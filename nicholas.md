This file contains the list of papers from [nicholas' blog](https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html).

## 2023-09-01

[Curating Naturally Adversarial Datasets for Trustworthy AI in Healthcare. (99%)](http://arxiv.org/abs/2309.00543)

Sydney Pugh, Ivan Ruchkin, Insup Lee, James Weimer


[Baseline Defenses for Adversarial Attacks Against Aligned Language Models. (98%)](http://arxiv.org/abs/2309.00614)

Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, Tom Goldstein


[Why do universal adversarial attacks work on large language models?: Geometry might be the answer. (83%)](http://arxiv.org/abs/2309.00254)

Varshini Subhash, Anna Bialas, Weiwei Pan, Finale Doshi-Velez


## 2023-08-31

[Adversarial Finetuning with Latent Representation Constraint to Mitigate Accuracy-Robustness Tradeoff. (98%)](http://arxiv.org/abs/2308.16454)

Satoshi Suzuki, Shin'ya Yamaguchi, Shoichiro Takeda, Sekitoshi Kanai, Naoki Makishima, Atsushi Ando, Ryo Masumura


[Image Hijacking: Adversarial Images can Control Generative Models at Runtime. (98%)](http://arxiv.org/abs/2309.00236)

Luke Bailey, Euan Ong, Stuart Russell, Scott Emmons


[The Power of MEME: Adversarial Malware Creation with Model-Based Reinforcement Learning. (93%)](http://arxiv.org/abs/2308.16562)

Maria Rigaki, Sebastian Garcia


[Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor Attack. (75%)](http://arxiv.org/abs/2308.16684)

Sze Jue Yang, Quang Nguyen, Chee Seng Chan, Khoa Doan


[Fault Injection and Safe-Error Attack for Extraction of Embedded Neural Network Models. (75%)](http://arxiv.org/abs/2308.16703)

Kevin Hector, Pierre-Alain Moellic, Mathieu Dumont, Jean-Max Dutertre


[FTA: Stealthy and Robust Backdoor Attack with Flexible Trigger on Federated Learning. (45%)](http://arxiv.org/abs/2309.00127)

Yanqi Qiao, Congwen Chen, Rui Wang, Kaitai Liang


## 2023-08-30

[Robust Principles: Architectural Design Principles for Adversarially Robust CNNs. (11%)](http://arxiv.org/abs/2308.16258)

ShengYun Peng, Weilin Xu, Cory Cornelius, Matthew Hull, Kevin Li, Rahul Duggal, Mansi Phute, Jason Martin, Duen Horng Chau


## 2023-08-29

[Adaptive Attack Detection in Text Classification: Leveraging Space Exploration Features for Text Sentiment Classification. (99%)](http://arxiv.org/abs/2308.15663)

Atefeh Mahdavi, Neda Keivandarian, Marco Carvalho


[Advancing Adversarial Robustness Through Adversarial Logit Update. (99%)](http://arxiv.org/abs/2308.15072)

Hao Xuan, Peican Zhu, Xingyu Li


[Imperceptible Adversarial Attack on Deep Neural Networks from Image Boundary. (99%)](http://arxiv.org/abs/2308.15344)

Fahad Alrasheedi, Xin Zhong


[A Classification-Guided Approach for Adversarial Attacks against Neural Machine Translation. (99%)](http://arxiv.org/abs/2308.15246)

Sahar Sadrizadeh, Ljiljana Dolamic, Pascal Frossard


[MDTD: A Multi Domain Trojan Detector for Deep Neural Networks. (97%)](http://arxiv.org/abs/2308.15673)

Arezoo Rajabi, Surudhi Asokraj, Fengqing Jiang, Luyao Niu, Bhaskar Ramasubramanian, Jim Ritcey, Radha Poovendran


[3D Adversarial Augmentations for Robust Out-of-Domain Predictions. (87%)](http://arxiv.org/abs/2308.15479)

Alexander Lehner, Stefano Gasperini, Alvaro Marcos-Ramiro, Michael Schmidt, Nassir Navab, Benjamin Busam, Federico Tombari


[Everything Perturbed All at Once: Enabling Differentiable Graph Attacks. (84%)](http://arxiv.org/abs/2308.15614)

Haoran Liu, Bokun Wang, Jianling Wang, Xiangjue Dong, Tianbao Yang, James Caverlee


[Vulnerability of Machine Learning Approaches Applied in IoT-based Smart Grid: A Review. (70%)](http://arxiv.org/abs/2308.15736)

Zhenyong Zhang, Mengxiang Liu, Mingyang Sun, Ruilong Deng, Peng Cheng, Dusit Niyato, Mo-Yuen Chow, Jiming Chen


[Intriguing Properties of Diffusion Models: A Large-Scale Dataset for Evaluating Natural Attack Capability in Text-to-Image Generative Models. (67%)](http://arxiv.org/abs/2308.15692)

Takami Sato, Justin Yue, Nanze Chen, Ningfei Wang, Qi Alfred Chen


[Can We Rely on AI? (50%)](http://arxiv.org/abs/2308.15092)

Desmond J. Higham


[Uncertainty Aware Training to Improve Deep Learning Model Calibration for Classification of Cardiac MR Images. (1%)](http://arxiv.org/abs/2308.15141)

Tareen Dawood, Chen Chen, Baldeep S. Sidhua, Bram Ruijsink, Justin Goulda, Bradley Porter, Mark K. Elliott, Vishal Mehta, Christopher A. Rinaldi, Esther Puyol-Anton, Reza Razavi, Andrew P. King


## 2023-08-28

[Adversarial Attacks on Foundational Vision Models. (80%)](http://arxiv.org/abs/2308.14597)

Nathan Inkawhich, Gwendolyn McDonald, Ryan Luley


[Identifying and Mitigating the Security Risks of Generative AI. (45%)](http://arxiv.org/abs/2308.14840)

Clark Barrett, Brad Boyd, Ellie Burzstein, Nicholas Carlini, Brad Chen, Jihye Choi, Amrita Roy Chowdhury, Mihai Christodorescu, Anupam Datta, Soheil Feizi, Kathleen Fisher, Tatsunori Hashimoto, Dan Hendrycks, Somesh Jha, Daniel Kang, Florian Kerschbaum, Eric Mitchell, John Mitchell, Zulfikar Ramzan, Khawaja Shams, Dawn Song, Ankur Taly, Diyi Yang


[DiffSmooth: Certifiably Robust Learning via Diffusion Models and Local Smoothing. (45%)](http://arxiv.org/abs/2308.14333)

Jiawei Zhang, Zhongzhu Chen, Huan Zhang, Chaowei Xiao, Bo Li


[ReMAV: Reward Modeling of Autonomous Vehicles for Finding Likely Failure Events. (2%)](http://arxiv.org/abs/2308.14550)

Aizaz Sharif, Dusica Marijan


[Are Existing Out-Of-Distribution Techniques Suitable for Network Intrusion Detection? (1%)](http://arxiv.org/abs/2308.14376)

Andrea Corsini, Shanchieh Jay Yang


## 2023-08-27

[Detecting Language Model Attacks with Perplexity. (1%)](http://arxiv.org/abs/2308.14132)

Gabriel Alon, Michael Kamfonas


## 2023-08-24

[Exploring Transferability of Multimodal Adversarial Samples for Vision-Language Pre-training Models with Contrastive Learning. (99%)](http://arxiv.org/abs/2308.12636)

Youze Wang, Wenbo Hu, Yinpeng Dong, Richang Hong


[Don't Look into the Sun: Adversarial Solarization Attacks on Image Classifiers. (92%)](http://arxiv.org/abs/2308.12661)

Paul Gavrikov, Janis Keuper


[Evaluating the Vulnerabilities in ML systems in terms of adversarial attacks. (82%)](http://arxiv.org/abs/2308.12918)

John Harshith, Mantej Singh Gill, Madhan Jothimani


[Fast Adversarial Training with Smooth Convergence. (3%)](http://arxiv.org/abs/2308.12857)

Mengnan Zhao, Lihe Zhang, Yuqiu Kong, Baocai Yin


[WavMark: Watermarking for Audio Generation. (2%)](http://arxiv.org/abs/2308.12770)

Guangyu Chen, Yu Wu, Shujie Liu, Tao Liu, Xiaoyong Du, Furu Wei


## 2023-08-23

[On-Manifold Projected Gradient Descent. (99%)](http://arxiv.org/abs/2308.12279)

Aaron Mahler, Tyrus Berry, Tom Stephens, Harbir Antil, Michael Merritt, Jeanie Schreiber, Ioannis Kevrekidis


[Sample Complexity of Robust Learning against Evasion Attacks. (98%)](http://arxiv.org/abs/2308.12054)

Pascale Gourdeau


[LCANets++: Robust Audio Classification using Multi-layer Neural Networks with Lateral Competition. (92%)](http://arxiv.org/abs/2308.12882)

Sayanton V. Dibbo, Juston S. Moore, Garrett T. Kenyon, Michael A. Teti


[BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection. (74%)](http://arxiv.org/abs/2308.12439)

Tinghao Xie, Xiangyu Qi, Ping He, Yiming Li, Jiachen T. Wang, Prateek Mittal


[RemovalNet: DNN Fingerprint Removal Attacks. (69%)](http://arxiv.org/abs/2308.12319)

Hongwei Yao, Zheng Li, Kunzhe Huang, Jian Lou, Zhan Qin, Kui Ren


[Ensembling Uncertainty Measures to Improve Safety of Black-Box Classifiers. (1%)](http://arxiv.org/abs/2308.12065)

Tommaso Zoppi, Andrea Ceccarelli, Andrea Bondavalli


[Aparecium: Revealing Secrets from Physical Photographs. (1%)](http://arxiv.org/abs/2308.12141)

Zhe Lei, Jie Zhang, Jingtao Li, Weiming Zhang, Nenghai Yu


## 2023-08-22

[SEA: Shareable and Explainable Attribution for Query-based Black-box Attacks. (99%)](http://arxiv.org/abs/2308.11845)

Yue Gao, Ilia Shumailov, Kassem Fawaz


[Multi-Instance Adversarial Attack on GNN-Based Malicious Domain Detection. (99%)](http://arxiv.org/abs/2308.11754)

Mahmoud Nazzal, Issa Khalil, Abdallah Khreishah, NhatHai Phan, Yao Ma


[Does Physical Adversarial Example Really Matter to Autonomous Driving? Towards System-Level Effect of Adversarial Object Evasion Attack. (98%)](http://arxiv.org/abs/2308.11894)

Ningfei Wang, Yunpeng Luo, Takami Sato, Kaidi Xu, Qi Alfred Chen


[Protect Federated Learning Against Backdoor Attacks via Data-Free Trigger Generation. (86%)](http://arxiv.org/abs/2308.11333)

Yanxin Yang, Ming Hu, Yue Cao, Jun Xia, Yihao Huang, Yang Liu, Mingsong Chen


[Revisiting and Exploring Efficient Fast Adversarial Training via LAW: Lipschitz Regularization and Auto Weight Averaging. (76%)](http://arxiv.org/abs/2308.11443)

Xiaojun Jia, Yuefeng Chen, Xiaofeng Mao, Ranjie Duan, Jindong Gu, Rong Zhang, Hui Xue, Xiaochun Cao


[Designing an attack-defense game: how to increase robustness of financial transaction models via a competition. (75%)](http://arxiv.org/abs/2308.11406)

Alexey Zaytsev, Alex Natekin, Evgeni Vorsin, Valerii Smirnov, Oleg Sidorshin, Alexander Senin, Alexander Dudin, Dmitry Berestnev


[Adversarial Training Using Feedback Loops. (74%)](http://arxiv.org/abs/2308.11881)

Ali Haisam Muhammad Rafid, Adrian Sandu


[LEAP: Efficient and Automated Test Method for NLP Software. (31%)](http://arxiv.org/abs/2308.11284)

Mingxuan Xiao, Yan Xiao, Hai Dong, Shunhui Ji, Pengcheng Zhang


[PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification. (16%)](http://arxiv.org/abs/2308.11822)

Yizhen Institute for AI Industry Research Yuan, Rui Shanghai Jiao Tong University, Shanghai, China Kong, Shenghao Wuhan University, Wuhan, China Xie, Yuanchun Institute for AI Industry Research Shanghai AI Laboratory, Shanghai, China Li, Yunxin Institute for AI Industry Research Shanghai AI Laboratory, Shanghai, China Liu


## 2023-08-21

[Spear and Shield: Adversarial Attacks and Defense Methods for Model-Based Link Prediction on Continuous-Time Dynamic Graphs. (99%)](http://arxiv.org/abs/2308.10779)

Dongjin Lee, Juho Lee, Kijung Shin


[Boosting Adversarial Attack with Similar Target. (99%)](http://arxiv.org/abs/2308.10743)

Shuo Zhang, Ziruo Wang, Zikai Zhou, Huanran Chen


[Improving the Transferability of Adversarial Examples with Arbitrary Style Transfer. (99%)](http://arxiv.org/abs/2308.10601)

Zhijin Ge, Fanhua Shang, Hongying Liu, Yuanyuan Liu, Liang Wan, Wei Feng, Xiaosen Wang


[Adversarial Attacks on Code Models with Discriminative Graph Patterns. (96%)](http://arxiv.org/abs/2308.11161)

Thanh-Dat Pick Nguyen, Yang Pick Zhou, Xuan Bach D. Pick Le, Pick Patanamon, Thongtanunam, David Lo


[Temporal-Distributed Backdoor Attack Against Video Based Action Recognition. (88%)](http://arxiv.org/abs/2308.11070)

Xi Li, Songhe Wang, Ruiquan Huang, Mahanth Gowda, George Kesidis


[Measuring the Effect of Causal Disentanglement on the Adversarial Robustness of Neural Network Models. (76%)](http://arxiv.org/abs/2308.10708)

Preben M. Ness, Dusica Marijan, Sunanda Bose


[Single-User Injection for Invisible Shilling Attack against Recommender Systems. (62%)](http://arxiv.org/abs/2308.10467)

Chengzhi Huang, Hui Li


[On the Adversarial Robustness of Multi-Modal Foundation Models. (4%)](http://arxiv.org/abs/2308.10741)

Christian Schlarmann, Matthias Hein


[Unlocking Accuracy and Fairness in Differentially Private Image Classification. (2%)](http://arxiv.org/abs/2308.10888)

Leonard Berrada, Soham De, Judy Hanwen Shen, Jamie Hayes, Robert Stanforth, David Stutz, Pushmeet Kohli, Samuel L. Smith, Borja Balle


## 2023-08-20

[Boosting Adversarial Transferability by Block Shuffle and Rotation. (99%)](http://arxiv.org/abs/2308.10299)

Kunyu Wang, Xuanran He, Wenxuan Wang, Xiaosen Wang


[Improving Adversarial Robustness of Masked Autoencoders via Test-time Frequency-domain Prompting. (96%)](http://arxiv.org/abs/2308.10315)

Qidong Huang, Xiaoyi Dong, Dongdong Chen, Yinpeng Chen, Lu Yuan, Gang Hua, Weiming Zhang, Nenghai Yu


[HoSNN: Adversarially-Robust Homeostatic Spiking Neural Networks with Adaptive Firing Thresholds. (96%)](http://arxiv.org/abs/2308.10373)

Hejia Geng, Peng Li


[Hiding Backdoors within Event Sequence Data via Poisoning Attacks. (95%)](http://arxiv.org/abs/2308.10201)

Elizaveta Kovtun, Alina Ermilova, Dmitry Berestnev, Alexey Zaytsev


[Adversarial Collaborative Filtering for Free. (61%)](http://arxiv.org/abs/2308.13541)

Huiyuan Chen, Xiaoting Li, Vivian Lai, Chin-Chia Michael Yeh, Yujie Fan, Yan Zheng, Mahashweta Das, Hao Yang


[Efficient Joint Optimization of Layer-Adaptive Weight Pruning in Deep Neural Networks. (1%)](http://arxiv.org/abs/2308.10438)

Kaixin Xu, Zhe Wang, Xue Geng, Jie Lin, Min Wu, Xiaoli Li, Weisi Lin


[A Study on Robustness and Reliability of Large Language Model Code Generation. (1%)](http://arxiv.org/abs/2308.10335)

Li Zhong, Zilong Wang


## 2023-08-19

[A Comparison of Adversarial Learning Techniques for Malware Detection. (99%)](http://arxiv.org/abs/2308.09958)

Pavla Louthánová, Matouš Kozák, Martin Jureček, Mark Stamp


[Robust Mixture-of-Expert Training for Convolutional Neural Networks. (83%)](http://arxiv.org/abs/2308.10110)

Yihua Zhang, Ruisi Cai, Tianlong Chen, Guanhua Zhang, Huan Zhang, Pin-Yu Chen, Shiyu Chang, Zhangyang Wang, Sijia Liu


## 2023-08-18

[Attacking logo-based phishing website detectors with adversarial perturbations. (99%)](http://arxiv.org/abs/2308.09392)

Jehyun Lee, Zhe Xin, Melanie Ng Pei See, Kanav Sabharwal, Giovanni Apruzzese, Dinil Mon Divakaran


[Black-box Adversarial Attacks against Dense Retrieval Models: A Multi-view Contrastive Learning Method. (99%)](http://arxiv.org/abs/2308.09861)

Yu-An Liu, Ruqing Zhang, Jiafeng Guo, Rijke Maarten de, Wei Chen, Yixing Fan, Xueqi Cheng


[Compensating Removed Frequency Components: Thwarting Voice Spectrum Reduction Attacks. (92%)](http://arxiv.org/abs/2308.09546)

Shu Wang, Kun Sun, Qi Li


[Poison Dart Frog: A Clean-Label Attack with Low Poisoning Rate and High Attack Success Rate in the Absence of Training Data. (54%)](http://arxiv.org/abs/2308.09487)

Binhao Ma, Jiahui Wang, Dejun Wang, Bo Meng


[Backdoor Mitigation by Correcting the Distribution of Neural Activations. (11%)](http://arxiv.org/abs/2308.09850)

Xi Li, Zhen Xiang, David J. Miller, George Kesidis


[On Gradient-like Explanation under a Black-box Setting: When Black-box Explanations Become as Good as White-box. (9%)](http://arxiv.org/abs/2308.09381)

Yi Cai, Gerhard Wunder


[Towards Attack-tolerant Federated Learning via Critical Parameter Analysis. (9%)](http://arxiv.org/abs/2308.09318)

Sungwon Han, Sungwon Park, Fangzhao Wu, Sundong Kim, Bin Zhu, Xing Xie, Meeyoung Cha


[Defending Label Inference Attacks in Split Learning under Regression Setting. (4%)](http://arxiv.org/abs/2308.09448)

Haoze Qiu, Fei Zheng, Chaochao Chen, Xiaolin Zheng


[An Image is Worth a Thousand Toxic Words: A Metamorphic Testing Framework for Content Moderation Software. (1%)](http://arxiv.org/abs/2308.09810)

Wenxuan Wang, Jingyuan Huang, Jen-tse Huang, Chang Chen, Jiazhen Gu, Pinjia He, Michael R. Lyu


[Proceedings of the 2nd International Workshop on Adaptive Cyber Defense. (1%)](http://arxiv.org/abs/2308.09520)

Marco Carvalho, Damian Marriott, Mark Bilinski, Ahmad Ridley


## 2023-08-17

[Towards a Practical Defense against Adversarial Attacks on Deep Learning-based Malware Detectors via Randomized Smoothing. (99%)](http://arxiv.org/abs/2308.08906)

Daniel Gibert, Giulio Zizzo, Quan Le


[A White-Box False Positive Adversarial Attack Method on Contrastive Loss-Based Offline Handwritten Signature Verification Models. (98%)](http://arxiv.org/abs/2308.08925)

Zhongliang Guo, Yifei Qian, Ognjen Arandjelović, Lei Fang


[Causal Adversarial Perturbations for Individual Fairness and Robustness in Heterogeneous Data Spaces. (16%)](http://arxiv.org/abs/2308.08938)

Ahmad-Reza Ehyaei, Kiarash Mohammadi, Amir-Hossein Karimi, Samira Samadi, Golnoosh Farnadi


[That Doesn't Go There: Attacks on Shared State in Multi-User Augmented Reality Applications. (10%)](http://arxiv.org/abs/2308.09146)

Carter Slocum, Yicheng Zhang, Erfan Shayegani, Pedram Zaree, Nael Abu-Ghazaleh, Jiasi Chen


[Do you really follow me? Adversarial Instructions for Evaluating the Robustness of Large Language Models. (10%)](http://arxiv.org/abs/2308.10819)

Zekun Li, Baolin Peng, Pengcheng He, Xifeng Yan


## 2023-08-16

[Benchmarking Adversarial Robustness of Compressed Deep Learning Models. (81%)](http://arxiv.org/abs/2308.08160)

Brijesh Vora, Kartik Patwari, Syed Mahbub Hafiz, Zubair Shafiq, Chen-Nee Chuah


[Test-Time Poisoning Attacks Against Test-Time Adaptation Models. (73%)](http://arxiv.org/abs/2308.08505)

Tianshuo Cong, Xinlei He, Yun Shen, Yang Zhang


[Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models. (67%)](http://arxiv.org/abs/2308.11521)

Zhenhua Wang, Wei Xie, Kai Chen, Baosheng Wang, Zhiwen Gui, Enze Wang


[Dynamic Neural Network is All You Need: Understanding the Robustness of Dynamic Mechanisms in Neural Networks. (61%)](http://arxiv.org/abs/2308.08709)

Mirazul Haque, Wei Yang


[Expressivity of Graph Neural Networks Through the Lens of Adversarial Robustness. (33%)](http://arxiv.org/abs/2308.08173)

Francesco Campi, Lukas Gosch, Tom Wollschläger, Yan Scholten, Stephan Günnemann


## 2023-08-15

[SEDA: Self-Ensembling ViT with Defensive Distillation and Adversarial Training for robust Chest X-rays Classification. (99%)](http://arxiv.org/abs/2308.07874)

Raza Imam, Ibrahim Almakky, Salma Alrashdi, Baketah Alrashdi, Mohammad Yaqub


[Backpropagation Path Search On Adversarial Transferability. (99%)](http://arxiv.org/abs/2308.07625)

Zhuoer Xu, Zhangxuan Gu, Jianping Zhang, Shiwen Cui, Changhua Meng, Weiqiang Wang


[A Review of Adversarial Attacks in Computer Vision. (99%)](http://arxiv.org/abs/2308.07673)

Yutong Zhang, Yao Li, Yin Li, Zhichang Guo


[Robustness Over Time: Understanding Adversarial Examples' Effectiveness on Longitudinal Versions of Large Language Models. (95%)](http://arxiv.org/abs/2308.07847)

Yugeng Liu, Tianshuo Cong, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang


[Simple and Efficient Partial Graph Adversarial Attack: A New Perspective. (93%)](http://arxiv.org/abs/2308.07834)

Guanghui Zhu, Mengyu Chen, Chunfeng Yuan, Yihua Huang


## 2023-08-14

[3DHacker: Spectrum-based Decision Boundary Generation for Hard-label 3D Point Cloud Attack. (99%)](http://arxiv.org/abs/2308.07546)

Yunbo Tao, Daizong Liu, Pan Zhou, Yulai Xie, Wei Du, Wei Hu


[White-Box Adversarial Attacks on Deep Learning-Based Radio Frequency Fingerprint Identification. (99%)](http://arxiv.org/abs/2308.07433)

Jie Ma, Junqing Zhang, Guanxiong Shen, Alan Marshall, Chip-Hong Chang


[AdvCLIP: Downstream-agnostic Adversarial Examples in Multimodal Contrastive Learning. (99%)](http://arxiv.org/abs/2308.07026)

Ziqi Zhou, Shengshan Hu, Minghui Li, Hangtao Zhang, Yechao Zhang, Hai Jin


[Enhancing the Antidote: Improved Pointwise Certifications against Poisoning Attacks. (68%)](http://arxiv.org/abs/2308.07553)

Shijie Liu, Andrew C. Cullen, Paul Montague, Sarah M. Erfani, Benjamin I. P. Rubinstein


[LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked. (22%)](http://arxiv.org/abs/2308.07308)

Alec Helbling, Mansi Phute, Matthew Hull, Duen Horng Chau


[DISBELIEVE: Distance Between Client Models is Very Essential for Effective Local Model Poisoning Attacks. (13%)](http://arxiv.org/abs/2308.07387)

Indu Joshi, Priyank Upadhya, Gaurav Kumar Nayak, Peter Schüffler, Nassir Navab


[ACTIVE: Towards Highly Transferable 3D Physical Camouflage for Universal and Robust Vehicle Evasion. (10%)](http://arxiv.org/abs/2308.07009)

Naufal Suryanto, Yongsu Kim, Harashta Tatimma Larasati, Hyoeun Kang, Thi-Thu-Huong Le, Yoonyoung Hong, Hunmin Yang, Se-Yoon Oh, Howon Kim


[SAM Meets Robotic Surgery: An Empirical Study on Generalization, Robustness and Adaptation. (1%)](http://arxiv.org/abs/2308.07156)

An Wang, Mobarakol Islam, Mengya Xu, Yang Zhang, Hongliang Ren


## 2023-08-13

[SoK: Realistic Adversarial Attacks and Defenses for Intelligent Network Intrusion Detection. (99%)](http://arxiv.org/abs/2308.06819)

João Vitorino, Isabel Praça, Eva Maia


[Understanding the robustness difference between stochastic gradient descent and adaptive gradient methods. (45%)](http://arxiv.org/abs/2308.06703)

Avery Ma, Yangchen Pan, Amir-massoud Farahmand


[A Survey on Deep Neural Network Pruning-Taxonomy, Comparison, Analysis, and Recommendations. (1%)](http://arxiv.org/abs/2308.06767)

Hongrong Cheng, Miao Zhang, Javen Qinfeng Shi


[Robustified ANNs Reveal Wormholes Between Human Category Percepts. (1%)](http://arxiv.org/abs/2308.06887)

Guy Gaziv, Michael J. Lee, James J. DiCarlo


[Faithful to Whom? Questioning Interpretability Measures in NLP. (1%)](http://arxiv.org/abs/2308.06795)

Evan Crothers, Herna Viktor, Nathalie Japkowicz


## 2023-08-12

[Not So Robust After All: Evaluating the Robustness of Deep Neural Networks to Unseen Adversarial Attacks. (99%)](http://arxiv.org/abs/2308.06467)

Roman Garaev, Bader Rasheed, Adil Khan


[One-bit Flip is All You Need: When Bit-flip Attack Meets Model Training. (13%)](http://arxiv.org/abs/2308.07934)

Jianshuo Dong, Han Qiu, Yiming Li, Tianwei Zhang, Yuanjie Li, Zeqi Lai, Chao Zhang, Shu-Tao Xia


## 2023-08-11

[Enhancing Generalization of Universal Adversarial Perturbation through Gradient Aggregation. (98%)](http://arxiv.org/abs/2308.06015)

Xuannan Liu, Yaoyao Zhong, Yuhang Zhang, Lixiong Qin, Weihong Deng


[Physical Adversarial Attacks For Camera-based Smart Systems: Current Trends, Categorization, Applications, Research Challenges, and Future Outlook. (98%)](http://arxiv.org/abs/2308.06173)

Amira Guesmi, Muhammad Abdullah Hanif, Bassem Ouni, Muhammed Shafique


[Face Encryption via Frequency-Restricted Identity-Agnostic Attacks. (96%)](http://arxiv.org/abs/2308.05983)

Xin Dong, Rui Wang, Siyuan Liang, Aishan Liu, Lihua Jing


[White-box Membership Inference Attacks against Diffusion Models. (68%)](http://arxiv.org/abs/2308.06405)

Yan Pang, Tianhao Wang, Xuhui Kang, Mengdi Huai, Yang Zhang


[Test-Time Adaptation for Backdoor Defense. (10%)](http://arxiv.org/abs/2308.06107)

Jiyang Guan, Jian Liang, Ran He


[Continual Face Forgery Detection via Historical Distribution Preserving. (2%)](http://arxiv.org/abs/2308.06217)

Ke Sun, Shen Chen, Taiping Yao, Xiaoshuai Sun, Shouhong Ding, Rongrong Ji


[Fast and Accurate Transferability Measurement by Evaluating Intra-class Feature Variance. (1%)](http://arxiv.org/abs/2308.05986)

Huiwen Xu, U Kang


## 2023-08-10

[Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient. (99%)](http://arxiv.org/abs/2308.05681)

Zhengzhi Lu, He Wang, Ziyi Chang, Guoan Yang, Hubert P. H. Shum


[Symmetry Defense Against XGBoost Adversarial Perturbation Attacks. (96%)](http://arxiv.org/abs/2308.05575)

Blerta Lindqvist


[Complex Network Effects on the Robustness of Graph Convolutional Networks. (92%)](http://arxiv.org/abs/2308.05498)

Benjamin A. Miller, Kevin Chan, Tina Eliassi-Rad


[FLShield: A Validation Based Federated Learning Framework to Defend Against Poisoning Attacks. (45%)](http://arxiv.org/abs/2308.05832)

Ehsanul Kabir, Zeyu Song, Md Rafi Ur Rashid, Shagufta Mehnaz


[Critical Points ++: An Agile Point Cloud Importance Measure for Robust Classification, Adversarial Defense and Explainable AI. (5%)](http://arxiv.org/abs/2308.05525)

Meir Yossef Levi, Guy Gilboa


[Comprehensive Analysis of Network Robustness Evaluation Based on Convolutional Neural Networks with Spatial Pyramid Pooling. (1%)](http://arxiv.org/abs/2308.08012)

Wenjun Jiang, Tianlong Fan, Changhao Li, Chuanfu Zhang, Tao Zhang, Zong-fu Luo


## 2023-08-09

[Adv-Inpainting: Generating Natural and Transferable Adversarial Patch via Attention-guided Feature Fusion. (95%)](http://arxiv.org/abs/2308.05320)

Yanjie Li, Mingxing Duan, Bin Xiao


[Adversarial ModSecurity: Countering Adversarial SQL Injections with Robust Machine Learning. (93%)](http://arxiv.org/abs/2308.04964)

Biagio Montaruli, Luca Demetrio, Andrea Valenza, Battista Biggio, Luca Compagna, Davide Balzarotti, Davide Ariu, Luca Piras


[Adversarial Deep Reinforcement Learning for Cyber Security in Software Defined Networks. (81%)](http://arxiv.org/abs/2308.04909)

Luke Borchjes, Clement Nyirenda, Louise Leenen


[Data-Free Model Extraction Attacks in the Context of Object Detection. (41%)](http://arxiv.org/abs/2308.05127)

Harshit Shah, Aravindhan G, Pavan Kulkarni, Yuvaraj Govidarajulu, Manojkumar Parmar


## 2023-08-08

[Pelta: Shielding Transformers to Mitigate Evasion Attacks in Federated Learning. (99%)](http://arxiv.org/abs/2308.04373)

Simon Queyrut, Yérom-David Bromberg, Valerio Schiavoni


[Federated Zeroth-Order Optimization using Trajectory-Informed Surrogate Gradients. (81%)](http://arxiv.org/abs/2308.04077)

Yao Shu, Xiaoqiang Lin, Zhongxiang Dai, Bryan Kian Hsiang Low


[The Model Inversion Eavesdropping Attack in Semantic Communication Systems. (67%)](http://arxiv.org/abs/2308.04304)

Yuhao Chen, Qianqian Yang, Zhiguo Shi, Jiming Chen


[Comprehensive Assessment of the Performance of Deep Learning Classifiers Reveals a Surprising Lack of Robustness. (64%)](http://arxiv.org/abs/2308.04137)

Michael W. Spratling


[XGBD: Explanation-Guided Graph Backdoor Detection. (54%)](http://arxiv.org/abs/2308.04406)

Zihan Guan, Mengnan Du, Ninghao Liu


[Improved Activation Clipping for Universal Backdoor Mitigation and Test-Time Detection. (50%)](http://arxiv.org/abs/2308.04617)

Hang Wang, Zhen Xiang, David J. Miller, George Kesidis


[Backdoor Federated Learning by Poisoning Backdoor-Critical Layers. (15%)](http://arxiv.org/abs/2308.04466)

Haomin Zhuang, Mingxian Yu, Hao Wang, Yang Hua, Jian Li, Xu Yuan


[Evil Operation: Breaking Speaker Recognition with PaddingBack. (13%)](http://arxiv.org/abs/2308.04179)

Zhe Ye, Diqun Yan, Li Dong, Kailai Shen


## 2023-08-07

[Fixed Inter-Neuron Covariability Induces Adversarial Robustness. (98%)](http://arxiv.org/abs/2308.03956)

Muhammad Ahmed Shah, Bhiksha Raj


[Exploring the Physical World Adversarial Robustness of Vehicle Detection. (98%)](http://arxiv.org/abs/2308.03476)

Wei Jiang, Tianyuan Zhang, Shuangcheng Liu, Weiyu Ji, Zichao Zhang, Gang Xiao


[PAIF: Perception-Aware Infrared-Visible Image Fusion for Attack-Tolerant Semantic Segmentation. (86%)](http://arxiv.org/abs/2308.03979)

Zhu Liu, Jinyuan Liu, Benzhuang Zhang, Long Ma, Xin Fan, Risheng Liu


[A reading survey on adversarial machine learning: Adversarial attacks and their understanding. (81%)](http://arxiv.org/abs/2308.03363)

Shashank Kotyan


[A Four-Pronged Defense Against Byzantine Attacks in Federated Learning. (54%)](http://arxiv.org/abs/2308.03331)

Wei Wan, Shengshan Hu, Minghui Li, Jianrong Lu, Longling Zhang, Leo Yu Zhang, Hai Jin


[Improving Performance of Semi-Supervised Learning by Adversarial Attacks. (11%)](http://arxiv.org/abs/2308.04018)

Dongyoon Yang, Kunwoong Kim, Yongdai Kim


[Mondrian: Prompt Abstraction Attack Against Large Language Models for Cheaper API Pricing. (10%)](http://arxiv.org/abs/2308.03558)

Wai Man Si, Michael Backes, Yang Zhang


## 2023-08-06

[SAAM: Stealthy Adversarial Attack on Monoculor Depth Estimation. (99%)](http://arxiv.org/abs/2308.03108)

Amira Guesmi, Muhammad Abdullah Hanif, Bassem Ouni, Muhammad Shafique


[CGBA: Curvature-aware Geometric Black-box Attack. (99%)](http://arxiv.org/abs/2308.03163)

Md Farhamdur Reza, Ali Rahmati, Tianfu Wu, Huaiyu Dai


[APBench: A Unified Benchmark for Availability Poisoning Attacks and Defenses. (98%)](http://arxiv.org/abs/2308.03258)

Tianrui Qin, Xitong Gao, Juanjuan Zhao, Kejiang Ye, Cheng-Zhong Xu


[Unsupervised Adversarial Detection without Extra Model: Training Loss Should Change. (82%)](http://arxiv.org/abs/2308.03243)

Chien Cheng Chyou, Hung-Ting Su, Winston H. Hsu


[Using Overlapping Methods to Counter Adversaries in Community Detection. (50%)](http://arxiv.org/abs/2308.03081)

Benjamin A. Miller, Kevin Chan, Tina Eliassi-Rad


## 2023-08-05

[An Adaptive Model Ensemble Adversarial Attack for Boosting Adversarial Transferability. (99%)](http://arxiv.org/abs/2308.02897)

Bin Chen, Jia-Li Yin, Shukai Chen, Bo-Hao Chen, Ximeng Liu


[An AI-Enabled Framework to Defend Ingenious MDT-based Attacks on the Emerging Zero Touch Cellular Networks. (92%)](http://arxiv.org/abs/2308.02923)

Aneeqa Ijaz, Waseem Raza, Hasan Farooq, Marvin Manalastas, Ali Imran


[A Security and Usability Analysis of Local Attacks Against FIDO2. (1%)](http://arxiv.org/abs/2308.02973)

Tarun Kumar Yadav, Kent Seamons


[Approximating Positive Homogeneous Functions with Scale Invariant Neural Networks. (1%)](http://arxiv.org/abs/2308.02836)

Stefan Bamberger, Reinhard Heckel, Felix Krahmer


## 2023-08-04

[Multi-attacks: Many images $+$ the same adversarial attack $\to$ many target labels. (99%)](http://arxiv.org/abs/2308.03792)

Stanislav Fort


[RobustMQ: Benchmarking Robustness of Quantized Models. (75%)](http://arxiv.org/abs/2308.02350)

Yisong Xiao, Aishan Liu, Tianyuan Zhang, Haotong Qin, Jinyang Guo, Xianglong Liu


[Universal Defensive Underpainting Patch: Making Your Text Invisible to Optical Character Recognition. (31%)](http://arxiv.org/abs/2308.02369)

JiaCheng Deng, Li Dong, Jiahao Chen, Diqun Yan, Rangding Wang, Dengpan Ye, Lingchen Zhao, Jinyu Tian


[BlindSage: Label Inference Attacks against Node-level Vertical Federated Graph Neural Networks. (9%)](http://arxiv.org/abs/2308.02465)

Marco Arazzi, Mauro Conti, Stefanos Koffas, Marina Krcek, Antonino Nocera, Stjepan Picek, Jing Xu


[Vulnerabilities in AI Code Generators: Exploring Targeted Data Poisoning Attacks. (3%)](http://arxiv.org/abs/2308.04451)

Domenico Cotroneo, Cristina Improta, Pietro Liguori, Roberto Natella


## 2023-08-03

[Hard Adversarial Example Mining for Improving Robust Fairness. (99%)](http://arxiv.org/abs/2308.01823)

Chenhao Lin, Xiang Ji, Yulong Yang, Qian Li, Chao Shen, Run Wang, Liming Fang


[URET: Universal Robustness Evaluation Toolkit (for Evasion). (99%)](http://arxiv.org/abs/2308.01840)

Kevin Eykholt, Taesung Lee, Douglas Schales, Jiyong Jang, Ian Molloy, Masha Zorin


[AdvFAS: A robust face anti-spoofing framework against adversarial examples. (98%)](http://arxiv.org/abs/2308.02116)

Jiawei Chen, Xiao Yang, Heng Yin, Mingzhi Ma, Bihui Chen, Jianteng Peng, Yandong Guo, Zhaoxia Yin, Hang Su


[FROD: Robust Object Detection for Free. (67%)](http://arxiv.org/abs/2308.01888)

Muhammad, Awais, Weiming, Zhuang, Lingjuan, Lyu, Sung-Ho, Bae


[ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned Samples in NLP. (33%)](http://arxiv.org/abs/2308.02122)

Lu Yan, Zhuo Zhang, Guanhong Tao, Kaiyuan Zhang, Xuan Chen, Guangyu Shen, Xiangyu Zhang


[From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application? (4%)](http://arxiv.org/abs/2308.01990)

Rodrigo Pedro, Daniel Castro, Paulo Carreira, Nuno Santos


## 2023-08-02

[Inaudible Adversarial Perturbation: Manipulating the Recognition of User Speech in Real Time. (99%)](http://arxiv.org/abs/2308.01040)

Xinfeng Li, Chen Yan, Xuancun Lu, Zihan Zeng, Xiaoyu Ji, Wenyuan Xu


[Isolation and Induction: Training Robust Deep Neural Networks against Model Stealing Attacks. (98%)](http://arxiv.org/abs/2308.00958)

Jun Guo, Aishan Liu, Xingyu Zheng, Siyuan Liang, Yisong Xiao, Yichao Wu, Xianglong Liu


[Mercury: An Automated Remote Side-channel Attack to Nvidia Deep Learning Accelerator. (16%)](http://arxiv.org/abs/2308.01193)

Xiaobei Yan, Xiaoxuan Lou, Guowen Xu, Han Qiu, Shangwei Guo, Chip Hong Chang, Tianwei Zhang


[TEASMA: A Practical Approach for the Test Assessment of Deep Neural Networks using Mutation Analysis. (2%)](http://arxiv.org/abs/2308.01311)

Amin Abbasishahkoo, Mahboubeh Dadkhah, Lionel Briand, Dayi Lin


[LSF-IDM: Automotive Intrusion Detection Model with Lightweight Attribution and Semantic Fusion. (1%)](http://arxiv.org/abs/2308.01237)

Pengzhou Cheng, Lei Hua, Haobin Jiang, Mohammad Samie, Gongshen Liu


## 2023-08-01

[Dynamic ensemble selection based on Deep Neural Network Uncertainty Estimation for Adversarial Robustness. (99%)](http://arxiv.org/abs/2308.00346)

Ruoxi Qin, Linyuan Wang, Xuehui Du, Xingyuan Chen, Bin Yan


[LimeAttack: Local Explainable Method for Textual Hard-Label Adversarial Attack. (99%)](http://arxiv.org/abs/2308.00319)

Hai Zhu, Zhaoqing Yang, Weiwei Shang, Yuren Wu


[Improving Generalization of Adversarial Training via Robust Critical Fine-Tuning. (99%)](http://arxiv.org/abs/2308.02533)

Kaijie Zhu, Jindong Wang, Xixu Hu, Xing Xie, Ge Yang


[Doubly Robust Instance-Reweighted Adversarial Training. (82%)](http://arxiv.org/abs/2308.00311)

Daouda Sow, Sen Lin, Zhangyang Wang, Yingbin Liang


[Training on Foveated Images Improves Robustness to Adversarial Attacks. (82%)](http://arxiv.org/abs/2308.00854)

Muhammad A. Shah, Bhiksha Raj


[Kidnapping Deep Learning-based Multirotors using Optimized Flying Adversarial Patches. (47%)](http://arxiv.org/abs/2308.00344)

Pia Hanfeld, Khaled Wahba, Marina M. -C. Höhne, Michael Bussmann, Wolfgang Hönig


[Robust Linear Regression: Phase-Transitions and Precise Tradeoffs for General Norms. (22%)](http://arxiv.org/abs/2308.00556)

Elvis Dohmatob, Meyer Scetbon


[Zero-Shot Learning by Harnessing Adversarial Samples. (1%)](http://arxiv.org/abs/2308.00313)

Zhi Chen, Pengfei Zhang, Jingjing Li, Sen Wang, Zi Huang


[A Novel Cross-Perturbation for Single Domain Generalization. (1%)](http://arxiv.org/abs/2308.00918)

Dongjia Zhao, Lei Qi, Xiao Shi, Yinghuan Shi, Xin Geng


[Learning to Generate Training Datasets for Robust Semantic Segmentation. (1%)](http://arxiv.org/abs/2308.02535)

Marwane Hariat, Olivier Laurent, Rémi Kazmierczak, Andrei Bursuc, Angela Yao, Gianni Franchi


## 2023-07-31

[A Novel Deep Learning based Model to Defend Network Intrusion Detection System against Adversarial Attacks. (99%)](http://arxiv.org/abs/2308.00077)

Khushnaseeb Roshan, Aasim Zafar, Shiekh Burhan Ul Haque


[Transferable Attack for Semantic Segmentation. (99%)](http://arxiv.org/abs/2307.16572)

Mengqi He, Jing Zhang, Zhaoyuan Yang, Mingyi He, Nick Barnes, Yuchao Dai


[Universal Adversarial Defense in Remote Sensing Based on Pre-trained Denoising Diffusion Models. (99%)](http://arxiv.org/abs/2307.16865)

Weikang Yu, Yonghao Xu, Pedram Ghamisi


[Defense of Adversarial Ranking Attack in Text Retrieval: Benchmark and Baseline via Detection. (97%)](http://arxiv.org/abs/2307.16816)

Xuanang Chen, Ben He, Le Sun, Yingfei Sun


[Text-CRS: A Generalized Certified Robustness Framework against Textual Adversarial Attacks. (86%)](http://arxiv.org/abs/2307.16630)

Xinyu Zhang, Hanbin Hong, Yuan Hong, Peng Huang, Binghui Wang, Zhongjie Ba, Kui Ren


[Adversarially Robust Neural Legal Judgement Systems. (11%)](http://arxiv.org/abs/2308.00165)

Rohit Raj, V Susheela Devi


[Virtual Prompt Injection for Instruction-Tuned Large Language Models. (10%)](http://arxiv.org/abs/2307.16888)

Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang, Vijay Srinivasan, Xiang Ren, Hongxia Jin


[Noisy Self-Training with Data Augmentations for Offensive and Hate Speech Detection Tasks. (1%)](http://arxiv.org/abs/2307.16609)

João A. Leite, Carolina Scarton, Diego F. Silva


## 2023-07-30

[Theoretically Principled Trade-off for Stateful Defenses against Query-Based Black-Box Attacks. (99%)](http://arxiv.org/abs/2307.16331)

Ashish Hooda, Neal Mangaokar, Ryan Feng, Kassem Fawaz, Somesh Jha, Atul Prakash


[Benchmarking and Analyzing Robust Point Cloud Recognition: Bag of Tricks for Defending Adversarial Examples. (99%)](http://arxiv.org/abs/2307.16361)

Qiufan Ji, Lin Wang, Cong Shi, Shengshan Hu, Yingying Chen, Lichao Sun


[Probabilistically robust conformal prediction. (91%)](http://arxiv.org/abs/2307.16360)

Subhankar Ghosh, Yuanjie Shi, Taha Belkhouja, Yan Yan, Jana Doppa, Brian Jones


[On Updating Static Output Feedback Controllers Under State-Space Perturbation. (1%)](http://arxiv.org/abs/2307.16178)

MirSaleh Bahavarnia, Ahmad F. Taha


## 2023-07-29

[You Can Backdoor Personalized Federated Learning. (92%)](http://arxiv.org/abs/2307.15971)

Tiandi Ye, Cen Chen, Yinggui Wang, Xiang Li, Ming Gao


[On Neural Network approximation of ideal adversarial attack and convergence of adversarial training. (92%)](http://arxiv.org/abs/2307.16099)

Rajdeep Haldar, Qifan Song


[Exposing Hidden Attackers in Industrial Control Systems using Micro-distortions. (41%)](http://arxiv.org/abs/2307.15926)

Suman Sourav, Binbin Chen


## 2023-07-28

[Beating Backdoor Attack at Its Own Game. (97%)](http://arxiv.org/abs/2307.15539)

Min Liu, Alberto Sangiovanni-Vincentelli, Xiangyu Yue


[Adversarial training for tabular data with attack propagation. (67%)](http://arxiv.org/abs/2307.15677)

Tiago Leon Melo, João Bravo, Marco O. P. Sampaio, Paolo Romano, Hugo Ferreira, João Tiago Ascensão, Pedro Bizarro


[Improving Realistic Worst-Case Performance of NVCiM DNN Accelerators through Training with Right-Censored Gaussian Noise. (10%)](http://arxiv.org/abs/2307.15853)

Zheyu Yan, Yifan Qin, Wujie Wen, Xiaobo Sharon Hu, Yiyu Shi


[What can Discriminator do? Towards Box-free Ownership Verification of Generative Adversarial Network. (4%)](http://arxiv.org/abs/2307.15860)

Ziheng Huang, Boheng Li, Yan Cai, Run Wang, Shangwei Guo, Liming Fang, Jing Chen, Lina Wang


## 2023-07-27

[Universal and Transferable Adversarial Attacks on Aligned Language Models. (99%)](http://arxiv.org/abs/2307.15043)

Andy Zou, Zifan Wang, J. Zico Kolter, Matt Fredrikson


[R-LPIPS: An Adversarially Robust Perceptual Similarity Metric. (99%)](http://arxiv.org/abs/2307.15157)

Sara Ghazanfari, Siddharth Garg, Prashanth Krishnamurthy, Farshad Khorrami, Alexandre Araujo


[When Measures are Unreliable: Imperceptible Adversarial Perturbations toward Top-$k$ Multi-Label Learning. (99%)](http://arxiv.org/abs/2309.00007)

Yuchen Sun, Qianqian Xu, Zitai Wang, Qingming Huang


[Backdoor Attacks for In-Context Learning with Language Models. (97%)](http://arxiv.org/abs/2307.14692)

Nikhil Kandpal, Matthew Jagielski, Florian Tramèr, Nicholas Carlini


[FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks. (93%)](http://arxiv.org/abs/2307.14751)

Buse G. A. Tekgul, N. Asokan


[Unified Adversarial Patch for Visible-Infrared Cross-modal Attacks in the Physical World. (92%)](http://arxiv.org/abs/2307.14682)

Xingxing Wei, Yao Huang, Yitong Sun, Jie Yu


[NSA: Naturalistic Support Artifact to Boost Network Confidence. (62%)](http://arxiv.org/abs/2307.14917)

Abhijith Sharma, Phil Munz, Apurva Narayan


[SEV-Step: A Single-Stepping Framework for AMD-SEV. (3%)](http://arxiv.org/abs/2307.14757)

Luca Wilke, Jan Wichelmann, Anja Rabich, Thomas Eisenbarth


[Decoding the Secrets of Machine Learning in Malware Classification: A Deep Dive into Datasets, Feature Extraction, and Model Performance. (1%)](http://arxiv.org/abs/2307.14657)

Savino Dambra, Yufei Han, Simone Aonzo, Platon Kotzias, Antonino Vitale, Juan Caballero, Davide Balzarotti, Leyla Bilge


[AC-Norm: Effective Tuning for Medical Image Analysis via Affine Collaborative Normalization. (1%)](http://arxiv.org/abs/2307.15282)

Chuyan Zhang, Yuncheng Yang, Hao Zheng, Yun Gu


## 2023-07-26

[Enhanced Security against Adversarial Examples Using a Random Ensemble of Encrypted Vision Transformer Models. (99%)](http://arxiv.org/abs/2307.13985)

Ryota Iijima, Miki Tanaka, Sayaka Shiota, Hitoshi Kiya


[Set-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training Models. (99%)](http://arxiv.org/abs/2307.14061)

Dong Lu, Zhiqiang Wang, Teng Wang, Weili Guan, Hongchang Gao, Feng Zheng


[Defending Adversarial Patches via Joint Region Localizing and Inpainting. (99%)](http://arxiv.org/abs/2307.14242)

Junwen Chen, Xingxing Wei


[Lateral-Direction Localization Attack in High-Level Autonomous Driving: Domain-Specific Defense Opportunity via Lane Detection. (67%)](http://arxiv.org/abs/2307.14540)

Junjie Shen, Yunpeng Luo, Ziwen Wan, Qi Alfred Chen


[Plug and Pray: Exploiting off-the-shelf components of Multi-Modal Models. (16%)](http://arxiv.org/abs/2307.14539)

Erfan Shayegani, Yue Dong, Nael Abu-Ghazaleh


[Dual-Space Attacks against Random-Walk-based Anomaly Detection. (11%)](http://arxiv.org/abs/2307.14387)

Yuni Lai, Marcin Waniek, Yulin Zhu, Liying Li, Jingwen Wu, Tomasz P. Michalak, Talal Rahwan, Kai Zhou


[FakeTracer: Proactively Defending Against Face-swap DeepFakes via Implanting Traces in Training. (5%)](http://arxiv.org/abs/2307.14593)

Pu Sun, Honggang Qi, Yuezun Li, Siwei Lyu


[Open Image Content Disarm And Reconstruction. (1%)](http://arxiv.org/abs/2307.14057)

Eli Belkind, Ran Dubin, Amit Dvir


## 2023-07-25

[On the unreasonable vulnerability of transformers for image restoration -- and an easy fix. (99%)](http://arxiv.org/abs/2307.13856)

Shashank Agnihotri, Kanchana Vaishnavi Gandikota, Julia Grabinski, Paramanand Chandramouli, Margret Keuper


[Imperceptible Physical Attack against Face Recognition Systems via LED Illumination Modulation. (99%)](http://arxiv.org/abs/2307.13294)

Junbin Fang, Canjian Jiang, You Jiang, Puxi Lin, Zhaojie Chen, Yujing Sun, Siu-Ming Yiu, Zoe L. Jiang


[Foundational Models Defining a New Era in Vision: A Survey and Outlook. (10%)](http://arxiv.org/abs/2307.13721)

Muhammad Awais, Muzammal Naseer, Salman Khan, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, Fahad Shahbaz Khan


[Efficient Estimation of the Local Robustness of Machine Learning Models. (2%)](http://arxiv.org/abs/2307.13885)

Tessa Han, Suraj Srinivas, Himabindu Lakkaraju


## 2023-07-24

[Why Don't You Clean Your Glasses? Perception Attacks with Dynamic Optical Perturbations. (99%)](http://arxiv.org/abs/2307.13131)

Yi Han, Matthew Chan, Eric Wengrowski, Zhuohuan Li, Nils Ole Tippenhauer, Mani Srivastava, Saman Zonouz, Luis Garcia


[Lost In Translation: Generating Adversarial Examples Robust to Round-Trip Translation. (99%)](http://arxiv.org/abs/2307.12520)

Neel Bhandari, Pin-Yu Chen


[Data-free Black-box Attack based on Diffusion Model. (62%)](http://arxiv.org/abs/2307.12872)

Mingwen Shao, Lingzhuang Meng, Yuanjian Qiao, Lixu Zhang, Wangmeng Zuo


[Adaptive Certified Training: Towards Better Accuracy-Robustness Tradeoffs. (56%)](http://arxiv.org/abs/2307.13078)

Zhakshylyk Nurlanov, Frank R. Schmidt, Florian Bernard


[An Estimator for the Sensitivity to Perturbations of Deep Neural Networks. (31%)](http://arxiv.org/abs/2307.12679)

Naman Maheshwari, Nicholas Malaya, Scott Moe, Jaydeep P. Kulkarni, Sudhanva Gurumurthi


[Cyber Deception against Zero-day Attacks: A Game Theoretic Approach. (12%)](http://arxiv.org/abs/2307.13107)

Md Abu University of Texas at El Paso Sayed, Ahmed H. US Army Research Laboratory Anwar, Christopher University of Texas at El Paso Kiekintveld, Branislav Czech Technical University in Prague Bosansky, Charles US Army Research Laboratory Kamhoua


[Malware Resistant Data Protection in Hyper-connected Networks: A survey. (10%)](http://arxiv.org/abs/2307.13164)

Jannatul Ferdous, Rafiqul Islam, Maumita Bhattacharya, Md Zahidul Islam


[Digital Twins for Moving Target Defense Validation in AC Microgrids. (1%)](http://arxiv.org/abs/2307.13152)

Suman Rath, Subham Sahoo, Shamik Sengupta


[Towards Bridging the FL Performance-Explainability Trade-Off: A Trustworthy 6G RAN Slicing Use-Case. (1%)](http://arxiv.org/abs/2307.12903)

Swastika Roy, Hatim Chergui, Christos Verikoukis


[Learning Provably Robust Estimators for Inverse Problems via Jittering. (1%)](http://arxiv.org/abs/2307.12822)

Anselm Krainovic, Mahdi Soltanolkotabi, Reinhard Heckel


## 2023-07-23

[AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models. (99%)](http://arxiv.org/abs/2307.12499)

Xuelong Dai, Kaisheng Liang, Bin Xiao


[Towards Generic and Controllable Attacks Against Object Detection. (99%)](http://arxiv.org/abs/2307.12342)

Guopeng Li, Yue Xu, Jian Ding, Gui-Song Xia


[Downstream-agnostic Adversarial Examples. (99%)](http://arxiv.org/abs/2307.12280)

Ziqi Zhou, Shengshan Hu, Ruizhi Zhao, Qian Wang, Leo Yu Zhang, Junhui Hou, Hai Jin


[Gradient-Based Word Substitution for Obstinate Adversarial Examples Generation in Language Models. (98%)](http://arxiv.org/abs/2307.12507)

Yimu Wang, Peng Shi, Hongyang Zhang


[A First Look at On-device Models in iOS Apps. (84%)](http://arxiv.org/abs/2307.12328)

Han Hu, Yujin Huang, Qiuyuan Chen, Terry Tue Zhuo, Chunyang Chen


[Robust Automatic Speech Recognition via WavAugment Guided Phoneme Adversarial Training. (83%)](http://arxiv.org/abs/2307.12498)

Gege Qi, Yuefeng Chen, Xiaofeng Mao, Xiaojun Jia, Ranjie Duan, Rong Zhang, Hui Xue


[Cross Contrastive Feature Perturbation for Domain Generalization. (1%)](http://arxiv.org/abs/2307.12502)

Chenming Li, Daoan Zhang, Wenjian Huang, Jianguo Zhang


## 2023-07-22

[Backdoor Attacks against Voice Recognition Systems: A Survey. (13%)](http://arxiv.org/abs/2307.13643)

Baochen Yan, Jiahe Lan, Zheng Yan


## 2023-07-21

[Unveiling Vulnerabilities in Interpretable Deep Learning Systems with Query-Efficient Black-box Attacks. (99%)](http://arxiv.org/abs/2307.11906)

Eldor Abdukhamidov, Mohammed Abuhamad, Simon S. Woo, Eric Chan-Tin, Tamer Abuhmed


[Fast Adaptive Test-Time Defense with Robust Features. (98%)](http://arxiv.org/abs/2307.11672)

Anurag Singh, Mahalakshmi Sabanayagam, Krikamol Muandet, Debarghya Ghoshdastidar


[Improving Viewpoint Robustness for Visual Recognition via Adversarial Training. (80%)](http://arxiv.org/abs/2307.11528)

Shouwei Ruan, Yinpeng Dong, Hang Su, Jianteng Peng, Ning Chen, Xingxing Wei


[FMT: Removing Backdoor Feature Maps via Feature Map Testing in Deep Neural Networks. (76%)](http://arxiv.org/abs/2307.11565)

Dong Huang, Qingwen Bu, Yahao Qing, Yichao Fu, Heming Cui


[HybridAugment++: Unified Frequency Spectra Perturbations for Model Robustness. (26%)](http://arxiv.org/abs/2307.11823)

Mehmet Kerim Yucel, Ramazan Gokberk Cinbis, Pinar Duygulu


[OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples. (5%)](http://arxiv.org/abs/2307.11729)

Ryuto Koike, Masahiro Kaneko, Naoaki Okazaki


## 2023-07-20

[A LLM Assisted Exploitation of AI-Guardian. (98%)](http://arxiv.org/abs/2307.15008)

Nicholas Carlini


[Improving Transferability of Adversarial Examples via Bayesian Attacks. (98%)](http://arxiv.org/abs/2307.11334)

Qizhang Li, Yiwen Guo, Xiaochen Yang, Wangmeng Zuo, Hao Chen


[Adversarial attacks for mixtures of classifiers. (54%)](http://arxiv.org/abs/2307.10788)

Lucas Gnecco Heredia, Benjamin Negrevergne, Yann Chevaleyre


[PATROL: Privacy-Oriented Pruning for Collaborative Inference Against Model Inversion Attacks. (31%)](http://arxiv.org/abs/2307.10981)

Shiwei Ding, Lan Zhang, Miao Pan, Xiaoyong Yuan


[A Holistic Assessment of the Reliability of Machine Learning Systems. (4%)](http://arxiv.org/abs/2307.10586)

Anthony Corso, David Karamadian, Romeo Valentin, Mary Cooper, Mykel J. Kochenderfer


[Making Pre-trained Language Models both Task-solvers and Self-calibrators. (2%)](http://arxiv.org/abs/2307.11316)

Yangyi Chen, Xingyao Wang, Heng Ji


[Boundary State Generation for Testing and Improvement of Autonomous Driving Systems. (1%)](http://arxiv.org/abs/2307.10590)

Matteo Biagiola, Paolo Tonella


## 2023-07-19

[Backdoor Attack against Object Detection with Clean Annotation. (93%)](http://arxiv.org/abs/2307.10487)

Yize Cheng, Wenbin Hu, Minhao Cheng


[Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples. (92%)](http://arxiv.org/abs/2307.10562)

Shaokui Wei, Mingda Zhang, Hongyuan Zha, Baoyuan Wu


[Rethinking Backdoor Attacks. (83%)](http://arxiv.org/abs/2307.10163)

Alaa Khaddaj, Guillaume Leclerc, Aleksandar Makelov, Kristian Georgiev, Hadi Salman, Andrew Ilyas, Aleksander Madry


[Towards Building More Robust Models with Frequency Bias. (81%)](http://arxiv.org/abs/2307.09763)

Qingwen Bu, Dong Huang, Heming Cui


[Reinforcing POD based model reduction techniques in reaction-diffusion complex networks using stochastic filtering and pattern recognition. (26%)](http://arxiv.org/abs/2307.09762)

Abhishek Ajayakumar, Soumyendu Raha


## 2023-07-18

[CertPri: Certifiable Prioritization for Deep Neural Networks via Movement Cost in Feature Space. (67%)](http://arxiv.org/abs/2307.09375)

Haibin Zheng, Jinyin Chen, Haibo Jin


[FedDefender: Client-Side Attack-Tolerant Federated Learning. (50%)](http://arxiv.org/abs/2307.09048)

Sungwon Park, Sungwon Han, Fangzhao Wu, Sundong Kim, Bin Zhu, Xing Xie, Meeyoung Cha


[Can Neural Network Memorization Be Localized? (4%)](http://arxiv.org/abs/2307.09542)

Pratyush Maini, Michael C. Mozer, Hanie Sedghi, Zachary C. Lipton, J. Zico Kolter, Chiyuan Zhang


## 2023-07-17

[Analyzing the Impact of Adversarial Examples on Explainable Machine Learning. (99%)](http://arxiv.org/abs/2307.08327)

Prathyusha Devabhakthini, Sasmita Parida, Raj Mani Shukla, Suvendu Chandan Nayak


[Adversarial Attacks on Traffic Sign Recognition: A Survey. (98%)](http://arxiv.org/abs/2307.08278)

Svetlana Pavlitska, Nico Lambing, J. Marius Zöllner


[Discretization-based ensemble model for robust learning in IoT. (87%)](http://arxiv.org/abs/2307.08955)

Anahita Namvar, Chandra Thapa, Salil S. Kanhere


[Unstoppable Attack: Label-Only Model Inversion via Conditional Diffusion Model. (83%)](http://arxiv.org/abs/2307.08424)

Rongke Liu


[Experimental Security Analysis of DNN-based Adaptive Cruise Control under Context-Aware Perception Attacks. (11%)](http://arxiv.org/abs/2307.08939)

Xugui Zhou, Anqi Chen, Maxfield Kouzel, Haotian Ren, Morgan McCarty, Cristina Nita-Rotaru, Homa Alemzadeh


[On the Fly Neural Style Smoothing for Risk-Averse Domain Generalization. (2%)](http://arxiv.org/abs/2307.08551)

Akshay Mehra, Yunbei Zhang, Bhavya Kailkhura, Jihun Hamm


[A Machine Learning based Empirical Evaluation of Cyber Threat Actors High Level Attack Patterns over Low level Attack Patterns in Attributing Attacks. (1%)](http://arxiv.org/abs/2307.10252)

Umara Noor, Sawera Shahid, Rimsha Kanwal, Zahid Rashid


## 2023-07-16

[Towards Viewpoint-Invariant Visual Recognition via Adversarial Training. (83%)](http://arxiv.org/abs/2307.10235)

Shouwei Ruan, Yinpeng Dong, Hang Su, Jianteng Peng, Ning Chen, Xingxing Wei


[Towards Stealthy Backdoor Attacks against Speech Recognition via Elements of Sound. (73%)](http://arxiv.org/abs/2307.08208)

Hanbo Cai, Pengcheng Zhang, Hai Dong, Yan Xiao, Stefanos Koffas, Yiming Li


[Diffusion to Confusion: Naturalistic Adversarial Patch Generation Based on Diffusion Model for Object Detector. (10%)](http://arxiv.org/abs/2307.08076)

Shuo-Yen Lin, Ernie Chu, Che-Hsien Lin, Jun-Cheng Chen, Jia-Ching Wang


[Lipschitz Continuous Algorithms for Covering Problems. (1%)](http://arxiv.org/abs/2307.08213)

Soh Kumabe, Yuichi Yoshida


## 2023-07-15

[On the Robustness of Split Learning against Adversarial Attacks. (99%)](http://arxiv.org/abs/2307.07916)

Mingyuan Fan, Cen Chen, Chengyu Wang, Wenmeng Zhou, Jun Huang


[Why Does Little Robustness Help? Understanding and Improving Adversarial Transferability from Surrogate Training. (99%)](http://arxiv.org/abs/2307.07873)

Yechao Zhang, Shengshan Hu, Leo Yu Zhang, Junyu Shi, Minghui Li, Xiaogeng Liu, Wei Wan, Hai Jin


[Unified Adversarial Patch for Cross-modal Attacks in the Physical World. (92%)](http://arxiv.org/abs/2307.07859)

Xingxing Wei, Yao Huang, Yitong Sun, Jie Yu


[Jailbreaker: Automated Jailbreak Across Multiple Large Language Model Chatbots. (2%)](http://arxiv.org/abs/2307.08715)

Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, Yang Liu


## 2023-07-14

[Vulnerability-Aware Instance Reweighting For Adversarial Training. (99%)](http://arxiv.org/abs/2307.07167)

Olukorede Fakorede, Ashutosh Kumar Nirala, Modeste Atsague, Jin Tian


[Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning. (99%)](http://arxiv.org/abs/2307.07250)

Byung-Kwan Lee, Junho Kim, Yong Man Ro


[On the Sensitivity of Deep Load Disaggregation to Adversarial Attacks. (99%)](http://arxiv.org/abs/2307.10209)

Hafsa Bousbiat, Yassine Himeur, Abbes Amira, Wathiq Mansoor


[RFLA: A Stealthy Reflected Light Adversarial Attack in the Physical World. (98%)](http://arxiv.org/abs/2307.07653)

Donghua Wang, Wen Yao, Tingsong Jiang, Chao Li, Xiaoqian Chen


[Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation. (98%)](http://arxiv.org/abs/2307.07269)

Asif Hanif, Muzammal Naseer, Salman Khan, Mubarak Shah, Fahad Shahbaz Khan


[Adversarial Training Over Long-Tailed Distribution. (84%)](http://arxiv.org/abs/2307.10205)

Guanlin Li, Guowen Xu, Tianwei Zhang


[Structured Pruning of Neural Networks for Constraints Learning. (76%)](http://arxiv.org/abs/2307.07457)

Matteo Cacciola, Antonio Frangioni, Andrea Lodi


[Boosting Backdoor Attack with A Learnable Poisoning Sample Selection Strategy. (68%)](http://arxiv.org/abs/2307.07328)

Zihao Zhu, Mingda Zhang, Shaokui Wei, Li Shen, Yanbo Fan, Baoyuan Wu


[Erasing, Transforming, and Noising Defense Network for Occluded Person Re-Identification. (31%)](http://arxiv.org/abs/2307.07187)

Neng Dong, Liyan Zhang, Shuanglin Yan, Hao Tang, Jinhui Tang


[Certified Robustness for Large Language Models with Self-Denoising. (5%)](http://arxiv.org/abs/2307.07171)

Zhen Zhang, Guanhua Zhang, Bairu Hou, Wenqi Fan, Qing Li, Sijia Liu, Yang Zhang, Shiyu Chang


## 2023-07-13

[Multi-objective Evolutionary Search of Variable-length Composite Semantic Perturbations. (99%)](http://arxiv.org/abs/2307.06548)

Jialiang Suna, Wen Yao, Tingsong Jianga, Xiaoqian Chena


[Introducing Foundation Models as Surrogate Models: Advancing Towards More Practical Adversarial Attacks. (99%)](http://arxiv.org/abs/2307.06608)

Jiaming Zhang, Jitao Sang, Qi Yi, Changsheng Xu


[Defeating Proactive Jammers Using Deep Reinforcement Learning for Resource-Constrained IoT Networks. (1%)](http://arxiv.org/abs/2307.06796)

Abubakar Sani Ali, Shimaa Naser, Sami Muhaidat


## 2023-07-12

[Single-Class Target-Specific Attack against Interpretable Deep Learning Systems. (99%)](http://arxiv.org/abs/2307.06484)

Eldor Abdukhamidov, Mohammed Abuhamad, George K. Thiruvathukal, Hyoungshick Kim, Tamer Abuhmed


[Microbial Genetic Algorithm-based Black-box Attack against Interpretable Deep Learning Systems. (99%)](http://arxiv.org/abs/2307.06496)

Eldor Abdukhamidov, Mohammed Abuhamad, Simon S. Woo, Eric Chan-Tin, Tamer Abuhmed


[Rational Neural Network Controllers. (2%)](http://arxiv.org/abs/2307.06287)

Matthew Newton, Antonis Papachristodoulou


[A Bayesian approach to quantifying uncertainties and improving generalizability in traffic prediction models. (1%)](http://arxiv.org/abs/2307.05946)

Agnimitra Sengupta, Sudeepta Mondal, Adway Das, S. Ilgin Guler


[Misclassification in Automated Content Analysis Causes Bias in Regression. Can We Fix It? Yes We Can! (1%)](http://arxiv.org/abs/2307.06483)

Nathan TeBlunthuis, Valerie Hase, Chung-Hong Chan


## 2023-07-11

[ATWM: Defense against adversarial malware based on adversarial training. (99%)](http://arxiv.org/abs/2307.05095)

Kun Li, Fan Zhang, Wei Guo


[Membership Inference Attacks on DNNs using Adversarial Perturbations. (89%)](http://arxiv.org/abs/2307.05193)

Hassan Ali, Adnan Qayyum, Ala Al-Fuqaha, Junaid Qadir


[Random-Set Convolutional Neural Network (RS-CNN) for Epistemic Deep Learning. (4%)](http://arxiv.org/abs/2307.05772)

Shireen Kudukkil Manchingal, Muhammad Mubashar, Kaizheng Wang, Keivan Shariatmadar, Fabio Cuzzolin


[Differential Analysis of Triggers and Benign Features for Black-Box DNN Backdoor Detection. (2%)](http://arxiv.org/abs/2307.05422)

Hao Fu, Prashanth Krishnamurthy, Siddharth Garg, Farshad Khorrami


## 2023-07-10

[Practical Trustworthiness Model for DNN in Dedicated 6G Application. (33%)](http://arxiv.org/abs/2307.04677)

Anouar Nechi, Ahmed Mahmoudi, Christoph Herold, Daniel Widmer, Thomas Kürner, Mladen Berekovic, Saleh Mulhem


## 2023-07-09

[Enhancing Adversarial Robustness via Score-Based Optimization. (98%)](http://arxiv.org/abs/2307.04333)

Boya Zhang, Weijian Luo, Zhihua Zhang


[GNP Attack: Transferable Adversarial Examples via Gradient Norm Penalty. (98%)](http://arxiv.org/abs/2307.04099)

Tao Wu, Tie Luo, Donald C. Wunsch


## 2023-07-08

[Adversarial Self-Attack Defense and Spatial-Temporal Relation Mining for Visible-Infrared Video Person Re-Identification. (99%)](http://arxiv.org/abs/2307.03903)

Huafeng Li, Le Xu, Yafei Zhang, Dapeng Tao, Zhengtao Yu


[Random Position Adversarial Patch for Vision Transformers. (83%)](http://arxiv.org/abs/2307.04066)

Mingzhen Shao


[Robust Ranking Explanations. (38%)](http://arxiv.org/abs/2307.04024)

Chao Chen, Chenghua Guo, Guixiang Ma, Ming Zeng, Xi Zhang, Sihong Xie


## 2023-07-07

[A Theoretical Perspective on Subnetwork Contributions to Adversarial Robustness. (81%)](http://arxiv.org/abs/2307.03803)

Jovon Craig, Josh Andle, Theodore S. Nowak, Salimeh Yasaei Sekeh


[Scalable Membership Inference Attacks via Quantile Regression. (33%)](http://arxiv.org/abs/2307.03694)

Martin Bertran, Shuai Tang, Michael Kearns, Jamie Morgenstern, Aaron Roth, Zhiwei Steven Wu


[RADAR: Robust AI-Text Detection via Adversarial Learning. (5%)](http://arxiv.org/abs/2307.03838)

Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho


[Generation of Time-Varying Impedance Attacks Against Haptic Shared Control Steering Systems. (1%)](http://arxiv.org/abs/2307.12399)

Alireza Mohammadi, Hafiz Malik


## 2023-07-06

[Sampling-based Fast Gradient Rescaling Method for Highly Transferable Adversarial Attacks. (99%)](http://arxiv.org/abs/2307.02828)

Xu Han, Anmin Liu, Chenxuan Yao, Yanbo Fan, Kun He


[NatLogAttack: A Framework for Attacking Natural Language Inference Models with Natural Logic. (92%)](http://arxiv.org/abs/2307.02849)

Zi'ou Zheng, Xiaodan Zhu


[Quantification of Uncertainty with Adversarial Models. (68%)](http://arxiv.org/abs/2307.03217)

Kajetan Schweighofer, Lukas Aichberger, Mykyta Ielanskyi, Günter Klambauer, Sepp Hochreiter


[A Vulnerability of Attribution Methods Using Pre-Softmax Scores. (41%)](http://arxiv.org/abs/2307.03305)

Miguel Lerma, Mirtha Lucas


[Probabilistic and Semantic Descriptions of Image Manifolds and Their Applications. (2%)](http://arxiv.org/abs/2307.02881)

Peter Tu, Zhaoyuan Yang, Richard Hartley, Zhiwei Xu, Jing Zhang, Dylan Campbell, Jaskirat Singh, Tianyu Wang


[T-MARS: Improving Visual Representations by Circumventing Text Feature Learning. (1%)](http://arxiv.org/abs/2307.03132)

Pratyush Maini, Sachin Goyal, Zachary C. Lipton, J. Zico Kolter, Aditi Raghunathan


## 2023-07-05

[Adversarial Attacks on Image Classification Models: FGSM and Patch Attacks and their Impact. (98%)](http://arxiv.org/abs/2307.02055)

Jaydip Sen, Subhasis Dasgupta


[DARE: Towards Robust Text Explanations in Biomedical and Healthcare Applications. (69%)](http://arxiv.org/abs/2307.02094)

Adam Ivankay, Mattia Rigotti, Pascal Frossard


[Detecting Images Generated by Deep Diffusion Models using their Local Intrinsic Dimensionality. (67%)](http://arxiv.org/abs/2307.02347)

Peter Lorenz, Ricard Durall, Janis Keuper


[GIT: Detecting Uncertainty, Out-Of-Distribution and Adversarial Samples using Gradients and Invariance Transformations. (62%)](http://arxiv.org/abs/2307.02672)

Julia Lust, Alexandru P. Condurache


[Securing Cloud FPGAs Against Power Side-Channel Attacks: A Case Study on Iterative AES. (5%)](http://arxiv.org/abs/2307.02569)

Nithyashankari Gummidipoondi JV Jayasankaran, Hao JV Guo, Satwik JV Patnaik, JV Jeyavijayan, Rajendran, Jiang Hu


[On the Adversarial Robustness of Generative Autoencoders in the Latent Space. (3%)](http://arxiv.org/abs/2307.02202)

Mingfei Lu, Badong Chen


## 2023-07-04

[SCAT: Robust Self-supervised Contrastive Learning via Adversarial Training for Text Classification. (99%)](http://arxiv.org/abs/2307.01488)

Junjie Wu, Dit-Yan Yeung


[LEAT: Towards Robust Deepfake Disruption in Real-World Scenarios via Latent Ensemble Attack. (83%)](http://arxiv.org/abs/2307.01520)

Joonkyo Shim, Hyunsoo Yoon


[Interpretable Computer Vision Models through Adversarial Training: Unveiling the Robustness-Interpretability Connection. (68%)](http://arxiv.org/abs/2307.02500)

Delyan Boychev


[Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction. (45%)](http://arxiv.org/abs/2307.01610)

Zitao Chen, Karthik Pattabiraman


[Physically Realizable Natural-Looking Clothing Textures Evade Person Detectors via 3D Modeling. (26%)](http://arxiv.org/abs/2307.01778)

Zhanhao Hu, Wenda Chu, Xiaopei Zhu, Hui Zhang, Bo Zhang, Xiaolin Hu


[An Analysis of Untargeted Poisoning Attack and Defense Methods for Federated Online Learning to Rank Systems. (13%)](http://arxiv.org/abs/2307.01565)

Shuyi Wang, Guido Zuccon


[Machine Learning-Based Intrusion Detection: Feature Selection versus Feature Extraction. (1%)](http://arxiv.org/abs/2307.01570)

Vu-Duc Ngo, Tuan-Cuong Vuong, Luong Thien Van, Hung Tran


## 2023-07-03

[Pareto-Secure Machine Learning (PSML): Fingerprinting and Securing Inference Serving Systems. (99%)](http://arxiv.org/abs/2307.01292)

Debopam Georgia Institute of Technology Sanyal, Jui-Tse Georgia Institute of Technology Hung, Manav Georgia Institute of Technology Agrawal, Prahlad Georgia Institute of Technology Jasti, Shahab University of California, Riverside Nikkhoo, Somesh University of Wisconsin-Madison Jha, Tianhao University of Virginia Wang, Sibin George Washington University Mohan, Alexey Georgia Institute of Technology Tumanov


[A Dual Stealthy Backdoor: From Both Spatial and Frequency Perspectives. (83%)](http://arxiv.org/abs/2307.10184)

Yudong Gao, Honglong Chen, Peng Sun, Junjian Li, Anqing Zhang, Zhibo Wang


[When Can Linear Learners be Robust to Indiscriminate Poisoning Attacks? (62%)](http://arxiv.org/abs/2307.01073)

Fnu Suya, Xiao Zhang, Yuan Tian, David Evans


[Analyzing the vulnerabilities in SplitFed Learning: Assessing the robustness against Data Poisoning Attacks. (62%)](http://arxiv.org/abs/2307.03197)

Aysha Thahsin Zahir Ismail, Raj Mani Shukla


[Adversarial Learning in Real-World Fraud Detection: Challenges and Perspectives. (45%)](http://arxiv.org/abs/2307.01390)

Danele Lunghi, Alkis Simitsis, Olivier Caelen, Gianluca Bontempi


[Analysis of Task Transferability in Large Pre-trained Classifiers. (13%)](http://arxiv.org/abs/2307.00823)

Akshay Mehra, Yunbei Zhang, Jihun Hamm


[Enhancing the Robustness of QMIX against State-adversarial Attacks. (4%)](http://arxiv.org/abs/2307.00907)

Weiran Guo, Guanjun Liu, Ziyuan Zhou, Ling Wang, Jiacun Wang


[Towards Building Self-Aware Object Detectors via Reliable Uncertainty Quantification and Calibration. (1%)](http://arxiv.org/abs/2307.00934)

Kemal Oksuz, Tom Joy, Puneet K. Dokania


## 2023-07-02

[Query-Efficient Decision-based Black-Box Patch Attack. (99%)](http://arxiv.org/abs/2307.00477)

Zhaoyu Chen, Bo Li, Shuang Wu, Shouhong Ding, Wenqiang Zhang


[Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT). (99%)](http://arxiv.org/abs/2307.01225)

Bushra Sabir, M. Ali Babar, Sharif Abuadbba


[From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy. (10%)](http://arxiv.org/abs/2307.00691)

Maanak Gupta, CharanKumar Akiri, Kshitiz Aryal, Eli Parker, Lopamudra Praharaj


[CLIMAX: An exploration of Classifier-Based Contrastive Explanations. (2%)](http://arxiv.org/abs/2307.00680)

Praharsh Nanavati, Ranjitha Prasad


## 2023-07-01

[Common Knowledge Learning for Generating Transferable Adversarial Examples. (99%)](http://arxiv.org/abs/2307.00274)

Ruijie Yang, Yuanfang Guo, Junfu Wang, Jiantao Zhou, Yunhong Wang


[Adversarial Attacks and Defenses on 3D Point Cloud Classification: A Survey. (99%)](http://arxiv.org/abs/2307.00309)

Hanieh Naderi, Ivan V. Bajić


[Brightness-Restricted Adversarial Attack Patch. (75%)](http://arxiv.org/abs/2307.00421)

Mingzhen Shao


[Fedward: Flexible Federated Backdoor Defense Framework with Non-IID Data. (54%)](http://arxiv.org/abs/2307.00356)

Zekai Chen, Fuyi Wang, Zhiwei Zheng, Ximeng Liu, Yujie Lin


[Minimizing Energy Consumption of Deep Learning Models by Energy-Aware Training. (26%)](http://arxiv.org/abs/2307.00368)

Dario Lazzaro, Antonio Emanuele Cinà, Maura Pintor, Ambra Demontis, Battista Biggio, Fabio Roli, Marcello Pelillo


[SysNoise: Exploring and Benchmarking Training-Deployment System Inconsistency. (13%)](http://arxiv.org/abs/2307.00280)

Yan Wang, Yuhang Li, Ruihao Gong, Aishan Liu, Yanfei Wang, Jian Hu, Yongqiang Yao, Yunchen Zhang, Tianzi Xiao, Fengwei Yu, Xianglong Liu


[CasTGAN: Cascaded Generative Adversarial Network for Realistic Tabular Data Synthesis. (2%)](http://arxiv.org/abs/2307.00384)

Abdallah Alshantti, Damiano Varagnolo, Adil Rasheed, Aria Rahmati, Frank Westad


[FedDefender: Backdoor Attack Defense in Federated Learning. (2%)](http://arxiv.org/abs/2307.08672)

Waris Virginia Tech Gill, Ali University of Minnesota Twin Cities Anwar, Muhammad Ali Virginia Tech Gulzar


[Hiding in Plain Sight: Differential Privacy Noise Exploitation for Evasion-resilient Localized Poisoning Attacks in Multiagent Reinforcement Learning. (1%)](http://arxiv.org/abs/2307.00268)

Md Tamjid Hossain, Hung La


## 2023-06-30

[Defense against Adversarial Cloud Attack on Remote Sensing Salient Object Detection. (99%)](http://arxiv.org/abs/2306.17431)

Huiming Sun, Lan Fu, Jinlong Li, Qing Guo, Zibo Meng, Tianyun Zhang, Yuewei Lin, Hongkai Yu


[Efficient Backdoor Removal Through Natural Gradient Fine-tuning. (8%)](http://arxiv.org/abs/2306.17441)

Nazmul Karim, Abdullah Al Arafat, Umar Khalid, Zhishan Guo, Naznin Rahnavard


[Minimum-norm Sparse Perturbations for Opacity in Linear Systems. (1%)](http://arxiv.org/abs/2306.17606)

Varkey M John, Vaibhav Katewa


## 2023-06-29

[Defending Black-box Classifiers by Bayesian Boundary Correction. (99%)](http://arxiv.org/abs/2306.16979)

He Wang, Yunfeng Diao


[Towards Optimal Randomized Strategies in Adversarial Example Game. (96%)](http://arxiv.org/abs/2306.16738)

Jiahao Xie, Chao Zhang, Weijie Liu, Wensong Bai, Hui Qian


[Neural Polarizer: A Lightweight and Effective Backdoor Defense via Purifying Poisoned Features. (13%)](http://arxiv.org/abs/2306.16697)

Mingli Zhu, Shaokui Wei, Hongyuan Zha, Baoyuan Wu


[NeuralFuse: Learning to Improve the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes. (1%)](http://arxiv.org/abs/2306.16869)

Hao-Lun Sun, Lei Hsiung, Nandhini Chandramoorthy, Pin-Yu Chen, Tsung-Yi Ho


## 2023-06-28

[Mitigating the Accuracy-Robustness Trade-off via Multi-Teacher Adversarial Distillation. (99%)](http://arxiv.org/abs/2306.16170)

Shiji Zhao, Xizhe Wang, Xingxing Wei


[Boosting Adversarial Transferability with Learnable Patch-wise Masks. (99%)](http://arxiv.org/abs/2306.15931)

Xingxing Wei, Shiji Zhao


[Evaluating Similitude and Robustness of Deep Image Denoising Models via Adversarial Attack. (99%)](http://arxiv.org/abs/2306.16050)

Jie Ning, Yao Li, Zhichang Guo


[Group-based Robustness: A General Framework for Customized Robustness in the Real World. (98%)](http://arxiv.org/abs/2306.16614)

Weiran Lin, Keane Lucas, Neo Eyal, Lujo Bauer, Michael K. Reiter, Mahmood Sharif


[Distributional Modeling for Location-Aware Adversarial Patches. (98%)](http://arxiv.org/abs/2306.16131)

Xingxing Wei, Shouwei Ruan, Yinpeng Dong, Hang Su


[Enrollment-stage Backdoor Attacks on Speaker Recognition Systems via Adversarial Ultrasound. (96%)](http://arxiv.org/abs/2306.16022)

Xinfeng Li, Junning Ze, Chen Yan, Yushi Cheng, Xiaoyu Ji, Wenyuan Xu


[Does Saliency-Based Training bring Robustness for Deep Neural Networks in Image Classification? (93%)](http://arxiv.org/abs/2306.16581)

Ali Karkehabadi


[On Practical Aspects of Aggregation Defenses against Data Poisoning Attacks. (50%)](http://arxiv.org/abs/2306.16415)

Wenxiao Wang, Soheil Feizi


[On the Exploitability of Instruction Tuning. (13%)](http://arxiv.org/abs/2306.17194)

Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping, Chaowei Xiao, Tom Goldstein


## 2023-06-27

[Advancing Adversarial Training by Injecting Booster Signal. (98%)](http://arxiv.org/abs/2306.15451)

Hong Joo Lee, Youngjoon Yu, Yong Man Ro


[Adversarial Training for Graph Neural Networks. (92%)](http://arxiv.org/abs/2306.15427)

Lukas Gosch, Simon Geisler, Daniel Sturm, Bertrand Charpentier, Daniel Zügner, Stephan Günnemann


[Robust Proxy: Improving Adversarial Robustness by Robust Proxy Learning. (89%)](http://arxiv.org/abs/2306.15457)

Hong Joo Lee, Yong Man Ro


[Your Attack Is Too DUMB: Formalizing Attacker Scenarios for Adversarial Transferability. (87%)](http://arxiv.org/abs/2306.15363)

Marco Alecci, Mauro Conti, Francesco Marchiori, Luca Martinelli, Luca Pajola


[[Re] Double Sampling Randomized Smoothing. (69%)](http://arxiv.org/abs/2306.15221)

Aryan Gupta, Sarthak Gupta, Abhay Kumar, Harsh Dugar


[Cooperation or Competition: Avoiding Player Domination for Multi-Target Robustness via Adaptive Budgets. (68%)](http://arxiv.org/abs/2306.15482)

Yimu Wang, Dinghuai Zhang, Yihan Wu, Heng Huang, Hongyang Zhang


[IMPOSITION: Implicit Backdoor Attack through Scenario Injection. (41%)](http://arxiv.org/abs/2306.15755)

Mozhgan Pourkeshavarz, Mohammad Sabokrou, Amir Rasouli


[Catch Me If You Can: A New Low-Rate DDoS Attack Strategy Disguised by Feint. (26%)](http://arxiv.org/abs/2306.15248)

Tianyang Cai, Yuqi Li, Tao Jia, Leo Yu Zhang, Zheng Yang


[Shilling Black-box Review-based Recommender Systems through Fake Review Generation. (1%)](http://arxiv.org/abs/2306.16526)

Hung-Yun Chiang, Yi-Syuan Chen, Yun-Zhu Song, Hong-Han Shuai, Jason S. Chang


## 2023-06-26

[On the Universal Adversarial Perturbations for Efficient Data-free Adversarial Detection. (99%)](http://arxiv.org/abs/2306.15705)

Songyang Gao, Shihan Dou, Qi Zhang, Xuanjing Huang, Jin Ma, Ying Shan


[Are aligned neural networks adversarially aligned? (99%)](http://arxiv.org/abs/2306.15447)

Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramer, Ludwig Schmidt


[The race to robustness: exploiting fragile models for urban camouflage and the imperative for machine learning security. (92%)](http://arxiv.org/abs/2306.14609)

Harriet Farlow, Matthew Garratt, Gavin Mount, Tim Lynar


[3D-Aware Adversarial Makeup Generation for Facial Privacy Protection. (92%)](http://arxiv.org/abs/2306.14640)

Yueming Lyu, Yue Jiang, Ziwen He, Bo Peng, Yunfan Liu, Jing Dong


[Towards Sybil Resilience in Decentralized Learning. (80%)](http://arxiv.org/abs/2306.15044)

Thomas Werthenbach, Johan Pouwelse


[On the Resilience of Machine Learning-Based IDS for Automotive Networks. (78%)](http://arxiv.org/abs/2306.14782)

Ivo Zenden, Han Wang, Alfonso Iacovazzi, Arash Vahidi, Rolf Blom, Shahid Raza


[DSRM: Boost Textual Adversarial Training with Distribution Shift Risk Minimization. (75%)](http://arxiv.org/abs/2306.15164)

Songyang Gao, Shihan Dou, Yan Liu, Xiao Wang, Qi Zhang, Zhongyu Wei, Jin Ma, Ying Shan


[PWSHAP: A Path-Wise Explanation Model for Targeted Variables. (8%)](http://arxiv.org/abs/2306.14672)

Lucile Ter-Minassian, Oscar Clivio, Karla Diaz-Ordaz, Robin J. Evans, Chris Holmes


## 2023-06-25

[A Spectral Perspective towards Understanding and Improving Adversarial Robustness. (99%)](http://arxiv.org/abs/2306.14262)

Binxiao Huang, Rui Lin, Chaofan Tao, Ngai Wong


[On Evaluating the Adversarial Robustness of Semantic Segmentation Models. (99%)](http://arxiv.org/abs/2306.14217)

Levente Halmosi, Mark Jelasity


[Robust Spatiotemporal Traffic Forecasting with Reinforced Dynamic Adversarial Training. (98%)](http://arxiv.org/abs/2306.14126)

Fan Liu, Weijia Zhang, Hao Liu


[Enhancing Adversarial Training via Reweighting Optimization Trajectory. (97%)](http://arxiv.org/abs/2306.14275)

Tianjin Huang, Shiwei Liu, Tianlong Chen, Meng Fang, Li Shen, Vlaod Menkovski, Lu Yin, Yulong Pei, Mykola Pechenizkiy


[RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations. (87%)](http://arxiv.org/abs/2306.14321)

Yilun Zhao, Chen Zhao, Linyong Nan, Zhenting Qi, Wenlin Zhang, Xiangru Tang, Boyu Mi, Dragomir Radev


[Computational Asymmetries in Robust Classification. (80%)](http://arxiv.org/abs/2306.14326)

Samuele Marro, Michele Lombardi


## 2023-06-24

[Machine Learning needs its own Randomness Standard: Randomised Smoothing and PRNG-based attacks. (98%)](http://arxiv.org/abs/2306.14043)

Pranav Dahiya, Ilia Shumailov, Ross Anderson


[Boosting Model Inversion Attacks with Adversarial Examples. (98%)](http://arxiv.org/abs/2306.13965)

Shuai Zhou, Tianqing Zhu, Dayong Ye, Xin Yu, Wanlei Zhou


[Similarity Preserving Adversarial Graph Contrastive Learning. (96%)](http://arxiv.org/abs/2306.13854)

Yeonjun In, Kanghoon Yoon, Chanyoung Park


[Weighted Automata Extraction and Explanation of Recurrent Neural Networks for Natural Language Tasks. (70%)](http://arxiv.org/abs/2306.14040)

Zeming Wei, Xiyue Zhang, Yihao Zhang, Meng Sun


## 2023-06-23

[Creating Valid Adversarial Examples of Malware. (99%)](http://arxiv.org/abs/2306.13587)

Matouš Kozák, Martin Jureček, Mark Stamp, Troia Fabio Di


[Adversarial Robustness Certification for Bayesian Neural Networks. (92%)](http://arxiv.org/abs/2306.13614)

Matthew Wicker, Andrea Patane, Luca Laurenti, Marta Kwiatkowska


[A First Order Meta Stackelberg Method for Robust Federated Learning. (10%)](http://arxiv.org/abs/2306.13800)

Yunian Pan, Tao Li, Henger Li, Tianyi Xu, Zizhan Zheng, Quanyan Zhu


## 2023-06-22

[Visual Adversarial Examples Jailbreak Large Language Models. (99%)](http://arxiv.org/abs/2306.13213)

Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Mengdi Wang, Prateek Mittal


[Rethinking the Backward Propagation for Adversarial Transferability. (99%)](http://arxiv.org/abs/2306.12685)

Xiaosen Wang, Kangheng Tong, Kun He


[Towards quantum enhanced adversarial robustness in machine learning. (99%)](http://arxiv.org/abs/2306.12688)

Maxwell T. West, Shu-Lok Tsang, Jia S. Low, Charles D. Hill, Christopher Leckie, Lloyd C. L. Hollenberg, Sarah M. Erfani, Muhammad Usman


[Evading Forensic Classifiers with Attribute-Conditioned Adversarial Faces. (96%)](http://arxiv.org/abs/2306.13091)

Fahad Shamshad, Koushik Srivatsan, Karthik Nandakumar


[Adversarial Resilience in Sequential Prediction via Abstention. (93%)](http://arxiv.org/abs/2306.13119)

Surbhi Goel, Steve Hanneke, Shay Moran, Abhishek Shetty


[Document Image Cleaning using Budget-Aware Black-Box Approximation. (92%)](http://arxiv.org/abs/2306.13236)

Ganesh Tata, Katyani Singh, Oeveren Eric Van, Nilanjan Ray


[Anticipatory Thinking Challenges in Open Worlds: Risk Management. (81%)](http://arxiv.org/abs/2306.13157)

Adam Amos-Binks, Dustin Dannenhauer, Leilani H. Gilpin


[Robust Semantic Segmentation: Strong Adversarial Attacks and Fast Training of Robust Models. (75%)](http://arxiv.org/abs/2306.12941)

Francesco Croce, Naman D Singh, Matthias Hein


[A First Order Meta Stackelberg Method for Robust Federated Learning (Technical Report). (33%)](http://arxiv.org/abs/2306.13273)

Henger Li, Tianyi Xu, Tao Li, Yunian Pan, Quanyan Zhu, Zizhan Zheng


[Impacts and Risk of Generative AI Technology on Cyber Defense. (4%)](http://arxiv.org/abs/2306.13033)

Subash Neupane, Ivan A. Fernandez, Sudip Mittal, Shahram Rahimi


## 2023-06-21

[Adversarial Attacks Neutralization via Data Set Randomization. (99%)](http://arxiv.org/abs/2306.12161)

Mouna Rabhi, Pietro Roberto Di


[A Comprehensive Study on the Robustness of Image Classification and Object Detection in Remote Sensing: Surveying and Benchmarking. (92%)](http://arxiv.org/abs/2306.12111)

Shaohui Mei, Jiawei Lian, Xiaofei Wang, Yuru Su, Mingyang Ma, Lap-Pui Chau


[Sample Attackability in Natural Language Adversarial Attacks. (92%)](http://arxiv.org/abs/2306.12043)

Vyas Raina, Mark Gales


[Revisiting Image Classifier Training for Improved Certified Robust Defense against Adversarial Patches. (76%)](http://arxiv.org/abs/2306.12610)

Aniruddha Saha, Shuhua Yu, Arash Norouzzadeh, Wan-Yi Lin, Chaithanya Kumar Mummadi


[DP-BREM: Differentially-Private and Byzantine-Robust Federated Learning with Client Momentum. (47%)](http://arxiv.org/abs/2306.12608)

Xiaolan Gu, Ming Li, Li Xiong


[FFCV: Accelerating Training by Removing Data Bottlenecks. (3%)](http://arxiv.org/abs/2306.12517)

Guillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, Aleksander Madry


## 2023-06-20

[Evaluating Adversarial Robustness of Convolution-based Human Motion Prediction. (99%)](http://arxiv.org/abs/2306.11990)

Chengxu Duan, Zhicheng Zhang, Xiaoli Liu, Yonghao Dang, Jianqin Yin


[Universal adversarial perturbations for multiple classification tasks with quantum classifiers. (99%)](http://arxiv.org/abs/2306.11974)

Yun-Zhong Qiu


[Reversible Adversarial Examples with Beam Search Attack and Grayscale Invariance. (99%)](http://arxiv.org/abs/2306.11322)

Haodong Zhang, Chi Man Pun, Xia Du


[FDInet: Protecting against DNN Model Extraction via Feature Distortion Index. (50%)](http://arxiv.org/abs/2306.11338)

Hongwei Yao, Zheng Li, Haiqin Weng, Feng Xue, Kui Ren, Zhan Qin


[DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. (26%)](http://arxiv.org/abs/2306.11698)

Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, Bo Li


[Towards a robust and reliable deep learning approach for detection of compact binary mergers in gravitational wave data. (2%)](http://arxiv.org/abs/2306.11797)

Shreejit Jadhav, Mihir Shrivastava, Sanjit Mitra


[LVM-Med: Learning Large-Scale Self-Supervised Vision Models for Medical Imaging via Second-order Graph Matching. (1%)](http://arxiv.org/abs/2306.11925)

Duy M. H. Nguyen, Hoang Nguyen, Nghiem T. Diep, Tan N. Pham, Tri Cao, Binh T. Nguyen, Paul Swoboda, Nhat Ho, Shadi Albarqouni, Pengtao Xie, Daniel Sonntag, Mathias Niepert


[Mitigating Speculation-based Attacks through Configurable Hardware/Software Co-design. (1%)](http://arxiv.org/abs/2306.11291)

Ali Hajiabadi, Archit Agarwal, Andreas Diavastos, Trevor E. Carlson


## 2023-06-19

[Comparative Evaluation of Recent Universal Adversarial Perturbations in Image Classification. (99%)](http://arxiv.org/abs/2306.11261)

Juanjuan Weng, Zhiming Luo, Dazhen Lin, Shaozi Li


[Adversarial Robustness of Prompt-based Few-Shot Learning for Natural Language Understanding. (75%)](http://arxiv.org/abs/2306.11066)

Venkata Prabhakara Sarath Nookala, Gaurav Verma, Subhabrata Mukherjee, Srijan Kumar


[Adversarial Training Should Be Cast as a Non-Zero-Sum Game. (73%)](http://arxiv.org/abs/2306.11035)

Alexander Robey, Fabian Latorre, George J. Pappas, Hamed Hassani, Volkan Cevher


[Eigenpatches -- Adversarial Patches from Principal Components. (38%)](http://arxiv.org/abs/2306.10963)

Jens Bayer, Stefan Becker, David Münch, Michael Arens


[Practical and General Backdoor Attacks against Vertical Federated Learning. (13%)](http://arxiv.org/abs/2306.10746)

Yuexin Xuan, Xiaojun Chen, Zhendong Zhao, Bisheng Tang, Ye Dong


[BNN-DP: Robustness Certification of Bayesian Neural Networks via Dynamic Programming. (5%)](http://arxiv.org/abs/2306.10742)

Steven Adams, Andrea Patane, Morteza Lahijanian, Luca Laurenti


## 2023-06-17

[Edge Learning for 6G-enabled Internet of Things: A Comprehensive Survey of Vulnerabilities, Datasets, and Defenses. (98%)](http://arxiv.org/abs/2306.10309)

Mohamed Amine Ferrag, Othmane Friha, Burak Kantarci, Norbert Tihanyi, Lucas Cordeiro, Merouane Debbah, Djallel Hamouda, Muna Al-Hawawreh, Kim-Kwang Raymond Choo


[Understanding Certified Training with Interval Bound Propagation. (38%)](http://arxiv.org/abs/2306.10426)

Yuhao Mao, Mark Niklas Müller, Marc Fischer, Martin Vechev


[GlyphNet: Homoglyph domains dataset and detection using attention-based Convolutional Neural Networks. (9%)](http://arxiv.org/abs/2306.10392)

Akshat Gupta, Laxman Singh Tomar, Ridhima Garg


[Bkd-FedGNN: A Benchmark for Classification Backdoor Attacks on Federated Graph Neural Network. (1%)](http://arxiv.org/abs/2306.10351)

Fan Liu, Siqi Lai, Yansong Ning, Hao Liu


## 2023-06-16

[Wasserstein distributional robustness of neural networks. (99%)](http://arxiv.org/abs/2306.09844)

Xingjian Bai, Guangyi He, Yifan Jiang, Jan Obloj


[Query-Free Evasion Attacks Against Machine Learning-Based Malware Detectors with Generative Adversarial Networks. (99%)](http://arxiv.org/abs/2306.09925)

Daniel Gibert, Jordi Planes, Quan Le, Giulio Zizzo


[You Don't Need Robust Machine Learning to Manage Adversarial Attack Risks. (98%)](http://arxiv.org/abs/2306.09951)

Edward Raff, Michel Benaroch, Andrew L. Farris


[Towards Better Certified Segmentation via Diffusion Models. (73%)](http://arxiv.org/abs/2306.09949)

Othmane Laousy, Alexandre Araujo, Guillaume Chassagnon, Marie-Pierre Revel, Siddharth Garg, Farshad Khorrami, Maria Vakalopoulou


[Adversarially robust clustering with optimality guarantees. (4%)](http://arxiv.org/abs/2306.09977)

Soham Jana, Kun Yang, Sanjeev Kulkarni


[CLIP2Protect: Protecting Facial Privacy using Text-Guided Makeup via Adversarial Latent Search. (1%)](http://arxiv.org/abs/2306.10008)

Fahad Shamshad, Muzammal Naseer, Karthik Nandakumar


## 2023-06-15

[DIFFender: Diffusion-Based Adversarial Defense against Patch Attacks in the Physical World. (99%)](http://arxiv.org/abs/2306.09124)

Caixin Kang, Yinpeng Dong, Zhengyi Wang, Shouwei Ruan, Hang Su, Xingxing Wei


[OVLA: Neural Network Ownership Verification using Latent Watermarks. (64%)](http://arxiv.org/abs/2306.13215)

Feisi Fu, Wenchao Li


[Evaluating the Robustness of Text-to-image Diffusion Models against Real-world Attacks. (62%)](http://arxiv.org/abs/2306.13103)

Hongcheng Gao, Hao Zhang, Yinpeng Dong, Zhijie Deng


[On Strengthening and Defending Graph Reconstruction Attack with Markov Chain Approximation. (33%)](http://arxiv.org/abs/2306.09104)

Zhanke Zhou, Chenyu Zhou, Xuan Li, Jiangchao Yao, Quanming Yao, Bo Han


[Robustness Analysis on Foundational Segmentation Models. (9%)](http://arxiv.org/abs/2306.09278)

Madeline Chantry Schiappa, Sachidanand VS, Yunhao Ge, Ondrej Miksik, Yogesh S. Rawat, Vibhav Vineet


[Community Detection Attack against Collaborative Learning-based Recommender Systems. (1%)](http://arxiv.org/abs/2306.08929)

Yacine Belal, Sonia Ben Mokhtar, Mohamed Maouche, Anthony Simonet-Boulogne


[Concealing CAN Message Sequences to Prevent Schedule-based Bus-off Attacks. (1%)](http://arxiv.org/abs/2306.09206)

Sunandan Adhikary, Ipsita Koley, Arkaprava Sain, Soumyadeep das, Shuvam Saha, Soumyajit Dey


## 2023-06-14

[Reliable Evaluation of Adversarial Transferability. (99%)](http://arxiv.org/abs/2306.08565)

Wenqian Yu, Jindong Gu, Zhijiang Li, Philip Torr


[A Relaxed Optimization Approach for Adversarial Attacks against Neural Machine Translation Models. (99%)](http://arxiv.org/abs/2306.08492)

Sahar Sadrizadeh, Clément Barbier, Ljiljana Dolamic, Pascal Frossard


[X-Detect: Explainable Adversarial Patch Detection for Object Detectors in Retail. (98%)](http://arxiv.org/abs/2306.08422)

Omer Hofman, Amit Giloni, Yarin Hayun, Ikuya Morikawa, Toshiya Shimizu, Yuval Elovici, Asaf Shabtai


[Augment then Smooth: Reconciling Differential Privacy with Certified Robustness. (98%)](http://arxiv.org/abs/2306.08656)

Jiapeng Wu, Atiyeh Ashari Ghomi, David Glukhov, Jesse C. Cresswell, Franziska Boenisch, Nicolas Papernot


[Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios. (83%)](http://arxiv.org/abs/2306.08386)

Hong Sun, Ziqiang Li, Pengfei Xia, Heng Li, Beihao Xia, Yi Wu, Bin Li


[A Unified Framework of Graph Information Bottleneck for Robustness and Membership Privacy. (75%)](http://arxiv.org/abs/2306.08604)

Enyan Dai, Limeng Cui, Zhengyang Wang, Xianfeng Tang, Yinghan Wang, Monica Cheng, Bing Yin, Suhang Wang


[On the Robustness of Latent Diffusion Models. (73%)](http://arxiv.org/abs/2306.08257)

Jianping Zhang, Zhuoer Xu, Shiwen Cui, Changhua Meng, Weibin Wu, Michael R. Lyu


[Improving Selective Visual Question Answering by Learning from Your Peers. (1%)](http://arxiv.org/abs/2306.08751)

Corentin Dancette, Spencer Whitehead, Rishabh Maheshwary, Ramakrishna Vedantam, Stefan Scherer, Xinlei Chen, Matthieu Cord, Marcus Rohrbach


## 2023-06-13

[Theoretical Foundations of Adversarially Robust Learning. (99%)](http://arxiv.org/abs/2306.07723)

Omar Montasser


[Finite Gaussian Neurons: Defending against adversarial attacks by making neural networks say "I don't know". (99%)](http://arxiv.org/abs/2306.07796)

Felix Grezes


[I See Dead People: Gray-Box Adversarial Attack on Image-To-Text Models. (99%)](http://arxiv.org/abs/2306.07591)

Raz Lapid, Moshe Sipper


[Robustness of SAM: Segment Anything Under Corruptions and Beyond. (98%)](http://arxiv.org/abs/2306.07713)

Yu Qiao, Chaoning Zhang, Taegoo Kang, Donghun Kim, Shehbaz Tariq, Chenshuang Zhang, Choong Seon Hong


[Area is all you need: repeatable elements make stronger adversarial attacks. (98%)](http://arxiv.org/abs/2306.07768)

Dillon Niederhut


[Malafide: a novel adversarial convolutive noise attack against deepfake and spoofing detection systems. (96%)](http://arxiv.org/abs/2306.07655)

Michele Panariello, Wanying Ge, Hemlata Tak, Massimiliano Todisco, Nicholas Evans


[Generative Watermarking Against Unauthorized Subject-Driven Image Synthesis. (78%)](http://arxiv.org/abs/2306.07754)

Yihan Ma, Zhengyu Zhao, Xinlei He, Zheng Li, Michael Backes, Yang Zhang


[Rethinking Adversarial Training with A Simple Baseline. (22%)](http://arxiv.org/abs/2306.07613)

Hong Liu, Shin'ichi Satoh


[Privacy Inference-Empowered Stealthy Backdoor Attack on Federated Learning under Non-IID Scenarios. (22%)](http://arxiv.org/abs/2306.08011)

Haochen Mei, Gaolei Li, Jun Wu, Longfei Zheng


[DHBE: Data-free Holistic Backdoor Erasing in Deep Neural Networks via Restricted Adversarial Distillation. (22%)](http://arxiv.org/abs/2306.08009)

Zhicong Yan, Shenghong Li, Ruijie Zhao, Yuan Tian, Yuanyuan Zhao


[Temporal Gradient Inversion Attacks with Robust Optimization. (8%)](http://arxiv.org/abs/2306.07883)

Bowen Li, Hanlin Gu, Ruoxin Chen, Jie Li, Chentao Wu, Na Ruan, Xueming Si, Lixin Fan


[Few-shot Multi-domain Knowledge Rearming for Context-aware Defence against Advanced Persistent Threats. (2%)](http://arxiv.org/abs/2306.07685)

Gaolei Li, Yuanyuan Zhao, Wenqi Wei, Yuchen Liu


## 2023-06-12

[When Vision Fails: Text Attacks Against ViT and OCR. (99%)](http://arxiv.org/abs/2306.07033)

Nicholas Boucher, Jenny Blessing, Ilia Shumailov, Ross Anderson, Nicolas Papernot


[AROID: Improving Adversarial Robustness through Online Instance-wise Data Augmentation. (99%)](http://arxiv.org/abs/2306.07197)

Lin Li, Jianing Qiu, Michael Spratling


[How robust accuracy suffers from certified training with convex relaxations. (73%)](http://arxiv.org/abs/2306.06995)

Bartolomeis Piersilvio De, Jacob Clarysse, Amartya Sanyal, Fanny Yang


[Graph Agent Network: Empowering Nodes with Decentralized Communications Capabilities for Adversarial Resilience. (54%)](http://arxiv.org/abs/2306.06909)

Ao Liu, Wenshan Li, Tao Li, Beibei Li, Hanyuan Huang, Guangquan Xu, Pan Zhou


[Frequency-Based Vulnerability Analysis of Deep Learning Models against Image Corruptions. (13%)](http://arxiv.org/abs/2306.07178)

Harshitha Machiraju, Michael H. Herzog, Pascal Frossard


[On the Robustness of Removal-Based Feature Attributions. (10%)](http://arxiv.org/abs/2306.07462)

Chris Lin, Ian Covert, Su-In Lee


[VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models. (1%)](http://arxiv.org/abs/2306.06874)

Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho


## 2023-06-11

[Securing Visually-Aware Recommender Systems: An Adversarial Image Reconstruction and Detection Framework. (99%)](http://arxiv.org/abs/2306.07992)

Minglei Yin, Bin Liu, Neil Zhenqiang Gong, Xin Li


[Neural Architecture Design and Robustness: A Dataset. (76%)](http://arxiv.org/abs/2306.06712)

Steffen Jung, Jovita Lukasik, Margret Keuper


[TrojPrompt: A Black-box Trojan Attack on Pre-trained Language Models. (56%)](http://arxiv.org/abs/2306.06815)

Jiaqi Xue, Yepeng Liu, Mengxin Zheng, Ting Hua, Yilin Shen, Ladislau Boloni, Qian Lou


## 2023-06-10

[Boosting Adversarial Robustness using Feature Level Stochastic Smoothing. (92%)](http://arxiv.org/abs/2306.06462)

Sravanti Addepalli, Samyak Jain, Gaurang Sriramanan, R. Venkatesh Babu


[NeRFool: Uncovering the Vulnerability of Generalizable Neural Radiance Fields against Adversarial Perturbations. (83%)](http://arxiv.org/abs/2306.06359)

Yonggan Fu, Ye Yuan, Souvik Kundu, Shang Wu, Shunyao Zhang, Yingyan Lin


[The Defense of Networked Targets in General Lotto games. (13%)](http://arxiv.org/abs/2306.06485)

Adel Aghajan, Keith Paarporn, Jason R. Marden


## 2023-06-09

[Detecting Adversarial Directions in Deep Reinforcement Learning to Make Robust Decisions. (84%)](http://arxiv.org/abs/2306.05873)

Ezgi Korkmaz, Jonah Brown-Cohen


[GAN-CAN: A Novel Attack to Behavior-Based Driver Authentication Systems. (70%)](http://arxiv.org/abs/2306.05923)

Emad Efatinasab, Francesco Marchiori, Denis Donadel, Alessandro Brighente, Mauro Conti


[Overcoming Adversarial Attacks for Human-in-the-Loop Applications. (45%)](http://arxiv.org/abs/2306.05952)

Ryan McCoppin, Marla Kennedy, Platon Lukyanenko, Sean Kennedy


## 2023-06-08

[Adversarial Evasion Attacks Practicality in Networks: Testing the Impact of Dynamic Learning. (99%)](http://arxiv.org/abs/2306.05494)

Mohamed el Shehaby, Ashraf Matrawy


[Boosting Adversarial Transferability by Achieving Flat Local Maxima. (99%)](http://arxiv.org/abs/2306.05225)

Zhijin Ge, Fanhua Shang, Hongying Liu, Yuanyuan Liu, Xiaosen Wang


[COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in Language Models. (93%)](http://arxiv.org/abs/2306.05659)

Zihao Tan, Qingliang Chen, Wenbin Zhu, Yongjian Huang


[Generalizable Lightweight Proxy for Robust NAS against Diverse Perturbations. (83%)](http://arxiv.org/abs/2306.05031)

Hyeonjeong Ha, Minseon Kim, Sung Ju Hwang


[A Melting Pot of Evolution and Learning. (41%)](http://arxiv.org/abs/2306.04971)

Moshe Sipper, Achiya Elyasaf, Tomer Halperin, Zvika Haramaty, Raz Lapid, Eyal Segal, Itai Tzruia, Snir Vitrack Tamam


[PriSampler: Mitigating Property Inference of Diffusion Models. (12%)](http://arxiv.org/abs/2306.05208)

Hailong Hu, Jun Pang


[Robustness Testing for Multi-Agent Reinforcement Learning: State Perturbations on Critical Agents. (10%)](http://arxiv.org/abs/2306.06136)

Ziyuan Zhou, Guanjun Liu


[G$^2$uardFL: Safeguarding Federated Learning Against Backdoor Attacks through Attributed Client Graph Clustering. (10%)](http://arxiv.org/abs/2306.04984)

Hao Yu, Chuan Ma, Meng Liu, Xinwang Liu, Zhe Liu, Ming Ding


[Re-aligning Shadow Models can Improve White-box Membership Inference Attacks. (10%)](http://arxiv.org/abs/2306.05093)

Ana-Maria Cretu, Daniel Jones, Montjoye Yves-Alexandre de, Shruti Tople


[Conservative Prediction via Data-Driven Confidence Minimization. (8%)](http://arxiv.org/abs/2306.04974)

Caroline Choi, Fahim Tajwar, Yoonho Lee, Huaxiu Yao, Ananya Kumar, Chelsea Finn


[Robust Framework for Explanation Evaluation in Time Series Classification. (2%)](http://arxiv.org/abs/2306.05501)

Thu Trang Nguyen, Thach Le Nguyen, Georgiana Ifrim


[FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and LLMs. (1%)](http://arxiv.org/abs/2306.04959)

Shanshan Han, Baturalp Buyukates, Zijian Hu, Han Jin, Weizhao Jin, Lichao Sun, Xiaoyang Wang, Chulin Xie, Kai Zhang, Qifan Zhang, Yuhui Zhang, Chaoyang He, Salman Avestimehr


[Open Set Relation Extraction via Unknown-Aware Training. (1%)](http://arxiv.org/abs/2306.04950)

Jun Zhao, Xin Zhao, Wenyu Zhan, Qi Zhang, Tao Gui, Zhongyu Wei, Yunwen Chen, Xiang Gao, Xuanjing Huang


## 2023-06-07

[Extracting Cloud-based Model with Prior Knowledge. (99%)](http://arxiv.org/abs/2306.04192)

Shiqian Zhao, Kangjie Chen, Meng Hao, Jian Zhang, Guowen Xu, Hongwei Li, Tianwei Zhang


[Expanding Scope: Adapting English Adversarial Attacks to Chinese. (99%)](http://arxiv.org/abs/2306.04874)

Hanyu Liu, Chengyuan Cai, Yanjun Qi


[PromptAttack: Probing Dialogue State Trackers with Adversarial Prompts. (92%)](http://arxiv.org/abs/2306.04535)

Xiangjue Dong, Yun He, Ziwei Zhu, James Caverlee


[Optimal Transport Model Distributional Robustness. (78%)](http://arxiv.org/abs/2306.04178)

Van-Anh Nguyen, Trung Le, Anh Tuan Bui, Thanh-Toan Do, Dinh Phung


[PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts. (76%)](http://arxiv.org/abs/2306.04528)

Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, Xing Xie


[A Linearly Convergent GAN Inversion-based Algorithm for Reverse Engineering of Deceptions. (45%)](http://arxiv.org/abs/2306.04756)

Darshan Thaker, Paris Giampouras, René Vidal


[Faithful Knowledge Distillation. (41%)](http://arxiv.org/abs/2306.04431)

Tom A. Lamb, Rudy Brunel, Krishnamurthy DJ Dvijotham, M. Pawan Kumar, Philip H. S. Torr, Francisco Eiras


[Divide and Repair: Using Options to Improve Performance of Imitation Learning Against Adversarial Demonstrations. (16%)](http://arxiv.org/abs/2306.04581)

Prithviraj Dasgupta


[Can current NLI systems handle German word order? Investigating language model performance on a new German challenge set of minimal pairs. (15%)](http://arxiv.org/abs/2306.04523)

Ines Reinig, Katja Markert


[Adversarial Sample Detection Through Neural Network Transport Dynamics. (10%)](http://arxiv.org/abs/2306.04252)

Skander Karkar, Patrick Gallinari, Alain Rakotomamonjy


## 2023-06-06

[Revisiting the Trade-off between Accuracy and Robustness via Weight Distribution of Filters. (99%)](http://arxiv.org/abs/2306.03430)

Xingxing Wei, Shiji Zhao


[Avoid Adversarial Adaption in Federated Learning by Multi-Metric Investigations. (97%)](http://arxiv.org/abs/2306.03600)

Torsten University of Würzburg Krauß, Alexandra University of Würzburg Dmitrienko


[Transferable Adversarial Robustness for Categorical Data via Universal Robust Embeddings. (93%)](http://arxiv.org/abs/2306.04064)

Klim Kireev, Maksym Andriushchenko, Carmela Troncoso, Nicolas Flammarion


[Exploring Model Dynamics for Accumulative Poisoning Discovery. (62%)](http://arxiv.org/abs/2306.03726)

Jianing Zhu, Xiawei Guo, Jiangchao Yao, Chao Du, Li He, Shuo Yuan, Tongliang Liu, Liang Wang, Bo Han


[Adversarial Attacks and Defenses in Explainable Artificial Intelligence: A Survey. (50%)](http://arxiv.org/abs/2306.06123)

Hubert Baniecki, Przemyslaw Biecek


[Membership inference attack with relative decision boundary distance. (33%)](http://arxiv.org/abs/2306.04109)

JiaCheng Xu, ChengXiang Tan


[Performance-optimized deep neural networks are evolving into worse models of inferotemporal visual cortex. (8%)](http://arxiv.org/abs/2306.03779)

Drew Linsley, Ivan F. Rodriguez, Thomas Fel, Michael Arcaro, Saloni Sharma, Margaret Livingstone, Thomas Serre


[Adversarial Attacks and Defenses for Semantic Communication in Vehicular Metaverses. (1%)](http://arxiv.org/abs/2306.03528)

Jiawen Kang, Jiayi He, Hongyang Du, Zehui Xiong, Zhaohui Yang, Xumin Huang, Shengli Xie


## 2023-06-05

[Evading Black-box Classifiers Without Breaking Eggs. (99%)](http://arxiv.org/abs/2306.02895)

Edoardo Debenedetti, Nicholas Carlini, Florian Tramèr


[Adversarial alignment: Breaking the trade-off between the strength of an attack and its relevance to human perception. (99%)](http://arxiv.org/abs/2306.03229)

Drew Linsley, Pinyuan Feng, Thibaut Boissin, Alekh Karkada Ashok, Thomas Fel, Stephanie Olaiya, Thomas Serre


[Evaluating robustness of support vector machines with the Lagrangian dual approach. (97%)](http://arxiv.org/abs/2306.02639)

Yuting Liu, Hong Gu, Pan Qin


[A Robust Likelihood Model for Novelty Detection. (93%)](http://arxiv.org/abs/2306.03331)

Ranya Almohsen, Shivang Patel, Donald A. Adjeroh, Gianfranco Doretto


[Adversarial Ink: Componentwise Backward Error Attacks on Deep Learning. (86%)](http://arxiv.org/abs/2306.02918)

Lucas Beerens, Desmond J. Higham


[Enhance Diffusion to Improve Robust Generalization. (76%)](http://arxiv.org/abs/2306.02618)

Jianhui Sun, Sanchit Sinha, Aidong Zhang


[KNOW How to Make Up Your Mind! Adversarially Detecting and Alleviating Inconsistencies in Natural Language Explanations. (68%)](http://arxiv.org/abs/2306.02980)

Myeongjun Jang, Bodhisattwa Prasad Majumder, Julian McAuley, Thomas Lukasiewicz, Oana-Maria Camburu


[Stable Diffusion is Unstable. (45%)](http://arxiv.org/abs/2306.02583)

Chengbin Du, Yanxi Li, Zhongwei Qiu, Chang Xu


[Security Knowledge-Guided Fuzzing of Deep Learning Libraries. (1%)](http://arxiv.org/abs/2306.03269)

Nima Shiri Harzevili, Hung Viet Pham, Song Wang


## 2023-06-04

[Adversary for Social Good: Leveraging Adversarial Attacks to Protect Personal Attribute Privacy. (98%)](http://arxiv.org/abs/2306.02488)

Xiaoting Li, Lingwei Chen, Dinghao Wu


[Aerial Swarm Defense using Interception and Herding Strategies. (1%)](http://arxiv.org/abs/2306.02482)

Vishnu S. Chipade, Dimitra Panagou


## 2023-06-03

[Towards Black-box Adversarial Example Detection: A Data Reconstruction-based Method. (99%)](http://arxiv.org/abs/2306.02021)

Yifei Gao, Zhiyu Lin, Yunfan Yang, Jitao Sang


[Learning to Defend by Attacking (and Vice-Versa): Transfer of Learning in Cybersecurity Games. (67%)](http://arxiv.org/abs/2306.02165)

Tyler Malloy, Cleotilde Gonzalez


[Can Directed Graph Neural Networks be Adversarially Robust? (56%)](http://arxiv.org/abs/2306.02002)

Zhichao Hou, Xitong Zhang, Wei Wang, Charu C. Aggarwal, Xiaorui Liu


[Flew Over Learning Trap: Learn Unlearnable Samples by Progressive Staged Training. (13%)](http://arxiv.org/abs/2306.02064)

Pucheng Dang, Xing Hu, Kaidi Xu, Jinhao Duan, Di Huang, Husheng Han, Rui Zhang, Zidong Du, Qi Guo, Yunji Chen


[Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models. (1%)](http://arxiv.org/abs/2306.02080)

Shuo Chen, Jindong Gu, Zhen Han, Yunpu Ma, Philip Torr, Volker Tresp


## 2023-06-02

[Why Clean Generalization and Robust Overfitting Both Happen in Adversarial Training. (99%)](http://arxiv.org/abs/2306.01271)

Binghui Li, Yuanzhi Li


[A Closer Look at the Adversarial Robustness of Deep Equilibrium Models. (92%)](http://arxiv.org/abs/2306.01429)

Zonghan Yang, Tianyu Pang, Yang Liu


[Adaptive Attractors: A Defense Strategy against ML Adversarial Collusion Attacks. (83%)](http://arxiv.org/abs/2306.01400)

Jiyi Zhang, Han Fang, Ee-Chien Chang


[Poisoning Network Flow Classifiers. (61%)](http://arxiv.org/abs/2306.01655)

Giorgio Severi, Simona Boboila, Alina Oprea, John Holodnak, Kendra Kratkiewicz, Jason Matterer


[Hyperparameter Learning under Data Poisoning: Analysis of the Influence of Regularization via Multiobjective Bilevel Optimization. (54%)](http://arxiv.org/abs/2306.01613)

Javier Carnerero-Cano, Luis Muñoz-González, Phillippa Spencer, Emil C. Lupu


[Robust low-rank training via approximate orthonormal constraints. (22%)](http://arxiv.org/abs/2306.01485)

Dayana Savostianova, Emanuele Zangrando, Gianluca Ceruti, Francesco Tudisco


[Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations. (13%)](http://arxiv.org/abs/2306.01505)

Dou Hu, Yinan Bao, Lingwei Wei, Wei Zhou, Songlin Hu


[Improving Adversarial Robustness of DEQs with Explicit Regulations Along the Neural Dynamics. (11%)](http://arxiv.org/abs/2306.01435)

Zonghan Yang, Peng Li, Tianyu Pang, Yang Liu


[Covert Communication Based on the Poisoning Attack in Federated Learning. (10%)](http://arxiv.org/abs/2306.01342)

Junchuan Liang, Rong Wang


[Invisible Image Watermarks Are Provably Removable Using Generative AI. (10%)](http://arxiv.org/abs/2306.01953)

Xuandong Zhao, Kexun Zhang, Zihao Su, Saastha Vasan, Ilya Grishchenko, Christopher Kruegel, Giovanni Vigna, Yu-Xiang Wang, Lei Li


[VoteTRANS: Detecting Adversarial Text without Training by Voting on Hard Labels of Transformations. (3%)](http://arxiv.org/abs/2306.01273)

Hoang-Quoc Nguyen-Son, Seira Hidano, Kazuhide Fukushima, Shinsaku Kiyomoto, Isao Echizen


[Unlearnable Examples for Diffusion Models: Protect Data from Unauthorized Exploitation. (2%)](http://arxiv.org/abs/2306.01902)

Zhengyue Zhao, Jinhao Duan, Xing Hu, Kaidi Xu, Chenan Wang, Rui Zhang, Zidong Du, Qi Guo, Yunji Chen


[Towards Robust GAN-generated Image Detection: a Multi-view Completion Representation. (1%)](http://arxiv.org/abs/2306.01364)

Chi Liu, Tianqing Zhu, Sheng Shen, Wanlei Zhou


[Improving the generalizability and robustness of large-scale traffic signal control. (1%)](http://arxiv.org/abs/2306.01925)

Tianyu Shi, Francois-Xavier Devailly, Denis Larocque, Laurent Charlin


## 2023-06-01

[Adversarial Attack Based on Prediction-Correction. (99%)](http://arxiv.org/abs/2306.01809)

Chen Wan, Fangjun Huang


[Reconstruction Distortion of Learned Image Compression with Imperceptible Perturbations. (96%)](http://arxiv.org/abs/2306.01125)

Yang Sui, Zhuohang Li, Ding Ding, Xiang Pan, Xiaozhong Xu, Shan Liu, Zhenzhong Chen


[Intriguing Properties of Text-guided Diffusion Models. (92%)](http://arxiv.org/abs/2306.00974)

Qihao Liu, Adam Kortylewski, Yutong Bai, Song Bai, Alan Yuille


[Constructing Semantics-Aware Adversarial Examples with Probabilistic Perspective. (87%)](http://arxiv.org/abs/2306.00353)

Andi Zhang, Damon Wischik


[Robust Backdoor Attack with Visible, Semantic, Sample-Specific, and Compatible Triggers. (82%)](http://arxiv.org/abs/2306.00816)

Ruotong Wang, Hongrui Chen, Zihao Zhu, Li Liu, Yong Zhang, Yanbo Fan, Baoyuan Wu


[Improving the Robustness of Summarization Systems with Dual Augmentation. (76%)](http://arxiv.org/abs/2306.01090)

Xiuying Chen, Guodong Long, Chongyang Tao, Mingzhe Li, Xin Gao, Chengqi Zhang, Xiangliang Zhang


[Adversarial Robustness in Unsupervised Machine Learning: A Systematic Review. (38%)](http://arxiv.org/abs/2306.00687)

Mathias Lundteigen Mohus, Jinyue Li


[Does Black-box Attribute Inference Attacks on Graph Neural Networks Constitute Privacy Risk? (13%)](http://arxiv.org/abs/2306.00578)

Iyiola E. Olatunji, Anmar Hizber, Oliver Sihlovec, Megha Khosla


[CALICO: Self-Supervised Camera-LiDAR Contrastive Pre-training for BEV Perception. (13%)](http://arxiv.org/abs/2306.00349)

Jiachen Sun, Haizhong Zheng, Qingzhao Zhang, Atul Prakash, Z. Morley Mao, Chaowei Xiao


[ModelObfuscator: Obfuscating Model Information to Protect Deployed ML-based Systems. (4%)](http://arxiv.org/abs/2306.06112)

Mingyi Zhou, Xiang Gao, Jing Wu, John Grundy, Xiao Chen, Chunyang Chen, Li Li


## 2023-05-31

[Exploring the Vulnerabilities of Machine Learning and Quantum Machine Learning to Adversarial Attacks using a Malware Dataset: A Comparative Analysis. (98%)](http://arxiv.org/abs/2305.19593)

Mst Shapna Akter, Hossain Shahriar, Iysa Iqbal, MD Hossain, M. A. Karim, Victor Clincy, Razvan Voicu


[Graph-based methods coupled with specific distributional distances for adversarial attack detection. (98%)](http://arxiv.org/abs/2306.00042)

Dwight Nwaigwe, Lucrezia Carboni, Martial Mermillod, Sophie Achard, Michel Dojat


[Adversarial-Aware Deep Learning System based on a Secondary Classical Machine Learning Verification Approach. (98%)](http://arxiv.org/abs/2306.00314)

Mohammed Alkhowaiter, Hisham Kholidy, Mnassar Alyami, Abdulmajeed Alghamdi, Cliff Zou


[Adversarial Clean Label Backdoor Attacks and Defenses on Text Classification Systems. (54%)](http://arxiv.org/abs/2305.19607)

Ashim Gupta, Amrith Krishna


[Deception by Omission: Using Adversarial Missingness to Poison Causal Structure Learning. (26%)](http://arxiv.org/abs/2305.20043)

Deniz Koyuncu, Alex Gittens, Bülent Yener, Moti Yung


[Red Teaming Language Model Detectors with Language Models. (15%)](http://arxiv.org/abs/2305.19713)

Zhouxing Shi, Yihan Wang, Fan Yin, Xiangning Chen, Kai-Wei Chang, Cho-Jui Hsieh


[Ambiguity in solving imaging inverse problems with deep learning based operators. (1%)](http://arxiv.org/abs/2305.19774)

Davide Evangelista, Elena Morotti, Elena Loli Piccolomini, James Nagy


## 2023-05-30

[Pseudo-Siamese Network based Timbre-reserved Black-box Adversarial Attack in Speaker Identification. (99%)](http://arxiv.org/abs/2305.19020)

Qing Wang, Jixun Yao, Ziqian Wang, Pengcheng Guo, Lei Xie


[Breeding Machine Translations: Evolutionary approach to survive and thrive in the world of automated evaluation. (64%)](http://arxiv.org/abs/2305.19330)

Josef Jon, Ondřej Bojar


[Incremental Randomized Smoothing Certification. (33%)](http://arxiv.org/abs/2305.19521)

Shubham Ugare, Tarun Suresh, Debangshu Banerjee, Gagandeep Singh, Sasa Misailovic


[Defense Against Shortest Path Attacks. (16%)](http://arxiv.org/abs/2305.19083)

Benjamin A. Miller, Zohair Shafi, Wheeler Ruml, Yevgeniy Vorobeychik, Tina Eliassi-Rad, Scott Alfeld


[A Multilingual Evaluation of NER Robustness to Adversarial Inputs. (15%)](http://arxiv.org/abs/2305.18933)

Akshay Srinivasan, Sowmya Vajjala


[Which Models have Perceptually-Aligned Gradients? An Explanation via Off-Manifold Robustness. (10%)](http://arxiv.org/abs/2305.19101)

Suraj Srinivas, Sebastian Bordt, Hima Lakkaraju


[It begins with a boundary: A geometric view on probabilistically robust learning. (8%)](http://arxiv.org/abs/2305.18779)

Leon Bungert, Nicolás García Trillos, Matt Jacobs, Daniel McKenzie, Đorđe Nikolić, Qingsong Wang


[Adversarial Attacks on Online Learning to Rank with Stochastic Click Models. (2%)](http://arxiv.org/abs/2305.19218)

Zichen Wang, Rishab Balasubramanian, Hui Yuan, Chenyu Song, Mengdi Wang, Huazheng Wang


[Learning Perturbations to Explain Time Series Predictions. (1%)](http://arxiv.org/abs/2305.18840)

Joseph Enguehard


## 2023-05-29

[From Adversarial Arms Race to Model-centric Evaluation: Motivating a Unified Automatic Robustness Evaluation Framework. (99%)](http://arxiv.org/abs/2305.18503)

Yangyi Chen, Hongcheng Gao, Ganqu Cui, Lifan Yuan, Dehan Kong, Hanlu Wu, Ning Shi, Bo Yuan, Longtao Huang, Hui Xue, Zhiyuan Liu, Maosong Sun, Heng Ji


[Fourier Analysis on Robustness of Graph Convolutional Neural Networks for Skeleton-based Action Recognition. (92%)](http://arxiv.org/abs/2305.17939)

Nariki Tanaka, Hiroshi Kera, Kazuhiko Kawamoto


[Exploiting Explainability to Design Adversarial Attacks and Evaluate Attack Resilience in Hate-Speech Detection Models. (92%)](http://arxiv.org/abs/2305.18585)

Pranath Reddy Kumbam, Sohaib Uddin Syed, Prashanth Thamminedi, Suhas Harish, Ian Perera, Bonnie J. Dorr


[UMD: Unsupervised Model Detection for X2X Backdoor Attacks. (81%)](http://arxiv.org/abs/2305.18651)

Zhen Xiang, Zidi Xiong, Bo Li


[Membership Inference Attacks against Language Models via Neighbourhood Comparison. (73%)](http://arxiv.org/abs/2305.18462)

Justus Mattern, Fatemehsadat Mireshghallah, Zhijing Jin, Bernhard Schölkopf, Mrinmaya Sachan, Taylor Berg-Kirkpatrick


[Trustworthy Sensor Fusion against Inaudible Command Attacks in Advanced Driver-Assistance System. (41%)](http://arxiv.org/abs/2306.05358)

Jiwei Guan, Lei Pan, Chen Wang, Shui Yu, Longxiang Gao, Xi Zheng


[Explainability in Simplicial Map Neural Networks. (38%)](http://arxiv.org/abs/2306.00010)

Eduardo Paluzo-Hidalgo, Miguel A. Gutiérrez-Naranjo, Rocio Gonzalez-Diaz


[Robust Lipschitz Bandits to Adversarial Corruptions. (11%)](http://arxiv.org/abs/2305.18543)

Yue Kang, Cho-Jui Hsieh, Thomas C. M. Lee


[Towards minimizing efforts for Morphing Attacks -- Deep embeddings for morphing pair selection and improved Morphing Attack Detection. (8%)](http://arxiv.org/abs/2305.18216)

Roman Kessler, Kiran Raja, Juan Tapia, Christoph Busch


## 2023-05-28

[Amplification trojan network: Attack deep neural networks by amplifying their inherent weakness. (99%)](http://arxiv.org/abs/2305.17688)

Zhanhao Hu, Jun Zhu, Bo Zhang, Xiaolin Hu


[NaturalFinger: Generating Natural Fingerprint with Generative Adversarial Networks. (92%)](http://arxiv.org/abs/2305.17868)

Kang Yang, Kunhao Lai


[Backdoor Attacks Against Incremental Learners: An Empirical Evaluation Study. (41%)](http://arxiv.org/abs/2305.18384)

Yiqi Zhong, Xianming Liu, Deming Zhai, Junjun Jiang, Xiangyang Ji


[NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models. (38%)](http://arxiv.org/abs/2305.17826)

Kai Mei, Zheng Li, Zhenting Wang, Yang Zhang, Shiqing Ma


[Choose your Data Wisely: A Framework for Semantic Counterfactuals. (13%)](http://arxiv.org/abs/2305.17667)

Edmund Dervakos, Konstantinos Thomas, Giorgos Filandrianos, Giorgos Stamou


[BadLabel: A Robust Perspective on Evaluating and Enhancing Label-noise Learning. (5%)](http://arxiv.org/abs/2305.18377)

Jingfeng Zhang, Bo Song, Haohan Wang, Bo Han, Tongliang Liu, Lei Liu, Masashi Sugiyama


[Black-Box Anomaly Attribution. (1%)](http://arxiv.org/abs/2305.18440)

Tsuyoshi Idé, Naoki Abe


## 2023-05-27

[Adversarial Attack On Yolov5 For Traffic And Road Sign Detection. (99%)](http://arxiv.org/abs/2306.06071)

Sanyam Jain


[Pre-trained transformer for adversarial purification. (99%)](http://arxiv.org/abs/2306.01762)

Kai Wu, Yujian Betterest Li, Xiaoyu Zhang, Handing Wang, Jing Liu


[Two Heads are Better than One: Towards Better Adversarial Robustness by Combining Transduction and Rejection. (98%)](http://arxiv.org/abs/2305.17528)

Nils Palumbo, Yang Guo, Xi Wu, Jiefeng Chen, Yingyu Liang, Somesh Jha


[Modeling Adversarial Attack on Pre-trained Language Models as Sequential Decision Making. (92%)](http://arxiv.org/abs/2305.17440)

Xuanjie Fang, Sijie Cheng, Yang Liu, Wei Wang


[On the Importance of Backbone to the Adversarial Robustness of Object Detectors. (83%)](http://arxiv.org/abs/2305.17438)

Xiao Li, Hang Chen, Xiaolin Hu


[No-Regret Online Reinforcement Learning with Adversarial Losses and Transitions. (2%)](http://arxiv.org/abs/2305.17380)

Tiancheng Jin, Junyan Liu, Chloé Rouyer, William Chang, Chen-Yu Wei, Haipeng Luo


## 2023-05-26

[On Evaluating Adversarial Robustness of Large Vision-Language Models. (99%)](http://arxiv.org/abs/2305.16934)

Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man Cheung, Min Lin


[Leveraging characteristics of the output probability distribution for identifying adversarial audio examples. (98%)](http://arxiv.org/abs/2305.17000)

Matías P. Pizarro B., Dorothea Kolossa, Asja Fischer


[Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in Multi-Agent RL. (96%)](http://arxiv.org/abs/2305.17342)

Xiangyu Liu, Souradip Chakraborty, Yanchao Sun, Furong Huang


[A Tale of Two Approximations: Tightening Over-Approximation for DNN Robustness Verification via Under-Approximation. (45%)](http://arxiv.org/abs/2305.16998)

Zhiyi Xue, Si Liu, Zhaodi Zhang, Yiting Wu, Min Zhang


[Adversarial Attacks on Online Learning to Rank with Click Feedback. (38%)](http://arxiv.org/abs/2305.17071)

Jinhang Zuo, Zhiyao Zhang, Zhiyong Wang, Shuai Li, Mohammad Hajiesmaili, Adam Wierman


[DeepSeaNet: Improving Underwater Object Detection using EfficientDet. (2%)](http://arxiv.org/abs/2306.06075)

Sanyam Jain


[Trust-Aware Resilient Control and Coordination of Connected and Automated Vehicles. (1%)](http://arxiv.org/abs/2305.16818)

H M Sabbir Ahmad, Ehsan Sabouni, Wei Xiao, Christos G. Cassandras, Wenchao Li


[Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model. (1%)](http://arxiv.org/abs/2305.16617)

Zhijie Deng, Hongcheng Gao, Yibo Miao, Hao Zhang


## 2023-05-25

[IDEA: Invariant Causal Defense for Graph Adversarial Robustness. (99%)](http://arxiv.org/abs/2305.15792)

Shuchang Tao, Qi Cao, Huawei Shen, Yunfan Wu, Bingbing Xu, Xueqi Cheng


[Don't Retrain, Just Rewrite: Countering Adversarial Perturbations by Rewriting Text. (98%)](http://arxiv.org/abs/2305.16444)

Ashim Gupta, Carter Wood Blum, Temma Choji, Yingjie Fei, Shalin Shah, Alakananda Vempala, Vivek Srikumar


[Diffusion-Based Adversarial Sample Generation for Improved Stealthiness and Controllability. (98%)](http://arxiv.org/abs/2305.16494)

Haotian Xue, Alexandre Araujo, Bin Hu, Yongxin Chen


[PEARL: Preprocessing Enhanced Adversarial Robust Learning of Image Deraining for Semantic Segmentation. (96%)](http://arxiv.org/abs/2305.15709)

Xianghao Jiao, Yaohua Liu, Jiaxin Gao, Xinyuan Chu, Risheng Liu, Xin Fan


[Adversarial Attacks on Leakage Detectors in Water Distribution Networks. (86%)](http://arxiv.org/abs/2306.06107)

Paul Stahlhofen, André Artelt, Luca Hermes, Barbara Hammer


[CARSO: Counter-Adversarial Recall of Synthetic Observations. (78%)](http://arxiv.org/abs/2306.06081)

Emanuele Ballarin, Alessio Ansuini, Luca Bortolussi


[On the Robustness of Segment Anything. (73%)](http://arxiv.org/abs/2305.16220)

Yihao Huang, Yue Cao, Tianlin Li, Felix Juefei-Xu, Di Lin, Ivor W. Tsang, Yang Liu, Qing Guo


[Detecting Adversarial Data by Probing Multiple Perturbations Using Expected Perturbation Score. (67%)](http://arxiv.org/abs/2305.16035)

Shuhai Zhang, Feng Liu, Jiahao Yang, Yifan Yang, Changsheng Li, Bo Han, Mingkui Tan


[Rethink Diversity in Deep Learning Testing. (50%)](http://arxiv.org/abs/2305.15698)

Zi Wang, Jihye Choi, Somesh Jha


[IMBERT: Making BERT Immune to Insertion-based Backdoor Attacks. (13%)](http://arxiv.org/abs/2305.16503)

Xuanli He, Jun Wang, Benjamin Rubinstein, Trevor Cohn


[Securing Deep Generative Models with Universal Adversarial Signature. (2%)](http://arxiv.org/abs/2305.16310)

Yu Zeng, Mo Zhou, Yuan Xue, Vishal M. Patel


## 2023-05-24

[How do humans perceive adversarial text? A reality check on the validity and naturalness of word-based adversarial attacks. (99%)](http://arxiv.org/abs/2305.15587)

Salijona Dyrmishi, Salah Ghamizi, Maxime Cordy


[Robust Classification via a Single Diffusion Model. (99%)](http://arxiv.org/abs/2305.15241)

Huanran Chen, Yinpeng Dong, Zhengyi Wang, Xiao Yang, Chengqi Duan, Hang Su, Jun Zhu


[Introducing Competition to Boost the Transferability of Targeted Adversarial Examples through Clean Feature Mixup. (99%)](http://arxiv.org/abs/2305.14846)

Junyoung Byun, Myung-Joon Kwon, Seungju Cho, Yoonji Kim, Changick Kim


[Fantastic DNN Classifiers and How to Identify them without Data. (91%)](http://arxiv.org/abs/2305.15563)

Nathaniel Dean, Dilip Sarkar


[Relating Implicit Bias and Adversarial Attacks through Intrinsic Dimension. (86%)](http://arxiv.org/abs/2305.15203)

Lorenzo Basile, Nikos Karantzas, Alberto D'Onofrio, Luca Bortolussi, Alex Rodriguez, Fabio Anselmi


[Adversarial Demonstration Attacks on Large Language Models. (82%)](http://arxiv.org/abs/2305.14950)

Jiongxiao Wang, Zichen Liu, Keun Hee Park, Muhao Chen, Chaowei Xiao


[AdvFunMatch: When Consistent Teaching Meets Adversarial Robustness. (76%)](http://arxiv.org/abs/2305.14700)

Ziuhi Wu, Haichang Gao, Bingqian Zhou, Ping Wang


[Reconstructive Neuron Pruning for Backdoor Defense. (75%)](http://arxiv.org/abs/2305.14876)

Yige Li, Xixiang Lyu, Xingjun Ma, Nodens Koren, Lingjuan Lyu, Bo Li, Yu-Gang Jiang


[Another Dead End for Morphological Tags? Perturbed Inputs and Parsing. (74%)](http://arxiv.org/abs/2305.15119)

Alberto Muñoz-Ortiz, David Vilares


[From Shortcuts to Triggers: Backdoor Defense with Denoised PoE. (47%)](http://arxiv.org/abs/2305.14910)

Qin Liu, Fei Wang, Chaowei Xiao, Muhao Chen


[Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models. (31%)](http://arxiv.org/abs/2305.14710)

Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, Muhao Chen


[Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models. (22%)](http://arxiv.org/abs/2305.14763)

Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, Vered Shwartz


[Adversarial robustness of amortized Bayesian inference. (11%)](http://arxiv.org/abs/2305.14984)

Manuel Glöckler, Michael Deistler, Jakob H. Macke


[Sharpness-Aware Data Poisoning Attack. (10%)](http://arxiv.org/abs/2305.14851)

Pengfei He, Han Xu, Jie Ren, Yingqian Cui, Hui Liu, Charu C. Aggarwal, Jiliang Tang


[Improving selective classification performance of deep neural networks through post-hoc logit normalization and temperature scaling. (1%)](http://arxiv.org/abs/2305.15508)

Luís Felipe P. Cattelan, Danilo Silva


## 2023-05-23

[The Best Defense is a Good Offense: Adversarial Augmentation against Adversarial Attacks. (99%)](http://arxiv.org/abs/2305.14188)

Iuri Frosio, Jan Kautz


[Enhancing Accuracy and Robustness through Adversarial Training in Class Incremental Continual Learning. (99%)](http://arxiv.org/abs/2305.13678)

Minchan Kwon, Kangil Kim


[QFA2SR: Query-Free Adversarial Transfer Attacks to Speaker Recognition Systems. (98%)](http://arxiv.org/abs/2305.14097)

Guangke Chen, Yedi Zhang, Zhe Zhao, Fu Song


[Expressive Losses for Verified Robustness via Convex Combinations. (95%)](http://arxiv.org/abs/2305.13991)

Palma Alessandro De, Rudy Bunel, Krishnamurthy Dvijotham, M. Pawan Kumar, Robert Stanforth, Alessio Lomuscio


[Impact of Light and Shadow on Robustness of Deep Neural Networks. (87%)](http://arxiv.org/abs/2305.14165)

Chengyin Hu, Weiwen Shi, Chao Li, Jialiang Sun, Donghua Wang, Junqi Wu, Guijian Tang


## 2023-05-22

[Uncertainty-based Detection of Adversarial Attacks in Semantic Segmentation. (99%)](http://arxiv.org/abs/2305.12825)

Kira Maag, Asja Fischer


[Latent Magic: An Investigation into Adversarial Examples Crafted in the Semantic Latent Space. (99%)](http://arxiv.org/abs/2305.12906)

BoYang Zheng


[FGAM:Fast Adversarial Malware Generation Method Based on Gradient Sign. (98%)](http://arxiv.org/abs/2305.12770)

Kun Li, Fan Zhang, Wei Guo


[Attribute-Guided Encryption with Facial Texture Masking. (98%)](http://arxiv.org/abs/2305.13548)

Chun Pong Lau, Jiang Liu, Rama Chellappa


[DiffProtect: Generate Adversarial Examples with Diffusion Models for Facial Privacy Protection. (98%)](http://arxiv.org/abs/2305.13625)

Jiang Liu, Chun Pong Lau, Rama Chellappa


[Byzantine Robust Cooperative Multi-Agent Reinforcement Learning as a Bayesian Game. (93%)](http://arxiv.org/abs/2305.12872)

Simin Li, Jun Guo, Jingqiao Xiu, Xini Yu, Jiakai Wang, Aishan Liu, Yaodong Yang, Xianglong Liu


[Towards Benchmarking and Assessing Visual Naturalness of Physical World Adversarial Attacks. (88%)](http://arxiv.org/abs/2305.12863)

Simin Li, Shuing Zhang, Gujun Chen, Dong Wang, Pu Feng, Jiakai Wang, Aishan Liu, Xin Yi, Xianglong Liu


[Flying Adversarial Patches: Manipulating the Behavior of Deep Learning-based Autonomous Multirotors. (54%)](http://arxiv.org/abs/2305.12859)

Pia Hanfeld, Marina M. -C. Höhne, Michael Bussmann, Wolfgang Hönig


[DeepBern-Nets: Taming the Complexity of Certifying Neural Networks using Bernstein Polynomial Activations and Precise Bound Propagation. (50%)](http://arxiv.org/abs/2305.13508)

Haitham Khedr, Yasser Shoukry


[The defender's perspective on automatic speaker verification: An overview. (22%)](http://arxiv.org/abs/2305.12804)

Haibin Wu, Jiawen Kang, Lingwei Meng, Helen Meng, Hung-yi Lee


[Model Stealing Attack against Multi-Exit Networks. (10%)](http://arxiv.org/abs/2305.13584)

Li Pan, Lv Peizhuo, Chen Kai, Cai Yuling, Xiang Fan, Zhang Shengzhi


[Adversarial Defenses via Vector Quantization. (8%)](http://arxiv.org/abs/2305.13651)

Zhiyi Dong, Yongyi Mao


[Adversarial Nibbler: A Data-Centric Challenge for Improving the Safety of Text-to-Image Models. (2%)](http://arxiv.org/abs/2305.14384)

Alicia Parrish, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Max Bartolo, Oana Inel, Juan Ciro, Rafael Mosquera, Addison Howard, Will Cukierski, D. Sculley, Vijay Janapa Reddi, Lora Aroyo


[Improving Classifier Robustness through Active Generation of Pairwise Counterfactuals. (1%)](http://arxiv.org/abs/2305.13535)

Ananth Balashankar, Xuezhi Wang, Yao Qin, Ben Packer, Nithum Thain, Jilin Chen, Ed H. Chi, Alex Beutel


[Tied-Augment: Controlling Representation Similarity Improves Data Augmentation. (1%)](http://arxiv.org/abs/2305.13520)

Emirhan Kurtulus, Zichao Li, Yann Dauphin, Ekin Dogus Cubuk


[Adaptive Face Recognition Using Adversarial Information Network. (1%)](http://arxiv.org/abs/2305.13605)

Mei Wang, Weihong Deng


## 2023-05-21

[Mist: Towards Improved Adversarial Examples for Diffusion Models. (99%)](http://arxiv.org/abs/2305.12683)

Chumeng Liang, Xiaoyu Wu


[FAQ: Mitigating the Impact of Faults in the Weight Memory of DNN Accelerators through Fault-Aware Quantization. (1%)](http://arxiv.org/abs/2305.12590)

Muhammad Abdullah Hanif, Muhammad Shafique


## 2023-05-20

[Dynamic Transformers Provide a False Sense of Efficiency. (92%)](http://arxiv.org/abs/2305.12228)

Yiming Chen, Simin Chen, Zexin Li, Wei Yang, Cong Liu, Robby T. Tan, Haizhou Li


[Annealing Self-Distillation Rectification Improves Adversarial Training. (76%)](http://arxiv.org/abs/2305.12118)

Yu-Yu Wu, Hung-Jui Wang, Shang-Tse Chen


[Stability, Generalization and Privacy: Precise Analysis for Random and NTK Features. (8%)](http://arxiv.org/abs/2305.12100)

Simone Bombari, Marco Mondelli


## 2023-05-19

[Dynamic Gradient Balancing for Enhanced Adversarial Attacks on Multi-Task Models. (98%)](http://arxiv.org/abs/2305.12066)

Lijun Zhang, Xiao Liu, Kaleel Mahmood, Caiwen Ding, Hui Guan


[DAP: A Dynamic Adversarial Patch for Evading Person Detectors. (92%)](http://arxiv.org/abs/2305.11618)

Amira Guesmi, Ruitian Ding, Muhammad Abdullah Hanif, Ihsen Alouani, Muhammad Shafique


[Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation. (8%)](http://arxiv.org/abs/2305.11596)

Xuanli He, Qiongkai Xu, Jun Wang, Benjamin Rubinstein, Trevor Cohn


[Long-tailed Visual Recognition via Gaussian Clouded Logit Adjustment. (5%)](http://arxiv.org/abs/2305.11733)

Mengke Li, Yiu-ming Cheung, Yang Lu


[SneakyPrompt: Evaluating Robustness of Text-to-image Generative Models' Safety Filters. (4%)](http://arxiv.org/abs/2305.12082)

Yuchen Yang, Bo Hui, Haolin Yuan, Neil Gong, Yinzhi Cao


[Latent Imitator: Generating Natural Individual Discriminatory Instances for Black-Box Fairness Testing. (2%)](http://arxiv.org/abs/2305.11602)

Yisong Xiao, Aishan Liu, Tianlin Li, Xianglong Liu


[Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning. (1%)](http://arxiv.org/abs/2305.11759)

Mustafa Safa Ozdayi, Charith Peris, Jack FitzGerald, Christophe Dupuy, Jimit Majmudar, Haidar Khan, Rahil Parikh, Rahul Gupta


## 2023-05-18

[Deep PackGen: A Deep Reinforcement Learning Framework for Adversarial Network Packet Generation. (99%)](http://arxiv.org/abs/2305.11039)

Soumyadeep Hore, Jalal Ghadermazi, Diwas Paudel, Ankit Shah, Tapas K. Das, Nathaniel D. Bastian


[Adversarial Amendment is the Only Force Capable of Transforming an Enemy into a Friend. (99%)](http://arxiv.org/abs/2305.10766)

Chong Yu, Tao Chen, Zhongxue Gan


[Architecture-agnostic Iterative Black-box Certified Defense against Adversarial Patches. (99%)](http://arxiv.org/abs/2305.10929)

Di Yang, Yihao Huang, Qing Guo, Felix Juefei-Xu, Ming Hu, Yang Liu, Geguang Pu


[Towards an Accurate and Secure Detector against Adversarial Perturbations. (99%)](http://arxiv.org/abs/2305.10856)

Chao Wang, Shuren Qi, Zhiqiu Huang, Yushu Zhang, Xiaochun Cao


[Quantifying the robustness of deep multispectral segmentation models against natural perturbations and data poisoning. (99%)](http://arxiv.org/abs/2305.11347)

Elise Bishoff, Charles Godfrey, Myles McKay, Eleanor Byler


[How Deep Learning Sees the World: A Survey on Adversarial Attacks & Defenses. (98%)](http://arxiv.org/abs/2305.10862)

Joana C. Costa, Tiago Roxo, Hugo Proença, Pedro R. M. Inácio


[RobustFair: Adversarial Evaluation through Fairness Confusion Directed Gradient Search. (93%)](http://arxiv.org/abs/2305.10906)

Xuran Li, Peng Wu, Kaixiang Dong, Zhen Zhang


[Attacks on Online Learners: a Teacher-Student Analysis. (54%)](http://arxiv.org/abs/2305.11132)

Riccardo Giuseppe Margiotta, Sebastian Goldt, Guido Sanguinetti


[Explaining V1 Properties with a Biologically Constrained Deep Learning Architecture. (47%)](http://arxiv.org/abs/2305.11275)

Galen Pogoncheff, Jacob Granley, Michael Beyeler


[Zero-Day Backdoor Attack against Text-to-Image Diffusion Models via Personalization. (2%)](http://arxiv.org/abs/2305.10701)

Yihao Huang, Qing Guo, Felix Juefei-Xu


[Large Language Models can be Guided to Evade AI-Generated Text Detection. (1%)](http://arxiv.org/abs/2305.10847)

Ning Lu, Shengcai Liu, Rui He, Ke Tang


[Re-thinking Data Availablity Attacks Against Deep Neural Networks. (1%)](http://arxiv.org/abs/2305.10691)

Bin Fang, Bo Li, Shuang Wu, Ran Yi, Shouhong Ding, Lizhuang Ma


[TrustSER: On the Trustworthiness of Fine-tuning Pre-trained Speech Embeddings For Speech Emotion Recognition. (1%)](http://arxiv.org/abs/2305.11229)

Tiantian Feng, Rajat Hebbar, Shrikanth Narayanan


## 2023-05-17

[Content-based Unrestricted Adversarial Attack. (99%)](http://arxiv.org/abs/2305.10665)

Zhaoyu Chen, Bo Li, Shuang Wu, Kaixun Jiang, Shouhong Ding, Wenqiang Zhang


[Raising the Bar for Certified Adversarial Robustness with Diffusion Models. (95%)](http://arxiv.org/abs/2305.10388)

Thomas Altstidl, David Dobre, Björn Eskofier, Gauthier Gidel, Leo Schwinn


[The Adversarial Consistency of Surrogate Risks for Binary Classification. (10%)](http://arxiv.org/abs/2305.09956)

Natalie Frank, Jonathan Niles-Weed


[PaLM 2 Technical Report. (1%)](http://arxiv.org/abs/2305.10403)

Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, Yonghui Wu


## 2023-05-16

[Iterative Adversarial Attack on Image-guided Story Ending Generation. (99%)](http://arxiv.org/abs/2305.13208)

Youze Wang, Wenbo Hu, Richang Hong


[Releasing Inequality Phenomena in $L_{\infty}$-Adversarial Training via Input Gradient Distillation. (98%)](http://arxiv.org/abs/2305.09305)

Junxi Chen, Junhao Dong, Xiaohua Xie


[Ortho-ODE: Enhancing Robustness and of Neural ODEs against Adversarial Attacks. (54%)](http://arxiv.org/abs/2305.09179)

Vishal Purohit


[Unlearnable Examples Give a False Sense of Security: Piercing through Unexploitable Data with Learnable Examples. (50%)](http://arxiv.org/abs/2305.09241)

Wan Jiang, Yunfeng Diao, He Wang, Jianxin Sun, Meng Wang, Richang Hong


## 2023-05-15

[Attacking Perceptual Similarity Metrics. (99%)](http://arxiv.org/abs/2305.08840)

Abhijay Ghildyal, Feng Liu


[Exploiting Frequency Spectrum of Adversarial Images for General Robustness. (96%)](http://arxiv.org/abs/2305.08439)

Chun Yang Tan, Kazuhiko Kawamoto, Hiroshi Kera


[Training Neural Networks without Backpropagation: A Deeper Dive into the Likelihood Ratio Method. (4%)](http://arxiv.org/abs/2305.08960)

Jinyang Jiang, Zeliang Zhang, Chenliang Xu, Zhaofei Yu, Yijie Peng


[Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility. (1%)](http://arxiv.org/abs/2305.10235)

Wentao Ye, Mingfeng Ou, Tianyi Li, Yipeng chen, Xuetao Ma, Yifan Yanggong, Sai Wu, Jie Fu, Gang Chen, Haobo Wang, Junbo Zhao


## 2023-05-14

[Diffusion Models for Imperceptible and Transferable Adversarial Attack. (99%)](http://arxiv.org/abs/2305.08192)

Jianqi Chen, Hao Chen, Keyan Chen, Yilan Zhang, Zhengxia Zou, Zhenwei Shi


[Improving Defensive Distillation using Teacher Assistant. (96%)](http://arxiv.org/abs/2305.08076)

Maniratnam Mandal, Suna Gao


[Manipulating Visually-aware Federated Recommender Systems and Its Countermeasures. (82%)](http://arxiv.org/abs/2305.08183)

Wei Yuan, Shilong Yuan, Chaoqun Yang, Quoc Viet Hung Nguyen, Hongzhi Yin


[Watermarking Text Generated by Black-Box Language Models. (9%)](http://arxiv.org/abs/2305.08883)

Xi Yang, Kejiang Chen, Weiming Zhang, Chang Liu, Yuang Qi, Jie Zhang, Han Fang, Nenghai Yu


## 2023-05-13

[DNN-Defender: An in-DRAM Deep Neural Network Defense Mechanism for Adversarial Weight Attack. (86%)](http://arxiv.org/abs/2305.08034)

Ranyang Zhou, Sabbir Ahmed, Adnan Siraj Rakin, Shaahin Angizi


[On enhancing the robustness of Vision Transformers: Defensive Diffusion. (76%)](http://arxiv.org/abs/2305.08031)

Raza Imam, Muhammad Huzaifa, Mohammed El-Amine Azz


[Decision-based iterative fragile watermarking for model integrity verification. (50%)](http://arxiv.org/abs/2305.09684)

Zhaoxia Yin, Heng Yin, Hang Su, Xinpeng Zhang, Zhenzhe Gao


## 2023-05-12

[Efficient Search of Comprehensively Robust Neural Architectures via Multi-fidelity Evaluation. (73%)](http://arxiv.org/abs/2305.07308)

Jialiang Sun, Wen Yao, Tingsong Jiang, Xiaoqian Chen


[Adversarial Security and Differential Privacy in mmWave Beam Prediction in 6G networks. (68%)](http://arxiv.org/abs/2305.09679)

Ghanta Sai Krishna, Kundrapu Supriya, Sanskar Singh, Sabur Baidya


[Mastering Percolation-like Games with Deep Learning. (1%)](http://arxiv.org/abs/2305.07687)

Michael M. Danziger, Omkar R. Gojala, Sean P. Cornelius


## 2023-05-11

[Distracting Downpour: Adversarial Weather Attacks for Motion Estimation. (74%)](http://arxiv.org/abs/2305.06716)

Jenny Schmalfuss, Lukas Mehl, Andrés Bruhn


[Backdoor Attack with Sparse and Invisible Trigger. (67%)](http://arxiv.org/abs/2306.06209)

Yinghua Gao, Yiming Li, Xueluan Gong, Shu-Tao Xia, Qian Wang


[Watch This Space: Securing Satellite Communication through Resilient Transmitter Fingerprinting. (1%)](http://arxiv.org/abs/2305.06947)

Joshua Smailes, Sebastian Kohler, Simon Birnbach, Martin Strohmeier, Ivan Martinovic


## 2023-05-10

[RNNS: Representation Nearest Neighbor Search Black-Box Attack on Code Models. (99%)](http://arxiv.org/abs/2305.05896)

Jie Zhang, Wei Ma, Qiang Hu, Xiaofei Xie, Yves Le Traon, Yang Liu


[Inter-frame Accelerate Attack against Video Interpolation Models. (99%)](http://arxiv.org/abs/2305.06540)

Junpei Liao, Zhikai Chen, Liang Yi, Wenyuan Yang, Baoyuan Wu, Xiaochun Cao


[Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications. (98%)](http://arxiv.org/abs/2305.06522)

Han Cheol Moon, Shafiq Joty, Ruochen Zhao, Megh Thakkar, Xu Chi


[Stealthy Low-frequency Backdoor Attack against Deep Neural Networks. (80%)](http://arxiv.org/abs/2305.09677)

Xinrui Liu, Yu-an Tan, Yajie Wang, Kefan Qiu, Yuanzhang Li


[Towards Invisible Backdoor Attacks in the Frequency Domain against Deep Neural Networks. (75%)](http://arxiv.org/abs/2305.10596)

Xinrui Liu, Yajie Wang, Yu-an Tan, Kefan Qiu, Yuanzhang Li


[The Robustness of Computer Vision Models against Common Corruptions: a Survey. (50%)](http://arxiv.org/abs/2305.06024)

Shunxin Wang, Raymond Veldhuis, Nicola Strisciuglio


[An Empirical Study on the Robustness of the Segment Anything Model (SAM). (22%)](http://arxiv.org/abs/2305.06422)

Yuqing Wang, Yun Zhao, Linda Petzold


[Robust multi-agent coordination via evolutionary generation of auxiliary adversarial attackers. (12%)](http://arxiv.org/abs/2305.05909)

Lei Yuan, Zi-Qian Zhang, Ke Xue, Hao Yin, Feng Chen, Cong Guan, Li-He Li, Chao Qian, Yang Yu


## 2023-05-09

[Quantization Aware Attack: Enhancing the Transferability of Adversarial Attacks across Target Models with Different Quantization Bitwidths. (99%)](http://arxiv.org/abs/2305.05875)

Yulong Yang, Chenhao Lin, Qian Li, Chao Shen, Dawei Zhou, Nannan Wang, Tongliang Liu


[Attack Named Entity Recognition by Entity Boundary Interference. (98%)](http://arxiv.org/abs/2305.05253)

Yifei Yang, Hongqiu Wu, Hai Zhao


[VSMask: Defending Against Voice Synthesis Attack via Real-Time Predictive Perturbation. (96%)](http://arxiv.org/abs/2305.05736)

Yuanda Wang, Hanqing Guo, Guangjing Wang, Bocheng Chen, Qiben Yan


[Investigating the Corruption Robustness of Image Classifiers with Random Lp-norm Corruptions. (75%)](http://arxiv.org/abs/2305.05400)

Georg Siedel, Silvia Vock, Andrey Morozov


[On the Relation between Sharpness-Aware Minimization and Adversarial Robustness. (56%)](http://arxiv.org/abs/2305.05392)

Zeming Wei, Jingyu Zhu, Yihao Zhang


[Turning Privacy-preserving Mechanisms against Federated Learning. (9%)](http://arxiv.org/abs/2305.05355)

Marco Arazzi, Mauro Conti, Antonino Nocera, Stjepan Picek


[BadCS: A Backdoor Attack Framework for Code search. (8%)](http://arxiv.org/abs/2305.05503)

Shiyi Qi, Yuanhang Yang, Shuzhzeng Gao, Cuiyun Gao, Zenglin Xu


[Quantum Machine Learning for Malware Classification. (1%)](http://arxiv.org/abs/2305.09674)

Grégoire Barrué, Tony Quertier


## 2023-05-08

[Toward Adversarial Training on Contextualized Language Representation. (93%)](http://arxiv.org/abs/2305.04557)

Hongqiu Wu, Yongxiang Liu, Hanwen Shi, Hai Zhao, Min Zhang


[Understanding Noise-Augmented Training for Randomized Smoothing. (64%)](http://arxiv.org/abs/2305.04746)

Ambar Pal, Jeremias Sulam


[TAPS: Connecting Certified and Adversarial Training. (33%)](http://arxiv.org/abs/2305.04574)

Yuhao Mao, Mark Niklas Müller, Marc Fischer, Martin Vechev


[Privacy-preserving Adversarial Facial Features. (22%)](http://arxiv.org/abs/2305.05391)

Zhibo Wang, He Wang, Shuaifan Jin, Wenwen Zhang, Jiahui Hu, Yan Wang, Peng Sun, Wei Yuan, Kaixin Liu, Kui Ren


[Communication-Robust Multi-Agent Learning by Adaptable Auxiliary Multi-Agent Adversary Generation. (1%)](http://arxiv.org/abs/2305.05116)

Lei Yuan, Feng Chen, Zhongzhang Zhang, Yang Yu


## 2023-05-07

[Adversarial Examples Detection with Enhanced Image Difference Features based on Local Histogram Equalization. (99%)](http://arxiv.org/abs/2305.04436)

Zhaoxia Yin, Shaowei Zhu, Hang Su, Jianteng Peng, Wanli Lyu, Bin Luo


[Pick your Poison: Undetectability versus Robustness in Data Poisoning Attacks against Deep Image Classification. (93%)](http://arxiv.org/abs/2305.09671)

Nils Lukas, Florian Kerschbaum


## 2023-05-06

[Reactive Perturbation Defocusing for Textual Adversarial Defense. (99%)](http://arxiv.org/abs/2305.04067)

Heng Yang, Ke Li


[Beyond the Model: Data Pre-processing Attack to Deep Learning Models in Android Apps. (92%)](http://arxiv.org/abs/2305.03963)

Ye Sang, Yujin Huang, Shuo Huang, Helei Cui


[Towards Prompt-robust Face Privacy Protection via Adversarial Decoupling Augmentation Framework. (38%)](http://arxiv.org/abs/2305.03980)

Ruijia Wu, Yuhang Wang, Huafeng Shi, Zhipeng Yu, Yichao Wu, Ding Liang


[Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning. (2%)](http://arxiv.org/abs/2305.04175)

Shengfang Zhai, Yinpeng Dong, Qingni Shen, Shi Pu, Yuejian Fang, Hang Su


## 2023-05-05

[White-Box Multi-Objective Adversarial Attack on Dialogue Generation. (99%)](http://arxiv.org/abs/2305.03655)

Yufei Li, Zexin Li, Yingfan Gao, Cong Liu


[Evading Watermark based Detection of AI-Generated Content. (87%)](http://arxiv.org/abs/2305.03807)

Zhengyuan Jiang, Jinghuai Zhang, Neil Zhenqiang Gong


[Verifiable Learning for Robust Tree Ensembles. (15%)](http://arxiv.org/abs/2305.03626)

Stefano Università Ca' Foscari Venezia, Italy Calzavara, Lorenzo Università Ca' Foscari Venezia, Italy Cazzaro, Giulio Ermanno Università Ca' Foscari Venezia, Italy Pibiri, Nicola Università Ca' Foscari Venezia, Italy Prezza


[Repairing Deep Neural Networks Based on Behavior Imitation. (4%)](http://arxiv.org/abs/2305.03365)

Zhen Liang, Taoran Wu, Changyuan Zhao, Wanwei Liu, Bai Xue, Wenjing Yang, Ji Wang


## 2023-05-04

[Madvex: Instrumentation-based Adversarial Attacks on Machine Learning Malware Detection. (99%)](http://arxiv.org/abs/2305.02559)

Nils Loose, Felix Mächtle, Claudius Pott, Volodymyr Bezsmertnyi, Thomas Eisenbarth


[IMAP: Intrinsically Motivated Adversarial Policy. (98%)](http://arxiv.org/abs/2305.02605)

Xiang Zheng, Xingjun Ma, Shengjie Wang, Xinyu Wang, Chao Shen, Cong Wang


[Single Node Injection Label Specificity Attack on Graph Neural Networks via Reinforcement Learning. (78%)](http://arxiv.org/abs/2305.02901)

Dayuan Chen, Jian Zhang, Yuqian Lv, Jinhuan Wang, Hongjie Ni, Shanqing Yu, Zhen Wang, Qi Xuan


[Faulting original McEliece's implementations is possible: How to mitigate this risk? (2%)](http://arxiv.org/abs/2305.02855)

Vincent Giraud, Guillaume Bouffard


## 2023-05-03

[New Adversarial Image Detection Based on Sentiment Analysis. (99%)](http://arxiv.org/abs/2305.03173)

Yulong Wang, Tianxiang Li, Shenghong Li, Xin Yuan, Wei Ni


[LearnDefend: Learning to Defend against Targeted Model-Poisoning Attacks on Federated Learning. (84%)](http://arxiv.org/abs/2305.02022)

Kiran Purohit, Soumi Das, Sourangshu Bhattacharya, Santu Rana


[Defending against Insertion-based Textual Backdoor Attacks via Attribution. (61%)](http://arxiv.org/abs/2305.02394)

Jiazhao Li, Zhuofeng Wu, Wei Ping, Chaowei Xiao, V. G. Vinod Vydiswaran


[On the Security Risks of Knowledge Graph Reasoning. (26%)](http://arxiv.org/abs/2305.02383)

Zhaohan Xi, Tianyu Du, Changjiang Li, Ren Pang, Shouling Ji, Xiapu Luo, Xusheng Xiao, Fenglong Ma, Ting Wang


[Backdoor Learning on Sequence to Sequence Models. (5%)](http://arxiv.org/abs/2305.02424)

Lichang Chen, Minhao Cheng, Heng Huang


[Rethinking Graph Lottery Tickets: Graph Sparsity Matters. (2%)](http://arxiv.org/abs/2305.02190)

Bo Hui, Da Yan, Xiaolong Ma, Wei-Shinn Ku


[PTP: Boosting Stability and Performance of Prompt Tuning with Perturbation-Based Regularizer. (1%)](http://arxiv.org/abs/2305.02423)

Lichang Chen, Heng Huang, Minhao Cheng


## 2023-05-02

[Boosting Adversarial Transferability via Fusing Logits of Top-1 Decomposed Feature. (99%)](http://arxiv.org/abs/2305.01361)

Juanjuan Weng, Zhiming Luo, Dazhen Lin, Shaozi Li, Zhun Zhong


[DABS: Data-Agnostic Backdoor attack at the Server in Federated Learning. (73%)](http://arxiv.org/abs/2305.01267)

Wenqiang Sun, Sen Li, Yuchang Sun, Jun Zhang


[Towards Imperceptible Document Manipulations against Neural Ranking Models. (67%)](http://arxiv.org/abs/2305.01860)

Xuanang Chen, Ben He, Zheng Ye, Le Sun, Yingfei Sun


[Sentiment Perception Adversarial Attacks on Neural Machine Translation Systems. (50%)](http://arxiv.org/abs/2305.01437)

Vyas Raina, Mark Gales


[Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. (8%)](http://arxiv.org/abs/2305.01219)

Shuai Zhao, Jinming Wen, Luu Anh Tuan, Junbo Zhao, Jie Fu


## 2023-05-01

[Attack-SAM: Towards Evaluating Adversarial Robustness of Segment Anything Model. (99%)](http://arxiv.org/abs/2305.00866)

Chenshuang Zhang, Chaoning Zhang, Taegoo Kang, Donghun Kim, Sung-Ho Bae, In So Kweon


[Physical Adversarial Attacks for Surveillance: A Survey. (98%)](http://arxiv.org/abs/2305.01074)

Kien Nguyen, Tharindu Fernando, Clinton Fookes, Sridha Sridharan


[Revisiting Robustness in Graph Machine Learning. (98%)](http://arxiv.org/abs/2305.00851)

Lukas Gosch, Daniel Sturm, Simon Geisler, Stephan Günnemann


[Stratified Adversarial Robustness with Rejection. (96%)](http://arxiv.org/abs/2305.01139)

Jiefeng Chen, Jayaram Raghuram, Jihye Choi, Xi Wu, Yingyu Liang, Somesh Jha


[Poisoning Language Models During Instruction Tuning. (2%)](http://arxiv.org/abs/2305.00944)

Alexander Wan, Eric Wallace, Sheng Shen, Dan Klein


## 2023-04-30

[Assessing Vulnerabilities of Adversarial Learning Algorithm through Poisoning Attacks. (98%)](http://arxiv.org/abs/2305.00399)

Jingfeng Zhang, Bo Song, Bo Han, Lei Liu, Gang Niu, Masashi Sugiyama


## 2023-04-29

[FedGrad: Mitigating Backdoor Attacks in Federated Learning Through Local Ultimate Gradients Inspection. (81%)](http://arxiv.org/abs/2305.00328)

Thuy Dung Nguyen, Anh Duy Nguyen, Kok-Seng Wong, Huy Hieu Pham, Thanh Hung Nguyen, Phi Le Nguyen, Truong Thao Nguyen


[Enhancing Adversarial Contrastive Learning via Adversarial Invariant Regularization. (33%)](http://arxiv.org/abs/2305.00374)

Xilie Xu, Jingfeng Zhang, Feng Liu, Masashi Sugiyama, Mohan Kankanhalli


[Adversarial Representation Learning for Robust Privacy Preservation in Audio. (1%)](http://arxiv.org/abs/2305.00011)

Shayan Gharib, Minh Tran, Diep Luong, Konstantinos Drossos, Tuomas Virtanen


## 2023-04-28

[Topic-oriented Adversarial Attacks against Black-box Neural Ranking Models. (99%)](http://arxiv.org/abs/2304.14867)

Yu-An Liu, Ruqing Zhang, Jiafeng Guo, Rijke Maarten de, Wei Chen, Yixing Fan, Xueqi Cheng


[On the existence of solutions to adversarial training in multiclass classification. (75%)](http://arxiv.org/abs/2305.00075)

Nicolas Garcia Trillos, Matt Jacobs, Jakwang Kim


[The Power of Typed Affine Decision Structures: A Case Study. (3%)](http://arxiv.org/abs/2304.14888)

Gerrit Nolte, Maximilian Schlüter, Alnis Murtovi, Bernhard Steffen


[faulTPM: Exposing AMD fTPMs' Deepest Secrets. (3%)](http://arxiv.org/abs/2304.14717)

Hans Niklas Jacob, Christian Werling, Robert Buhren, Jean-Pierre Seifert


[SAM Meets Robotic Surgery: An Empirical Study in Robustness Perspective. (1%)](http://arxiv.org/abs/2304.14674)

An Wang, Mobarakol Islam, Mengya Xu, Yang Zhang, Hongliang Ren


## 2023-04-27

[Adversary Aware Continual Learning. (80%)](http://arxiv.org/abs/2304.14483)

Muhammad Umer, Robi Polikar


[Fusion is Not Enough: Single-Modal Attacks to Compromise Fusion Models in Autonomous Driving. (75%)](http://arxiv.org/abs/2304.14614)

Zhiyuan Cheng, Hongjun Choi, James Liang, Shiwei Feng, Guanhong Tao, Dongfang Liu, Michael Zuzak, Xiangyu Zhang


[Boosting Big Brother: Attacking Search Engines with Encodings. (68%)](http://arxiv.org/abs/2304.14031)

Nicholas Boucher, Luca Pajola, Ilia Shumailov, Ross Anderson, Mauro Conti


[ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger. (62%)](http://arxiv.org/abs/2304.14475)

Jiazhao Li, Yijin Yang, Zhuofeng Wu, V. G. Vinod Vydiswaran, Chaowei Xiao


[Improve Video Representation with Temporal Adversarial Augmentation. (26%)](http://arxiv.org/abs/2304.14601)

Jinhao Duan, Quanfu Fan, Hao Cheng, Xiaoshuang Shi, Kaidi Xu


[Origin Tracing and Detecting of LLMs. (1%)](http://arxiv.org/abs/2304.14072)

Linyang Li, Pengyu Wang, Ke Ren, Tianxiang Sun, Xipeng Qiu


[Deep Intellectual Property Protection: A Survey. (1%)](http://arxiv.org/abs/2304.14613)

Yuchen Sun, Tianpeng Liu, Panhe Hu, Qing Liao, Shaojing Fu, Nenghai Yu, Deke Guo, Yongxiang Liu, Li Liu


## 2023-04-26

[Improving Adversarial Transferability via Intermediate-level Perturbation Decay. (98%)](http://arxiv.org/abs/2304.13410)

Qizhang Li, Yiwen Guo, Wangmeng Zuo, Hao Chen


[Detection of Adversarial Physical Attacks in Time-Series Image Data. (92%)](http://arxiv.org/abs/2304.13919)

Ramneet Kaur, Yiannis Kantaros, Wenwen Si, James Weimer, Insup Lee


[Blockchain-based Federated Learning with SMPC Model Verification Against Poisoning Attack for Healthcare Systems. (13%)](http://arxiv.org/abs/2304.13360)

Aditya Pribadi Kalapaaking, Ibrahim Khalil, Xun Yi


## 2023-04-25

[Improving Robustness Against Adversarial Attacks with Deeply Quantized Neural Networks. (99%)](http://arxiv.org/abs/2304.12829)

Ferheen Ayaz, Idris Zakariyya, José Cano, Sye Loong Keoh, Jeremy Singer, Danilo Pau, Mounia Kharbouche-Harrari


[Generating Adversarial Examples with Task Oriented Multi-Objective Optimization. (99%)](http://arxiv.org/abs/2304.13229)

Anh Bui, Trung Le, He Zhao, Quan Tran, Paul Montague, Dinh Phung


[SHIELD: Thwarting Code Authorship Attribution. (98%)](http://arxiv.org/abs/2304.13255)

Mohammed Abuhamad, Changhun Jung, David Mohaisen, DaeHun Nyang


[Learning Robust Deep Equilibrium Models. (82%)](http://arxiv.org/abs/2304.12707)

Haoyu Chu, Shikui Wei, Ting Liu, Yao Zhao


[LSTM-based Load Forecasting Robustness Against Noise Injection Attack in Microgrid. (1%)](http://arxiv.org/abs/2304.13104)

Amirhossein Nazeri, Pierluigi Pisu


## 2023-04-24

[Evaluating Adversarial Robustness on Document Image Classification. (99%)](http://arxiv.org/abs/2304.12486)

Timothée Fronteau, Arnaud Paran, Aymen Shabou


[Combining Adversaries with Anti-adversaries in Training. (64%)](http://arxiv.org/abs/2304.12550)

Xiaoling Zhou, Nan Yang, Ou Wu


[Enhancing Fine-Tuning Based Backdoor Defense with Sharpness-Aware Minimization. (41%)](http://arxiv.org/abs/2304.11823)

Mingli Zhu, Shaokui Wei, Li Shen, Yanbo Fan, Baoyuan Wu


[Opinion Control under Adversarial Network Perturbation: A Stackelberg Game Approach. (10%)](http://arxiv.org/abs/2304.12540)

Yuejiang Li, Zhanjiang Chen, H. Vicky Zhao


[Robust Tickets Can Transfer Better: Drawing More Transferable Subnetworks in Transfer Learning. (1%)](http://arxiv.org/abs/2304.11834)

Yonggan Fu, Ye Yuan, Shang Wu, Jiayi Yuan, Yingyan Lin


## 2023-04-23

[StyLess: Boosting the Transferability of Adversarial Examples. (99%)](http://arxiv.org/abs/2304.11579)

Kaisheng Liang, Bin Xiao


[Evading DeepFake Detectors via Adversarial Statistical Consistency. (98%)](http://arxiv.org/abs/2304.11670)

Yang Hou, Qing Guo, Yihao Huang, Xiaofei Xie, Lei Ma, Jianjun Zhao


## 2023-04-22

[Detecting Adversarial Faces Using Only Real Face Self-Perturbations. (98%)](http://arxiv.org/abs/2304.11359)

Qian Wang, Yongqin Xian, Hefei Ling, Jinyuan Zhang, Xiaorui Lin, Ping Li, Jiazhong Chen, Ning Yu


[Universal Adversarial Backdoor Attacks to Fool Vertical Federated Learning in Cloud-Edge Collaboration. (70%)](http://arxiv.org/abs/2304.11432)

Peng Chen, Xin Du, Zhihui Lu, Hongfeng Chai


## 2023-04-21

[Launching a Robust Backdoor Attack under Capability Constrained Scenarios. (88%)](http://arxiv.org/abs/2304.10985)

Ming Yi, Yixiao Xu, Kangyi Ding, Mingyong Yin, Xiaolei Liu


[Individual Fairness in Bayesian Neural Networks. (69%)](http://arxiv.org/abs/2304.10828)

Alice Doherty, Matthew Wicker, Luca Laurenti, Andrea Patane


[Denial-of-Service or Fine-Grained Control: Towards Flexible Model Poisoning Attacks on Federated Learning. (64%)](http://arxiv.org/abs/2304.10783)

Hangtao Zhang, Zeming Yao, Leo Yu Zhang, Shengshan Hu, Chao Chen, Alan Liew, Zhetao Li


[Interpretable and Robust AI in EEG Systems: A Survey. (12%)](http://arxiv.org/abs/2304.10755)

Xinliang Zhou, Chenyu Liu, Liming Zhai, Ziyu Jia, Cuntai Guan, Yang Liu


[MAWSEO: Adversarial Wiki Search Poisoning for Illicit Online Promotion. (2%)](http://arxiv.org/abs/2304.11300)

Zilong Lin, Zhengyi Li, Xiaojing Liao, XiaoFeng Wang, Xiaozhong Liu


## 2023-04-20

[Diversifying the High-level Features for better Adversarial Transferability. (99%)](http://arxiv.org/abs/2304.10136)

Zhiyuan Wang, Zeliang Zhang, Siyuan Liang, Xiaosen Wang


[Towards the Universal Defense for Query-Based Audio Adversarial Attacks. (99%)](http://arxiv.org/abs/2304.10088)

Feng Guo, Zheng Sun, Yuxuan Chen, Lei Ju


[Using Z3 for Formal Modeling and Verification of FNN Global Robustness. (98%)](http://arxiv.org/abs/2304.10558)

Yihao Zhang, Zeming Wei, Xiyue Zhang, Meng Sun


[Certified Adversarial Robustness Within Multiple Perturbation Bounds. (96%)](http://arxiv.org/abs/2304.10446)

Soumalya Nandi, Sravanti Addepalli, Harsh Rangwani, R. Venkatesh Babu


[Can Perturbations Help Reduce Investment Risks? Risk-Aware Stock Recommendation via Split Variational Adversarial Training. (93%)](http://arxiv.org/abs/2304.11043)

Jiezhu Cheng, Kaizhu Huang, Zibin Zheng


[Adversarial Infrared Blocks: A Black-box Attack to Thermal Infrared Detectors at Multiple Angles in Physical World. (89%)](http://arxiv.org/abs/2304.10712)

Chengyin Hu, Weiwen Shi, Tingsong Jiang, Wen Yao, Ling Tian, Xiaoqian Chen


[An Analysis of the Completion Time of the BB84 Protocol. (22%)](http://arxiv.org/abs/2304.10218)

Sounak Kar, Jean-Yves Le Boudec


[A Plug-and-Play Defensive Perturbation for Copyright Protection of DNN-based Applications. (13%)](http://arxiv.org/abs/2304.10679)

Donghua Wang, Wen Yao, Tingsong Jiang, Weien Zhou, Lang Lin, Xiaoqian Chen


[Enhancing object detection robustness: A synthetic and natural perturbation approach. (12%)](http://arxiv.org/abs/2304.10622)

Nilantha Premakumara, Brian Jalaian, Niranjan Suri, Hooman Samani


[RoCOCO: Robustness Benchmark of MS-COCO to Stress-test Image-Text Matching Models. (8%)](http://arxiv.org/abs/2304.10727)

Seulki Park, Daeho Um, Hajung Yoon, Sanghyuk Chun, Sangdoo Yun, Jin Young Choi


[Get Rid Of Your Trail: Remotely Erasing Backdoors in Federated Learning. (2%)](http://arxiv.org/abs/2304.10638)

Manaar Alam, Hithem Lamri, Michail Maniatakos


## 2023-04-19

[Jedi: Entropy-based Localization and Removal of Adversarial Patches. (84%)](http://arxiv.org/abs/2304.10029)

Bilel Tarchoun, Anouar Ben Khalifa, Mohamed Ali Mahjoub, Nael Abu-Ghazaleh, Ihsen Alouani


[GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models. (81%)](http://arxiv.org/abs/2304.09875)

Zaitang Li, Pin-Yu Chen, Tsung-Yi Ho


[Secure Split Learning against Property Inference, Data Reconstruction, and Feature Space Hijacking Attacks. (5%)](http://arxiv.org/abs/2304.09515)

Yunlong Mao, Zexi Xin, Zhenyu Li, Jue Hong, Qingyou Yang, Sheng Zhong


[Density-Insensitive Unsupervised Domain Adaption on 3D Object Detection. (1%)](http://arxiv.org/abs/2304.09446)

Qianjiang Hu, Daizong Liu, Wei Hu


[On the Robustness of Aspect-based Sentiment Analysis: Rethinking Model, Data, and Training. (1%)](http://arxiv.org/abs/2304.09563)

Hao Fei, Tat-Seng Chua, Chenliang Li, Donghong Ji, Meishan Zhang, Yafeng Ren


[Fundamental Limitations of Alignment in Large Language Models. (1%)](http://arxiv.org/abs/2304.11082)

Yotam Wolf, Noam Wies, Oshri Avnery, Yoav Levine, Amnon Shashua


## 2023-04-18

[Wavelets Beat Monkeys at Adversarial Robustness. (99%)](http://arxiv.org/abs/2304.09403)

Jingtong Su, Julia Kempe


[Towards the Transferable Audio Adversarial Attack via Ensemble Methods. (99%)](http://arxiv.org/abs/2304.08811)

Feng Guo, Zheng Sun, Yuxuan Chen, Lei Ju


[Masked Language Model Based Textual Adversarial Example Detection. (99%)](http://arxiv.org/abs/2304.08767)

Xiaomei Zhang, Zhaoxi Zhang, Qi Zhong, Xufei Zheng, Yanjun Zhang, Shengshan Hu, Leo Yu Zhang


[In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT. (68%)](http://arxiv.org/abs/2304.08979)

Xinyue Shen, Zeyuan Chen, Michael Backes, Yang Zhang


[Generative models improve fairness of medical classifiers under distribution shifts. (13%)](http://arxiv.org/abs/2304.09218)

Ira Ktena, Olivia Wiles, Isabela Albuquerque, Sylvestre-Alvise Rebuffi, Ryutaro Tanno, Abhijit Guha Roy, Shekoofeh Azizi, Danielle Belgrave, Pushmeet Kohli, Alan Karthikesalingam, Taylan Cemgil, Sven Gowal


## 2023-04-17

[Evil from Within: Machine Learning Backdoors through Hardware Trojans. (15%)](http://arxiv.org/abs/2304.08411)

Alexander Warnecke, Julian Speith, Jan-Niklas Möller, Konrad Rieck, Christof Paar


[GrOVe: Ownership Verification of Graph Neural Networks using Embeddings. (13%)](http://arxiv.org/abs/2304.08566)

Asim Waheed, Vasisht Duddu, N. Asokan


[OOD-CV-v2: An extended Benchmark for Robustness to Out-of-Distribution Shifts of Individual Nuisances in Natural Images. (1%)](http://arxiv.org/abs/2304.10266)

Bingchen Zhao, Jiahao Wang, Wufei Ma, Artur Jesslen, Siwei Yang, Shaozuo Yu, Oliver Zendel, Christian Theobalt, Alan Yuille, Adam Kortylewski


## 2023-04-16

[A Random-patch based Defense Strategy Against Physical Attacks for Face Recognition Systems. (98%)](http://arxiv.org/abs/2304.07822)

JiaHao Xie, Ye Luo, Jianwei Lu


[RNN-Guard: Certified Robustness Against Multi-frame Attacks for Recurrent Neural Networks. (96%)](http://arxiv.org/abs/2304.07980)

Yunruo Zhang, Tianyu Du, Shouling Ji, Peng Tang, Shanqing Guo


[JoB-VS: Joint Brain-Vessel Segmentation in TOF-MRA Images. (15%)](http://arxiv.org/abs/2304.07744)

Natalia Valderrama, Ioannis Pitsiorlas, Luisa Vargas, Pablo Arbeláez, Maria A. Zuluaga


## 2023-04-14

[Interpretability is a Kind of Safety: An Interpreter-based Ensemble for Adversary Defense. (99%)](http://arxiv.org/abs/2304.06919)

Jingyuan Wang, Yufan Wu, Mingxuan Li, Xin Lin, Junjie Wu, Chao Li


[Combining Generators of Adversarial Malware Examples to Increase Evasion Rate. (99%)](http://arxiv.org/abs/2304.07360)

Matouš Kozák, Martin Jureček


[Cross-Entropy Loss Functions: Theoretical Analysis and Applications. (3%)](http://arxiv.org/abs/2304.07288)

Anqi Mao, Mehryar Mohri, Yutao Zhong


[Pool Inference Attacks on Local Differential Privacy: Quantifying the Privacy Guarantees of Apple's Count Mean Sketch in Practice. (2%)](http://arxiv.org/abs/2304.07134)

Andrea Gadotti, Florimond Houssiau, Meenatchi Sundaram Muthu Selva Annamalai, Montjoye Yves-Alexandre de


## 2023-04-13

[Generating Adversarial Examples with Better Transferability via Masking Unimportant Parameters of Surrogate Model. (99%)](http://arxiv.org/abs/2304.06908)

Dingcheng Yang, Wenjian Yu, Zihao Xiao, Jiaqi Luo


[Certified Zeroth-order Black-Box Defense with Robust UNet Denoiser. (96%)](http://arxiv.org/abs/2304.06430)

Astha Verma, Siddhesh Bangar, A V Subramanyam, Naman Lal, Rajiv Ratn Shah, Shin'ichi Satoh


[False Claims against Model Ownership Resolution. (93%)](http://arxiv.org/abs/2304.06607)

Jian Liu, Rui Zhang, Sebastian Szyller, Kui Ren, N. Asokan


[Adversarial Examples from Dimensional Invariance. (45%)](http://arxiv.org/abs/2304.06575)

Benjamin L. Badger


[Understanding Overfitting in Adversarial Training in Kernel Regression. (1%)](http://arxiv.org/abs/2304.06326)

Teng Zhang, Kang Li


[LSFSL: Leveraging Shape Information in Few-shot Learning. (1%)](http://arxiv.org/abs/2304.06672)

Deepan Chakravarthi Padmanabhan, Shruthi Gowda, Elahe Arani, Bahram Zonooz


## 2023-04-12

[Generative Adversarial Networks-Driven Cyber Threat Intelligence Detection Framework for Securing Internet of Things. (92%)](http://arxiv.org/abs/2304.05644)

Mohamed Amine Ferrag, Djallel Hamouda, Merouane Debbah, Leandros Maglaras, Abderrahmane Lakas


[Exploiting Logic Locking for a Neural Trojan Attack on Machine Learning Accelerators. (1%)](http://arxiv.org/abs/2304.06017)

Hongye Xu, Dongfang Liu, Cory Merkel, Michael Zuzak


## 2023-04-11

[RecUP-FL: Reconciling Utility and Privacy in Federated Learning via User-configurable Privacy Defense. (99%)](http://arxiv.org/abs/2304.05135)

Yue Cui, Syed Irfan Ali Meerza, Zhuohang Li, Luyang Liu, Jiaxin Zhang, Jian Liu


[Simultaneous Adversarial Attacks On Multiple Face Recognition System Components. (98%)](http://arxiv.org/abs/2304.05048)

Inderjeet Singh, Kazuya Kakizaki, Toshinori Araki


[Boosting Cross-task Transferability of Adversarial Patches with Visual Relations. (98%)](http://arxiv.org/abs/2304.05402)

Tony Ma, Songze Li, Yisong Xiao, Shunchang Liu


[Benchmarking the Physical-world Adversarial Robustness of Vehicle Detection. (92%)](http://arxiv.org/abs/2304.05098)

Tianyuan Zhang, Yisong Xiao, Xiaoya Zhang, Hao Li, Lu Wang


[On the Adversarial Inversion of Deep Biometric Representations. (67%)](http://arxiv.org/abs/2304.05561)

Gioacchino Tangari, Shreesh Keskar, Hassan Jameel Asghar, Dali Kaafar


[Overload: Latency Attacks on Object Detection for Edge Devices. (33%)](http://arxiv.org/abs/2304.05370)

Erh-Chung Chen, Pin-Yu Chen, I-Hsin Chung, Che-rung Lee


[Towards More Robust and Accurate Sequential Recommendation with Cascade-guided Adversarial Training. (9%)](http://arxiv.org/abs/2304.05492)

Juntao Tan, Shelby Heinecke, Zhiwei Liu, Yongjun Chen, Yongfeng Zhang, Huan Wang


## 2023-04-10

[Generating Adversarial Attacks in the Latent Space. (98%)](http://arxiv.org/abs/2304.04386)

Nitish Shukla, Sudipta Banerjee


[Reinforcement Learning-Based Black-Box Model Inversion Attacks. (67%)](http://arxiv.org/abs/2304.04625)

Gyojin Han, Jaehyun Choi, Haeil Lee, Junmo Kim


[Defense-Prefix for Preventing Typographic Attacks on CLIP. (16%)](http://arxiv.org/abs/2304.04512)

Hiroki Azuma, Yusuke Matsui


[Helix++: A platform for efficiently securing software. (1%)](http://arxiv.org/abs/2304.04846)

Jack W. Davidson, Jason D. Hiser, Anh Nguyen-Tuong


## 2023-04-09

[Certifiable Black-Box Attack: Ensuring Provably Successful Attack for Adversarial Examples. (99%)](http://arxiv.org/abs/2304.04343)

Hanbin Hong, Yuan Hong


[Adversarially Robust Neural Architecture Search for Graph Neural Networks. (80%)](http://arxiv.org/abs/2304.04168)

Beini Xie, Heng Chang, Ziwei Zhang, Xin Wang, Daixin Wang, Zhiqiang Zhang, Rex Ying, Wenwu Zhu


[Unsupervised Multi-Criteria Adversarial Detection in Deep Image Retrieval. (68%)](http://arxiv.org/abs/2304.04228)

Yanru Xiao, Cong Wang, Xing Gao


## 2023-04-08

[Robust Deep Learning Models Against Semantic-Preserving Adversarial Attack. (99%)](http://arxiv.org/abs/2304.03955)

Dashan Gao, Yunce Zhao, Yinghua Yao, Zeqi Zhang, Bifei Mao, Xin Yao


[RobCaps: Evaluating the Robustness of Capsule Networks against Affine Transformations and Adversarial Attacks. (98%)](http://arxiv.org/abs/2304.03973)

Alberto Marchisio, Marco Antonio De, Alessio Colucci, Maurizio Martina, Muhammad Shafique


[Exploring the Connection between Robust and Generative Models. (67%)](http://arxiv.org/abs/2304.04033)

Senad Beadini, Iacopo Masi


[Benchmarking the Robustness of Quantized Models. (47%)](http://arxiv.org/abs/2304.03968)

Yisong Xiao, Tianyuan Zhang, Shunchang Liu, Haotong Qin


[Attack is Good Augmentation: Towards Skeleton-Contrastive Representation Learning. (13%)](http://arxiv.org/abs/2304.04023)

Binqian Xu, Xiangbo Shu, Rui Yan, Guo-Sen Xie, Yixiao Ge, Mike Zheng Shou


[Deep Prototypical-Parts Ease Morphological Kidney Stone Identification and are Competitively Robust to Photometric Perturbations. (4%)](http://arxiv.org/abs/2304.04077)

Daniel Flores-Araiza, Francisco Lopez-Tiro, Jonathan El-Beze, Jacques Hubert, Miguel Gonzalez-Mendoza, Gilberto Ochoa-Ruiz, Christian Daul


[EMP-SSL: Towards Self-Supervised Learning in One Training Epoch. (1%)](http://arxiv.org/abs/2304.03977)

Shengbang Tong, Yubei Chen, Yi Ma, Yann Lecun


## 2023-04-07

[Architecture-Preserving Provable Repair of Deep Neural Networks. (1%)](http://arxiv.org/abs/2304.03496)

Zhe Tao, Stephanie Nawas, Jacqueline Mitchell, Aditya V. Thakur


[ASPEST: Bridging the Gap Between Active Learning and Selective Prediction. (1%)](http://arxiv.org/abs/2304.03870)

Jiefeng Chen, Jinsung Yoon, Sayna Ebrahimi, Sercan Arik, Somesh Jha, Tomas Pfister


## 2023-04-06

[Quantifying and Defending against Privacy Threats on Federated Knowledge Graph Embedding. (45%)](http://arxiv.org/abs/2304.02932)

Yuke Hu, Wei Liang, Ruofan Wu, Kai Xiao, Weiqiang Wang, Xiaochen Li, Jinfei Liu, Zhan Qin


[Manipulating Federated Recommender Systems: Poisoning with Synthetic Users and Its Countermeasures. (45%)](http://arxiv.org/abs/2304.03054)

Wei Yuan, Quoc Viet Hung Nguyen, Tieke He, Liang Chen, Hongzhi Yin


[Improving Visual Question Answering Models through Robustness Analysis and In-Context Learning with a Chain of Basic Questions. (10%)](http://arxiv.org/abs/2304.03147)

Jia-Hong Huang, Modar Alfadly, Bernard Ghanem, Marcel Worring


[EZClone: Improving DNN Model Extraction Attack via Shape Distillation from GPU Execution Profiles. (4%)](http://arxiv.org/abs/2304.03388)

Jonah O'Brien Weiss, Tiago Alves, Sandip Kundu


[Evaluating the Robustness of Machine Reading Comprehension Models to Low Resource Entity Renaming. (2%)](http://arxiv.org/abs/2304.03145)

Clemencia Siro, Tunde Oluwaseyi Ajayi


[Rethinking Evaluation Protocols of Visual Representations Learned via Self-supervised Learning. (1%)](http://arxiv.org/abs/2304.03456)

Jae-Hun Lee, Doyoung Yoon, ByeongMoon Ji, Kyungyul Kim, Sangheum Hwang


[Reliable Learning for Test-time Attacks and Distribution Shift. (1%)](http://arxiv.org/abs/2304.03370)

Maria-Florina Balcan, Steve Hanneke, Rattana Pukdee, Dravyansh Sharma


[Benchmarking Robustness to Text-Guided Corruptions. (1%)](http://arxiv.org/abs/2304.02963)

Mohammadreza Mofayezi, Yasamin Medghalchi


## 2023-04-05

[A Certified Radius-Guided Attack Framework to Image Segmentation Models. (99%)](http://arxiv.org/abs/2304.02693)

Wenjie Qu, Youqi Li, Binghui Wang


[Going Further: Flatness at the Rescue of Early Stopping for Adversarial Example Transferability. (99%)](http://arxiv.org/abs/2304.02688)

Martin Gubri, Maxime Cordy, Yves Le Traon


[How to choose your best allies for a transferable attack? (99%)](http://arxiv.org/abs/2304.02312)

Thibault Maho, Seyed-Mohsen Moosavi-Dezfooli, Teddy Furon


[Robust Neural Architecture Search. (92%)](http://arxiv.org/abs/2304.02845)

Xunyu Zhu, Jian Li, Yong Liu, Weiping Wang


[Hyper-parameter Tuning for Adversarially Robust Models. (62%)](http://arxiv.org/abs/2304.02497)

Pedro Mendes, Paolo Romano, David Garlan


[JPEG Compressed Images Can Bypass Protections Against AI Editing. (15%)](http://arxiv.org/abs/2304.02234)

Pedro Sandoval-Segura, Jonas Geiping, Tom Goldstein


[FACE-AUDITOR: Data Auditing in Facial Recognition Systems. (1%)](http://arxiv.org/abs/2304.02782)

Min Chen, Zhikun Zhang, Tianhao Wang, Michael Backes, Yang Zhang


## 2023-04-04

[CGDTest: A Constrained Gradient Descent Algorithm for Testing Neural Networks. (31%)](http://arxiv.org/abs/2304.01826)

Vineel Nagisetty, Laura Graves, Guanting Pan, Piyush Jha, Vijay Ganesh


[Selective Knowledge Sharing for Privacy-Preserving Federated Distillation without A Good Teacher. (1%)](http://arxiv.org/abs/2304.01731)

Jiawei Shao, Fangzhao Wu, Jun Zhang


[EGC: Image Generation and Classification via a Single Energy-Based Model. (1%)](http://arxiv.org/abs/2304.02012)

Qiushan Guo, Chuofan Ma, Yi Jiang, Zehuan Yuan, Yizhou Yu, Ping Luo


## 2023-04-03

[Defending Against Patch-based Backdoor Attacks on Self-Supervised Learning. (76%)](http://arxiv.org/abs/2304.01482)

Ajinkya Tejankar, Maziar Sanjabi, Qifan Wang, Sinong Wang, Hamed Firooz, Hamed Pirsiavash, Liang Tan


[Model-Agnostic Reachability Analysis on Deep Neural Networks. (75%)](http://arxiv.org/abs/2304.00813)

Chi Zhang, Wenjie Ruan, Fu Wang, Peipei Xu, Geyong Min, Xiaowei Huang


[NetFlick: Adversarial Flickering Attacks on Deep Learning Based Video Compression. (69%)](http://arxiv.org/abs/2304.01441)

Jung-Woo Chang, Nojan Sheybani, Shehzeen Samarah Hussain, Mojan Javaheripi, Seira Hidano, Farinaz Koushanfar


[Learning About Simulated Adversaries from Human Defenders using Interactive Cyber-Defense Games. (1%)](http://arxiv.org/abs/2304.01142)

Baptiste Prebot, Yinuo Du, Cleotilde Gonzalez


## 2023-04-01

[GradMDM: Adversarial Attack on Dynamic Networks. (84%)](http://arxiv.org/abs/2304.06724)

Jianhong Pan, Lin Geng Foo, Qichen Zheng, Zhipeng Fan, Hossein Rahmani, Qiuhong Ke, Jun Liu


[Instance-level Trojan Attacks on Visual Question Answering via Adversarial Learning in Neuron Activation Space. (61%)](http://arxiv.org/abs/2304.00436)

Yuwei Sun, Hideya Ochiai, Jun Sakuma


## 2023-03-31

[Improving Fast Adversarial Training with Prior-Guided Knowledge. (99%)](http://arxiv.org/abs/2304.00202)

Xiaojun Jia, Yong Zhang, Xingxing Wei, Baoyuan Wu, Ke Ma, Jue Wang, Xiaochun Sr Cao


[To be Robust and to be Fair: Aligning Fairness with Robustness. (93%)](http://arxiv.org/abs/2304.00061)

Junyi Chai, Xiaoqian Wang


[Fooling Polarization-based Vision using Locally Controllable Polarizing Projection. (91%)](http://arxiv.org/abs/2303.17890)

Zhuoxiao Li, Zhihang Zhong, Shohei Nobuhara, Ko Nishino, Yinqiang Zheng


[Per-Example Gradient Regularization Improves Learning Signals from Noisy Data. (3%)](http://arxiv.org/abs/2303.17940)

Xuran Meng, Yuan Cao, Difan Zou


[Secure Federated Learning against Model Poisoning Attacks via Client Filtering. (2%)](http://arxiv.org/abs/2304.00160)

Duygu Nur Yaldiz, Tuo Zhang, Salman Avestimehr


[DIME-FM: DIstilling Multimodal and Efficient Foundation Models. (1%)](http://arxiv.org/abs/2303.18232)

Ximeng Sun, Pengchuan Zhang, Peizhao Zhang, Hardik Shah, Kate Saenko, Xide Xia


[Fides: A Generative Framework for Result Validation of Outsourced Machine Learning Workloads via TEE. (1%)](http://arxiv.org/abs/2304.00083)

Abhinav Kumar, Miguel A. Guirao Aguilera, Reza Tourani, Satyajayant Misra


## 2023-03-30

[Adversarial Attack and Defense for Dehazing Networks. (97%)](http://arxiv.org/abs/2303.17255)

Jie Gui, Xiaofeng Cong, Chengwei Peng, Yuan Yan Tang, James Tin-Yau Kwok


[Generating Adversarial Samples in Mini-Batches May Be Detrimental To Adversarial Robustness. (96%)](http://arxiv.org/abs/2303.17720)

Timothy Redgrave, Colton Crum


[Towards Adversarially Robust Continual Learning. (95%)](http://arxiv.org/abs/2303.17764)

Tao Bai, Chen Chen, Lingjuan Lyu, Jun Zhao, Bihan Wen


[Understanding the Robustness of 3D Object Detection with Bird's-Eye-View Representations in Autonomous Driving. (81%)](http://arxiv.org/abs/2303.17297)

Zijian Zhu, Yichi Zhang, Hai Chen, Yinpeng Dong, Shu Zhao, Wenbo Ding, Jiachen Zhong, Shibao Zheng


[Robo3D: Towards Robust and Reliable 3D Perception against Corruptions. (2%)](http://arxiv.org/abs/2303.17597)

Lingdong Kong, Youquan Liu, Xin Li, Runnan Chen, Wenwei Zhang, Jiawei Ren, Liang Pan, Kai Chen, Ziwei Liu


[Establishing baselines and introducing TernaryMixOE for fine-grained out-of-distribution detection. (1%)](http://arxiv.org/abs/2303.17658)

Noah Fleischmann, Walter Bennette, Nathan Inkawhich


[Explainable Intrusion Detection Systems Using Competitive Learning Techniques. (1%)](http://arxiv.org/abs/2303.17387)

Jesse Ables, Thomas Kirby, Sudip Mittal, Ioana Banicescu, Shahram Rahimi, William Anderson, Maria Seale


[Differential Area Analysis for Ransomware: Attacks, Countermeasures, and Limitations. (1%)](http://arxiv.org/abs/2303.17351)

Marco Venturini, Francesco Freda, Emanuele Miotto, Alberto Giaretta, Mauro Conti


## 2023-03-29

[Latent Feature Relation Consistency for Adversarial Robustness. (99%)](http://arxiv.org/abs/2303.16697)

Xingbin Liu, Huafeng Kuang, Hong Liu, Xianming Lin, Yongjian Wu, Rongrong Ji


[Beyond Empirical Risk Minimization: Local Structure Preserving Regularization for Improving Adversarial Robustness. (99%)](http://arxiv.org/abs/2303.16861)

Wei Wei, Jiahuan Zhou, Ying Wu


[Targeted Adversarial Attacks on Wind Power Forecasts. (88%)](http://arxiv.org/abs/2303.16633)

René Heinrich, Christoph Scholz, Stephan Vogt, Malte Lehna


[Towards Reasonable Budget Allocation in Untargeted Graph Structure Attacks via Gradient Debias. (67%)](http://arxiv.org/abs/2304.00010)

Zihan Liu, Yun Luo, Lirong Wu, Zicheng Liu, Stan Z. Li


[ImageNet-E: Benchmarking Neural Network Robustness via Attribute Editing. (56%)](http://arxiv.org/abs/2303.17096)

Xiaodan Li, Yuefeng Chen, Yao Zhu, Shuhui Wang, Rong Zhang, Hui Xue


[Graph Neural Networks for Hardware Vulnerability Analysis -- Can you Trust your GNN? (16%)](http://arxiv.org/abs/2303.16690)

Lilas Alrahis, Ozgur Sinanoglu


[Mole Recruitment: Poisoning of Image Classifiers via Selective Batch Sampling. (10%)](http://arxiv.org/abs/2303.17080)

Ethan Wisdom, Tejas Gokhale, Chaowei Xiao, Yezhou Yang


[A Tensor-based Convolutional Neural Network for Small Dataset Classification. (2%)](http://arxiv.org/abs/2303.17061)

Zhenhua Chen, David Crandall


[ALUM: Adversarial Data Uncertainty Modeling from Latent Model Uncertainty Compensation. (1%)](http://arxiv.org/abs/2303.16866)

Wei Wei, Jiahuan Zhou, Hongze Li, Ying Wu


## 2023-03-28

[A Pilot Study of Query-Free Adversarial Attack against Stable Diffusion. (99%)](http://arxiv.org/abs/2303.16378)

Haomin Zhuang, Yihua Zhang, Sijia Liu


[Improving the Transferability of Adversarial Samples by Path-Augmented Method. (99%)](http://arxiv.org/abs/2303.15735)

Jianping Zhang, Jen-tse Huang, Wenxuan Wang, Yichen Li, Weibin Wu, Xiaosen Wang, Yuxin Su, Michael R. Lyu


[Towards Effective Adversarial Textured 3D Meshes on Physical Face Recognition. (99%)](http://arxiv.org/abs/2303.15818)

Xiao Yang, Chang Liu, Longlong Xu, Yikai Wang, Yinpeng Dong, Ning Chen, Hang Su, Jun Zhu


[Transferable Adversarial Attacks on Vision Transformers with Token Gradient Regularization. (98%)](http://arxiv.org/abs/2303.15754)

Jianping Zhang, Yizhan Huang, Weibin Wu, Michael R. Lyu


[Denoising Autoencoder-based Defensive Distillation as an Adversarial Robustness Algorithm. (98%)](http://arxiv.org/abs/2303.15901)

Bakary Badjie, José Cecílio, António Casimiro


[TransAudio: Towards the Transferable Adversarial Audio Attack via Learning Contextualized Perturbations. (98%)](http://arxiv.org/abs/2303.15940)

Qi Gege, Yuefeng Chen, Xiaofeng Mao, Yao Zhu, Binyuan Hui, Xiaodan Li, Rong Zhang, Hui Xue


[A Survey on Malware Detection with Graph Representation Learning. (41%)](http://arxiv.org/abs/2303.16004)

Tristan Bilot, Nour El Madhoun, Khaldoun Al Agha, Anis Zouaoui


[Provable Robustness for Streaming Models with a Sliding Window. (15%)](http://arxiv.org/abs/2303.16308)

Aounon Kumar, Vinu Sankar Sadasivan, Soheil Feizi


[Machine-learned Adversarial Attacks against Fault Prediction Systems in Smart Electrical Grids. (9%)](http://arxiv.org/abs/2303.18136)

Carmelo Ardito, Yashar Deldjoo, Noia Tommaso Di, Sciascio Eugenio Di, Fatemeh Nazary, Giovanni Servedio


[On the Use of Reinforcement Learning for Attacking and Defending Load Frequency Control. (3%)](http://arxiv.org/abs/2303.15736)

Amr S. Mohamed, Deepa Kundur


[A Universal Identity Backdoor Attack against Speaker Verification based on Siamese Network. (1%)](http://arxiv.org/abs/2303.16031)

Haodong Zhao, Wei Du, Junjie Guo, Gongshen Liu


## 2023-03-27

[Classifier Robustness Enhancement Via Test-Time Transformation. (99%)](http://arxiv.org/abs/2303.15409)

Tsachi Blau, Roy Ganz, Chaim Baskin, Michael Elad, Alex Bronstein


[Improving the Transferability of Adversarial Examples via Direction Tuning. (99%)](http://arxiv.org/abs/2303.15109)

Xiangyuan Yang, Jie Lin, Hanlin Zhang, Xinyu Yang, Peng Zhao


[EMShepherd: Detecting Adversarial Samples via Side-channel Leakage. (99%)](http://arxiv.org/abs/2303.15571)

Ruyi Ding, Cheng Gongye, Siyue Wang, Aidong Ding, Yunsi Fei


[Learning the Unlearnable: Adversarial Augmentations Suppress Unlearnable Example Attacks. (97%)](http://arxiv.org/abs/2303.15127)

Tianrui Qin, Xitong Gao, Juanjuan Zhao, Kejiang Ye, Cheng-Zhong Xu


[Detecting Backdoors During the Inference Stage Based on Corruption Robustness Consistency. (76%)](http://arxiv.org/abs/2303.18191)

Xiaogeng Liu, Minghui Li, Haoyu Wang, Shengshan Hu, Dengpan Ye, Hai Jin, Libing Wu, Chaowei Xiao


[CAT:Collaborative Adversarial Training. (69%)](http://arxiv.org/abs/2303.14922)

Xingbin Liu, Huafeng Kuang, Xianming Lin, Yongjian Wu, Rongrong Ji


[Diffusion Denoised Smoothing for Certified and Adversarial Robust Out-Of-Distribution Detection. (67%)](http://arxiv.org/abs/2303.14961)

Nicola Franco, Daniel Korth, Jeanette Miriam Lorenz, Karsten Roscher, Stephan Guennemann


[Personalized Federated Learning on Long-Tailed Data via Adversarial Feature Augmentation. (41%)](http://arxiv.org/abs/2303.15168)

Yang Lu, Pinxin Qian, Gang Huang, Hanzi Wang


[Mask and Restore: Blind Backdoor Defense at Test Time with Masked Autoencoder. (41%)](http://arxiv.org/abs/2303.15564)

Tao Sun, Lu Pang, Chao Chen, Haibin Ling


[Sequential training of GANs against GAN-classifiers reveals correlated "knowledge gaps" present among independently trained GAN instances. (41%)](http://arxiv.org/abs/2303.15533)

Arkanath Pathak, Nicholas Dufour


[Anti-DreamBooth: Protecting users from personalized text-to-image synthesis. (5%)](http://arxiv.org/abs/2303.15433)

Le Thanh Van, Hao Phung, Thuan Hoang Nguyen, Quan Dao, Ngoc Tran, Anh Tran


## 2023-03-26

[MGTBench: Benchmarking Machine-Generated Text Detection. (26%)](http://arxiv.org/abs/2303.14822)

Xinlei He, Xinyue Shen, Zeyuan Chen, Michael Backes, Yang Zhang


## 2023-03-25

[AdvCheck: Characterizing Adversarial Examples via Local Gradient Checking. (99%)](http://arxiv.org/abs/2303.18131)

Ruoxi Chen, Haibo Jin, Jinyin Chen, Haibin Zheng


[CFA: Class-wise Calibrated Fair Adversarial Training. (98%)](http://arxiv.org/abs/2303.14460)

Zeming Wei, Yifei Wang, Yiwen Guo, Yisen Wang


[PORE: Provably Robust Recommender Systems against Data Poisoning Attacks. (68%)](http://arxiv.org/abs/2303.14601)

Jinyuan Jia, Yupei Liu, Yuepeng Hu, Neil Zhenqiang Gong


[Improving robustness of jet tagging algorithms with adversarial training: exploring the loss surface. (12%)](http://arxiv.org/abs/2303.14511)

Annika Stein


## 2023-03-24

[PIAT: Parameter Interpolation based Adversarial Training for Image Classification. (99%)](http://arxiv.org/abs/2303.13955)

Kun He, Xin Liu, Yichen Yang, Zhou Qin, Weigao Wen, Hui Xue, John E. Hopcroft


[How many dimensions are required to find an adversarial example? (99%)](http://arxiv.org/abs/2303.14173)

Charles Godfrey, Henry Kvinge, Elise Bishoff, Myles Mckay, Davis Brown, Tim Doster, Eleanor Byler


[Effective black box adversarial attack with handcrafted kernels. (99%)](http://arxiv.org/abs/2303.13887)

Petr Dvořáček, Petr Hurtik, Petra Števuliáková


[Adversarial Attack and Defense for Medical Image Analysis: Methods and Applications. (99%)](http://arxiv.org/abs/2303.14133)

Junhao Dong, Junxi Chen, Xiaohua Xie, Jianhuang Lai, Hao Chen


[Improved Adversarial Training Through Adaptive Instance-wise Loss Smoothing. (99%)](http://arxiv.org/abs/2303.14077)

Lin Li, Michael Spratling


[Feature Separation and Recalibration for Adversarial Robustness. (98%)](http://arxiv.org/abs/2303.13846)

Woo Jae Kim, Yoonki Cho, Junsik Jung, Sung-Eui Yoon


[Physically Adversarial Infrared Patches with Learnable Shapes and Locations. (97%)](http://arxiv.org/abs/2303.13868)

Wei Xingxing, Yu Jie, Huang Yao


[Generalist: Decoupling Natural and Robust Generalization. (96%)](http://arxiv.org/abs/2303.13813)

Hongjun Wang, Yisen Wang


[Ensemble-based Blackbox Attacks on Dense Prediction. (86%)](http://arxiv.org/abs/2303.14304)

Zikui Cai, Yaoteng Tan, M. Salman Asif


[Backdoor Attacks with Input-unique Triggers in NLP. (54%)](http://arxiv.org/abs/2303.14325)

Xukun Zhou, Jiwei Li, Tianwei Zhang, Lingjuan Lyu, Muqiao Yang, Jun He


[PoisonedGNN: Backdoor Attack on Graph Neural Networks-based Hardware Security Systems. (22%)](http://arxiv.org/abs/2303.14009)

Lilas Alrahis, Satwik Patnaik, Muhammad Abdullah Hanif, Muhammad Shafique, Ozgur Sinanoglu


[Enhancing Multiple Reliability Measures via Nuisance-extended Information Bottleneck. (5%)](http://arxiv.org/abs/2303.14096)

Jongheon Jeong, Sihyun Yu, Hankook Lee, Jinwoo Shin


[Optimal Smoothing Distribution Exploration for Backdoor Neutralization in Deep Learning-based Traffic Systems. (2%)](http://arxiv.org/abs/2303.14197)

Yue Wang, Wending Li, Michail Maniatakos, Saif Eddin Jabari


[TRAK: Attributing Model Behavior at Scale. (1%)](http://arxiv.org/abs/2303.14186)

Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, Aleksander Madry


## 2023-03-23

[Watch Out for the Confusing Faces: Detecting Face Swapping with the Probability Distribution of Face Identification Models. (68%)](http://arxiv.org/abs/2303.13131)

Yuxuan Duan, Xuhong Zhang, Chuer Yu, Zonghui Wang, Shouling Ji, Wenzhi Chen


[Quadratic Graph Attention Network (Q-GAT) for Robust Construction of Gene Regulatory Networks. (50%)](http://arxiv.org/abs/2303.14193)

Hui Zhang, Xuexin An, Qiang He, Yudong Yao, Feng-Lei Fan, Yueyang Teng


[Optimization and Optimizers for Adversarial Robustness. (41%)](http://arxiv.org/abs/2303.13401)

Hengyue Liang, Buyun Liang, Le Peng, Ying Cui, Tim Mitchell, Ju Sun


[Adversarial Robustness and Feature Impact Analysis for Driver Drowsiness Detection. (41%)](http://arxiv.org/abs/2303.13649)

João Vitorino, Lourenço Rodrigues, Eva Maia, Isabel Praça, André Lourenço


[Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense. (15%)](http://arxiv.org/abs/2303.13408)

Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, Mohit Iyyer


[Decentralized Adversarial Training over Graphs. (13%)](http://arxiv.org/abs/2303.13326)

Ying Cao, Elsa Rizk, Stefan Vlaski, Ali H. Sayed


[Don't FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor Poisoned Samples in DNNs. (8%)](http://arxiv.org/abs/2303.13211)

Hasan Abed Al Kader Hammoud, Adel Bibi, Philip H. S. Torr, Bernard Ghanem


[Low-frequency Image Deep Steganography: Manipulate the Frequency Distribution to Hide Secrets with Tenacious Robustness. (1%)](http://arxiv.org/abs/2303.13713)

Huajie Chen, Tianqing Zhu, Yuan Zhao, Bo Liu, Xin Yu, Wanlei Zhou


[Efficient Symbolic Reasoning for Neural-Network Verification. (1%)](http://arxiv.org/abs/2303.13588)

Zi Dj Wang, Somesh Dj Jha, Dj Krishnamurthy, Dvijotham


## 2023-03-22

[Reliable and Efficient Evaluation of Adversarial Robustness for Deep Hashing-Based Retrieval. (99%)](http://arxiv.org/abs/2303.12658)

Xunguang Wang, Jiawang Bai, Xinyue Xu, Xiaomeng Li


[Semantic Image Attack for Visual Model Diagnosis. (99%)](http://arxiv.org/abs/2303.13010)

Jinqi Luo, Zhaoning Wang, Chen Henry Wu, Dong Huang, la Torre Fernando De


[Revisiting DeepFool: generalization and improvement. (99%)](http://arxiv.org/abs/2303.12481)

Alireza Abdollahpourrostam, Mahed Abroshan, Seyed-Mohsen Moosavi-Dezfooli


[Wasserstein Adversarial Examples on Univariant Time Series Data. (99%)](http://arxiv.org/abs/2303.12357)

Wenjie Wang, Li Xiong, Jian Lou


[Test-time Defense against Adversarial Attacks: Detection and Reconstruction of Adversarial Examples via Masked Autoencoder. (99%)](http://arxiv.org/abs/2303.12848)

Yun-Yun Tsai, Ju-Chin Chao, Albert Wen, Zhaoyuan Yang, Chengzhi Mao, Tapan Shah, Junfeng Yang


[Sibling-Attack: Rethinking Transferable Adversarial Attacks against Face Recognition. (78%)](http://arxiv.org/abs/2303.12512)

Zexin Li, Bangjie Yin, Taiping Yao, Juefeng Guo, Shouhong Ding, Simin Chen, Cong Liu


[An Extended Study of Human-like Behavior under Adversarial Training. (76%)](http://arxiv.org/abs/2303.12669)

Paul Gavrikov, Janis Keuper, Margret Keuper


[Distribution-restrained Softmax Loss for the Model Robustness. (38%)](http://arxiv.org/abs/2303.12363)

Hao Wang, Chen Li, Jinzhe Jiang, Xin Zhang, Yaqian Zhao, Weifeng Gong


[Backdoor Defense via Adaptively Splitting Poisoned Dataset. (16%)](http://arxiv.org/abs/2303.12993)

Kuofeng Gao, Yang Bai, Jindong Gu, Yong Yang, Shu-Tao Xia


[Edge Deep Learning Model Protection via Neuron Authorization. (11%)](http://arxiv.org/abs/2303.12397)

Jinyin Chen, Haibin Zheng, Tao Liu, Rongchang Li, Yao Cheng, Xuhong Zhang, Shouling Ji


## 2023-03-21

[State-of-the-art optical-based physical adversarial attacks for deep learning computer vision systems. (99%)](http://arxiv.org/abs/2303.12249)

Junbin Fang, You Jiang, Canjian Jiang, Zoe L. Jiang, Siu-Ming Yiu, Chuanyi Liu


[Information-containing Adversarial Perturbation for Combating Facial Manipulation Systems. (99%)](http://arxiv.org/abs/2303.11625)

Yao Zhu, Yuefeng Chen, Xiaodan Li, Rong Zhang, Xiang Tian, Bolun Zheng, Yaowu Chen


[OTJR: Optimal Transport Meets Optimal Jacobian Regularization for Adversarial Robustness. (99%)](http://arxiv.org/abs/2303.11793)

Binh M. Le, Shahroz Tariq, Simon S. Woo


[Efficient Decision-based Black-box Patch Attacks on Video Recognition. (98%)](http://arxiv.org/abs/2303.11917)

Kaixun Jiang, Zhaoyu Chen, Hao Huang, Jiafeng Wang, Dingkang Yang, Bo Li, Yan Wang, Wenqiang Zhang


[Black-box Backdoor Defense via Zero-shot Image Purification. (86%)](http://arxiv.org/abs/2303.12175)

Yucheng Shi, Mengnan Du, Xuansheng Wu, Zihan Guan, Ninghao Liu


[Model Robustness Meets Data Privacy: Adversarial Robustness Distillation without Original Data. (10%)](http://arxiv.org/abs/2303.11611)

Yuzheng Wang, Zhaoyu Chen, Dingkang Yang, Pinxue Guo, Kaixun Jiang, Wenqiang Zhang, Lizhe Qi


[Influencer Backdoor Attack on Semantic Segmentation. (8%)](http://arxiv.org/abs/2303.12054)

Haoheng Lan, Jindong Gu, Philip Torr, Hengshuang Zhao


[Poisoning Attacks in Federated Edge Learning for Digital Twin 6G-enabled IoTs: An Anticipatory Study. (1%)](http://arxiv.org/abs/2303.11745)

Mohamed Amine Ferrag, Burak Kantarci, Lucas C. Cordeiro, Merouane Debbah, Kim-Kwang Raymond Choo


## 2023-03-20

[TWINS: A Fine-Tuning Framework for Improved Transferability of Adversarial Robustness and Generalization. (99%)](http://arxiv.org/abs/2303.11135)

Ziquan Liu, Yi Xu, Xiangyang Ji, Antoni B. Chan


[Adversarial Attacks against Binary Similarity Systems. (99%)](http://arxiv.org/abs/2303.11143)

Gianluca Capozzi, Daniele Cono D'Elia, Luna Giuseppe Antonio Di, Leonardo Querzoni


[Adversarial Robustness of Learning-based Static Malware Classifiers. (99%)](http://arxiv.org/abs/2303.13372)

Shoumik Saha, Wenxiao Wang, Yigitcan Kaya, Soheil Feizi, Tudor Dumitras


[Translate your gibberish: black-box adversarial attack on machine translation systems. (83%)](http://arxiv.org/abs/2303.10974)

Andrei Chertkov, Olga Tsymboi, Mikhail Pautov, Ivan Oseledets


[GNN-Ensemble: Towards Random Decision Graph Neural Networks. (56%)](http://arxiv.org/abs/2303.11376)

Wenqi Wei, Mu Qiao, Divyesh Jadav


[Benchmarking Robustness of 3D Object Detection to Common Corruptions in Autonomous Driving. (41%)](http://arxiv.org/abs/2303.11040)

Yinpeng Dong, Caixin Kang, Jinlai Zhang, Zijian Zhu, Yikai Wang, Xiao Yang, Hang Su, Xingxing Wei, Jun Zhu


[Did You Train on My Dataset? Towards Public Dataset Protection with Clean-Label Backdoor Watermarking. (9%)](http://arxiv.org/abs/2303.11470)

Ruixiang Tang, Qizhang Feng, Ninghao Liu, Fan Yang, Xia Hu


[Boosting Semi-Supervised Learning by Exploiting All Unlabeled Data. (2%)](http://arxiv.org/abs/2303.11066)

Yuhao Chen, Xin Tan, Borui Zhao, Zhaowei Chen, Renjie Song, Jiajun Liang, Xuequan Lu


[Make Landscape Flatter in Differentially Private Federated Learning. (1%)](http://arxiv.org/abs/2303.11242)

Yifan Shi, Yingqi Liu, Kang Wei, Li Shen, Xueqian Wang, Dacheng Tao


[Robustifying Token Attention for Vision Transformers. (1%)](http://arxiv.org/abs/2303.11126)

Yong Guo, David Stutz, Bernt Schiele


## 2023-03-19

[Randomized Adversarial Training via Taylor Expansion. (99%)](http://arxiv.org/abs/2303.10653)

Gaojie Jin, Xinping Yi, Dengyu Wu, Ronghui Mu, Xiaowei Huang


[AdaptGuard: Defending Against Universal Attacks for Model Adaptation. (82%)](http://arxiv.org/abs/2303.10594)

Lijun Sheng, Jian Liang, Ran He, Zilei Wang, Tieniu Tan


## 2023-03-18

[NoisyHate: Benchmarking Content Moderation Machine Learning Models with Human-Written Perturbations Online. (98%)](http://arxiv.org/abs/2303.10430)

Yiran Ye, Thai Le, Dongwon Lee


[FedRight: An Effective Model Copyright Protection for Federated Learning. (96%)](http://arxiv.org/abs/2303.10399)

Jinyin Chen, Mingjun Li, Mingjun Li, Haibin Zheng


## 2023-03-17

[Fuzziness-tuned: Improving the Transferability of Adversarial Examples. (99%)](http://arxiv.org/abs/2303.10078)

Xiangyuan Yang, Jie Lin, Hanlin Zhang, Xinyu Yang, Peng Zhao


[It Is All About Data: A Survey on the Effects of Data on Adversarial Robustness. (99%)](http://arxiv.org/abs/2303.09767)

Peiyu Xiong, Michael Tegegn, Jaskeerat Singh Sarin, Shubhraneel Pal, Julia Rubin


[Robust Mode Connectivity-Oriented Adversarial Defense: Enhancing Neural Network Robustness Against Diversified $\ell_p$ Attacks. (99%)](http://arxiv.org/abs/2303.10225)

Ren Wang, Yuxuan Li, Sijia Liu


[Detection of Uncertainty in Exceedance of Threshold (DUET): An Adversarial Patch Localizer. (83%)](http://arxiv.org/abs/2303.10291)

Terence Jie Chua, Wenhan Yu, Jun Zhao


[Can AI-Generated Text be Reliably Detected? (33%)](http://arxiv.org/abs/2303.11156)

Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, Soheil Feizi


[Adversarial Counterfactual Visual Explanations. (31%)](http://arxiv.org/abs/2303.09962)

Guillaume Jeanneret, Loïc Simon, Frédéric Jurie


[MedLocker: A Transferable Adversarial Watermarking for Preventing Unauthorized Analysis of Medical Image Dataset. (16%)](http://arxiv.org/abs/2303.09858)

Bangzheng Pu, Xingxing Wei, Shiji Zhao, Huazhu Fu


[Mobile Edge Adversarial Detection for Digital Twinning to the Metaverse with Deep Reinforcement Learning. (9%)](http://arxiv.org/abs/2303.10288)

Terence Jie Chua, Wenhan Yu, Jun Zhao


[Moving Target Defense for Service-oriented Mission-critical Networks. (1%)](http://arxiv.org/abs/2303.09893)

Doğanalp Ergenç, Florian Schneider, Peter Kling, Mathias Fischer


## 2023-03-16

[Rethinking Model Ensemble in Transfer-based Adversarial Attacks. (99%)](http://arxiv.org/abs/2303.09105)

Huanran Chen, Yichi Zhang, Yinpeng Dong, Jun Zhu


[Class Attribute Inference Attacks: Inferring Sensitive Class Information by Diffusion-Based Attribute Manipulations. (68%)](http://arxiv.org/abs/2303.09289)

Lukas Struppek, Dominik Hintersdorf, Felix Friedrich, Manuel Brack, Patrick Schramowski, Kristian Kersting


[Among Us: Adversarially Robust Collaborative Perception by Consensus. (67%)](http://arxiv.org/abs/2303.09495)

Yiming Li, Qi Fang, Jiamu Bai, Siheng Chen, Felix Juefei-Xu, Chen Feng


[Exorcising ''Wraith'': Protecting LiDAR-based Object Detector in Automated Driving System from Appearing Attacks. (50%)](http://arxiv.org/abs/2303.09731)

Qifan Xiao, Xudong Pan, Yifan Lu, Mi Zhang, Jiarun Dai, Min Yang


[Rethinking White-Box Watermarks on Deep Learning Models under Neural Structural Obfuscation. (11%)](http://arxiv.org/abs/2303.09732)

Yifan Yan, Xudong Pan, Mi Zhang, Min Yang


## 2023-03-15

[Black-box Adversarial Example Attack towards FCG Based Android Malware Detection under Incomplete Feature Information. (99%)](http://arxiv.org/abs/2303.08509)

Heng Li, Zhang Cheng, Bang Wu, Liheng Yuan, Cuiying Gao, Wei Yuan, Xiapu Luo


[Robust Evaluation of Diffusion-Based Adversarial Purification. (83%)](http://arxiv.org/abs/2303.09051)

Minjong Lee, Dongwoo Kim


[DeeBBAA: A benchmark Deep Black Box Adversarial Attack against Cyber-Physical Power Systems. (81%)](http://arxiv.org/abs/2303.09024)

Arnab Bhattacharjee, Tapan K. Saha, Ashu Verma, Sukumar Mishra


[The Devil's Advocate: Shattering the Illusion of Unexploitable Data using Diffusion Models. (62%)](http://arxiv.org/abs/2303.08500)

Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie


[EvalAttAI: A Holistic Approach to Evaluating Attribution Maps in Robust and Non-Robust Models. (45%)](http://arxiv.org/abs/2303.08866)

Ian E. Nielsen, Ravi P. Ramachandran, Nidhal Bouaynaya, Hassan M. Fathallah-Shaykh, Ghulam Rasool


[Certifiable (Multi)Robustness Against Patch Attacks Using ERM. (10%)](http://arxiv.org/abs/2303.08944)

Saba Ahmadi, Avrim Blum, Omar Montasser, Kevin Stangl


[Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement. (1%)](http://arxiv.org/abs/2303.08983)

Fartash Faghri, Hadi Pouransari, Sachin Mehta, Mehrdad Farajtabar, Ali Farhadi, Mohammad Rastegari, Oncel Tuzel


## 2023-03-14

[Verifying the Robustness of Automatic Credibility Assessment. (99%)](http://arxiv.org/abs/2303.08032)

Piotr Przybyła, Alexander Shvets, Horacio Saggion


[Resilient Dynamic Average Consensus based on Trusted agents. (69%)](http://arxiv.org/abs/2303.08171)

Shamik Bhattacharyya, Rachel Kalpana Kalaimani


[Improving Adversarial Robustness with Hypersphere Embedding and Angular-based Regularizations. (31%)](http://arxiv.org/abs/2303.08289)

Olukorede Fakorede, Ashutosh Nirala, Modeste Atsague, Jin Tian


## 2023-03-13

[Constrained Adversarial Learning and its applicability to Automated Software Testing: a systematic review. (99%)](http://arxiv.org/abs/2303.07546)

João Vitorino, Tiago Dias, Tiago Fonseca, Eva Maia, Isabel Praça


[Can Adversarial Examples Be Parsed to Reveal Victim Model Information? (99%)](http://arxiv.org/abs/2303.07474)

Yuguang Yao, Jiancheng Liu, Yifan Gong, Xiaoming Liu, Yanzhi Wang, Xue Lin, Sijia Liu


[Review on the Feasibility of Adversarial Evasion Attacks and Defenses for Network Intrusion Detection Systems. (99%)](http://arxiv.org/abs/2303.07003)

Islam Debicha, Benjamin Cochez, Tayeb Kenaza, Thibault Debatty, Jean-Michel Dricot, Wim Mees


[SMUG: Towards robust MRI reconstruction by smoothed unrolling. (98%)](http://arxiv.org/abs/2303.12735)

Hui Li, Jinghan Jia, Shijun Liang, Yuguang Yao, Saiprasad Ravishankar, Sijia Liu


[Model-tuning Via Prompts Makes NLP Models Adversarially Robust. (93%)](http://arxiv.org/abs/2303.07320)

Mrigank Raman, Pratyush Maini, J. Zico Kolter, Zachary C. Lipton, Danish Pruthi


[Robust Contrastive Language-Image Pretraining against Adversarial Attacks. (76%)](http://arxiv.org/abs/2303.06854)

Wenhan Yang, Baharan Mirzasoleiman


[Model Extraction Attacks on Split Federated Learning. (47%)](http://arxiv.org/abs/2303.08581)

Jingtao Li, Adnan Siraj Rakin, Xing Chen, Li Yang, Zhezhi He, Deliang Fan, Chaitali Chakrabarti


[WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminative Analysis. (1%)](http://arxiv.org/abs/2303.07543)

Yiye Chen, Yunzhi Lin, Ruinian Xu, Patricio A. Vela


## 2023-03-12

[Adv-Bot: Realistic Adversarial Botnet Attacks against Network Intrusion Detection Systems. (99%)](http://arxiv.org/abs/2303.06664)

Islam Debicha, Benjamin Cochez, Tayeb Kenaza, Thibault Debatty, Jean-Michel Dricot, Wim Mees


[Adaptive Local Adversarial Attacks on 3D Point Clouds for Augmented Reality. (99%)](http://arxiv.org/abs/2303.06641)

Weiquan Liu, Shijun Zheng, Cheng Wang


[DNN-Alias: Deep Neural Network Protection Against Side-Channel Attacks via Layer Balancing. (96%)](http://arxiv.org/abs/2303.06746)

Mahya Morid Ahmadi, Lilas Alrahis, Ozgur Sinanoglu, Muhammad Shafique


[Multi-metrics adaptively identifies backdoors in Federated learning. (92%)](http://arxiv.org/abs/2303.06601)

Siquan Huang, Yijiang Li, Chong Chen, Leyu Shi, Ying Gao


[Adversarial Attacks to Direct Data-driven Control for Destabilization. (91%)](http://arxiv.org/abs/2303.06837)

Hampei Sasahara


[Backdoor Defense via Deconfounded Representation Learning. (83%)](http://arxiv.org/abs/2303.06818)

Zaixi Zhang, Qi Liu, Zhicai Wang, Zepu Lu, Qingyong Hu


[Interpreting Hidden Semantics in the Intermediate Layers of 3D Point Cloud Classification Neural Network. (76%)](http://arxiv.org/abs/2303.06652)

Weiquan Liu, Minghao Liu, Shijun Zheng, Cheng Wang


[Boosting Source Code Learning with Data Augmentation: An Empirical Study. (11%)](http://arxiv.org/abs/2303.06808)

Zeming Dong, Qiang Hu, Yuejun Guo, Zhenya Zhang, Maxime Cordy, Mike Papadakis, Yves Le Traon, Jianjun Zhao


## 2023-03-11

[Improving the Robustness of Deep Convolutional Neural Networks Through Feature Learning. (99%)](http://arxiv.org/abs/2303.06425)

Jin Ding, Jie-Chao Zhao, Yong-Zhi Sun, Ping Tan, Ji-En Ma, You-Tong Fang


[SHIELD: An Adaptive and Lightweight Defense against the Remote Power Side-Channel Attacks on Multi-tenant FPGAs. (8%)](http://arxiv.org/abs/2303.06486)

Mahya Morid Ahmadi, Faiq Khalid, Radha Vaidya, Florian Kriebel, Andreas Steininger, Muhammad Shafique


## 2023-03-10

[Turning Strengths into Weaknesses: A Certified Robustness Inspired Attack Framework against Graph Neural Networks. (99%)](http://arxiv.org/abs/2303.06199)

Binghui Wang, Meng Pang, Yun Dong


[Boosting Adversarial Attacks by Leveraging Decision Boundary Information. (99%)](http://arxiv.org/abs/2303.05719)

Boheng Zeng, LianLi Gao, QiLong Zhang, ChaoQun Li, JingKuan Song, ShuaiQi Jing


[Adversarial Attacks and Defenses in Machine Learning-Powered Networks: A Contemporary Survey. (99%)](http://arxiv.org/abs/2303.06302)

Yulong Wang, Tong Sun, Shenghong Li, Xin Yuan, Wei Ni, Ekram Hossain, H. Vincent Poor


[Investigating Stateful Defenses Against Black-Box Adversarial Examples. (99%)](http://arxiv.org/abs/2303.06280)

Ryan Feng, Ashish Hooda, Neal Mangaokar, Kassem Fawaz, Somesh Jha, Atul Prakash


[MIXPGD: Hybrid Adversarial Training for Speech Recognition Systems. (99%)](http://arxiv.org/abs/2303.05758)

Aminul Huq, Weiyi Zhang, Xiaolin Hu


[Do we need entire training data for adversarial training? (99%)](http://arxiv.org/abs/2303.06241)

Vipul Gupta, Apurva Narayan


[TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets. (61%)](http://arxiv.org/abs/2303.05762)

Weixin Chen, Dawn Song, Bo Li


## 2023-03-09

[NoiseCAM: Explainable AI for the Boundary Between Noise and Adversarial Attacks. (99%)](http://arxiv.org/abs/2303.06151)

Wenkai Tan, Justus Renkhoff, Alvaro Velasquez, Ziyu Wang, Lusi Li, Jian Wang, Shuteng Niu, Fan Yang, Yongxin Liu, Houbing Song


[Evaluating the Robustness of Conversational Recommender Systems by Adversarial Examples. (92%)](http://arxiv.org/abs/2303.05575)

Ali Montazeralghaem, James Allan


[Identification of Systematic Errors of Image Classifiers on Rare Subgroups. (83%)](http://arxiv.org/abs/2303.05072)

Jan Hendrik Metzen, Robin Hutmacher, N. Grace Hua, Valentyn Boreiko, Dan Zhang


[Learning the Legibility of Visual Text Perturbations. (78%)](http://arxiv.org/abs/2303.05077)

Dev Seth, Rickard Stureborg, Danish Pruthi, Bhuwan Dhingra


[Efficient Certified Training and Robustness Verification of Neural ODEs. (75%)](http://arxiv.org/abs/2303.05246)

Mustafa Zeqiri, Mark Niklas Müller, Marc Fischer, Martin Vechev


[Feature Unlearning for Pre-trained GANs and VAEs. (62%)](http://arxiv.org/abs/2303.05699)

Saemi Moon, Seunghyuk Cho, Dongwoo Kim


## 2023-03-08

[Immune Defense: A Novel Adversarial Defense Mechanism for Preventing the Generation of Adversarial Examples. (99%)](http://arxiv.org/abs/2303.04502)

Jinwei Wang, Hao Wu, Haihua Wang, Jiawei Zhang, Xiangyang Luo, Bin Ma


[Decision-BADGE: Decision-based Adversarial Batch Attack with Directional Gradient Estimation. (99%)](http://arxiv.org/abs/2303.04980)

Geunhyeok Yu, Minwoo Jeon, Hyoseok Hwang


[Exploring Adversarial Attacks on Neural Networks: An Explainable Approach. (99%)](http://arxiv.org/abs/2303.06032)

Justus Renkhoff, Wenkai Tan, Alvaro Velasquez, illiam Yichen Wang, Yongxin Liu, Jian Wang, Shuteng Niu, Lejla Begic Fazlic, Guido Dartmann, Houbing Song


[BeamAttack: Generating High-quality Textual Adversarial Examples through Beam Search and Mixed Semantic Spaces. (99%)](http://arxiv.org/abs/2303.07199)

Hai Zhu, Qingyang Zhao, Yuren Wu


[DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep Neural Networks. (3%)](http://arxiv.org/abs/2303.04878)

Zohreh Aghababaeyan, Manel Abdellatif, Mahboubeh Dadkhah, Lionel Briand


## 2023-03-07

[Logit Margin Matters: Improving Transferable Targeted Adversarial Attack by Logit Calibration. (99%)](http://arxiv.org/abs/2303.03680)

Juanjuan Weng, Zhiming Luo, Zhun Zhong, Shaozi Li, Nicu Sebe


[Robustness-preserving Lifelong Learning via Dataset Condensation. (96%)](http://arxiv.org/abs/2303.04183)

Jinghan Jia, Yihua Zhang, Dogyoon Song, Sijia Liu, Alfred Hero


[Patch of Invisibility: Naturalistic Black-Box Adversarial Attacks on Object Detectors. (93%)](http://arxiv.org/abs/2303.04238)

Raz Lapid, Moshe Sipper


[CUDA: Convolution-based Unlearnable Datasets. (82%)](http://arxiv.org/abs/2303.04278)

Vinu Sankar Sadasivan, Mahdi Soltanolkotabi, Soheil Feizi


[EavesDroid: Eavesdropping User Behaviors via OS Side-Channels on Smartphones. (11%)](http://arxiv.org/abs/2303.03700)

Quancheng Wang, Ming Tang, Jianming Fu


[Stabilized training of joint energy-based models and their practical applications. (2%)](http://arxiv.org/abs/2303.04187)

Martin Sustek, Samik Sadhu, Lukas Burget, Hynek Hermansky, Jesus Villalba, Laureano Moro-Velazquez, Najim Dehak


## 2023-03-06

[CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning. (41%)](http://arxiv.org/abs/2303.03323)

Hritik Bansal, Nishad Singhi, Yu Yang, Fan Yin, Aditya Grover, Kai-Wei Chang


[Students Parrot Their Teachers: Membership Inference on Model Distillation. (31%)](http://arxiv.org/abs/2303.03446)

Matthew Jagielski, Milad Nasr, Christopher Choquette-Choo, Katherine Lee, Nicholas Carlini


[A Unified Algebraic Perspective on Lipschitz Neural Networks. (15%)](http://arxiv.org/abs/2303.03169)

Alexandre Araujo, Aaron Havens, Blaise Delattre, Alexandre Allauzen, Bin Hu


[Learning to Backdoor Federated Learning. (15%)](http://arxiv.org/abs/2303.03320)

Henger Li, Chen Wu, Sencun Zhu, Zizhan Zheng


[Partial-Information, Longitudinal Cyber Attacks on LiDAR in Autonomous Vehicles. (10%)](http://arxiv.org/abs/2303.03470)

R. Spencer Hallyburton, Qingzhao Zhang, Z. Morley Mao, Miroslav Pajic


[On the Feasibility of Specialized Ability Extracting for Large Language Code Models. (2%)](http://arxiv.org/abs/2303.03012)

Zongjie Li, Chaozheng Wang, Pingchuan Ma, Chaowei Liu, Shuai Wang, Daoyuan Wu, Cuiyun Gao


[ALMOST: Adversarial Learning to Mitigate Oracle-less ML Attacks via Synthesis Tuning. (1%)](http://arxiv.org/abs/2303.03372)

Animesh Basak Chowdhury, Lilas Alrahis, Luca Collini, Johann Knechtel, Ramesh Karri, Siddharth Garg, Ozgur Sinanoglu, Benjamin Tan


[Rethinking Confidence Calibration for Failure Prediction. (1%)](http://arxiv.org/abs/2303.02970)

Fei Zhu, Zhen Cheng, Xu-Yao Zhang, Cheng-Lin Liu


## 2023-03-05

[Consistent Valid Physically-Realizable Adversarial Attack against Crowd-flow Prediction Models. (99%)](http://arxiv.org/abs/2303.02669)

Hassan Ali, Muhammad Atif Butt, Fethi Filali, Ala Al-Fuqaha, Junaid Qadir


[Visual Analytics of Neuron Vulnerability to Adversarial Attacks on Convolutional Neural Networks. (99%)](http://arxiv.org/abs/2303.02814)

Yiran Li, Junpeng Wang, Takanori Fujiwara, Kwan-Liu Ma


[Adversarial Sampling for Fairness Testing in Deep Neural Network. (98%)](http://arxiv.org/abs/2303.02874)

Tosin Ige, William Marfo, Justin Tonkinson, Sikiru Adewale, Bolanle Hafiz Matti


[Local Environment Poisoning Attacks on Federated Reinforcement Learning. (12%)](http://arxiv.org/abs/2303.02725)

Evelyn Ma, Rasoul Etesami


[Robustness, Evaluation and Adaptation of Machine Learning Models in the Wild. (10%)](http://arxiv.org/abs/2303.02781)

Vihari Piratla


[Knowledge-Based Counterfactual Queries for Visual Question Answering. (3%)](http://arxiv.org/abs/2303.02601)

Theodoti Stoikou, Maria Lymperaiou, Giorgos Stamou


## 2023-03-04

[Improved Robustness Against Adaptive Attacks With Ensembles and Error-Correcting Output Codes. (68%)](http://arxiv.org/abs/2303.02322)

Thomas Philippon, Christian Gagné


## 2023-03-03

[PointCert: Point Cloud Classification with Deterministic Certified Robustness Guarantees. (91%)](http://arxiv.org/abs/2303.01959)

Jinghuai Zhang, Jinyuan Jia, Hongbin Liu, Neil Zhenqiang Gong


[Certified Robust Neural Networks: Generalization and Corruption Resistance. (82%)](http://arxiv.org/abs/2303.02251)

Amine Bennouna, Ryan Lucas, Parys Bart Van


[Backdoor Attacks and Defenses in Federated Learning: Survey, Challenges and Future Research Directions. (47%)](http://arxiv.org/abs/2303.02213)

Thuy Dung Nguyen, Tuan Nguyen, Phi Le Nguyen, Hieu H. Pham, Khoa Doan, Kok-Seng Wong


[Adversarial Attacks on Machine Learning in Embedded and IoT Platforms. (38%)](http://arxiv.org/abs/2303.02214)

Christian Westbrook, Sudeep Pasricha


[Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models. (33%)](http://arxiv.org/abs/2303.01870)

Naman D Singh, Francesco Croce, Matthias Hein


[Stealthy Perception-based Attacks on Unmanned Aerial Vehicles. (16%)](http://arxiv.org/abs/2303.02112)

Amir Khazraei, Haocheng Meng, Miroslav Pajic


[AdvART: Adversarial Art for Camouflaged Object Detection Attacks. (15%)](http://arxiv.org/abs/2303.01734)

Amira Guesmi, Ioan Marius Bilasco, Muhammad Shafique, Ihsen Alouani


[TrojText: Test-time Invisible Textual Trojan Insertion. (2%)](http://arxiv.org/abs/2303.02242)

Qian Lou, Yepeng Liu, Bo Feng


## 2023-03-02

[Defending against Adversarial Audio via Diffusion Model. (99%)](http://arxiv.org/abs/2303.01507)

Shutong Wu, Jiongxiao Wang, Wei Ping, Weili Nie, Chaowei Xiao


[Demystifying Causal Features on Adversarial Examples and Causal Inoculation for Robust Network by Adversarial Instrumental Variable Regression. (99%)](http://arxiv.org/abs/2303.01052)

Junho Kim. Byung-Kwan Lee, Yong Man Ro


[APARATE: Adaptive Adversarial Patch for CNN-based Monocular Depth Estimation for Autonomous Navigation. (99%)](http://arxiv.org/abs/2303.01351)

Amira Guesmi, Muhammad Abdullah Hanif, Ihsen Alouani, Muhammad Shafique


[AdvRain: Adversarial Raindrops to Attack Camera-based Smart Vision Systems. (99%)](http://arxiv.org/abs/2303.01338)

Amira Guesmi, Muhammad Abdullah Hanif, Muhammad Shafique


[Targeted Adversarial Attacks against Neural Machine Translation. (98%)](http://arxiv.org/abs/2303.01068)

Sahar Sadrizadeh, AmirHossein Dabiri Aghdam, Ljiljana Dolamic, Pascal Frossard


[The Double-Edged Sword of Implicit Bias: Generalization vs. Robustness in ReLU Networks. (93%)](http://arxiv.org/abs/2303.01456)

Spencer Frei, Gal Vardi, Peter L. Bartlett, Nathan Srebro


[Feature Perturbation Augmentation for Reliable Evaluation of Importance Estimators. (10%)](http://arxiv.org/abs/2303.01538)

Lennart Brocki, Neo Christopher Chung


[D-Score: An Expert-Based Method for Assessing the Detectability of IoT-Related Cyber-Attacks. (3%)](http://arxiv.org/abs/2303.01041)

Yair Meidan, Daniel Benatar, Ron Bitton, Dan Avraham, Asaf Shabtai


[Interpretable System Identification and Long-term Prediction on Time-Series Data. (1%)](http://arxiv.org/abs/2303.01193)

Xiaoyi Liu, Duxin Chen, Wenjia Wei, Xia Zhu, Wenwu Yu


[Consistency Models. (1%)](http://arxiv.org/abs/2303.01469)

Yang Song, Prafulla Dhariwal, Mark Chen, Ilya Sutskever


[CADeSH: Collaborative Anomaly Detection for Smart Homes. (1%)](http://arxiv.org/abs/2303.01021)

Yair Meidan, Dan Avraham, Hanan Libhaber, Asaf Shabtai


[Conflict-Based Cross-View Consistency for Semi-Supervised Semantic Segmentation. (1%)](http://arxiv.org/abs/2303.01276)

Zicheng Wang, Zhen Zhao, Xiaoxia Xing, Dong Xu, Xiangyu Kong, Luping Zhou


## 2023-03-01

[To Make Yourself Invisible with Adversarial Semantic Contours. (99%)](http://arxiv.org/abs/2303.00284)

Yichi Zhang, Zijian Zhu, Hang Su, Jun Zhu, Shibao Zheng, Yuan He, Hui Xue


[Adversarial Examples Exist in Two-Layer ReLU Networks for Low Dimensional Data Manifolds. (98%)](http://arxiv.org/abs/2303.00783)

Odelia Melamed, Gilad Yehudai, Gal Vardi


[Frauds Bargain Attack: Generating Adversarial Text Samples via Word Manipulation Process. (95%)](http://arxiv.org/abs/2303.01234)

Mingze Ni, Zhensu Sun, Wei Liu


[A Practical Upper Bound for the Worst-Case Attribution Deviations. (70%)](http://arxiv.org/abs/2303.00340)

Fan Wang, Adams Wai-Kin Kong


[Combating Exacerbated Heterogeneity for Robust Models in Federated Learning. (54%)](http://arxiv.org/abs/2303.00250)

Jianing Zhu, Jiangchao Yao, Tongliang Liu, Quanming Yao, Jianliang Xu, Bo Han


[Poster: Sponge ML Model Attacks of Mobile Apps. (8%)](http://arxiv.org/abs/2303.01243)

Souvik Paul, Nicolas Kourtellis


[DOLOS: A Novel Architecture for Moving Target Defense. (8%)](http://arxiv.org/abs/2303.00387)

Giulio Pagnotta, Gaspari Fabio De, Dorjan Hitaj, Mauro Andreolini, Michele Colajanni, Luigi V. Mancini


[Mitigating Backdoors in Federated Learning with FLD. (2%)](http://arxiv.org/abs/2303.00302)

Yihang Lin, Pengyuan Zhou, Zhiqian Wu, Yong Liao


[Competence-Based Analysis of Language Models. (1%)](http://arxiv.org/abs/2303.00333)

Adam Davies, Jize Jiang, ChengXiang Zhai


## 2023-02-28

[A semantic backdoor attack against Graph Convolutional Networks. (98%)](http://arxiv.org/abs/2302.14353)

Jiazhu Dai, Zhipeng Xiong


[Feature Extraction Matters More: Universal Deepfake Disruption through Attacking Ensemble Feature Extractors. (67%)](http://arxiv.org/abs/2303.00200)

Long Tang, Dengpan Ye, Zhenhao Lu, Yunming Zhang, Shengshan Hu, Yue Xu, Chuanxi Chen


[Single Image Backdoor Inversion via Robust Smoothed Classifiers. (22%)](http://arxiv.org/abs/2303.00215)

Mingjie Sun, Zico Kolter


[Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger. (11%)](http://arxiv.org/abs/2302.14677)

Yi Yu, Yufei Wang, Wenhan Yang, Shijian Lu, Yap-peng Tan, Alex C. Kot


[FreeEagle: Detecting Complex Neural Trojans in Data-Free Cases. (1%)](http://arxiv.org/abs/2302.14500)

Chong Fu, Xuhong Zhang, Shouling Ji, Ting Wang, Peng Lin, Yanghe Feng, Jianwei Yin


## 2023-02-27

[A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking. (99%)](http://arxiv.org/abs/2302.14301)

Chang Liu, Yinpeng Dong, Wenzhao Xiang, Xiao Yang, Hang Su, Jun Zhu, Yuefeng Chen, Yuan He, Hui Xue, Shibao Zheng


[Adversarial Attack with Raindrops. (99%)](http://arxiv.org/abs/2302.14267)

Jiyuan Liu, Bingyi Lu, Mingkang Xiong, Tao Zhang, Huilin Xiong


[Physical Adversarial Attacks on Deep Neural Networks for Traffic Sign Recognition: A Feasibility Study. (99%)](http://arxiv.org/abs/2302.13570)

Fabian Woitschek, Georg Schneider


[Aegis: Mitigating Targeted Bit-flip Attacks against Deep Neural Networks. (98%)](http://arxiv.org/abs/2302.13520)

Jialai Wang, Ziyuan Zhang, Meiqi Wang, Han Qiu, Tianwei Zhang, Qi Li, Zongpeng Li, Tao Wei, Chao Zhang


[CBA: Contextual Background Attack against Optical Aerial Detection in the Physical World. (98%)](http://arxiv.org/abs/2302.13519)

Jiawei Lian, Xiaofei Wang, Yuru Su, Mingyang Ma, Shaohui Mei


[Improving Model Generalization by On-manifold Adversarial Augmentation in the Frequency Domain. (96%)](http://arxiv.org/abs/2302.14302)

Chang Liu, Wenzhao Xiang, Yuan He, Hui Xue, Shibao Zheng, Hang Su


[Efficient and Low Overhead Website Fingerprinting Attacks and Defenses based on TCP/IP Traffic. (83%)](http://arxiv.org/abs/2302.13763)

Guodong Huang, Chuan Ma, Ming Ding, Yuwen Qian, Chunpeng Ge, Liming Fang, Zhe Liu


[GLOW: Global Layout Aware Attacks on Object Detection. (81%)](http://arxiv.org/abs/2302.14166)

Buyu Liu, BaoJun, Jianping Fan, Xi Peng, Kui Ren, Jun Yu


[Online Black-Box Confidence Estimation of Deep Neural Networks. (16%)](http://arxiv.org/abs/2302.13578)

Fabian Woitschek, Georg Schneider


[Implicit Poisoning Attacks in Two-Agent Reinforcement Learning: Adversarial Policies for Training-Time Attacks. (15%)](http://arxiv.org/abs/2302.13851)

Mohammad Mohammadi, Jonathan Nöther, Debmalya Mandal, Adish Singla, Goran Radanovic


[Differentially Private Diffusion Models Generate Useful Synthetic Images. (10%)](http://arxiv.org/abs/2302.13861)

Sahra Ghalebikesabi, Leonard Berrada, Sven Gowal, Ira Ktena, Robert Stanforth, Jamie Hayes, Soham De, Samuel L. Smith, Olivia Wiles, Borja Balle


[Learning to Retain while Acquiring: Combating Distribution-Shift in Adversarial Data-Free Knowledge Distillation. (5%)](http://arxiv.org/abs/2302.14290)

Gaurav Patel, Konda Reddy Mopuri, Qiang Qiu


## 2023-02-26

[Contextual adversarial attack against aerial detection in the physical world. (99%)](http://arxiv.org/abs/2302.13487)

Jiawei Lian, Xiaofei Wang, Yuru Su, Mingyang Ma, Shaohui Mei


[Randomness in ML Defenses Helps Persistent Attackers and Hinders Evaluators. (96%)](http://arxiv.org/abs/2302.13464)

Keane Lucas, Matthew Jagielski, Florian Tramèr, Lujo Bauer, Nicholas Carlini


## 2023-02-25

[Deep Learning-based Multi-Organ CT Segmentation with Adversarial Data Augmentation. (99%)](http://arxiv.org/abs/2302.13172)

Shaoyan Pan, Shao-Yuan Lo, Min Huang, Chaoqiong Ma, Jacob Wynne, Tonghe Wang, Tian Liu, Xiaofeng Yang


[Scalable Attribution of Adversarial Attacks via Multi-Task Learning. (99%)](http://arxiv.org/abs/2302.14059)

Zhongyi Guo, Keji Han, Yao Ge, Wei Ji, Yun Li


[SATBA: An Invisible Backdoor Attack Based On Spatial Attention. (67%)](http://arxiv.org/abs/2302.13056)

Huasong Zhou, Xiaowei Xu, Xiaodong Wang, Leon Bevan Bullock


## 2023-02-24

[Defending Against Backdoor Attacks by Layer-wise Feature Analysis. (68%)](http://arxiv.org/abs/2302.12758)

Najeeb Moharram Jebreel, Josep Domingo-Ferrer, Yiming Li


[Chaotic Variational Auto encoder-based Adversarial Machine Learning. (54%)](http://arxiv.org/abs/2302.12959)

Pavan Venkata Sainadh Reddy, Yelleti Vivek, Gopi Pranay, Vadlamani Ravi


[Robust Weight Signatures: Gaining Robustness as Easy as Patching Weights? (12%)](http://arxiv.org/abs/2302.12480)

Ruisi Cai, Zhenyu Zhang, Zhangyang Wang


## 2023-02-23

[Less is More: Data Pruning for Faster Adversarial Training. (99%)](http://arxiv.org/abs/2302.12366)

Yize Li, Pu Zhao, Xue Lin, Bhavya Kailkhura, Ryan Goldhahn


[A Plot is Worth a Thousand Words: Model Information Stealing Attacks via Scientific Plots. (99%)](http://arxiv.org/abs/2302.11982)

Boyang Zhang, Xinlei He, Yun Shen, Tianhao Wang, Yang Zhang


[Boosting Adversarial Transferability using Dynamic Cues. (99%)](http://arxiv.org/abs/2302.12252)

Muzammal Naseer, Ahmad Mahmood, Salman Khan, Fahad Khan


[HyperAttack: Multi-Gradient-Guided White-box Adversarial Structure Attack of Hypergraph Neural Networks. (98%)](http://arxiv.org/abs/2302.12407)

Chao Hu, Ruishi Yu, Binqi Zeng, Yu Zhan, Ying Fu, Quan Zhang, Rongkai Liu, Heyuan Shi


[Investigating Catastrophic Overfitting in Fast Adversarial Training: A Self-fitting Perspective. (84%)](http://arxiv.org/abs/2302.11963)

Zhengbao He, Tao Li, Sizhe Chen, Xiaolin Huang


[More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models. (70%)](http://arxiv.org/abs/2302.12173)

Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, Mario Fritz


[On the Hardness of Robustness Transfer: A Perspective from Rademacher Complexity over Symmetric Difference Hypothesis Space. (68%)](http://arxiv.org/abs/2302.12351)

Yuyang Deng, Nidham Gazagnadou, Junyuan Hong, Mehrdad Mahdavi, Lingjuan Lyu


[Harnessing the Speed and Accuracy of Machine Learning to Advance Cybersecurity. (2%)](http://arxiv.org/abs/2302.12415)

Khatoon Mohammed


## 2023-02-22

[Mitigating Adversarial Attacks in Deepfake Detection: An Exploration of Perturbation and AI Techniques. (98%)](http://arxiv.org/abs/2302.11704)

Saminder Dhesi, Laura Fontes, Pedro Machado, Isibor Kennedy Ihianle, Farhad Fassihi Tash, David Ada Adama


[PAD: Towards Principled Adversarial Malware Detection Against Evasion Attacks. (98%)](http://arxiv.org/abs/2302.11328)

Deqiang Li, Shicheng Cui, Yun Li, Jia Xu, Fu Xiao, Shouhuai Xu


[Feature Partition Aggregation: A Fast Certified Defense Against a Union of Sparse Adversarial Attacks. (97%)](http://arxiv.org/abs/2302.11628)

Zayd Hammoudeh, Daniel Lowd


[ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms. (33%)](http://arxiv.org/abs/2302.11408)

Minzhou Pan, Yi Zeng, Lingjuan Lyu, Xue Lin, Ruoxi Jia


[On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective. (12%)](http://arxiv.org/abs/2302.12095)

Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, Binxin Jiao, Yue Zhang, Xing Xie


## 2023-02-21

[MultiRobustBench: Benchmarking Robustness Against Multiple Attacks. (99%)](http://arxiv.org/abs/2302.10980)

Sihui Dai, Saeed Mahloujifar, Chong Xiang, Vikash Sehwag, Pin-Yu Chen, Prateek Mittal


[MalProtect: Stateful Defense Against Adversarial Query Attacks in ML-based Malware Detection. (99%)](http://arxiv.org/abs/2302.10739)

Aqib Rashid, Jose Such


[Interpretable Spectrum Transformation Attacks to Speaker Recognition. (98%)](http://arxiv.org/abs/2302.10686)

Jiadi Yao, Hong Luo, Xiao-Lei Zhang


[Characterizing the Optimal 0-1 Loss for Multi-class Classification with a Test-time Attacker. (97%)](http://arxiv.org/abs/2302.10722)

Sihui Dai, Wenxin Ding, Arjun Nitin Bhagoji, Daniel Cullina, Ben Y. Zhao, Haitao Zheng, Prateek Mittal


[Generalization Bounds for Adversarial Contrastive Learning. (31%)](http://arxiv.org/abs/2302.10633)

Xin Zou, Weiwei Liu


## 2023-02-20

[An Incremental Gray-box Physical Adversarial Attack on Neural Network Training. (98%)](http://arxiv.org/abs/2303.01245)

Rabiah Al-qudah, Moayad Aloqaily, Bassem Ouni, Mohsen Guizani, Thierry Lestable


[Variation Enhanced Attacks Against RRAM-based Neuromorphic Computing System. (97%)](http://arxiv.org/abs/2302.09902)

Hao Lv, Bing Li, Lei Zhang, Cheng Liu, Ying Wang


[Seasoning Model Soups for Robustness to Adversarial and Natural Distribution Shifts. (88%)](http://arxiv.org/abs/2302.10164)

Francesco Croce, Sylvestre-Alvise Rebuffi, Evan Shelhamer, Sven Gowal


[Poisoning Web-Scale Training Datasets is Practical. (83%)](http://arxiv.org/abs/2302.10149)

Nicholas Carlini, Matthew Jagielski, Christopher A. Choquette-Choo, Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, Florian Tramèr


[Pseudo Label-Guided Model Inversion Attack via Conditional Generative Adversarial Network. (47%)](http://arxiv.org/abs/2302.09814)

Xiaojian Yuan, Kejiang Chen, Jie Zhang, Weiming Zhang, Nenghai Yu, Yang Zhang


[Model-based feature selection for neural networks: A mixed-integer programming approach. (22%)](http://arxiv.org/abs/2302.10344)

Shudian Zhao, Calvin Tsay, Jan Kronqvist


[Take Me Home: Reversing Distribution Shifts using Reinforcement Learning. (8%)](http://arxiv.org/abs/2302.10341)

Vivian Lin, Kuk Jin Jang, Souradeep Dutta, Michele Caprio, Oleg Sokolsky, Insup Lee


## 2023-02-19

[X-Adv: Physical Adversarial Object Attacks against X-ray Prohibited Item Detection. (99%)](http://arxiv.org/abs/2302.09491)

Aishan Liu, Jun Guo, Jiakai Wang, Siyuan Liang, Renshuai Tao, Wenbo Zhou, Cong Liu, Xianglong Liu, Dacheng Tao


[Stationary Point Losses for Robust Model. (93%)](http://arxiv.org/abs/2302.09575)

Weiwei Gao, Dazhi Zhang, Yao Li, Zhichang Guo, Ovanes Petrosian


[On Feasibility of Server-side Backdoor Attacks on Split Learning. (76%)](http://arxiv.org/abs/2302.09578)

Behrad Tajalli, Oguzhan Ersoy, Stjepan Picek


## 2023-02-18

[Adversarial Machine Learning: A Systematic Survey of Backdoor Attack, Weight Attack and Adversarial Example. (99%)](http://arxiv.org/abs/2302.09457)

Baoyuan Wu, Li Liu, Zihao Zhu, Qingshan Liu, Zhaofeng He, Siwei Lyu


[Delving into the Adversarial Robustness of Federated Learning. (98%)](http://arxiv.org/abs/2302.09479)

Jie Zhang, Bo Li, Chen Chen, Lingjuan Lyu, Shuang Wu, Shouhong Ding, Chao Wu


[Meta Style Adversarial Training for Cross-Domain Few-Shot Learning. (83%)](http://arxiv.org/abs/2302.09309)

Yuqian Fu, Yu Xie, Yanwei Fu, Yu-Gang Jiang


[MedViT: A Robust Vision Transformer for Generalized Medical Image Classification. (12%)](http://arxiv.org/abs/2302.09462)

Omid Nejati Manzari, Hamid Ahmadabadi, Hossein Kashiani, Shahriar B. Shokouhi, Ahmad Ayatollahi


[RobustNLP: A Technique to Defend NLP Models Against Backdoor Attacks. (11%)](http://arxiv.org/abs/2302.09420)

Marwan Omar


## 2023-02-17

[Measuring Equality in Machine Learning Security Defenses. (96%)](http://arxiv.org/abs/2302.08973)

Luke E. Richards, Edward Raff, Cynthia Matuszek


[Function Composition in Trustworthy Machine Learning: Implementation Choices, Insights, and Questions. (5%)](http://arxiv.org/abs/2302.09190)

Manish Nagireddy, Moninder Singh, Samuel C. Hoffman, Evaline Ju, Karthikeyan Natesan Ramamurthy, Kush R. Varshney


[RetVec: Resilient and Efficient Text Vectorizer. (1%)](http://arxiv.org/abs/2302.09207)

Elie Bursztein, Marina Zhang, Owen Vallis, Xinyu Jia, Alexey Kurakin


## 2023-02-16

[On the Effect of Adversarial Training Against Invariance-based Adversarial Examples. (99%)](http://arxiv.org/abs/2302.08257)

Roland Rauter, Martin Nocker, Florian Merkle, Pascal Schöttle


[High-frequency Matters: An Overwriting Attack and defense for Image-processing Neural Network Watermarking. (67%)](http://arxiv.org/abs/2302.08637)

Huajie Chen, Tianqing Zhu, Chi Liu, Shui Yu, Wanlei Zhou


[Marich: A Query-efficient Distributionally Equivalent Model Extraction Attack using Public Data. (3%)](http://arxiv.org/abs/2302.08466)

Pratik Karmakar, Debabrota Basu


[A Novel Noise Injection-based Training Scheme for Better Model Robustness. (2%)](http://arxiv.org/abs/2302.10802)

Zeliang Zhang, Jinyang Jiang, Minjie Chen, Zhiyuan Wang, Yijie Peng, Zhaofei Yu


## 2023-02-15

[Masking and Mixing Adversarial Training. (99%)](http://arxiv.org/abs/2302.08066)

Hiroki Adachi, Tsubasa Hirakawa, Takayoshi Yamashita, Hironobu Fujiyoshi, Yasunori Ishii, Kazuki Kozuka


[Robust Mid-Pass Filtering Graph Convolutional Networks. (98%)](http://arxiv.org/abs/2302.08048)

Jincheng Huang, Lun Du, Xu Chen, Qiang Fu, Shi Han, Dongmei Zhang


[Graph Adversarial Immunization for Certifiable Robustness. (98%)](http://arxiv.org/abs/2302.08051)

Shuchang Tao, Huawei Shen, Qi Cao, Yunfan Wu, Liang Hou, Xueqi Cheng


[XploreNAS: Explore Adversarially Robust & Hardware-efficient Neural Architectures for Non-ideal Xbars. (87%)](http://arxiv.org/abs/2302.07769)

Abhiroop Bhattacharjee, Abhishek Moitra, Priyadarshini Panda


[Tight Auditing of Differentially Private Machine Learning. (41%)](http://arxiv.org/abs/2302.07956)

Milad Nasr, Jamie Hayes, Thomas Steinke, Borja Balle, Florian Tramèr, Matthew Jagielski, Nicholas Carlini, Andreas Terzis


[Field-sensitive Data Flow Integrity. (1%)](http://arxiv.org/abs/2302.07717)

So Shizukuishi, Yoshitaka Arahori, Katsuhiko Gondow


[Uncertainty-Estimation with Normalized Logits for Out-of-Distribution Detection. (1%)](http://arxiv.org/abs/2302.07608)

Mouxiao Huang, Yu Qiao


## 2023-02-14

[Regret-Based Optimization for Robust Reinforcement Learning. (99%)](http://arxiv.org/abs/2302.06912)

Roman Belaire, Pradeep Varakantham, David Lo


[On the Role of Randomization in Adversarially Robust Classification. (99%)](http://arxiv.org/abs/2302.07221)

Lucas Gnecco-Heredia, Yann Chevaleyre, Benjamin Negrevergne, Laurent Meunier, Muni Sreenivas Pydi


[Attacking Fake News Detectors via Manipulating News Social Engagement. (83%)](http://arxiv.org/abs/2302.07363)

Haoran Wang, Yingtong Dou, Canyu Chen, Lichao Sun, Philip S. Yu, Kai Shu


[An Experimental Study of Byzantine-Robust Aggregation Schemes in Federated Learning. (31%)](http://arxiv.org/abs/2302.07173)

Shenghui Li, Edith C. -H. Ngai, Thiemo Voigt


[A Modern Look at the Relationship between Sharpness and Generalization. (10%)](http://arxiv.org/abs/2302.07011)

Maksym Andriushchenko, Francesco Croce, Maximilian Müller, Matthias Hein, Nicolas Flammarion


[Bounding Training Data Reconstruction in DP-SGD. (8%)](http://arxiv.org/abs/2302.07225)

Jamie Hayes, Saeed Mahloujifar, Borja Balle


[Security Defense For Smart Contracts: A Comprehensive Survey. (1%)](http://arxiv.org/abs/2302.07347)

Nikolay Ivanov, Chenning Li, Qiben Yan, Zhiyuan Sun, Zhichao Cao, Xiapu Luo


[READIN: A Chinese Multi-Task Benchmark with Realistic and Diverse Input Noises. (1%)](http://arxiv.org/abs/2302.07324)

Chenglei Si, Zhengyan Zhang, Yingfa Chen, Xiaozhi Wang, Zhiyuan Liu, Maosong Sun


## 2023-02-13

[Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data. (98%)](http://arxiv.org/abs/2302.06279)

Gorka Abad, Oguzhan Ersoy, Stjepan Picek, Aitor Urbieta


[Raising the Cost of Malicious AI-Powered Image Editing. (82%)](http://arxiv.org/abs/2302.06588)

Hadi Salman, Alaa Khaddaj, Guillaume Leclerc, Andrew Ilyas, Aleksander Madry


[Targeted Attack on GPT-Neo for the SATML Language Model Data Extraction Challenge. (8%)](http://arxiv.org/abs/2302.07735)

Ali Al-Kaswan, Maliheh Izadi, Deursen Arie van


[Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions. (1%)](http://arxiv.org/abs/2302.06801)

Marwan Omar


## 2023-02-12

[TextDefense: Adversarial Text Detection based on Word Importance Entropy. (99%)](http://arxiv.org/abs/2302.05892)

Lujia Shen, Xuhong Zhang, Shouling Ji, Yuwen Pu, Chunpeng Ge, Xing Yang, Yanghe Feng


## 2023-02-11

[Mutation-Based Adversarial Attacks on Neural Text Detectors. (69%)](http://arxiv.org/abs/2302.05794)

Gongbo Liang, Jesus Guerrero, Izzat Alsmadi


[HateProof: Are Hateful Meme Detection Systems really Robust? (13%)](http://arxiv.org/abs/2302.05703)

Piush Aggarwal, Pranit Chawla, Mithun Das, Punyajoy Saha, Binny Mathew, Torsten Zesch, Animesh Mukherjee


[MTTM: Metamorphic Testing for Textual Content Moderation Software. (2%)](http://arxiv.org/abs/2302.05706)

Wenxuan Wang, Jen-tse Huang, Weibin Wu, Jianping Zhang, Yizhan Huang, Shuqing Li, Pinjia He, Michael Lyu


[Pushing the Accuracy-Group Robustness Frontier with Introspective Self-play. (1%)](http://arxiv.org/abs/2302.05807)

Jeremiah Zhe Liu, Krishnamurthy Dj Dvijotham, Jihyeon Lee, Quan Yuan, Martin Strobel, Balaji Lakshminarayanan, Deepak Ramachandran


[High Recovery with Fewer Injections: Practical Binary Volumetric Injection Attacks against Dynamic Searchable Encryption. (1%)](http://arxiv.org/abs/2302.05628)

Xianglong Zhang, Wei Wang, Peng Xu, Laurence T. Yang, Kaitai Liang


## 2023-02-10

[Making Substitute Models More Bayesian Can Enhance Transferability of Adversarial Examples. (98%)](http://arxiv.org/abs/2302.05086)

Qizhang Li, Yiwen Guo, Wangmeng Zuo, Hao Chen


[Unnoticeable Backdoor Attacks on Graph Neural Networks. (80%)](http://arxiv.org/abs/2303.01263)

Enyan Dai, Minhua Lin, Xiang Zhang, Suhang Wang


[Step by Step Loss Goes Very Far: Multi-Step Quantization for Adversarial Text Attacks. (73%)](http://arxiv.org/abs/2302.05120)

Piotr Gaiński, Klaudia Bałazy


## 2023-02-09

[IB-RAR: Information Bottleneck as Regularizer for Adversarial Robustness. (98%)](http://arxiv.org/abs/2302.10896)

Xiaoyun Xu, Guilherme Perin, Stjepan Picek


[Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples. (98%)](http://arxiv.org/abs/2302.04578)

Chumeng Liang, Xiaoyu Wu, Yang Hua, Jiaru Zhang, Yiming Xue, Tao Song, Zhengui Xue, Ruhui Ma, Haibing Guan


[Hyperparameter Search Is All You Need For Training-Agnostic Backdoor Robustness. (75%)](http://arxiv.org/abs/2302.04977)

Eugene Bagdasaryan, Vitaly Shmatikov


[Imperceptible Sample-Specific Backdoor to DNN with Denoising Autoencoder. (62%)](http://arxiv.org/abs/2302.04457)

Jiliang Zhang, Jing Xu, Zhi Zhang, Yansong Gao


[Better Diffusion Models Further Improve Adversarial Training. (22%)](http://arxiv.org/abs/2302.04638)

Zekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu, Shuicheng Yan


[Augmenting NLP data to counter Annotation Artifacts for NLI Tasks. (16%)](http://arxiv.org/abs/2302.04700)

Armaan Singh Bhullar


[Incremental Satisfiability Modulo Theory for Verification of Deep Neural Networks. (1%)](http://arxiv.org/abs/2302.06455)

Pengfei Yang, Zhiming Chi, Zongxin Liu, Mengyu Zhao, Cheng-Chao Huang, Shaowei Cai, Lijun Zhang


## 2023-02-08

[WAT: Improve the Worst-class Robustness in Adversarial Training. (99%)](http://arxiv.org/abs/2302.04025)

Boqi Li, Weiwei Liu


[Exploiting Certified Defences to Attack Randomised Smoothing. (99%)](http://arxiv.org/abs/2302.04379)

Andrew C. Cullen, Paul Montague, Shijie Liu, Sarah M. Erfani, Benjamin I. P. Rubinstein


[Shortcut Detection with Variational Autoencoders. (13%)](http://arxiv.org/abs/2302.04246)

Nicolas M. Müller, Simon Roschmann, Shahbaz Khan, Philip Sperl, Konstantin Böttinger


[Continuous Learning for Android Malware Detection. (13%)](http://arxiv.org/abs/2302.04332)

Yizheng Chen, Zhoujie Ding, David Wagner


[Training-free Lexical Backdoor Attacks on Language Models. (8%)](http://arxiv.org/abs/2302.04116)

Yujin Huang, Terry Yue Zhuo, Qiongkai Xu, Han Hu, Xingliang Yuan, Chunyang Chen


[On Function-Coupled Watermarks for Deep Neural Networks. (2%)](http://arxiv.org/abs/2302.10296)

Xiangyu Wen, Yu Li, Wei Jiang, Qiang Xu


[Unsupervised Learning of Initialization in Deep Neural Networks via Maximum Mean Discrepancy. (1%)](http://arxiv.org/abs/2302.04369)

Cheolhyoung Lee, Kyunghyun Cho


## 2023-02-07

[Toward Face Biometric De-identification using Adversarial Examples. (98%)](http://arxiv.org/abs/2302.03657)

Mahdi Ghafourian, Julian Fierrez, Luis Felipe Gomez, Ruben Vera-Rodriguez, Aythami Morales, Zohra Rezgui, Raymond Veldhuis


[Attacking Cooperative Multi-Agent Reinforcement Learning by Adversarial Minority Influence. (83%)](http://arxiv.org/abs/2302.03322)

Simin Li, Jun Guo, Jingqiao Xiu, Pu Feng, Xin Yu, Jiakai Wang, Aishan Liu, Wenjun Wu, Xianglong Liu


[Membership Inference Attacks against Diffusion Models. (64%)](http://arxiv.org/abs/2302.03262)

Tomoya Matsumoto, Takayuki Miura, Naoto Yanai


[Temporal Robustness against Data Poisoning. (12%)](http://arxiv.org/abs/2302.03684)

Wenxiao Wang, Soheil Feizi


[Robustness Implies Fairness in Casual Algorithmic Recourse. (2%)](http://arxiv.org/abs/2302.03465)

Ahmad-Reza Ehyaei, Amir-Hossein Karimi, Bernhard Schölkopf, Setareh Maghsudi


[Low-Latency Communication using Delay-Aware Relays Against Reactive Adversaries. (1%)](http://arxiv.org/abs/2302.03335)

Vivek Chaudhary, J. Harshan


## 2023-02-06

[Less is More: Understanding Word-level Textual Adversarial Attack via n-gram Frequency Descend. (99%)](http://arxiv.org/abs/2302.02568)

Ning Lu, Shengcai Liu, Zhirui Zhang, Qi Wang, Haifeng Liu, Ke Tang


[SCALE-UP: An Efficient Black-box Input-level Backdoor Detection via Analyzing Scaled Prediction Consistency. (92%)](http://arxiv.org/abs/2302.03251)

Junfeng Guo, Yiming Li, Xun Chen, Hanqing Guo, Lichao Sun, Cong Liu


[Exploring and Exploiting Decision Boundary Dynamics for Adversarial Robustness. (87%)](http://arxiv.org/abs/2302.03015)

Yuancheng Xu, Yanchao Sun, Micah Goldblum, Tom Goldstein, Furong Huang


[Collective Robustness Certificates: Exploiting Interdependence in Graph Neural Networks. (75%)](http://arxiv.org/abs/2302.02829)

Jan Schuchardt, Aleksandar Bojchevski, Johannes Gasteiger, Stephan Günnemann


[GAT: Guided Adversarial Training with Pareto-optimal Auxiliary Tasks. (67%)](http://arxiv.org/abs/2302.02907)

Salah Ghamizi, Jingfeng Zhang, Maxime Cordy, Mike Papadakis, Masashi Sugiyama, Yves Le Traon


[Target-based Surrogates for Stochastic Optimization. (1%)](http://arxiv.org/abs/2302.02607)

Jonathan Wilder Lavington, Sharan Vaswani, Reza Babanezhad, Mark Schmidt, Nicolas Le Roux


[Dropout Injection at Test Time for Post Hoc Uncertainty Quantification in Neural Networks. (1%)](http://arxiv.org/abs/2302.02924)

Emanuele Ledda, Giorgio Fumera, Fabio Roli


## 2023-02-05

[On the Role of Contrastive Representation Learning in Adversarial Robustness: An Empirical Study. (54%)](http://arxiv.org/abs/2302.02502)

Fatemeh Ghofrani, Mehdi Yaghouti, Pooyan Jamshidi


[Leaving Reality to Imagination: Robust Classification via Generated Datasets. (2%)](http://arxiv.org/abs/2302.02503)

Hritik Bansal, Aditya Grover


## 2023-02-04

[CosPGD: a unified white-box adversarial attack for pixel-wise prediction tasks. (99%)](http://arxiv.org/abs/2302.02213)

Shashank Agnihotri, Steffen Jung, Margret Keuper


[A Minimax Approach Against Multi-Armed Adversarial Attacks Detection. (86%)](http://arxiv.org/abs/2302.02216)

Federica Granese, Marco Romanelli, Siddharth Garg, Pablo Piantanida


[Run-Off Election: Improved Provable Defense against Data Poisoning Attacks. (83%)](http://arxiv.org/abs/2302.02300)

Keivan Rezaei, Kiarash Banihashem, Atoosa Chegini, Soheil Feizi


[AUTOLYCUS: Exploiting Explainable AI (XAI) for Model Extraction Attacks against Decision Tree Models. (80%)](http://arxiv.org/abs/2302.02162)

Abdullah Caglar Oksuz, Anisa Halimi, Erman Ayday


[Certified Robust Control under Adversarial Perturbations. (78%)](http://arxiv.org/abs/2302.02208)

Jinghan Yang, Hunmin Kim, Wenbin Wan, Naira Hovakimyan, Yevgeniy Vorobeychik


## 2023-02-03

[TextShield: Beyond Successfully Detecting Adversarial Sentences in Text Classification. (96%)](http://arxiv.org/abs/2302.02023)

Lingfeng Shen, Ze Zhang, Haiyun Jiang, Ying Chen


[DeTorrent: An Adversarial Padding-only Traffic Analysis Defense. (73%)](http://arxiv.org/abs/2302.02012)

James K Holland, Jason Carpenter, Se Eun Oh, Nicholas Hopper


[SoK: A Systematic Evaluation of Backdoor Trigger Characteristics in Image Classification. (61%)](http://arxiv.org/abs/2302.01740)

Gorka Abad, Jing Xu, Stefanos Koffas, Behrad Tajalli, Stjepan Picek, Mauro Conti


[Beyond the Universal Law of Robustness: Sharper Laws for Random Features and Neural Tangent Kernels. (15%)](http://arxiv.org/abs/2302.01629)

Simone Bombari, Shayan Kiyani, Marco Mondelli


[Asymmetric Certified Robustness via Feature-Convex Neural Networks. (8%)](http://arxiv.org/abs/2302.01961)

Samuel Pfrommer, Brendon G. Anderson, Julien Piet, Somayeh Sojoudi


[Revisiting Personalized Federated Learning: Robustness Against Backdoor Attacks. (2%)](http://arxiv.org/abs/2302.01677)

Zeyu Qin, Liuyi Yao, Daoyuan Chen, Yaliang Li, Bolin Ding, Minhao Cheng


[BarrierBypass: Out-of-Sight Clean Voice Command Injection Attacks through Physical Barriers. (2%)](http://arxiv.org/abs/2302.02042)

Payton Walker, Tianfang Zhang, Cong Shi, Nitesh Saxena, Yingying Chen


[From Robustness to Privacy and Back. (2%)](http://arxiv.org/abs/2302.01855)

Hilal Asi, Jonathan Ullman, Lydia Zakynthinou


[DCA: Delayed Charging Attack on the Electric Shared Mobility System. (1%)](http://arxiv.org/abs/2302.01972)

Shuocheng Guo, Hanlin Chen, Mizanur Rahman, Xinwu Qian


[Augmenting Rule-based DNS Censorship Detection at Scale with Machine Learning. (1%)](http://arxiv.org/abs/2302.02031)

Jacob Alexander Markson Brown, Xi Jiang, Van Tran, Arjun Nitin Bhagoji, Nguyen Phong Hoang, Nick Feamster, Prateek Mittal, Vinod Yegneswaran


## 2023-02-02

[Beyond Pretrained Features: Noisy Image Modeling Provides Adversarial Defense. (99%)](http://arxiv.org/abs/2302.01056)

Zunzhi You, Daochang Liu, Chang Xu


[TransFool: An Adversarial Attack against Neural Machine Translation Models. (99%)](http://arxiv.org/abs/2302.00944)

Sahar Sadrizadeh, Ljiljana Dolamic, Pascal Frossard


[On the Robustness of Randomized Ensembles to Adversarial Perturbations. (75%)](http://arxiv.org/abs/2302.01375)

Hassan Dbouk, Naresh R. Shanbhag


[A sliced-Wasserstein distance-based approach for out-of-class-distribution detection. (62%)](http://arxiv.org/abs/2302.01459)

Mohammad Shifat E Rabbi, Abu Hasnat Mohammad Rubaiyat, Yan Zhuang, Gustavo K Rohde


[Effective Robustness against Natural Distribution Shifts for Models with Different Training Data. (13%)](http://arxiv.org/abs/2302.01381)

Zhouxing Shi, Nicholas Carlini, Ananth Balashankar, Ludwig Schmidt, Cho-Jui Hsieh, Alex Beutel, Yao Qin


[SPECWANDS: An Efficient Priority-based Scheduler Against Speculation Contention Attacks. (10%)](http://arxiv.org/abs/2302.00947)

Bowen Tang, Chenggang Wu, Pen-Chung Yew, Yinqian Zhang, Mengyao Xie, Yuanming Lai, Yan Kang, Wei Wang, Qiang Wei, Zhe Wang


[Provably Bounding Neural Network Preimages. (8%)](http://arxiv.org/abs/2302.01404)

Suhas Dj Kotha, Christopher Dj Brix, Zico Dj Kolter, Dj Krishnamurthy, Dvijotham, Huan Zhang


[Defensive ML: Defending Architectural Side-channels with Adversarial Obfuscation. (2%)](http://arxiv.org/abs/2302.01474)

Hyoungwook Nam, Raghavendra Pradyumna Pothukuchi, Bo Li, Nam Sung Kim, Josep Torrellas


[Generalized Uncertainty of Deep Neural Networks: Taxonomy and Applications. (1%)](http://arxiv.org/abs/2302.01440)

Chengyu Dong


[Dataset Distillation Fixes Dataset Reconstruction Attacks. (1%)](http://arxiv.org/abs/2302.01428)

Noel Loo, Ramin Hasani, Mathias Lechner, Daniela Rus


## 2023-02-01

[Universal Soldier: Using Universal Adversarial Perturbations for Detecting Backdoor Attacks. (99%)](http://arxiv.org/abs/2302.00747)

Xiaoyun Xu, Oguzhan Ersoy, Stjepan Picek


[Effectiveness of Moving Target Defenses for Adversarial Attacks in ML-based Malware Detection. (92%)](http://arxiv.org/abs/2302.00537)

Aqib Rashid, Jose Such


[Exploring Semantic Perturbations on Grover. (56%)](http://arxiv.org/abs/2302.00509)

Pranav Kulkarni, Ziqing Ji, Yan Xu, Marko Neskovic, Kevin Nolan


[BackdoorBox: A Python Toolbox for Backdoor Learning. (10%)](http://arxiv.org/abs/2302.01762)

Yiming Li, Mengxi Ya, Yang Bai, Yong Jiang, Shu-Tao Xia


## 2023-01-31

[Reverse engineering adversarial attacks with fingerprints from adversarial examples. (99%)](http://arxiv.org/abs/2301.13869)

David Aaron Embedded Intelligence Nicholson, Vincent Embedded Intelligence Emanuele


[The Impacts of Unanswerable Questions on the Robustness of Machine Reading Comprehension Models. (97%)](http://arxiv.org/abs/2302.00094)

Son Quoc Tran, Phong Nguyen-Thuan Do, Uyen Le, Matt Kretchmar


[Are Defenses for Graph Neural Networks Robust? (80%)](http://arxiv.org/abs/2301.13694)

Felix Mujkanovic, Simon Geisler, Stephan Günnemann, Aleksandar Bojchevski


[Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks. (75%)](http://arxiv.org/abs/2301.13487)

Zhiyuan Cheng, James Liang, Guanhong Tao, Dongfang Liu, Xiangyu Zhang


[Robust Linear Regression: Gradient-descent, Early-stopping, and Beyond. (47%)](http://arxiv.org/abs/2301.13486)

Meyer Scetbon, Elvis Dohmatob


[Fairness-aware Vision Transformer via Debiased Self-Attention. (47%)](http://arxiv.org/abs/2301.13803)

Yao Qiang, Chengyin Li, Prashant Khanduri, Dongxiao Zhu


[Image Shortcut Squeezing: Countering Perturbative Availability Poisons with Compression. (12%)](http://arxiv.org/abs/2301.13838)

Zhuoran Liu, Zhengyu Zhao, Martha Larson


[Identifying the Hazard Boundary of ML-enabled Autonomous Systems Using Cooperative Co-Evolutionary Search. (1%)](http://arxiv.org/abs/2301.13807)

Sepehr Sharifi, Donghwan Shin, Lionel C. Briand, Nathan Aschbacher


## 2023-01-30

[Feature-Space Bayesian Adversarial Learning Improved Malware Detector Robustness. (99%)](http://arxiv.org/abs/2301.12680)

Bao Gia Doan, Shuiqiao Yang, Paul Montague, Vel Olivier De, Tamas Abraham, Seyit Camtepe, Salil S. Kanhere, Ehsan Abbasnejad, Damith C. Ranasinghe


[Improving Adversarial Transferability with Scheduled Step Size and Dual Example. (99%)](http://arxiv.org/abs/2301.12968)

Zeliang Zhang, Peihan Liu, Xiaosen Wang, Chenliang Xu


[Towards Adversarial Realism and Robust Learning for IoT Intrusion Detection and Classification. (99%)](http://arxiv.org/abs/2301.13122)

João Vitorino, Isabel Praça, Eva Maia


[Certified Robustness of Learning-based Static Malware Detectors. (99%)](http://arxiv.org/abs/2302.01757)

Zhuoqun Huang, Neil G. Marchant, Keane Lucas, Lujo Bauer, Olga Ohrimenko, Benjamin I. P. Rubinstein


[Identifying Adversarially Attackable and Robust Samples. (99%)](http://arxiv.org/abs/2301.12896)

Vyas Raina, Mark Gales


[On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex. (98%)](http://arxiv.org/abs/2301.12868)

Terry Yue Zhuo, Zhuang Li, Yujin Huang, Fatemeh Shiri, Weiqing Wang, Gholamreza Haffari, Yuan-Fang Li


[Anchor-Based Adversarially Robust Zero-Shot Learning Driven by Language. (96%)](http://arxiv.org/abs/2301.13096)

Xiao Li, Wei Zhang, Yining Liu, Zhanhao Hu, Bo Zhang, Xiaolin Hu


[Inference Time Evidences of Adversarial Attacks for Forensic on Transformers. (87%)](http://arxiv.org/abs/2301.13356)

Hugo Lemarchant, Liangzi Li, Yiming Qian, Yuta Nakashima, Hajime Nagahara


[On the Efficacy of Metrics to Describe Adversarial Attacks. (82%)](http://arxiv.org/abs/2301.13028)

Tommaso Puccetti, Tommaso Zoppi, Andrea Ceccarelli


[Benchmarking Robustness to Adversarial Image Obfuscations. (74%)](http://arxiv.org/abs/2301.12993)

Florian Stimberg, Ayan Chakrabarti, Chun-Ta Lu, Hussein Hazimeh, Otilia Stretcu, Wei Qiao, Yintao Liu, Merve Kaya, Cyrus Rashtchian, Ariel Fuxman, Mehmet Tek, Sven Gowal


[Extracting Training Data from Diffusion Models. (5%)](http://arxiv.org/abs/2301.13188)

Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramèr, Borja Balle, Daphne Ippolito, Eric Wallace


[M3FAS: An Accurate and Robust MultiModal Mobile Face Anti-Spoofing System. (1%)](http://arxiv.org/abs/2301.12831)

Chenqi Kong, Kexin Zheng, Yibing Liu, Shiqi Wang, Anderson Rocha, Haoliang Li


## 2023-01-29

[Scaling in Depth: Unlocking Robustness Certification on ImageNet. (98%)](http://arxiv.org/abs/2301.12549)

Kai Hu, Andy Zou, Zifan Wang, Klas Leino, Matt Fredrikson


[Mitigating Adversarial Effects of False Data Injection Attacks in Power Grid. (93%)](http://arxiv.org/abs/2301.12487)

Farhin Farhad Riya, Shahinul Hoque, Jinyuan Stella Sun, Jiangnan Li, Hairong Qi


[Improving the Accuracy-Robustness Trade-off of Classifiers via Adaptive Smoothing. (82%)](http://arxiv.org/abs/2301.12554)

Yatong Bai, Brendon G. Anderson, Aerin Kim, Somayeh Sojoudi


[Uncovering Adversarial Risks of Test-Time Adaptation. (82%)](http://arxiv.org/abs/2301.12576)

Tong Wu, Feiran Jia, Xiangyu Qi, Jiachen T. Wang, Vikash Sehwag, Saeed Mahloujifar, Prateek Mittal


[Adversarial Attacks on Adversarial Bandits. (69%)](http://arxiv.org/abs/2301.12595)

Yuzhe Ma, Zhijin Zhou


[Towards Verifying the Geometric Robustness of Large-scale Neural Networks. (54%)](http://arxiv.org/abs/2301.12456)

Fu Wang, Peipei Xu, Wenjie Ruan, Xiaowei Huang


[Lateralized Learning for Multi-Class Visual Classification Tasks. (13%)](http://arxiv.org/abs/2301.12637)

Abubakar Siddique, Will N. Browne, Gina M. Grimshaw


[Diverse, Difficult, and Odd Instances (D2O): A New Test Set for Object Classification. (3%)](http://arxiv.org/abs/2301.12527)

Ali Borji


[Adversarial Style Augmentation for Domain Generalization. (2%)](http://arxiv.org/abs/2301.12643)

Yabin Zhang, Bin Deng, Ruihuang Li, Kui Jia, Lei Zhang


[Confidence-Aware Calibration and Scoring Functions for Curriculum Learning. (1%)](http://arxiv.org/abs/2301.12589)

Shuang Ao, Stefan Rueger, Advaith Siddharthan


## 2023-01-28

[Node Injection for Class-specific Network Poisoning. (82%)](http://arxiv.org/abs/2301.12277)

Ansh Kumar Sharma, Rahul Kukreja, Mayank Kharbanda, Tanmoy Chakraborty


[Out-of-distribution Detection with Energy-based Models. (82%)](http://arxiv.org/abs/2302.12002)

Sven Elflein


[Gradient Shaping: Enhancing Backdoor Attack Against Reverse Engineering. (13%)](http://arxiv.org/abs/2301.12318)

Rui Zhu, Di Tang, Siyuan Tang, Guanhong Tao, Shiqing Ma, Xiaofeng Wang, Haixu Tang


[Selecting Models based on the Risk of Damage Caused by Adversarial Attacks. (1%)](http://arxiv.org/abs/2301.12151)

Jona Klemenc, Holger Trittenbach


## 2023-01-27

[Semantic Adversarial Attacks on Face Recognition through Significant Attributes. (99%)](http://arxiv.org/abs/2301.12046)

Yasmeen M. Khedr, Yifeng Xiong, Kun He


[Targeted Attacks on Timeseries Forecasting. (99%)](http://arxiv.org/abs/2301.11544)

Yuvaraj Govindarajulu, Avinash Amballa, Pavan Kulkarni, Manojkumar Parmar


[Adapting Step-size: A Unified Perspective to Analyze and Improve Gradient-based Methods for Adversarial Attacks. (98%)](http://arxiv.org/abs/2301.11546)

Wei Tao, Lei Bao, Long Sheng, Gaowei Wu, Qing Tao


[PECAN: A Deterministic Certified Defense Against Backdoor Attacks. (97%)](http://arxiv.org/abs/2301.11824)

Yuhao Zhang, Aws Albarghouthi, Loris D'Antoni


[Vertex-based reachability analysis for verifying ReLU deep neural networks. (93%)](http://arxiv.org/abs/2301.12001)

João Zago, Eduardo Camponogara, Eric Antonelo


[OccRob: Efficient SMT-Based Occlusion Robustness Verification of Deep Neural Networks. (92%)](http://arxiv.org/abs/2301.11912)

Xingwu Guo, Ziwei Zhou, Yueling Zhang, Guy Katz, Min Zhang


[PCV: A Point Cloud-Based Network Verifier. (88%)](http://arxiv.org/abs/2301.11806)

Arup Kumar Sarker, Farzana Yasmin Ahmad, Matthew B. Dwyer


[Robust Transformer with Locality Inductive Bias and Feature Normalization. (88%)](http://arxiv.org/abs/2301.11553)

Omid Nejati Manzari, Hossein Kashiani, Hojat Asgarian Dehkordi, Shahriar Baradaran Shokouhi


[Analyzing Robustness of the Deep Reinforcement Learning Algorithm in Ramp Metering Applications Considering False Data Injection Attack and Defense. (87%)](http://arxiv.org/abs/2301.12036)

Diyi Liu, Lanmin Liu, Lee D Han


[Learning to Unlearn: Instance-wise Unlearning for Pre-trained Classifiers. (80%)](http://arxiv.org/abs/2301.11578)

Sungmin Cha, Sungjun Cho, Dasol Hwang, Honglak Lee, Taesup Moon, Moontae Lee


[Certified Invertibility in Neural Networks via Mixed-Integer Programming. (76%)](http://arxiv.org/abs/2301.11783)

Tianqi Cui, Thomas Bertalan, George J. Pappas, Manfred Morari, Ioannis G. Kevrekidis, Mahyar Fazlyab


## 2023-01-26

[Attacking Important Pixels for Anchor-free Detectors. (99%)](http://arxiv.org/abs/2301.11457)

Yunxu Xie, Shu Hu, Xin Wang, Quanyu Liao, Bin Zhu, Xi Wu, Siwei Lyu


[Certified Interpretability Robustness for Class Activation Mapping. (92%)](http://arxiv.org/abs/2301.11324)

Alex Gu, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, Luca Daniel


[Interaction-level Membership Inference Attack Against Federated Recommender Systems. (31%)](http://arxiv.org/abs/2301.10964)

Wei Yuan, Chaoqun Yang, Quoc Viet Hung Nguyen, Lizhen Cui, Tieke He, Hongzhi Yin


[Minerva: A File-Based Ransomware Detector. (13%)](http://arxiv.org/abs/2301.11050)

Dorjan Hitaj, Giulio Pagnotta, Gaspari Fabio De, Carli Lorenzo De, Luigi V. Mancini


## 2023-01-25

[RobustPdM: Designing Robust Predictive Maintenance against Adversarial Attacks. (99%)](http://arxiv.org/abs/2301.10822)

Ayesha Siddique, Ripan Kumar Kundu, Gautam Raj Mode, Khaza Anuarul Hoque


[BDMMT: Backdoor Sample Detection for Language Models through Model Mutation Testing. (98%)](http://arxiv.org/abs/2301.10412)

Jiali Wei, Ming Fan, Wenjing Jiao, Wuxia Jin, Ting Liu


[A Data-Centric Approach for Improving Adversarial Training Through the Lens of Out-of-Distribution Detection. (96%)](http://arxiv.org/abs/2301.10454)

Mohammad Azizmalayeri, Arman Zarei, Alireza Isavand, Mohammad Taghi Manzuri, Mohammad Hossein Rohban


[On the Adversarial Robustness of Camera-based 3D Object Detection. (81%)](http://arxiv.org/abs/2301.10766)

Shaoyuan Xie, Zichao Li, Zeyu Wang, Cihang Xie


[A Study on FGSM Adversarial Training for Neural Retrieval. (75%)](http://arxiv.org/abs/2301.10576)

Simon Lupart, Stéphane Clinchant


[Distilling Cognitive Backdoor Patterns within an Image. (5%)](http://arxiv.org/abs/2301.10908)

Hanxun Huang, Xingjun Ma, Sarah Erfani, James Bailey


[Connecting metrics for shape-texture knowledge in computer vision. (1%)](http://arxiv.org/abs/2301.10608)

Tiago Oliveira, Tiago Marques, Arlindo L. Oliveira


## 2023-01-24

[Blockchain-aided Secure Semantic Communication for AI-Generated Content in Metaverse. (13%)](http://arxiv.org/abs/2301.11289)

Yijing Lin, Hongyang Du, Dusit Niyato, Jiangtian Nie, Jiayi Zhang, Yanyu Cheng, Zhaohui Yang


[Learning Effective Strategies for Moving Target Defense with Switching Costs. (1%)](http://arxiv.org/abs/2301.09892)

Vignesh Viswanathan, Megha Bose, Praveen Paruchuri


[Data Augmentation Alone Can Improve Adversarial Training. (1%)](http://arxiv.org/abs/2301.09879)

Lin Li, Michael Spratling


## 2023-01-23

[DODEM: DOuble DEfense Mechanism Against Adversarial Attacks Towards Secure Industrial Internet of Things Analytics. (99%)](http://arxiv.org/abs/2301.09740)

Onat Gungor, Tajana Rosing, Baris Aksanli


[Practical Adversarial Attacks Against AI-Driven Power Allocation in a Distributed MIMO Network. (92%)](http://arxiv.org/abs/2301.09305)

Ömer Faruk Tuna, Fehmi Emre Kadan, Leyli Karaçay


[BayBFed: Bayesian Backdoor Defense for Federated Learning. (78%)](http://arxiv.org/abs/2301.09508)

Kavita Kumari, Phillip Rieger, Hossein Fereidooni, Murtuza Jadliwala, Ahmad-Reza Sadeghi


[Backdoor Attacks in Peer-to-Peer Federated Learning. (68%)](http://arxiv.org/abs/2301.09732)

Gokberk Yar, Cristina Nita-Rotaru, Alina Oprea


## 2023-01-22

[Provable Unrestricted Adversarial Training without Compromise with Generalizability. (99%)](http://arxiv.org/abs/2301.09069)

Lilin Zhang, Ning Yang, Yanchao Sun, Philip S. Yu


[ContraBERT: Enhancing Code Pre-trained Models via Contrastive Learning. (8%)](http://arxiv.org/abs/2301.09072)

Shangqing Liu, Bozhi Wu, Xiaofei Xie, Guozhu Meng, Yang Liu


## 2023-01-20

[Limitations of Piecewise Linearity for Efficient Robustness Certification. (95%)](http://arxiv.org/abs/2301.08842)

Klas Leino


[Towards Understanding How Self-training Tolerates Data Backdoor Poisoning. (16%)](http://arxiv.org/abs/2301.08751)

Soumyadeep Pal, Ren Wang, Yuguang Yao, Sijia Liu


[Dr.Spider: A Diagnostic Evaluation Benchmark towards Text-to-SQL Robustness. (8%)](http://arxiv.org/abs/2301.08881)

Shuaichen Chang, Jun Wang, Mingwen Dong, Lin Pan, Henghui Zhu, Alexander Hanbo Li, Wuwei Lan, Sheng Zhang, Jiarong Jiang, Joseph Lilien, Steve Ash, William Yang Wang, Zhiguo Wang, Vittorio Castelli, Patrick Ng, Bing Xiang


[Defending SDN against packet injection attacks using deep learning. (2%)](http://arxiv.org/abs/2301.08428)

Anh Tuan Phu, Bo Li, Faheem Ullah, Tanvir Ul Huque, Ranesh Naha, Ali Babar, Hung Nguyen


## 2023-01-19

[On the Vulnerability of Backdoor Defenses for Federated Learning. (62%)](http://arxiv.org/abs/2301.08170)

Pei Fang, Jinghui Chen


[On the Relationship Between Information-Theoretic Privacy Metrics And Probabilistic Information Privacy. (31%)](http://arxiv.org/abs/2301.08401)

Chong Xiao Wang, Wee Peng Tay


[RNAS-CL: Robust Neural Architecture Search by Cross-Layer Knowledge Distillation. (16%)](http://arxiv.org/abs/2301.08092)

Utkarsh Nath, Yancheng Wang, Yingzhen Yang


[Enhancing Deep Learning with Scenario-Based Override Rules: a Case Study. (1%)](http://arxiv.org/abs/2301.08114)

Adiel Ashrov, Guy Katz


## 2023-01-17

[Denoising Diffusion Probabilistic Models as a Defense against Adversarial Attacks. (98%)](http://arxiv.org/abs/2301.06871)

Lars Lien Ankile, Anna Midgley, Sebastian Weisshaar


[Adversarial Robust Deep Reinforcement Learning Requires Redefining Robustness. (68%)](http://arxiv.org/abs/2301.07487)

Ezgi Korkmaz


[Label Inference Attack against Split Learning under Regression Setting. (8%)](http://arxiv.org/abs/2301.07284)

Shangyu Xie, Xin Yang, Yuanshun Yao, Tianyi Liu, Taiqing Wang, Jiankai Sun


## 2023-01-16

[$\beta$-DARTS++: Bi-level Regularization for Proxy-robust Differentiable Architecture Search. (1%)](http://arxiv.org/abs/2301.06393)

Peng Ye, Tong He, Baopu Li, Tao Chen, Lei Bai, Wanli Ouyang


[Modeling Uncertain Feature Representation for Domain Generalization. (1%)](http://arxiv.org/abs/2301.06442)

Xiaotong Li, Zixuan Hu, Jun Liu, Yixiao Ge, Yongxing Dai, Ling-Yu Duan


## 2023-01-15

[BEAGLE: Forensics of Deep Learning Backdoor Attack for Better Defense. (4%)](http://arxiv.org/abs/2301.06241)

Siyuan Cheng, Guanhong Tao, Yingqi Liu, Shengwei An, Xiangzhe Xu, Shiwei Feng, Guangyu Shen, Kaiyuan Zhang, Qiuling Xu, Shiqing Ma, Xiangyu Zhang


## 2023-01-13

[On the feasibility of attacking Thai LPR systems with adversarial examples. (99%)](http://arxiv.org/abs/2301.05506)

Chissanupong Jiamsuchon, Jakapan Suaboot, Norrathep Rattanavipanon


## 2023-01-12

[Security-Aware Approximate Spiking Neural Networks. (87%)](http://arxiv.org/abs/2301.05264)

Syed Tihaam Ahmad, Ayesha Siddique, Khaza Anuarul Hoque


[Jamming Attacks on Decentralized Federated Learning in General Multi-Hop Wireless Networks. (3%)](http://arxiv.org/abs/2301.05250)

Yi Shi, Yalin E. Sagduyu, Tugba Erpek


## 2023-01-11

[Phase-shifted Adversarial Training. (82%)](http://arxiv.org/abs/2301.04785)

Yeachan Kim, Seongyeon Kim, Ihyeok Seo, Bonggun Shin


[Universal Detection of Backdoor Attacks via Density-based Clustering and Centroids Analysis. (68%)](http://arxiv.org/abs/2301.04554)

Wei Guo, Benedetta Tondi, Mauro Barni


## 2023-01-10

[On the Robustness of AlphaFold: A COVID-19 Case Study. (73%)](http://arxiv.org/abs/2301.04093)

Ismail Alkhouri, Sumit Jha, Andre Beckus, George Atia, Alvaro Velasquez, Rickard Ewetz, Arvind Ramanathan, Susmit Jha


[CDA: Contrastive-adversarial Domain Adaptation. (38%)](http://arxiv.org/abs/2301.03826)

Nishant Yadav, Mahbubul Alam, Ahmed Farahat, Dipanjan Ghosh, Chetan Gupta, Auroop R. Ganguly


[User-Centered Security in Natural Language Processing. (12%)](http://arxiv.org/abs/2301.04230)

Chris Emmery


[Leveraging Diffusion For Strong and High Quality Face Morphing Attacks. (3%)](http://arxiv.org/abs/2301.04218)

Zander Blasingame, Chen Liu


## 2023-01-09

[Over-The-Air Adversarial Attacks on Deep Learning Wi-Fi Fingerprinting. (99%)](http://arxiv.org/abs/2301.03760)

Fei Xiao, Yong Huang, Yingying Zuo, Wei Kuang, Wei Wang


[On the Susceptibility and Robustness of Time Series Models through Adversarial Attack and Defense. (98%)](http://arxiv.org/abs/2301.03703)

Asadullah Hill Galib, Bidhan Bashyal


[Is Federated Learning a Practical PET Yet? (13%)](http://arxiv.org/abs/2301.04017)

Franziska Boenisch, Adam Dziedzic, Roei Schuster, Ali Shahin Shamsabadi, Ilia Shumailov, Nicolas Papernot


[SoK: Hardware Defenses Against Speculative Execution Attacks. (1%)](http://arxiv.org/abs/2301.03724)

Guangyuan Hu, Zecheng He, Ruby Lee


## 2023-01-08

[RobArch: Designing Robust Architectures against Adversarial Attacks. (76%)](http://arxiv.org/abs/2301.03110)

ShengYun Peng, Weilin Xu, Cory Cornelius, Kevin Li, Rahul Duggal, Duen Horng Chau, Jason Martin


[MoreauGrad: Sparse and Robust Interpretation of Neural Networks via Moreau Envelope. (1%)](http://arxiv.org/abs/2302.05294)

Jingwei Zhang, Farzan Farnia


## 2023-01-07

[REaaS: Enabling Adversarially Robust Downstream Classifiers via Robust Encoder as a Service. (99%)](http://arxiv.org/abs/2301.02905)

Wenjie Qu, Jinyuan Jia, Neil Zhenqiang Gong


[Adversarial training with informed data selection. (99%)](http://arxiv.org/abs/2301.04472)

Marcele O. K. Mendonça, Javier Maroto, Pascal Frossard, Paulo S. R. Diniz


## 2023-01-06

[Code Difference Guided Adversarial Example Generation for Deep Code Models. (99%)](http://arxiv.org/abs/2301.02412)

Zhao Tian, Junjie Chen, Zhi Jin


[Stealthy Backdoor Attack for Code Models. (98%)](http://arxiv.org/abs/2301.02496)

Zhou Yang, Bowen Xu, Jie M. Zhang, Hong Jin Kang, Jieke Shi, Junda He, David Lo


## 2023-01-05

[Silent Killer: Optimizing Backdoor Trigger Yields a Stealthy and Powerful Data Poisoning Attack. (98%)](http://arxiv.org/abs/2301.02615)

Tzvi Lederer, Gallil Maimon, Lior Rokach


[gRoMA: a Tool for Measuring Deep Neural Networks Global Robustness. (96%)](http://arxiv.org/abs/2301.02288)

Natan Levy, Raz Yerushalmi, Guy Katz


[Randomized Message-Interception Smoothing: Gray-box Certificates for Graph Neural Networks. (61%)](http://arxiv.org/abs/2301.02039)

Yan Scholten, Jan Schuchardt, Simon Geisler, Aleksandar Bojchevski, Stephan Günnemann


[Can Large Language Models Change User Preference Adversarially? (1%)](http://arxiv.org/abs/2302.10291)

Varshini Subhash


[TrojanPuzzle: Covertly Poisoning Code-Suggestion Models. (1%)](http://arxiv.org/abs/2301.02344)

Hojjat Aghakhani, Wei Dai, Andre Manoel, Xavier Fernandes, Anant Kharkar, Christopher Kruegel, Giovanni Vigna, David Evans, Ben Zorn, Robert Sim


## 2023-01-04

[Availability Adversarial Attack and Countermeasures for Deep Learning-based Load Forecasting. (98%)](http://arxiv.org/abs/2301.01832)

Wangkun Xu, Fei Teng


[Beckman Defense. (84%)](http://arxiv.org/abs/2301.01495)

A. V. Subramanyam


[GUAP: Graph Universal Attack Through Adversarial Patching. (81%)](http://arxiv.org/abs/2301.01731)

Xiao Zang, Jie Chen, Bo Yuan


[Enhancement attacks in biomedical machine learning. (1%)](http://arxiv.org/abs/2301.01885)

Matthew Rosenblatt, Javid Dadashkarimi, Dustin Scheinost


## 2023-01-03

[Explainability and Robustness of Deep Visual Classification Models. (92%)](http://arxiv.org/abs/2301.01343)

Jindong Gu


[Look, Listen, and Attack: Backdoor Attacks Against Video Action Recognition. (83%)](http://arxiv.org/abs/2301.00986)

Hasan Abed Al Kader Hammoud, Shuming Liu, Mohammed Alkhrashi, Fahad AlBalawi, Bernard Ghanem


[Backdoor Attacks Against Dataset Distillation. (50%)](http://arxiv.org/abs/2301.01197)

Yugeng Liu, Zheng Li, Michael Backes, Yun Shen, Yang Zhang


[Analysis of Label-Flip Poisoning Attack on Machine Learning Based Malware Detector. (33%)](http://arxiv.org/abs/2301.01044)

Kshitiz Aryal, Maanak Gupta, Mahmoud Abdelsalam


## 2023-01-02

[Efficient Robustness Assessment via Adversarial Spatial-Temporal Focus on Videos. (92%)](http://arxiv.org/abs/2301.00896)

Wei Xingxing, Wang Songping, Yan Huanqian


## 2023-01-01

[Generalizable Black-Box Adversarial Attack with Meta Learning. (99%)](http://arxiv.org/abs/2301.00364)

Fei Yin, Yong Zhang, Baoyuan Wu, Yan Feng, Jingyi Zhang, Yanbo Fan, Yujiu Yang


[ExploreADV: Towards exploratory attack for Neural Networks. (99%)](http://arxiv.org/abs/2301.01223)

Tianzuo Luo, Yuyi Zhong, Siaucheng Khoo


[Trojaning semi-supervised learning model via poisoning wild images on the web. (47%)](http://arxiv.org/abs/2301.00435)

Le Feng, Zhenxing Qian, Sheng Li, Xinpeng Zhang


## 2022-12-30

[Tracing the Origin of Adversarial Attack for Forensic Investigation and Deterrence. (99%)](http://arxiv.org/abs/2301.01218)

Han Fang, Jiyi Zhang, Yupeng Qiu, Ke Xu, Chengfang Fang, Ee-Chien Chang


[Guidance Through Surrogate: Towards a Generic Diagnostic Attack. (99%)](http://arxiv.org/abs/2212.14875)

Muzammal Naseer, Salman Khan, Fatih Porikli, Fahad Shahbaz Khan


[Defense Against Adversarial Attacks on Audio DeepFake Detection. (91%)](http://arxiv.org/abs/2212.14597)

Piotr Kawa, Marcin Plata, Piotr Syga


[Adversarial attacks and defenses on ML- and hardware-based IoT device fingerprinting and identification. (82%)](http://arxiv.org/abs/2212.14677)

Pedro Miguel Sánchez Sánchez, Alberto Huertas Celdrán, Gérôme Bovet, Gregorio Martínez Pérez


[Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples. (22%)](http://arxiv.org/abs/2301.01217)

Jiaming Zhang, Xingjun Ma, Qi Yi, Jitao Sang, Yugang Jiang, Yaowei Wang, Changsheng Xu


[Targeted k-node Collapse Problem: Towards Understanding the Robustness of Local k-core Structure. (1%)](http://arxiv.org/abs/2301.00108)

Yuqian Lv, Bo Zhou, Jinhuan Wang, Qi Xuan


## 2022-12-29

["Real Attackers Don't Compute Gradients": Bridging the Gap Between Adversarial ML Research and Practice. (68%)](http://arxiv.org/abs/2212.14315)

Giovanni Apruzzese, Hyrum S. Anderson, Savino Dambra, David Freeman, Fabio Pierazzi, Kevin A. Roundy


[Detection of out-of-distribution samples using binary neuron activation patterns. (11%)](http://arxiv.org/abs/2212.14268)

Bartlomiej Olber, Krystian Radlak, Adam Popowicz, Michal Szczepankiewicz, Krystian Chachula


## 2022-12-28

[Thermal Heating in ReRAM Crossbar Arrays: Challenges and Solutions. (99%)](http://arxiv.org/abs/2212.13707)

Kamilya Smagulova, Mohammed E. Fouda, Ahmed Eltawil


[Certifying Safety in Reinforcement Learning under Adversarial Perturbation Attacks. (98%)](http://arxiv.org/abs/2212.14115)

Junlin Wu, Hussein Sibai, Yevgeniy Vorobeychik


[Publishing Efficient On-device Models Increases Adversarial Vulnerability. (95%)](http://arxiv.org/abs/2212.13700)

Sanghyun Hong, Nicholas Carlini, Alexey Kurakin


[Differentiable Search of Accurate and Robust Architectures. (92%)](http://arxiv.org/abs/2212.14049)

Yuwei Ou, Xiangning Xie, Shangce Gao, Yanan Sun, Kay Chen Tan, Jiancheng Lv


[Robust Ranking Explanations. (76%)](http://arxiv.org/abs/2212.14106)

Chao Chen, Chenghua Guo, Guixiang Ma, Xi Zhang, Sihong Xie


[Evaluating Generalizability of Deep Learning Models Using Indian-COVID-19 CT Dataset. (1%)](http://arxiv.org/abs/2212.13929)

Suba S, Nita Parekh, Ramesh Loganathan, Vikram Pudi, Chinnababu Sunkavalli


## 2022-12-27

[EDoG: Adversarial Edge Detection For Graph Neural Networks. (98%)](http://arxiv.org/abs/2212.13607)

Xiaojun Xu, Yue Yu, Hanzhang Wang, Alok Lal, Carl A. Gunter, Bo Li


[Learning When to Use Adaptive Adversarial Image Perturbations against Autonomous Vehicles. (86%)](http://arxiv.org/abs/2212.13667)

Hyung-Jin Yoon, Hamidreza Jafarnejadsani, Petros Voulgaris


[Sparse Mixture Once-for-all Adversarial Training for Efficient In-Situ Trade-Off Between Accuracy and Robustness of DNNs. (62%)](http://arxiv.org/abs/2302.03523)

Souvik Kundu, Sairam Sundaresan, Sharath Nittur Sridhar, Shunlin Lu, Han Tang, Peter A. Beerel


[XMAM:X-raying Models with A Matrix to Reveal Backdoor Attacks for Federated Learning. (56%)](http://arxiv.org/abs/2212.13675)

Jianyi Zhang, Fangjiao Zhang, Qichao Jin, Zhiqiang Wang, Xiaodong Lin, Xiali Hei


## 2022-12-25

[Simultaneously Optimizing Perturbations and Positions for Black-box Adversarial Patch Attacks. (99%)](http://arxiv.org/abs/2212.12995)

Xingxing Wei, Ying Guo, Jie Yu, Bo Zhang


## 2022-12-24

[Frequency Regularization for Improving Adversarial Robustness. (99%)](http://arxiv.org/abs/2212.12732)

Binxiao Huang, Chaofan Tao, Rui Lin, Ngai Wong


## 2022-12-23

[Out-of-Distribution Detection with Reconstruction Error and Typicality-based Penalty. (61%)](http://arxiv.org/abs/2212.12641)

Genki Osada, Takahashi Tsubasa, Budrul Ahsan, Takashi Nishide


[Towards Scalable Physically Consistent Neural Networks: an Application to Data-driven Multi-zone Thermal Building Models. (1%)](http://arxiv.org/abs/2212.12380)

Natale Loris Di, Bratislav Svetozarevic, Philipp Heer, Colin Neil Jones


## 2022-12-22

[Adversarial Machine Learning and Defense Game for NextG Signal Classification with Deep Learning. (98%)](http://arxiv.org/abs/2212.11778)

Yalin E. Sagduyu


[Aliasing is a Driver of Adversarial Attacks. (80%)](http://arxiv.org/abs/2212.11760)

Adrián Rodríguez-Muñoz, Antonio Torralba


[GAN-based Domain Inference Attack. (2%)](http://arxiv.org/abs/2212.11810)

Yuechun Gu, Keke Chen


[Hybrid Quantum-Classical Generative Adversarial Network for High Resolution Image Generation. (1%)](http://arxiv.org/abs/2212.11614)

Shu Lok Tsang, Maxwell T. West, Sarah M. Erfani, Muhammad Usman


## 2022-12-21

[Revisiting Residual Networks for Adversarial Robustness: An Architectural Perspective. (80%)](http://arxiv.org/abs/2212.11005)

Shihua Huang, Zhichao Lu, Kalyanmoy Deb, Vishnu Naresh Boddeti


[Vulnerabilities of Deep Learning-Driven Semantic Communications to Backdoor (Trojan) Attacks. (67%)](http://arxiv.org/abs/2212.11205)

Yalin E. Sagduyu, Tugba Erpek, Sennur Ulukus, Aylin Yener


[A Theoretical Study of The Effects of Adversarial Attacks on Sparse Regression. (13%)](http://arxiv.org/abs/2212.11209)

Deepak Maurya, Jean Honorio


## 2022-12-20

[A Comprehensive Study and Comparison of the Robustness of 3D Object Detectors Against Adversarial Attacks. (98%)](http://arxiv.org/abs/2212.10230)

Yifan Zhang, Junhui Hou, Yixuan Yuan


[Multi-head Uncertainty Inference for Adversarial Attack Detection. (98%)](http://arxiv.org/abs/2212.10006)

Yuqi Yang, Songyun Yang, Jiyang Xie. Zhongwei Si, Kai Guo, Ke Zhang, Kongming Liang


[In and Out-of-Domain Text Adversarial Robustness via Label Smoothing. (98%)](http://arxiv.org/abs/2212.10258)

Yahan Yang, Soham Dan, Dan Roth, Insup Lee


[Is Semantic Communications Secure? A Tale of Multi-Domain Adversarial Attacks. (96%)](http://arxiv.org/abs/2212.10438)

Yalin E. Sagduyu, Tugba Erpek, Sennur Ulukus, Aylin Yener


[Unleashing the Power of Visual Prompting At the Pixel Level. (92%)](http://arxiv.org/abs/2212.10556)

Junyang Wu, Xianhang Li, Chen Wei, Huiyu Wang, Alan Yuille, Yuyin Zhou, Cihang Xie


[Learned Systems Security. (78%)](http://arxiv.org/abs/2212.10318)

Roei Schuster, Jin Peng Zhou, Paul Grubbs, Thorsten Eisenhofer, Nicolas Papernot


[Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks. (22%)](http://arxiv.org/abs/2212.10717)

Jimmy Z. Di, Jack Douglas, Jayadev Acharya, Gautam Kamath, Ayush Sekhari


[ReCode: Robustness Evaluation of Code Generation Models. (10%)](http://arxiv.org/abs/2212.10264)

Shiqi Wang, Zheng Li, Haifeng Qian, Chenghao Yang, Zijian Wang, Mingyue Shang, Varun Kumar, Samson Tan, Baishakhi Ray, Parminder Bhatia, Ramesh Nallapati, Murali Krishna Ramanathan, Dan Roth, Bing Xiang


[Defending Against Poisoning Attacks in Open-Domain Question Answering. (8%)](http://arxiv.org/abs/2212.10002)

Orion Weller, Aleem Khan, Nathaniel Weir, Dawn Lawrie, Durme Benjamin Van


[SoK: Analysis of Root Causes and Defense Strategies for Attacks on Microarchitectural Optimizations. (5%)](http://arxiv.org/abs/2212.10221)

Nadja Ramhöj Holtryd, Madhavan Manivannan, Per Stenström


[DISCO: Distilling Phrasal Counterfactuals with Large Language Models. (1%)](http://arxiv.org/abs/2212.10534)

Zeming Chen, Qiyue Gao, Kyle Richardson, Antoine Bosselut, Ashish Sabharwal


## 2022-12-19

[TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven Optimization. (99%)](http://arxiv.org/abs/2212.09254)

Bairu Hou, Jinghan Jia, Yihua Zhang, Guanhua Zhang, Yang Zhang, Sijia Liu, Shiyu Chang


[Towards Robustness of Text-to-SQL Models Against Natural and Realistic Adversarial Table Perturbation. (75%)](http://arxiv.org/abs/2212.09994)

Xinyu Pi, Bing Wang, Yan Gao, Jiaqi Guo, Zhoujun Li, Jian-Guang Lou


[AI Security for Geoscience and Remote Sensing: Challenges and Future Trends. (50%)](http://arxiv.org/abs/2212.09360)

Yonghao Xu, Tao Bai, Weikang Yu, Shizhen Chang, Peter M. Atkinson, Pedram Ghamisi


[Task-Oriented Communications for NextG: End-to-End Deep Learning and AI Security Aspects. (26%)](http://arxiv.org/abs/2212.09668)

Yalin E. Sagduyu, Sennur Ulukus, Aylin Yener


[Flareon: Stealthy any2any Backdoor Injection via Poisoned Augmentation. (2%)](http://arxiv.org/abs/2212.09979)

Tianrui Qin, Xianghuan He, Xitong Gao, Yiren Zhao, Kejiang Ye, Cheng-Zhong Xu


[Exploring Optimal Substructure for Out-of-distribution Generalization via Feature-targeted Model Pruning. (1%)](http://arxiv.org/abs/2212.09458)

Yingchun Wang, Jingcai Guo, Song Guo, Weizhan Zhang, Jie Zhang


## 2022-12-18

[Estimating the Adversarial Robustness of Attributions in Text with Transformers. (99%)](http://arxiv.org/abs/2212.09155)

Adam Ivankay, Mattia Rigotti, Ivan Girardi, Chiara Marchiori, Pascal Frossard


[Minimizing Maximum Model Discrepancy for Transferable Black-box Targeted Attacks. (99%)](http://arxiv.org/abs/2212.09035)

Anqi Zhao, Tong Chu, Yahao Liu, Wen Li, Jingjing Li, Lixin Duan


[Discrete Point-wise Attack Is Not Enough: Generalized Manifold Adversarial Attack for Face Recognition. (99%)](http://arxiv.org/abs/2301.06083)

Qian Li, Yuxiao Hu, Ye Liu, Dongxiao Zhang, Xin Jin, Yuntian Chen


[Fine-Tuning Is All You Need to Mitigate Backdoor Attacks. (4%)](http://arxiv.org/abs/2212.09067)

Zeyang Sha, Xinlei He, Pascal Berrang, Mathias Humbert, Yang Zhang


## 2022-12-17

[Confidence-aware Training of Smoothed Classifiers for Certified Robustness. (86%)](http://arxiv.org/abs/2212.09000)

Jongheon Jeong, Seojin Kim, Jinwoo Shin


[A Review of Speech-centric Trustworthy Machine Learning: Privacy, Safety, and Fairness. (2%)](http://arxiv.org/abs/2212.09006)

Tiantian Feng, Rajat Hebbar, Nicholas Mehlman, Xuan Shi, Aditya Kommineni, and Shrikanth Narayanan


[HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation. (1%)](http://arxiv.org/abs/2212.08853)

Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang, Songfang Huang


## 2022-12-16

[Adversarial Example Defense via Perturbation Grading Strategy. (99%)](http://arxiv.org/abs/2212.08341)

Shaowei Zhu, Wanli Lyu, Bin Li, Zhaoxia Yin, Bin Luo


[WebAssembly Diversification for Malware Evasion. (5%)](http://arxiv.org/abs/2212.08427)

Javier Cabrera-Arteaga, Martin Monperrus, Tim Toady, Benoit Baudry


[Biomedical image analysis competitions: The state of current participation practice. (4%)](http://arxiv.org/abs/2212.08568)

Matthias Eisenmann, Annika Reinke, Vivienn Weru, Minu Dietlinde Tizabi, Fabian Isensee, Tim J. Adler, Patrick Godau, Veronika Cheplygina, Michal Kozubek, Sharib Ali, Anubha Gupta, Jan Kybic, Alison Noble, Solórzano Carlos Ortiz de, Samiksha Pachade, Caroline Petitjean, Daniel Sage, Donglai Wei, Elizabeth Wilden, Deepak Alapatt, Vincent Andrearczyk, Ujjwal Baid, Spyridon Bakas, Niranjan Balu, Sophia Bano, Vivek Singh Bawa, Jorge Bernal, Sebastian Bodenstedt, Alessandro Casella, Jinwook Choi, Olivier Commowick, Marie Daum, Adrien Depeursinge, Reuben Dorent, Jan Egger, Hannah Eichhorn, Sandy Engelhardt, Melanie Ganz, Gabriel Girard, Lasse Hansen, Mattias Heinrich, Nicholas Heller, Alessa Hering, Arnaud Huaulmé, Hyunjeong Kim, Bennett Landman, Hongwei Bran Li, Jianning Li, Jun Ma, Anne Martel, Carlos Martín-Isla, Bjoern Menze, Chinedu Innocent Nwoye, Valentin Oreiller, Nicolas Padoy, Sarthak Pati, Kelly Payette, Carole Sudre, Wijnen Kimberlin van, Armine Vardazaryan, Tom Vercauteren, Martin Wagner, Chuanbo Wang, Moi Hoon Yap, Zeyun Yu, Chun Yuan, Maximilian Zenk, Aneeq Zia, David Zimmerer, Rina Bao, Chanyeol Choi, Andrew Cohen, Oleh Dzyubachyk, Adrian Galdran, Tianyuan Gan, Tianqi Guo, Pradyumna Gupta, Mahmood Haithami, Edward Ho, Ikbeom Jang, Zhili Li, Zhengbo Luo, Filip Lux, Sokratis Makrogiannis, Dominik Müller, Young-tack Oh, Subeen Pang, Constantin Pape, Gorkem Polat, Charlotte Rosalie Reed, Kanghyun Ryu, Tim Scherr, Vajira Thambawita, Haoyu Wang, Xinliang Wang, Kele Xu, Hung Yeh, Doyeob Yeo, Yixuan Yuan, Yan Zeng, Xin Zhao, Julian Abbing, Jannes Adam, Nagesh Adluru, Niklas Agethen, Salman Ahmed, Yasmina Al Khalil, Mireia Alenyà, Esa Alhoniemi, Chengyang An, Talha Anwar, Tewodros Weldebirhan Arega, Netanell Avisdris, Dogu Baran Aydogan, Yingbin Bai, Maria Baldeon Calisto, Berke Doga Basaran, Marcel Beetz, Cheng Bian, Hao Bian, Kevin Blansit, Louise Bloch, Robert Bohnsack, Sara Bosticardo, Jack Breen, Mikael Brudfors, Raphael Brüngel, Mariano Cabezas, Alberto Cacciola, Zhiwei Chen, Yucong Chen, Daniel Tianming Chen, Minjeong Cho, Min-Kook Choi, Chuantao Xie Chuantao Xie, Dana Cobzas, Julien Cohen-Adad, Jorge Corral Acero, Sujit Kumar Das, Oliveira Marcela de, Hanqiu Deng, Guiming Dong, Lars Doorenbos, Cory Efird, Di Fan, Mehdi Fatan Serj, Alexandre Fenneteau, Lucas Fidon, Patryk Filipiak, René Finzel, Nuno R. Freitas, Christoph M. Friedrich, Mitchell Fulton, Finn Gaida, Francesco Galati, Christoforos Galazis, Chang Hee Gan, Zheyao Gao, Shengbo Gao, Matej Gazda, Beerend Gerats, Neil Getty, Adam Gibicar, Ryan Gifford, Sajan Gohil, Maria Grammatikopoulou, Daniel Grzech, Orhun Güley, Timo Günnemann, Chunxu Guo, Sylvain Guy, Heonjin Ha, Luyi Han, Il Song Han, Ali Hatamizadeh, Tian He, Jimin Heo, Sebastian Hitziger, SeulGi Hong, SeungBum Hong, Rian Huang, Ziyan Huang, Markus Huellebrand, Stephan Huschauer, Mustaffa Hussain, Tomoo Inubushi, Ece Isik Polat, Mojtaba Jafaritadi, SeongHun Jeong, Bailiang Jian, Yuanhong Jiang, Zhifan Jiang, Yueming Jin, Smriti Joshi, Abdolrahim Kadkhodamohammadi, Reda Abdellah Kamraoui, Inha Kang, Junghwa Kang, Davood Karimi, April Khademi, Muhammad Irfan Khan, Suleiman A. Khan, Rishab Khantwal, Kwang-Ju Kim, Timothy Kline, Satoshi Kondo, Elina Kontio, Adrian Krenzer, Artem Kroviakov, Hugo Kuijf, Satyadwyoom Kumar, Rosa Francesco La, Abhi Lad, Doohee Lee, Minho Lee, Chiara Lena, Hao Li, Ling Li, Xingyu Li, Fuyuan Liao, KuanLun Liao, Arlindo Limede Oliveira, Chaonan Lin, Shan Lin, Akis Linardos, Marius George Linguraru, Han Liu, Tao Liu, Di Liu, Yanling Liu, João Lourenço-Silva, Jingpei Lu, Jiangshan Lu, Imanol Luengo, Christina B. Lund, Huan Minh Luu, Yi Lv, Yi Lv, Uzay Macar, Leon Maechler, Sina Mansour L., Kenji Marshall, Moona Mazher, Richard McKinley, Alfonso Medela, Felix Meissen, Mingyuan Meng, Dylan Miller, Seyed Hossein Mirjahanmardi, Arnab Mishra, Samir Mitha, Hassan Mohy-ud-Din, Tony Chi Wing Mok, Gowtham Krishnan Murugesan, Enamundram Naga Karthik, Sahil Nalawade, Jakub Nalepa, Mohamed Naser, Ramin Nateghi, Hammad Naveed, Quang-Minh Nguyen, Cuong Nguyen Quoc, Brennan Nichyporuk, Bruno Oliveira, David Owen, Jimut Bahan Pal, Junwen Pan, Wentao Pan, Winnie Pang, Bogyu Park, Vivek Pawar, Kamlesh Pawar, Michael Peven, Lena Philipp, Tomasz Pieciak, Szymon Plotka, Marcel Plutat, Fattaneh Pourakpour, Domen Preložnik, Kumaradevan Punithakumar, Abdul Qayyum, Sandro Queirós, Arman Rahmim, Salar Razavi, Jintao Ren, Mina Rezaei, Jonathan Adam Rico, ZunHyan Rieu, Markus Rink, Johannes Roth, Yusely Ruiz-Gonzalez, Numan Saeed, Anindo Saha, Mostafa Salem, Ricardo Sanchez-Matilla, Kurt Schilling, Wei Shao, Zhiqiang Shen, Ruize Shi, Pengcheng Shi, Daniel Sobotka, Théodore Soulier, Bella Specktor Fadida, Danail Stoyanov, Timothy Sum Hon Mun, Xiaowu Sun, Rong Tao, Franz Thaler, Antoine Théberge, Felix Thielke, Helena Torres, Kareem A. Wahid, Jiacheng Wang, YiFei Wang, Wei Wang, Xiong Wang, Jianhui Wen, Ning Wen, Marek Wodzinski, Ye Wu, Fangfang Xia, Tianqi Xiang, Chen Xiaofei, Lizhan Xu, Tingting Xue, Yuxuan Yang, Lin Yang, Kai Yao, Huifeng Yao, Amirsaeed Yazdani, Michael Yip, Hwanseung Yoo, Fereshteh Yousefirizi, Shunkai Yu, Lei Yu, Jonathan Zamora, Ramy Ashraf Zeineldin, Dewen Zeng, Jianpeng Zhang, Bokai Zhang, Jiapeng Zhang, Fan Zhang, Huahong Zhang, Zhongchen Zhao, Zixuan Zhao, Jiachen Zhao, Can Zhao, Qingshuo Zheng, Yuheng Zhi, Ziqi Zhou, Baosheng Zou, Klaus Maier-Hein, Paul F. Jäger, Annette Kopp-Schneider, Lena Maier-Hein


[Better May Not Be Fairer: Can Data Augmentation Mitigate Subgroup Degradation? (1%)](http://arxiv.org/abs/2212.08649)

Ming-Chang Chiu, Pin-Yu Chen, Xuezhe Ma


[On Human Visual Contrast Sensitivity and Machine Vision Robustness: A Comparative Study. (1%)](http://arxiv.org/abs/2212.08650)

Ming-Chang Chiu, Yingfei Wang, Derrick Eui Gyu Kim, Pin-Yu Chen, Xuezhe Ma


## 2022-12-15

[Alternating Objectives Generates Stronger PGD-Based Adversarial Attacks. (98%)](http://arxiv.org/abs/2212.07992)

Nikolaos Antoniou, Efthymios Georgiou, Alexandros Potamianos


[On Evaluating Adversarial Robustness of Chest X-ray Classification: Pitfalls and Best Practices. (84%)](http://arxiv.org/abs/2212.08130)

Salah Ghamizi, Maxime Cordy, Michail Papadakis, Yves Le Traon


[Are Multimodal Models Robust to Image and Text Perturbations? (5%)](http://arxiv.org/abs/2212.08044)

Jielin Qiu, Yi Zhu, Xingjian Shi, Florian Wenzel, Zhiqiang Tang, Ding Zhao, Bo Li, Mu Li


[Holistic risk assessment of inference attacks in machine learning. (4%)](http://arxiv.org/abs/2212.10628)

Yang Yang


[Defending against cybersecurity threats to the payments and banking system. (2%)](http://arxiv.org/abs/2212.12307)

Williams Haruna, Toyin Ajiboro Aremu, Yetunde Ajao Modupe


[White-box Inference Attacks against Centralized Machine Learning and Federated Learning. (1%)](http://arxiv.org/abs/2301.03595)

Jingyi Ge


## 2022-12-14

[SAIF: Sparse Adversarial and Interpretable Attack Framework. (98%)](http://arxiv.org/abs/2212.07495)

Tooba Imtiaz, Morgan Kohler, Jared Miller, Zifeng Wang, Mario Sznaier, Octavia Camps, Jennifer Dy


[Dissecting Distribution Inference. (88%)](http://arxiv.org/abs/2212.07591)

Anshuman Suri, Yifu Lu, Yanjin Chen, David Evans


[Generative Robust Classification. (11%)](http://arxiv.org/abs/2212.07283)

Xuwang Yin


[Synthesis of Adversarial DDOS Attacks Using Tabular Generative Adversarial Networks. (8%)](http://arxiv.org/abs/2212.14109)

Abdelmageed Ahmed Hassan, Mohamed Sayed Hussein, Ahmed Shehata AboMoustafa, Sarah Hossam Elmowafy


[DOC-NAD: A Hybrid Deep One-class Classifier for Network Anomaly Detection. (1%)](http://arxiv.org/abs/2212.07558)

Mohanad Sarhan, Gayan Kulatilleke, Wai Weng Lo, Siamak Layeghy, Marius Portmann


## 2022-12-13

[Unfolding Local Growth Rate Estimates for (Almost) Perfect Adversarial Detection. (99%)](http://arxiv.org/abs/2212.06776)

Peter Lorenz, Margret Keuper, Janis Keuper


[Object-fabrication Targeted Attack for Object Detection. (99%)](http://arxiv.org/abs/2212.06431)

Xuchong Zhang, Changfeng Sun, Haoliang Han, Hang Wang, Hongbin Sun, Nanning Zheng


[Adversarial Attacks and Defences for Skin Cancer Classification. (99%)](http://arxiv.org/abs/2212.06822)

Vinay Jogani, Joy Purohit, Ishaan Shivhare, Samina Attari, Shraddha Surtkar


[Towards Efficient and Domain-Agnostic Evasion Attack with High-dimensional Categorical Inputs. (80%)](http://arxiv.org/abs/2212.06836)

Hongyan Bao, Yufei Han, Yujun Zhou, Xin Gao, Xiangliang Zhang


[Understanding Zero-Shot Adversarial Robustness for Large-Scale Models. (73%)](http://arxiv.org/abs/2212.07016)

Chengzhi Mao, Scott Geng, Junfeng Yang, Xin Wang, Carl Vondrick


[Pixel is All You Need: Adversarial Trajectory-Ensemble Active Learning for Salient Object Detection. (56%)](http://arxiv.org/abs/2212.06493)

Zhenyu Wu, Lin Wang, Wei Wang, Qing Xia, Chenglizhao Chen, Aimin Hao, Shuo Li


[AdvCat: Domain-Agnostic Robustness Assessment for Cybersecurity-Critical Applications with Categorical Inputs. (56%)](http://arxiv.org/abs/2212.13989)

Helene Orsini, Hongyan Bao, Yujun Zhou, Xiangrui Xu, Yufei Han, Longyang Yi, Wei Wang, Xin Gao, Xiangliang Zhang


[Privacy-preserving Security Inference Towards Cloud-Edge Collaborative Using Differential Privacy. (1%)](http://arxiv.org/abs/2212.06428)

Yulong Wang, Xingshu Chen, Qixu Wang


[Boosting Semi-Supervised Learning with Contrastive Complementary Labeling. (1%)](http://arxiv.org/abs/2212.06643)

Qinyi Deng, Yong Guo, Zhibang Yang, Haolin Pan, Jian Chen


## 2022-12-12

[SRoUDA: Meta Self-training for Robust Unsupervised Domain Adaptation. (98%)](http://arxiv.org/abs/2212.05917)

Wanqing Zhu, Jia-Li Yin, Bo-Hao Chen, Ximeng Liu


[Adversarially Robust Video Perception by Seeing Motion. (98%)](http://arxiv.org/abs/2212.07815)

Lingyu Zhang, Chengzhi Mao, Junfeng Yang, Carl Vondrick


[A Survey on Reinforcement Learning Security with Application to Autonomous Driving. (96%)](http://arxiv.org/abs/2212.06123)

Ambra Demontis, Maura Pintor, Luca Demetrio, Kathrin Grosse, Hsiao-Ying Lin, Chengfang Fang, Battista Biggio, Fabio Roli


[HOTCOLD Block: Fooling Thermal Infrared Detectors with a Novel Wearable Design. (96%)](http://arxiv.org/abs/2212.05709)

Hui Wei, Zhixiang Wang, Xuemei Jia, Yinqiang Zheng, Hao Tang, Shin'ichi Satoh, Zheng Wang


[Robust Perception through Equivariance. (96%)](http://arxiv.org/abs/2212.06079)

Chengzhi Mao, Lingyu Zhang, Abhishek Joshi, Junfeng Yang, Hao Wang, Carl Vondrick


[Despite "super-human" performance, current LLMs are unsuited for decisions about ethics and safety. (75%)](http://arxiv.org/abs/2212.06295)

Joshua Albrecht, Ellie Kitanidis, Abraham J. Fetterman


[AFLGuard: Byzantine-robust Asynchronous Federated Learning. (15%)](http://arxiv.org/abs/2212.06325)

Minghong Fang, Jia Liu, Neil Zhenqiang Gong, Elizabeth S. Bentley


[Carpet-bombing patch: attacking a deep network without usual requirements. (2%)](http://arxiv.org/abs/2212.05827)

Pol Labarbarie, Adrien Chan-Hon-Tong, Stéphane Herbin, Milad Leyli-Abadi


## 2022-12-11

[DISCO: Adversarial Defense with Local Implicit Functions. (99%)](http://arxiv.org/abs/2212.05630)

Chih-Hui Ho, Nuno Vasconcelos


[REAP: A Large-Scale Realistic Adversarial Patch Benchmark. (98%)](http://arxiv.org/abs/2212.05680)

Nabeel Hingun, Chawin Sitawarin, Jerry Li, David Wagner


## 2022-12-10

[General Adversarial Defense Against Black-box Attacks via Pixel Level and Feature Level Distribution Alignments. (99%)](http://arxiv.org/abs/2212.05387)

Xiaogang Xu, Hengshuang Zhao, Philip Torr, Jiaya Jia


[Untargeted Attack against Federated Recommendation Systems via Poisonous Item Embeddings and the Defense. (93%)](http://arxiv.org/abs/2212.05399)

Yang Yu, Qi Liu, Likang Wu, Runlong Yu, Sanshi Lei Yu, Zaixi Zhang


[Targeted Adversarial Attacks on Deep Reinforcement Learning Policies via Model Checking. (93%)](http://arxiv.org/abs/2212.05337)

Dennis Gross, Thiago D. Simao, Nils Jansen, Guillermo A. Perez


[Mitigating Adversarial Gray-Box Attacks Against Phishing Detectors. (54%)](http://arxiv.org/abs/2212.05380)

Giovanni Apruzzese, V. S. Subrahmanian


[How to Backdoor Diffusion Models? (12%)](http://arxiv.org/abs/2212.05400)

Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho


[Identifying the Source of Vulnerability in Explanation Discrepancy: A Case Study in Neural Text Classification. (1%)](http://arxiv.org/abs/2212.05327)

Ruixuan Tang, Hanjie Chen, Yangfeng Ji


## 2022-12-09

[Understanding and Combating Robust Overfitting via Input Loss Landscape Analysis and Regularization. (98%)](http://arxiv.org/abs/2212.04985)

Lin Li, Michael Spratling


[Expeditious Saliency-guided Mix-up through Random Gradient Thresholding. (2%)](http://arxiv.org/abs/2212.04875)

Minh-Long Luu, Zeyi Huang, Eric P. Xing, Yong Jae Lee, Haohan Wang


[Spurious Features Everywhere -- Large-Scale Detection of Harmful Spurious Features in ImageNet. (1%)](http://arxiv.org/abs/2212.04871)

Yannic Neuhaus, Maximilian Augustin, Valentyn Boreiko, Matthias Hein


[Robustness Implies Privacy in Statistical Estimation. (1%)](http://arxiv.org/abs/2212.05015)

Samuel B. Hopkins, Gautam Kamath, Mahbod Majid, Shyam Narayanan


[Selective Amnesia: On Efficient, High-Fidelity and Blind Suppression of Backdoor Effects in Trojaned Machine Learning Models. (1%)](http://arxiv.org/abs/2212.04687)

Rui Zhu, Di Tang, Siyuan Tang, XiaoFeng Wang, Haixu Tang


[QVIP: An ILP-based Formal Verification Approach for Quantized Neural Networks. (1%)](http://arxiv.org/abs/2212.11138)

Yedi Zhang, Zhe Zhao, Fu Song, Min Zhang, Taolue Chen, Jun Sun


## 2022-12-08

[Targeted Adversarial Attacks against Neural Network Trajectory Predictors. (99%)](http://arxiv.org/abs/2212.04138)

Kaiyuan Tan, Jun Wang, Yiannis Kantaros


[XRand: Differentially Private Defense against Explanation-Guided Attacks. (68%)](http://arxiv.org/abs/2212.04454)

Truc Nguyen, Phung Lai, NhatHai Phan, My T. Thai


[Robust Graph Representation Learning via Predictive Coding. (22%)](http://arxiv.org/abs/2212.04656)

Billy Byiringiro, Tommaso Salvatori, Thomas Lukasiewicz


## 2022-12-07

[Use of Cryptography in Malware Obfuscation. (1%)](http://arxiv.org/abs/2212.04008)

Hassan Jameel Asghar, Benjamin Zi Hao Zhao, Muhammad Ikram, Giang Nguyen, Dali Kaafar, Sean Lamont, Daniel Coscia


## 2022-12-06

[Pre-trained Encoders in Self-Supervised Learning Improve Secure and Privacy-preserving Supervised Learning. (96%)](http://arxiv.org/abs/2212.03334)

Hongbin Liu, Wenjie Qu, Jinyuan Jia, Neil Zhenqiang Gong


## 2022-12-05

[Enhancing Quantum Adversarial Robustness by Randomized Encodings. (99%)](http://arxiv.org/abs/2212.02531)

Weiyuan Gong, Dong Yuan, Weikang Li, Dong-Ling Deng


[Multiple Perturbation Attack: Attack Pixelwise Under Different $\ell_p$-norms For Better Adversarial Performance. (99%)](http://arxiv.org/abs/2212.03069)

Ngoc N. Tran, Anh Tuan Bui, Dinh Phung, Trung Le


[FaceQAN: Face Image Quality Assessment Through Adversarial Noise Exploration. (92%)](http://arxiv.org/abs/2212.02127)

Žiga Babnik, Peter Peer, Vitomir Štruc


[Refiner: Data Refining against Gradient Leakage Attacks in Federated Learning. (47%)](http://arxiv.org/abs/2212.02042)

Mingyuan Fan, Cen Chen, Chengyu Wang, Wenmeng Zhou, Jun Huang, Ximeng Liu, Wenzhong Guo


[Blessings and Curses of Covariate Shifts: Adversarial Learning Dynamics, Directional Convergence, and Equilibria. (8%)](http://arxiv.org/abs/2212.02457)

Tengyuan Liang


[What is the Solution for State-Adversarial Multi-Agent Reinforcement Learning? (3%)](http://arxiv.org/abs/2212.02705)

Songyang Han, Sanbao Su, Sihong He, Shuo Han, Haizhao Yang, Fei Miao


[Spuriosity Rankings: Sorting Data for Spurious Correlation Robustness. (1%)](http://arxiv.org/abs/2212.02648)

Mazda Moayeri, Wenxiao Wang, Sahil Singla, Soheil Feizi


[Efficient Malware Analysis Using Metric Embeddings. (1%)](http://arxiv.org/abs/2212.02663)

Ethan M. Rudd, David Krisiloff, Scott Coull, Daniel Olszewski, Edward Raff, James Holt


## 2022-12-04

[Bayesian Learning with Information Gain Provably Bounds Risk for a Robust Adversarial Defense. (98%)](http://arxiv.org/abs/2212.02003)

Bao Gia Doan, Ehsan Abbasnejad, Javen Qinfeng Shi, Damith C. Ranasinghe


[Recognizing Object by Components with Human Prior Knowledge Enhances Adversarial Robustness of Deep Neural Networks. (88%)](http://arxiv.org/abs/2212.01806)

Xiao Li, Ziqi Wang, Bo Zhang, Fuchun Sun, Xiaolin Hu


[CSTAR: Towards Compact and STructured Deep Neural Networks with Adversarial Robustness. (82%)](http://arxiv.org/abs/2212.01957)

Huy Phan, Miao Yin, Yang Sui, Bo Yuan, Saman Zonouz


[FedCC: Robust Federated Learning against Model Poisoning Attacks. (45%)](http://arxiv.org/abs/2212.01976)

Hyejun Jeong, Hamin Son, Seohu Lee, Jayun Hyun, Tai-Myoung Chung


[ConfounderGAN: Protecting Image Data Privacy with Causal Confounder. (8%)](http://arxiv.org/abs/2212.01767)

Qi Tian, Kun Kuang, Kelu Jiang, Furui Liu, Zhihua Wang, Fei Wu


## 2022-12-03

[LDL: A Defense for Label-Based Membership Inference Attacks. (83%)](http://arxiv.org/abs/2212.01688)

Arezoo Rajabi, Dinuka Sahabandu, Luyao Niu, Bhaskar Ramasubramanian, Radha Poovendran


[Security Analysis of SplitFed Learning. (8%)](http://arxiv.org/abs/2212.01716)

Momin Ahmad Khan, Virat Shejwalkar, Amir Houmansadr, Fatima Muhammad Anwar


## 2022-12-02

[Membership Inference Attacks Against Semantic Segmentation Models. (45%)](http://arxiv.org/abs/2212.01082)

Tomas Chobola, Dmitrii Usynin, Georgios Kaissis


[Guaranteed Conformance of Neurosymbolic Models to Natural Constraints. (1%)](http://arxiv.org/abs/2212.01346)

Kaustubh Sridhar, Souradeep Dutta, James Weimer, Insup Lee


## 2022-12-01

[Purifier: Defending Data Inference Attacks via Transforming Confidence Scores. (89%)](http://arxiv.org/abs/2212.00612)

Ziqi Yang, Lijin Wang, Da Yang, Jie Wan, Ziming Zhao, Ee-Chien Chang, Fan Zhang, Kui Ren


[Pareto Regret Analyses in Multi-objective Multi-armed Bandit. (41%)](http://arxiv.org/abs/2212.00884)

Mengfan Xu, Diego Klabjan


[All You Need Is Hashing: Defending Against Data Reconstruction Attack in Vertical Federated Learning. (2%)](http://arxiv.org/abs/2212.00325)

Pengyu Qiu, Xuhong Zhang, Shouling Ji, Yuwen Pu, Ting Wang


[Generalizing and Improving Jacobian and Hessian Regularization. (1%)](http://arxiv.org/abs/2212.00311)

Chenwei Cui, Zehao Yan, Guangshen Liu, Liangfu Lu


[On the Limit of Explaining Black-box Temporal Graph Neural Networks. (1%)](http://arxiv.org/abs/2212.00952)

Minh N. Vu, My T. Thai


[SimpleMind adds thinking to deep neural networks. (1%)](http://arxiv.org/abs/2212.00951)

Youngwon Choi, M. Wasil Wahi-Anwar, Matthew S. Brown


## 2022-11-30

[Towards Interpreting Vulnerability of Multi-Instance Learning via Customized and Universal Adversarial Perturbations. (97%)](http://arxiv.org/abs/2211.17071)

Yu-Xuan Zhang, Hua Meng, Xue-Mei Cao, Zhengchun Zhou, Mei Yang, Avik Ranjan Adhikary


[Interpretation of Neural Networks is Susceptible to Universal Adversarial Perturbations. (84%)](http://arxiv.org/abs/2212.03095)

Haniyeh Ehsani Oskouie, Farzan Farnia


[Efficient Adversarial Input Generation via Neural Net Patching. (73%)](http://arxiv.org/abs/2211.16808)

Tooba Khan, Kumar Madhukar, Subodh Vishnu Sharma


[Toward Robust Diagnosis: A Contour Attention Preserving Adversarial Defense for COVID-19 Detection. (69%)](http://arxiv.org/abs/2211.16806)

Kun Xiang, Xing Zhang, Jinwen She, Jinpeng Liu, Haohan Wang, Shiqi Deng, Shancheng Jiang


[Tight Certification of Adversarially Trained Neural Networks via Nonconvex Low-Rank Semidefinite Relaxations. (38%)](http://arxiv.org/abs/2211.17244)

Hong-Ming Chiu, Richard Y. Zhang


[Improved Smoothed Analysis of 2-Opt for the Euclidean TSP. (8%)](http://arxiv.org/abs/2211.16908)

Bodo Manthey, Rhijn Jesse van


## 2022-11-29

[Ada3Diff: Defending against 3D Adversarial Point Clouds via Adaptive Diffusion. (99%)](http://arxiv.org/abs/2211.16247)

Kui Zhang, Hang Zhou, Jie Zhang, Qidong Huang, Weiming Zhang, Nenghai Yu


[Understanding and Enhancing Robustness of Concept-based Models. (99%)](http://arxiv.org/abs/2211.16080)

Sanchit Sinha, Mengdi Huai, Jianhui Sun, Aidong Zhang


[Advancing Deep Metric Learning Through Multiple Batch Norms And Multi-Targeted Adversarial Examples. (88%)](http://arxiv.org/abs/2211.16253)

Inderjeet Singh, Kazuya Kakizaki, Toshinori Araki


[Penalizing Confident Predictions on Largely Perturbed Inputs Does Not Improve Out-of-Distribution Generalization in Question Answering. (83%)](http://arxiv.org/abs/2211.16093)

Kazutoshi Shinoda, Saku Sugawara, Akiko Aizawa


[Quantization-aware Interval Bound Propagation for Training Certifiably Robust Quantized Neural Networks. (73%)](http://arxiv.org/abs/2211.16187)

Mathias Lechner, Đorđe Žikelić, Krishnendu Chatterjee, Thomas A. Henzinger, Daniela Rus


[AdvMask: A Sparse Adversarial Attack Based Data Augmentation Method for Image Classification. (54%)](http://arxiv.org/abs/2211.16040)

Suorong Yang, Jinqiao Li, Jian Zhao, Furao Shen


[A3T: Accuracy Aware Adversarial Training. (10%)](http://arxiv.org/abs/2211.16316)

Enes Altinisik, Safa Messaoud, Husrev Taha Sencar, Sanjay Chawla


[Building Resilience to Out-of-Distribution Visual Data via Input Optimization and Model Finetuning. (1%)](http://arxiv.org/abs/2211.16228)

Christopher J. Holder, Majid Khonji, Jorge Dias, Muhammad Shafique


## 2022-11-28

[Adversarial Artifact Detection in EEG-Based Brain-Computer Interfaces. (99%)](http://arxiv.org/abs/2212.00727)

Xiaoqing Chen, Dongrui Wu


[Interpretations Cannot Be Trusted: Stealthy and Effective Adversarial Perturbations against Interpretable Deep Learning. (95%)](http://arxiv.org/abs/2211.15926)

Eldor Abdukhamidov, Mohammed Abuhamad, Simon S. Woo, Eric Chan-Tin, Tamer Abuhmed


[Training Time Adversarial Attack Aiming the Vulnerability of Continual Learning. (83%)](http://arxiv.org/abs/2211.15875)

Gyojin Han, Jaehyun Choi, Hyeong Gwon Hong, Junmo Kim


[Towards More Robust Interpretation via Local Gradient Alignment. (76%)](http://arxiv.org/abs/2211.15900)

Sunghwan Joo, Seokhyeon Jeong, Juyeon Heo, Adrian Weller, Taesup Moon


[Understanding the Impact of Adversarial Robustness on Accuracy Disparity. (31%)](http://arxiv.org/abs/2211.15762)

Yuzheng Hu, Fan Wu, Hongyang Zhang, Han Zhao


[How Important are Good Method Names in Neural Code Generation? A Model Robustness Perspective. (13%)](http://arxiv.org/abs/2211.15844)

Guang Yang, Yu Zhou, Wenhua Yang, Tao Yue, Xiang Chen, Taolue Chen


[Rethinking the Number of Shots in Robust Model-Agnostic Meta-Learning. (8%)](http://arxiv.org/abs/2211.15180)

Xiaoyue Duan, Guoliang Kang, Runqi Wang, Shumin Han, Song Xue, Tian Wang, Baochang Zhang


[Attack on Unfair ToS Clause Detection: A Case Study using Universal Adversarial Triggers. (8%)](http://arxiv.org/abs/2211.15556)

Shanshan Xu, Irina Broda, Rashid Haddad, Marco Negrini, Matthias Grabmair


[Gamma-convergence of a nonlocal perimeter arising in adversarial machine learning. (3%)](http://arxiv.org/abs/2211.15223)

Leon Bungert, Kerrek Stinson


[CoNAL: Anticipating Outliers with Large Language Models. (1%)](http://arxiv.org/abs/2211.15718)

Albert Xu, Xiang Ren, Robin Jia


[Learning Antidote Data to Individual Unfairness. (1%)](http://arxiv.org/abs/2211.15897)

Peizhao Li, Ethan Xia, Hongfu Liu


## 2022-11-27

[Imperceptible Adversarial Attack via Invertible Neural Networks. (99%)](http://arxiv.org/abs/2211.15030)

Zihan Chen, Ziyue Wang, Junjie Huang, Wentao Zhao, Xiao Liu, Dejian Guan


[Foiling Explanations in Deep Neural Networks. (98%)](http://arxiv.org/abs/2211.14860)

Snir Vitrack Tamam, Raz Lapid, Moshe Sipper


[Navigation as the Attacker Wishes? Towards Building Byzantine-Robust Embodied Agents under Federated Learning. (84%)](http://arxiv.org/abs/2211.14769)

Yunchao Zhang, Zonglin Di, Kaiwen Zhou, Cihang Xie, Xin Wang


[Traditional Classification Neural Networks are Good Generators: They are Competitive with DDPMs and GANs. (50%)](http://arxiv.org/abs/2211.14794)

Guangrun Wang, Philip H. S. Torr


[Federated Learning Attacks and Defenses: A Survey. (47%)](http://arxiv.org/abs/2211.14952)

Yao Chen, Yijie Gui, Hong Lin, Wensheng Gan, Yongdong Wu


[Adversarial Rademacher Complexity of Deep Neural Networks. (47%)](http://arxiv.org/abs/2211.14966)

Jiancong Xiao, Yanbo Fan, Ruoyu Sun, Zhi-Quan Luo


## 2022-11-26

[Game Theoretic Mixed Experts for Combinational Adversarial Machine Learning. (99%)](http://arxiv.org/abs/2211.14669)

Ethan Rathbun, Kaleel Mahmood, Sohaib Ahmad, Caiwen Ding, Dijk Marten van


## 2022-11-25

[Boundary Adversarial Examples Against Adversarial Overfitting. (99%)](http://arxiv.org/abs/2211.14088)

Muhammad Zaid Hameed, Beat Buesser


[Supervised Contrastive Prototype Learning: Augmentation Free Robust Neural Network. (98%)](http://arxiv.org/abs/2211.14424)

Iordanis Fostiropoulos, Laurent Itti


[Beyond Smoothing: Unsupervised Graph Representation Learning with Edge Heterophily Discriminating. (3%)](http://arxiv.org/abs/2211.14065)

Yixin Liu, Yizhen Zheng, Daokun Zhang, Vincent CS Lee, Shirui Pan


[TrustGAN: Training safe and trustworthy deep learning models through generative adversarial networks. (1%)](http://arxiv.org/abs/2211.13991)

Hélion du Mas des Bourboux


## 2022-11-24

[SAGA: Spectral Adversarial Geometric Attack on 3D Meshes. (98%)](http://arxiv.org/abs/2211.13775)

Tomer Stolik, Itai Lang, Shai Avidan


[Explainable and Safe Reinforcement Learning for Autonomous Air Mobility. (92%)](http://arxiv.org/abs/2211.13474)

Lei Wang, Hongyu Yang, Yi Lin, Suwan Yin, Yuankai Wu


[Tracking Dataset IP Use in Deep Neural Networks. (76%)](http://arxiv.org/abs/2211.13535)

Seonhye Park, Alsharif Abuadbba, Shuo Wang, Kristen Moore, Yansong Gao, Hyoungshick Kim, Surya Nepal


[Neural Network Complexity of Chaos and Turbulence. (41%)](http://arxiv.org/abs/2211.15382)

Tim Whittaker, Romuald A. Janik, Yaron Oz


[Seeds Don't Lie: An Adaptive Watermarking Framework for Computer Vision Models. (8%)](http://arxiv.org/abs/2211.13644)

Jacob Shams, Ben Nassi, Ikuya Morikawa, Toshiya Shimizu, Asaf Shabtai, Yuval Elovici


[Generative Joint Source-Channel Coding for Semantic Image Transmission. (1%)](http://arxiv.org/abs/2211.13772)

Ecenaz Erdemir, Tze-Yang Tung, Pier Luigi Dragotti, Deniz Gunduz


[CycleGANWM: A CycleGAN watermarking method for ownership verification. (1%)](http://arxiv.org/abs/2211.13737)

Dongdong Lin, Benedetta Tondi, Bin Li, Mauro Barni


## 2022-11-23

[Query Efficient Cross-Dataset Transferable Black-Box Attack on Action Recognition. (99%)](http://arxiv.org/abs/2211.13171)

Rohit Gupta, Naveed Akhtar, Gaurav Kumar Nayak, Ajmal Mian, Mubarak Shah


[Adversarial Attacks are a Surprisingly Strong Baseline for Poisoning Few-Shot Meta-Learners. (99%)](http://arxiv.org/abs/2211.12990)

Elre T. Oldewage, John Bronskill, Richard E. Turner


[Reliable Robustness Evaluation via Automatically Constructed Attack Ensembles. (76%)](http://arxiv.org/abs/2211.12713)

Shengcai Liu, Fu Peng, Ke Tang


[Dual Graphs of Polyhedral Decompositions for the Detection of Adversarial Attacks. (62%)](http://arxiv.org/abs/2211.13305)

Huma Jamil, Yajing Liu, Christina Cole, Nathaniel Blanchard, Emily J. King, Michael Kirby, Christopher Peterson


[Privacy-Enhancing Optical Embeddings for Lensless Classification. (11%)](http://arxiv.org/abs/2211.12864)

Eric Bezzam, Martin Vetterli, Matthieu Simeoni


[Principled Data-Driven Decision Support for Cyber-Forensic Investigations. (1%)](http://arxiv.org/abs/2211.13345)

Soodeh Atefi, Sakshyam Panda, Manos Panaousis, Aron Laszka


[Data Provenance Inference in Machine Learning. (1%)](http://arxiv.org/abs/2211.13416)

Mingxue Xu, Xiang-Yang Li


## 2022-11-22

[Benchmarking Adversarially Robust Quantum Machine Learning at Scale. (99%)](http://arxiv.org/abs/2211.12681)

Maxwell T. West, Sarah M. Erfani, Christopher Leckie, Martin Sevior, Lloyd C. L. Hollenberg, Muhammad Usman


[PointCA: Evaluating the Robustness of 3D Point Cloud Completion Models Against Adversarial Examples. (99%)](http://arxiv.org/abs/2211.12294)

Shengshan Hu, Junwei Zhang, Wei Liu, Junhui Hou, Minghui Li, Leo Yu Zhang, Hai Jin, Lichao Sun


[Attacking Image Splicing Detection and Localization Algorithms Using Synthetic Traces. (98%)](http://arxiv.org/abs/2211.12314)

Shengbang Fang, Matthew C Stamm


[Backdoor Cleansing with Unlabeled Data. (75%)](http://arxiv.org/abs/2211.12044)

Lu Pang, Tao Sun, Haibin Ling, Chao Chen


[Improving Robust Generalization by Direct PAC-Bayesian Bound Minimization. (70%)](http://arxiv.org/abs/2211.12624)

Zifan Wang, Nan Ding, Tomer Levinboim, Xi Chen, Radu Soricut


[SoK: Inference Attacks and Defenses in Human-Centered Wireless Sensing. (69%)](http://arxiv.org/abs/2211.12087)

Wei Sun, Tingjun Chen, Neil Gong


## 2022-11-21

[Boosting the Transferability of Adversarial Attacks with Global Momentum Initialization. (99%)](http://arxiv.org/abs/2211.11236)

Jiafeng Wang, Zhaoyu Chen, Kaixun Jiang, Dingkang Yang, Lingyi Hong, Yan Wang, Wenqiang Zhang


[Understanding the Vulnerability of Skeleton-based Human Activity Recognition via Black-box Attack. (99%)](http://arxiv.org/abs/2211.11312)

Yunfeng Diao, He Wang, Tianjia Shao, Yong-Liang Yang, Kun Zhou, David Hogg


[Self-Ensemble Protection: Training Checkpoints Are Good Data Protectors. (99%)](http://arxiv.org/abs/2211.12005)

Sizhe Chen, Geng Yuan, Xinwen Cheng, Yifan Gong, Minghai Qin, Yanzhi Wang, Xiaolin Huang


[Addressing Mistake Severity in Neural Networks with Semantic Knowledge. (92%)](http://arxiv.org/abs/2211.11880)

Natalie Abreu, Nathan Vaska, Victoria Helus


[Efficient Generalization Improvement Guided by Random Weight Perturbation. (68%)](http://arxiv.org/abs/2211.11489)

Tao Li, Weihao Yan, Zehao Lei, Yingwen Wu, Kun Fang, Ming Yang, Xiaolin Huang


[CLAWSAT: Towards Both Robust and Accurate Code Models. (56%)](http://arxiv.org/abs/2211.11711)

Jinghan Jia, Shashank Srikant, Tamara Mitrovska, Chuang Gan, Shiyu Chang, Sijia Liu, Una-May O'Reilly


[Fairness Increases Adversarial Vulnerability. (54%)](http://arxiv.org/abs/2211.11835)

Cuong Tran, Keyu Zhu, Ferdinando Fioretto, Henternyck Pascal Van


[Don't Watch Me: A Spatio-Temporal Trojan Attack on Deep-Reinforcement-Learning-Augment Autonomous Driving. (10%)](http://arxiv.org/abs/2211.14440)

Yinbo Yu, Jiajia Liu


[SPIN: Simulated Poisoning and Inversion Network for Federated Learning-Based 6G Vehicular Networks. (8%)](http://arxiv.org/abs/2211.11321)

Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev, Angelos Antonopoulos


[A Survey on Backdoor Attack and Defense in Natural Language Processing. (2%)](http://arxiv.org/abs/2211.11958)

Xuan Sheng, Zhaoyang Han, Piji Li, Xiangmao Chang


[Understanding and Improving Visual Prompting: A Label-Mapping Perspective. (2%)](http://arxiv.org/abs/2211.11635)

Aochuan Chen, Yuguang Yao, Pin-Yu Chen, Yihua Zhang, Sijia Liu


[Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text. (1%)](http://arxiv.org/abs/2211.11300)

Qianhui Wu, Huiqiang Jiang, Haonan Yin, Börje F. Karlsson, Chin-Yew Lin


[Privacy in Practice: Private COVID-19 Detection in X-Ray Images. (1%)](http://arxiv.org/abs/2211.11434)

Lucas Lange, Maja Schneider, Erhard Rahm


[A Tale of Frozen Clouds: Quantifying the Impact of Algorithmic Complexity Vulnerabilities in Popular Web Servers. (1%)](http://arxiv.org/abs/2211.11357)

Masudul Hasan Masud Bhuiyan, Cristian-Alexandru Staicu


## 2022-11-20

[Spectral Adversarial Training for Robust Graph Neural Network. (99%)](http://arxiv.org/abs/2211.10896)

Jintang Li, Jiaying Peng, Liang Chen, Zibin Zheng, Tingting Liang, Qing Ling


[Invisible Backdoor Attack with Dynamic Triggers against Person Re-identification. (81%)](http://arxiv.org/abs/2211.10933)

Wenli Sun, Xinyang Jiang, Shuguang Dou, Dongsheng Li, Duoqian Miao, Cheng Deng, Cairong Zhao


[Adversarial Cheap Talk. (8%)](http://arxiv.org/abs/2211.11030)

Chris Lu, Timon Willi, Alistair Letcher, Jakob Foerster


[Deep Composite Face Image Attacks: Generation, Vulnerability and Detection. (2%)](http://arxiv.org/abs/2211.11039)

Jag Mohan Singh, Raghavendra Ramachandra


[AI-KD: Adversarial learning and Implicit regularization for self-Knowledge Distillation. (2%)](http://arxiv.org/abs/2211.10938)

Hyungmin Kim, Sungho Suh, Sunghyun Baek, Daehwan Kim, Daun Jeong, Hansang Cho, Junmo Kim


## 2022-11-19

[Towards Adversarial Robustness of Deep Vision Algorithms. (92%)](http://arxiv.org/abs/2211.10670)

Hanshu Yan


[Phonemic Adversarial Attack against Audio Recognition in Real World. (87%)](http://arxiv.org/abs/2211.10661)

Jiakai Wang, Zhendong Chen, Zixin Yin, Qinghong Yang, Xianglong Liu


[Towards Robust Dataset Learning. (82%)](http://arxiv.org/abs/2211.10752)

Yihan Wu, Xinda Li, Florian Kerschbaum, Heng Huang, Hongyang Zhang


[Let Graph be the Go Board: Gradient-free Node Injection Attack for Graph Neural Networks via Reinforcement Learning. (80%)](http://arxiv.org/abs/2211.10782)

Mingxuan Ju, Yujie Fan, Chuxu Zhang, Yanfang Ye


[Mask Off: Analytic-based Malware Detection By Transfer Learning and Model Personalization. (9%)](http://arxiv.org/abs/2211.10843)

Amirmohammad Pasdar, Young Choon Lee, Seok-Hee Hong


[Investigating the Security of EV Charging Mobile Applications As an Attack Surface. (1%)](http://arxiv.org/abs/2211.10603)

K. Sarieddine, M. A. Sayed, S. Torabi, R. Atallah, C. Assi


## 2022-11-18

[Adversarial Stimuli: Attacking Brain-Computer Interfaces via Perturbed Sensory Events. (98%)](http://arxiv.org/abs/2211.10033)

Bibek Upadhayay, Vahid Behzadan


[Adversarial Detection by Approximation of Ensemble Boundary. (75%)](http://arxiv.org/abs/2211.10227)

T. Windeatt


[Leveraging Algorithmic Fairness to Mitigate Blackbox Attribute Inference Attacks. (68%)](http://arxiv.org/abs/2211.10209)

Jan Aalmoes, Vasisht Duddu, Antoine Boutet


[Invariant Learning via Diffusion Dreamed Distribution Shifts. (10%)](http://arxiv.org/abs/2211.10370)

Priyatham Kattakinda, Alexander Levine, Soheil Feizi


[Intrusion Detection in Internet of Things using Convolutional Neural Networks. (1%)](http://arxiv.org/abs/2211.10062)

Martin Kodys, Zhi Lu, Kar Wai Fok, Vrizlynn L. L. Thing


[Improving Robustness of TCM-based Robust Steganography with Variable Robustness. (1%)](http://arxiv.org/abs/2211.10095)

Jimin Zhang, Xianfeng Zhao, Xiaolei He


[Provable Defense against Backdoor Policies in Reinforcement Learning. (1%)](http://arxiv.org/abs/2211.10530)

Shubham Kumar Bharti, Xuezhou Zhang, Adish Singla, Xiaojin Zhu


## 2022-11-17

[Diagnostics for Deep Neural Networks with Automated Copy/Paste Attacks. (99%)](http://arxiv.org/abs/2211.10024)

Stephen Casper, Kaivalya Hariharan, Dylan Hadfield-Menell


[Towards Good Practices in Evaluating Transfer Adversarial Attacks. (93%)](http://arxiv.org/abs/2211.09565)

Zhengyu Zhao, Hanwei Zhang, Renjue Li, Ronan Sicre, Laurent Amsaleg, Michael Backes


[Assessing Neural Network Robustness via Adversarial Pivotal Tuning. (92%)](http://arxiv.org/abs/2211.09782)

Peter Ebert Christensen, Vésteinn Snæbjarnarson, Andrea Dittadi, Serge Belongie, Sagie Benaim


[UPTON: Unattributable Authorship Text via Data Poisoning. (86%)](http://arxiv.org/abs/2211.09717)

Ziyao Wang, Thai Le, Dongwon Lee


[Generalizable Deepfake Detection with Phase-Based Motion Analysis. (50%)](http://arxiv.org/abs/2211.09363)

Ekta Prashnani, Michael Goebel, B. S. Manjunath


[More Effective Centrality-Based Attacks on Weighted Networks. (15%)](http://arxiv.org/abs/2211.09345)

Balume Mburano, Weisheng Si, Qing Cao, Wei Xing Zheng


[Potential Auto-driving Threat: Universal Rain-removal Attack. (2%)](http://arxiv.org/abs/2211.09959)

Jinchegn Hu, Jihao Li, Zhuoran Hou, Jingjing Jiang, Cunjia Liu, Yuanjian Zhang


[Data-Centric Debugging: mitigating model failures via targeted data collection. (1%)](http://arxiv.org/abs/2211.09859)

Sahil Singla, Atoosa Malemir Chegini, Mazda Moayeri, Soheil Feiz


[A Tale of Two Cities: Data and Configuration Variances in Robust Deep Learning. (1%)](http://arxiv.org/abs/2211.10012)

Guanqin Zhang, Jiankun Sun, Feng Xu, H. M. N. Dilum Bandara, Shiping Chen, Yulei Sui, Tim Menzies


[VeriSparse: Training Verified Locally Robust Sparse Neural Networks from Scratch. (1%)](http://arxiv.org/abs/2211.09945)

Sawinder Kaur, Yi Xiao, Asif Salekin


## 2022-11-16

[T-SEA: Transfer-based Self-Ensemble Attack on Object Detection. (99%)](http://arxiv.org/abs/2211.09773)

Hao Huang, Ziyan Chen, Huanran Chen, Yongtao Wang, Kevin Zhang


[Efficiently Finding Adversarial Examples with DNN Preprocessing. (99%)](http://arxiv.org/abs/2211.08706)

Avriti Chauhan, Mohammad Afzal, Hrishikesh Karmarkar, Yizhak Elboher, Kumar Madhukar, Guy Katz


[Improving Interpretability via Regularization of Neural Activation Sensitivity. (92%)](http://arxiv.org/abs/2211.08686)

Ofir Moshe, Gil Fidel, Ron Bitton, Asaf Shabtai


[Attacking Object Detector Using A Universal Targeted Label-Switch Patch. (86%)](http://arxiv.org/abs/2211.08859)

Avishag Shapira, Ron Bitton, Dan Avraham, Alon Zolfi, Yuval Elovici, Asaf Shabtai


[Differentially Private Optimizers Can Learn Adversarially Robust Models. (83%)](http://arxiv.org/abs/2211.08942)

Yuan Zhang, Zhiqi Bu


[Interpretable Dimensionality Reduction by Feature Preserving Manifold Approximation and Projection. (56%)](http://arxiv.org/abs/2211.09321)

Yang Yang, Hongjian Sun, Jialei Gong, Yali Du, Di Yu


[Privacy against Real-Time Speech Emotion Detection via Acoustic Adversarial Evasion of Machine Learning. (38%)](http://arxiv.org/abs/2211.09273)

Brian Testa, Yi Xiao, Avery Gump, Asif Salekin


[Holistic Evaluation of Language Models. (2%)](http://arxiv.org/abs/2211.09110)

Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, Yuta Koreeda


[Analysis and Detectability of Offline Data Poisoning Attacks on Linear Systems. (1%)](http://arxiv.org/abs/2211.08804)

Alessio Russo, Alexandre Proutiere


## 2022-11-15

[Resisting Graph Adversarial Attack via Cooperative Homophilous Augmentation. (99%)](http://arxiv.org/abs/2211.08068)

Zhihao Zhu, Chenwang Wu, Min Zhou, Hao Liao, Defu Lian, Enhong Chen


[Universal Distributional Decision-based Black-box Adversarial Attack with Reinforcement Learning. (99%)](http://arxiv.org/abs/2211.08384)

Yiran Huang, Yexu Zhou, Michael Hefenbrock, Till Riedel, Likun Fang, Michael Beigl


[MORA: Improving Ensemble Robustness Evaluation with Model-Reweighing Attack. (99%)](http://arxiv.org/abs/2211.08008)

Yunrui Yu, Xitong Gao, Cheng-Zhong Xu


[Person Text-Image Matching via Text-Featur Interpretability Embedding and External Attack Node Implantation. (92%)](http://arxiv.org/abs/2211.08657)

Fan Li, Hang Zhou, Huafeng Li, Yafei Zhang, Zhengtao Yu


[Backdoor Attacks on Time Series: A Generative Approach. (70%)](http://arxiv.org/abs/2211.07915)

Yujing Jiang, Xingjun Ma, Sarah Monazam Erfani, James Bailey


[Improved techniques for deterministic l2 robustness. (22%)](http://arxiv.org/abs/2211.08453)

Sahil Singla, Soheil Feizi


[CorruptEncoder: Data Poisoning based Backdoor Attacks to Contrastive Learning. (22%)](http://arxiv.org/abs/2211.08229)

Jinghuai Zhang, Hongbin Liu, Jinyuan Jia, Neil Zhenqiang Gong


[Backdoor Attacks for Remote Sensing Data with Wavelet Transform. (12%)](http://arxiv.org/abs/2211.08044)

Nikolaus Dräger, Yonghao Xu, Pedram Ghamisi


## 2022-11-14

[Efficient Adversarial Training with Robust Early-Bird Tickets. (92%)](http://arxiv.org/abs/2211.07263)

Zhiheng Xi, Rui Zheng, Tao Gui, Qi Zhang, Xuanjing Huang


[Attacking Face Recognition with T-shirts: Database, Vulnerability Assessment and Detection. (13%)](http://arxiv.org/abs/2211.07383)

M. Ibsen, C. Rathgeb, F. Brechtel, R. Klepp, K. Pöppelmann, A. George, S. Marcel, C. Busch


[Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems. (5%)](http://arxiv.org/abs/2211.07455)

Jialiang Xu, Mengyu Zhou, Xinyi He, Shi Han, Dongmei Zhang


[Explainer Divergence Scores (EDS): Some Post-Hoc Explanations May be Effective for Detecting Unknown Spurious Correlations. (5%)](http://arxiv.org/abs/2211.07650)

Shea Cardozo, Gabriel Islas Montero, Dmitry Kazhdan, Botty Dimanov, Maleakhi Wijaya, Mateja Jamnik, Pietro Lio


[Robustifying Deep Vision Models Through Shape Sensitization. (2%)](http://arxiv.org/abs/2211.07277)

Aditay Tripathi, Rishubh Singh, Anirban Chakraborty, Pradeep Shenoy


## 2022-11-13

[Certifying Robustness of Convolutional Neural Networks with Tight Linear Approximation. (26%)](http://arxiv.org/abs/2211.09810)

Yuan Xiao, Tongtong Bai, Mingzheng Gu, Chunrong Fang, Zhenyu Chen


## 2022-11-12

[Adversarial and Random Transformations for Robust Domain Adaptation and Generalization. (75%)](http://arxiv.org/abs/2211.06788)

Liang Xiao, Jiaolong Xu, Dawei Zhao, Erke Shang, Qi Zhu, Bin Dai


[DriftRec: Adapting diffusion models to blind JPEG restoration. (1%)](http://arxiv.org/abs/2211.06757)

Simon Welker, Henry N. Chapman, Timo Gerkmann


## 2022-11-11

[Generating Textual Adversaries with Minimal Perturbation. (98%)](http://arxiv.org/abs/2211.06571)

Xingyi Zhao, Lu Zhang, Depeng Xu, Shuhan Yuan


[On the robustness of non-intrusive speech quality model by adversarial examples. (98%)](http://arxiv.org/abs/2211.06508)

Hsin-Yi Lin, Huan-Hsin Tseng, Yu Tsao


[An investigation of security controls and MITRE ATT\&CK techniques. (47%)](http://arxiv.org/abs/2211.06500)

Md Rayhanur Rahman, Laurie Williams


[Investigating co-occurrences of MITRE ATT\&CK Techniques. (12%)](http://arxiv.org/abs/2211.06495)

Md Rayhanur Rahman, Laurie Williams


[Remapped Cache Layout: Thwarting Cache-Based Side-Channel Attacks with a Hardware Defense. (9%)](http://arxiv.org/abs/2211.06056)

Wei Song, Rui Hou, Peng Liu, Xiaoxin Li, Peinan Li, Lutan Zhao, Xiaofei Fu, Yifei Sun, Dan Meng


## 2022-11-10

[Test-time adversarial detection and robustness for localizing humans using ultra wide band channel impulse responses. (99%)](http://arxiv.org/abs/2211.05854)

Abhiram Kolli, Muhammad Jehanzeb Mirza, Horst Possegger, Horst Bischof


[Impact of Adversarial Training on Robustness and Generalizability of Language Models. (99%)](http://arxiv.org/abs/2211.05523)

Enes Altinisik, Hassan Sajjad, Husrev Taha Sencar, Safa Messaoud, Sanjay Chawla


[Privacy-Utility Balanced Voice De-Identification Using Adversarial Examples. (98%)](http://arxiv.org/abs/2211.05446)

Meng Chen, Li Lu, Jiadi Yu, Yingying Chen, Zhongjie Ba, Feng Lin, Kui Ren


[Stay Home Safe with Starving Federated Data. (80%)](http://arxiv.org/abs/2211.05410)

Jaechul Roh, Yajun Fang


[MSDT: Masked Language Model Scoring Defense in Text Domain. (38%)](http://arxiv.org/abs/2211.05371)

Jaechul Roh, Minhao Cheng, Yajun Fang


[Robust DNN Surrogate Models with Uncertainty Quantification via Adversarial Training. (3%)](http://arxiv.org/abs/2211.09954)

Lixiang Zhang, Jia Li


[Mitigating Forgetting in Online Continual Learning via Contrasting Semantically Distinct Augmentations. (1%)](http://arxiv.org/abs/2211.05347)

Sheng-Feng Yu, Wei-Chen Chiu


## 2022-11-09

[On the Robustness of Explanations of Deep Neural Network Models: A Survey. (50%)](http://arxiv.org/abs/2211.04780)

Amlan Jyoti, Karthik Balaji Ganesh, Manoj Gayala, Nandita Lakshmi Tunuguntla, Sandesh Kamath, Vineeth N Balasubramanian


[Are All Edges Necessary? A Unified Framework for Graph Purification. (5%)](http://arxiv.org/abs/2211.05184)

Zishan Gu, Jintang Li, Liang Chen


[QuerySnout: Automating the Discovery of Attribute Inference Attacks against Query-Based Systems. (3%)](http://arxiv.org/abs/2211.05249)

Ana-Maria Cretu, Florimond Houssiau, Antoine Cully, Montjoye Yves-Alexandre de


[Accountable and Explainable Methods for Complex Reasoning over Text. (2%)](http://arxiv.org/abs/2211.04946)

Pepa Atanasova


[Directional Privacy for Deep Learning. (1%)](http://arxiv.org/abs/2211.04686)

Pedro Faustini, Natasha Fernandes, Shakila Tonni, Annabelle McIver, Mark Dras


## 2022-11-08

[Preserving Semantics in Textual Adversarial Attacks. (99%)](http://arxiv.org/abs/2211.04205)

David Herel, Hugo Cisneros, Tomas Mikolov


[NaturalAdversaries: Can Naturalistic Adversaries Be as Effective as Artificial Adversaries? (98%)](http://arxiv.org/abs/2211.04364)

Saadia Gabriel, Hamid Palangi, Yejin Choi


[How Fraudster Detection Contributes to Robust Recommendation. (67%)](http://arxiv.org/abs/2211.11534)

Yuni Lai, Kai Zhou


[Lipschitz Continuous Algorithms for Graph Problems. (16%)](http://arxiv.org/abs/2211.04674)

Soh Kumabe, Yuichi Yoshida


[Learning advisor networks for noisy image classification. (1%)](http://arxiv.org/abs/2211.04177)

Simone Ricci, Tiberio Uricchio, Bimbo Alberto Del


## 2022-11-07

[Are AlphaZero-like Agents Robust to Adversarial Perturbations? (99%)](http://arxiv.org/abs/2211.03769)

Li-Cheng Lan, Huan Zhang, Ti-Rong Wu, Meng-Yu Tsai, I-Chen Wu, Cho-Jui Hsieh


[Black-Box Attack against GAN-Generated Image Detector with Contrastive Perturbation. (82%)](http://arxiv.org/abs/2211.03509)

Zijie Lou, Gang Cao, Man Lin


[Deviations in Representations Induced by Adversarial Attacks. (70%)](http://arxiv.org/abs/2211.03714)

Daniel Steinberg, Paul Munro


[A Hypergraph-Based Machine Learning Ensemble Network Intrusion Detection System. (1%)](http://arxiv.org/abs/2211.03933)

Zong-Zhi Lin, Thomas D. Pike, Mark M. Bailey, Nathaniel D. Bastian


[Interpreting deep learning output for out-of-distribution detection. (1%)](http://arxiv.org/abs/2211.03637)

Damian Matuszewski, Ida-Maria Sintorn


[Resilience of Wireless Ad Hoc Federated Learning against Model Poisoning Attacks. (1%)](http://arxiv.org/abs/2211.03489)

Naoya Tezuka, Hideya Ochiai, Yuwei Sun, Hiroshi Esaki


## 2022-11-06

[Contrastive Weighted Learning for Near-Infrared Gaze Estimation. (31%)](http://arxiv.org/abs/2211.03073)

Adam Lee


## 2022-11-05

[Textual Manifold-based Defense Against Natural Language Adversarial Examples. (99%)](http://arxiv.org/abs/2211.02878)

Dang Minh Nguyen, Luu Anh Tuan


[Stateful Detection of Adversarial Reprogramming. (96%)](http://arxiv.org/abs/2211.02885)

Yang Zheng, Xiaoyi Feng, Zhaoqiang Xia, Xiaoyue Jiang, Maura Pintor, Ambra Demontis, Battista Biggio, Fabio Roli


[Robust Lottery Tickets for Pre-trained Language Models. (83%)](http://arxiv.org/abs/2211.03013)

Rui Zheng, Rong Bao, Yuhao Zhou, Di Liang, Sirui Wang, Wei Wu, Tao Gui, Qi Zhang, Xuanjing Huang


## 2022-11-04

[Improving Adversarial Robustness to Sensitivity and Invariance Attacks with Deep Metric Learning. (99%)](http://arxiv.org/abs/2211.02468)

Anaelia Ovalle, Evan Czyzycki, Cho-Jui Hsieh


[Logits are predictive of network type. (68%)](http://arxiv.org/abs/2211.02272)

Ali Borji


[An Adversarial Robustness Perspective on the Topology of Neural Networks. (64%)](http://arxiv.org/abs/2211.02675)

Morgane Goibert, Thomas Ricatte, Elvis Dohmatob


[Fairness-aware Regression Robust to Adversarial Attacks. (38%)](http://arxiv.org/abs/2211.04449)

Yulu Jin, Lifeng Lai


[Extension of Simple Algorithms to the Matroid Secretary Problem. (9%)](http://arxiv.org/abs/2211.02755)

Simon Park


[Robustness of Fusion-based Multimodal Classifiers to Cross-Modal Content Dilutions. (3%)](http://arxiv.org/abs/2211.02646)

Gaurav Verma, Vishwa Vinay, Ryan A. Rossi, Srijan Kumar


[Data Models for Dataset Drift Controls in Machine Learning With Images. (1%)](http://arxiv.org/abs/2211.02578)

Luis Oala, Marco Aversa, Gabriel Nobis, Kurt Willis, Yoan Neuenschwander, Michèle Buck, Christian Matek, Jerome Extermann, Enrico Pomarico, Wojciech Samek, Roderick Murray-Smith, Christoph Clausen, Bruno Sanguinetti


## 2022-11-03

[Physically Adversarial Attacks and Defenses in Computer Vision: A Survey. (99%)](http://arxiv.org/abs/2211.01671)

Xingxing Wei, Bangzheng Pu, Jiefan Lu, Baoyuan Wu


[Adversarial Defense via Neural Oscillation inspired Gradient Masking. (98%)](http://arxiv.org/abs/2211.02223)

Chunming Jiang, Yilei Zhang


[M-to-N Backdoor Paradigm: A Stealthy and Fuzzy Attack to Deep Learning Models. (98%)](http://arxiv.org/abs/2211.01875)

Linshan Hou, Zhongyun Hua, Yuhong Li, Leo Yu Zhang


[Robust Few-shot Learning Without Using any Adversarial Samples. (89%)](http://arxiv.org/abs/2211.01598)

Gaurav Kumar Nayak, Ruchit Rawal, Inder Khatri, Anirban Chakraborty


[Data-free Defense of Black Box Models Against Adversarial Attacks. (84%)](http://arxiv.org/abs/2211.01579)

Gaurav Kumar Nayak, Inder Khatri, Shubham Randive, Ruchit Rawal, Anirban Chakraborty


[Leveraging Domain Features for Detecting Adversarial Attacks Against Deep Speech Recognition in Noise. (38%)](http://arxiv.org/abs/2211.01621)

Christian Heider Nielsen, Zheng-Hua Tan


[Try to Avoid Attacks: A Federated Data Sanitization Defense for Healthcare IoMT Systems. (33%)](http://arxiv.org/abs/2211.01592)

Chong Chen, Ying Gao, Leyu Shi, Siquan Huang


[Unintended Memorization and Timing Attacks in Named Entity Recognition Models. (12%)](http://arxiv.org/abs/2211.02245)

Rana Salal Ali, Benjamin Zi Hao Zhao, Hassan Jameel Asghar, Tham Nguyen, Ian David Wood, Dali Kaafar


## 2022-11-02

[Defending with Errors: Approximate Computing for Robustness of Deep Neural Networks. (99%)](http://arxiv.org/abs/2211.01182)

Amira Guesmi, Ihsen Alouani, Khaled N. Khasawneh, Mouna Baklouti, Tarek Frikha, Mohamed Abid, Nael Abu-Ghazaleh


[Improving transferability of 3D adversarial attacks with scale and shear transformations. (99%)](http://arxiv.org/abs/2211.01093)

Jinali Zhang, Yinpeng Dong, Jun Zhu, Jihong Zhu, Minchi Kuang, Xiaming Yuan


[Certified Robustness of Quantum Classifiers against Adversarial Examples through Quantum Noise. (99%)](http://arxiv.org/abs/2211.00887)

Jhih-Cing Huang, Yu-Lin Tsai, Chao-Han Huck Yang, Cheng-Fang Su, Chia-Mu Yu, Pin-Yu Chen, Sy-Yen Kuo


[Adversarial Attack on Radar-based Environment Perception Systems. (99%)](http://arxiv.org/abs/2211.01112)

Amira Guesmi, Ihsen Alouani


[Isometric Representations in Neural Networks Improve Robustness. (62%)](http://arxiv.org/abs/2211.01236)

Kosio Beshkov, Jonas Verhellen, Mikkel Elle Lepperød


[BATT: Backdoor Attack with Transformation-based Triggers. (56%)](http://arxiv.org/abs/2211.01806)

Tong Xu, Yiming Li, Yong Jiang, Shu-Tao Xia


[Untargeted Backdoor Attack against Object Detection. (50%)](http://arxiv.org/abs/2211.05638)

Chengxiao Luo, Yiming Li, Yong Jiang, Shu-Tao Xia


[Generative Adversarial Training Can Improve Neural Language Models. (33%)](http://arxiv.org/abs/2211.09728)

Sajad Movahedi, Azadeh Shakery


[Backdoor Defense via Suppressing Model Shortcuts. (3%)](http://arxiv.org/abs/2211.05631)

Sheng Yang, Yiming Li, Yong Jiang, Shu-Tao Xia


[Human-in-the-Loop Mixup. (1%)](http://arxiv.org/abs/2211.01202)

Katherine M. Collins, Umang Bhatt, Weiyang Liu, Vihari Piratla, Ilia Sucholutsky, Bradley Love, Adrian Weller


## 2022-11-01

[The Enemy of My Enemy is My Friend: Exploring Inverse Adversaries for Improving Adversarial Training. (99%)](http://arxiv.org/abs/2211.00525)

Junhao Dong, Seyed-Mohsen Moosavi-Dezfooli, Jianhuang Lai, Xiaohua Xie


[LMD: A Learnable Mask Network to Detect Adversarial Examples for Speaker Verification. (99%)](http://arxiv.org/abs/2211.00825)

Xing Chen, Jie Wang, Xiao-Lei Zhang, Wei-Qiang Zhang, Kunde Yang


[DensePure: Understanding Diffusion Models towards Adversarial Robustness. (98%)](http://arxiv.org/abs/2211.00322)

Chaowei Xiao, Zhongzhu Chen, Kun Jin, Jiongxiao Wang, Weili Nie, Mingyan Liu, Anima Anandkumar, Bo Li, Dawn Song


[Adversarial Training with Complementary Labels: On the Benefit of Gradually Informative Attacks. (87%)](http://arxiv.org/abs/2211.00269)

Jianan Zhou, Jianing Zhu, Jingfeng Zhang, Tongliang Liu, Gang Niu, Bo Han, Masashi Sugiyama


[Universal Perturbation Attack on Differentiable No-Reference Image- and Video-Quality Metrics. (82%)](http://arxiv.org/abs/2211.00366)

Ekaterina Shumitskaya, Anastasia Antsiferova, Dmitriy Vatolin


[The Perils of Learning From Unlabeled Data: Backdoor Attacks on Semi-supervised Learning. (80%)](http://arxiv.org/abs/2211.00453)

Virat Shejwalkar, Lingjuan Lyu, Amir Houmansadr


[Maximum Likelihood Distillation for Robust Modulation Classification. (69%)](http://arxiv.org/abs/2211.00748)

Javier Maroto, Gérôme Bovet, Pascal Frossard


[FRSUM: Towards Faithful Abstractive Summarization via Enhancing Factual Robustness. (45%)](http://arxiv.org/abs/2211.00294)

Wenhao Wu, Wei Li, Jiachen Liu, Xinyan Xiao, Ziqiang Cao, Sujian Li, Hua Wu


[Amplifying Membership Exposure via Data Poisoning. (22%)](http://arxiv.org/abs/2211.00463)

Yufei Chen, Chao Shen, Yun Shen, Cong Wang, Yang Zhang


[ActGraph: Prioritization of Test Cases Based on Deep Neural Network Activation Graph. (13%)](http://arxiv.org/abs/2211.00273)

Jinyin Chen, Jie Ge, Haibin Zheng


## 2022-10-31

[Scoring Black-Box Models for Adversarial Robustness. (98%)](http://arxiv.org/abs/2210.17140)

Jian Vora, Pranay Reddy Samala


[ARDIR: Improving Robustness using Knowledge Distillation of Internal Representation. (88%)](http://arxiv.org/abs/2211.00239)

Tomokatsu Takahashi, Masanori Yamada, Yuuki Yamanaka, Tomoya Yamashita


[SoK: Modeling Explainability in Security Analytics for Interpretability, Trustworthiness, and Usability. (33%)](http://arxiv.org/abs/2210.17376)

Dipkamal Bhusal, Rosalyn Shin, Ajay Ashok Shewale, Monish Kumar Manikya Veerabhadran, Michael Clifford, Sara Rampazzi, Nidhi Rastogi


[Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy. (16%)](http://arxiv.org/abs/2210.17546)

Daphne Ippolito, Florian Tramèr, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher A. Choquette-Choo, Nicholas Carlini


## 2022-10-30

[Poison Attack and Defense on Deep Source Code Processing Models. (99%)](http://arxiv.org/abs/2210.17029)

Jia Li, Zhuo Li, Huangzhao Zhang, Ge Li, Zhi Jin, Xing Hu, Xin Xia


[Character-level White-Box Adversarial Attacks against Transformers via Attachable Subwords Substitution. (99%)](http://arxiv.org/abs/2210.17004)

Aiwei Liu, Honghai Yu, Xuming Hu, Shu'ang Li, Li Lin, Fukun Ma, Yawen Yang, Lijie Wen


[Benchmarking Adversarial Patch Against Aerial Detection. (99%)](http://arxiv.org/abs/2210.16765)

Jiawei Lian, Shaohui Mei, Shun Zhang, Mingyang Ma


[Symmetric Saliency-based Adversarial Attack To Speaker Identification. (92%)](http://arxiv.org/abs/2210.16777)

Jiadi Yao, Xing Chen, Xiao-Lei Zhang, Wei-Qiang Zhang, Kunde Yang


[FI-ODE: Certified and Robust Forward Invariance in Neural ODEs. (61%)](http://arxiv.org/abs/2210.16940)

Yujia Huang, Ivan Dario Jimenez Rodriguez, Huan Zhang, Yuanyuan Shi, Yisong Yue


[Imitating Opponent to Win: Adversarial Policy Imitation Learning in Two-player Competitive Games. (9%)](http://arxiv.org/abs/2210.16915)

The Viet Bui, Tien Mai, Thanh H. Nguyen


## 2022-10-29

[On the Need of Neuromorphic Twins to Detect Denial-of-Service Attacks on Communication Networks. (10%)](http://arxiv.org/abs/2210.16690)

Holger Boche, Rafael F. Schaefer, H. Vincent Poor, Frank H. P. Fitzek


## 2022-10-28

[Universal Adversarial Directions. (99%)](http://arxiv.org/abs/2210.15997)

Ching Lam Choi, Farzan Farnia


[Improving the Transferability of Adversarial Attacks on Face Recognition with Beneficial Perturbation Feature Augmentation. (99%)](http://arxiv.org/abs/2210.16117)

Fengfan Zhou, Hefei Ling, Yuxuan Shi, Jiazhong Chen, Zongyi Li, Ping Li


[Improving Hyperspectral Adversarial Robustness Under Multiple Attacks. (98%)](http://arxiv.org/abs/2210.16346)

Nicholas Soucy, Salimeh Yasaei Sekeh


[Distributed Black-box Attack against Image Classification Cloud Services. (95%)](http://arxiv.org/abs/2210.16371)

Han Wu, Sareh Rowlands, Johan Wahlstrom


[RoChBert: Towards Robust BERT Fine-tuning for Chinese. (75%)](http://arxiv.org/abs/2210.15944)

Zihan Zhang, Jinfeng Li, Ning Shi, Bo Yuan, Xiangyu Liu, Rong Zhang, Hui Xue, Donghong Sun, Chao Zhang


[Robust Boosting Forests with Richer Deep Feature Hierarchy. (56%)](http://arxiv.org/abs/2210.16451)

Jianqiao Wangni


[Localized Randomized Smoothing for Collective Robustness Certification. (26%)](http://arxiv.org/abs/2210.16140)

Jan Schuchardt, Tom Wollschläger, Aleksandar Bojchevski, Stephan Günnemann


[Towards Reliable Neural Specifications. (11%)](http://arxiv.org/abs/2210.16114)

Chuqin Geng, Nham Le, Xiaojie Xu, Zhaoyue Wang, Arie Gurfinkel, Xujie Si


[On the Vulnerability of Data Points under Multiple Membership Inference Attacks and Target Models. (1%)](http://arxiv.org/abs/2210.16258)

Mauro Conti, Jiaxin Li, Stjepan Picek


## 2022-10-27

[TAD: Transfer Learning-based Multi-Adversarial Detection of Evasion Attacks against Network Intrusion Detection Systems. (99%)](http://arxiv.org/abs/2210.15700)

Islam Debicha, Richard Bauwens, Thibault Debatty, Jean-Michel Dricot, Tayeb Kenaza, Wim Mees


[Isometric 3D Adversarial Examples in the Physical World. (99%)](http://arxiv.org/abs/2210.15291)

Yibo Miao, Yinpeng Dong, Jun Zhu, Xiao-Shan Gao


[LeNo: Adversarial Robust Salient Object Detection Networks with Learnable Noise. (92%)](http://arxiv.org/abs/2210.15392)

He Tang, He Wang


[TASA: Deceiving Question Answering Models by Twin Answer Sentences Attack. (92%)](http://arxiv.org/abs/2210.15221)

Yu Cao, Dianqi Li, Meng Fang, Tianyi Zhou, Jun Gao, Yibing Zhan, Dacheng Tao


[Efficient and Effective Augmentation Strategy for Adversarial Training. (56%)](http://arxiv.org/abs/2210.15318)

Sravanti Addepalli, Samyak Jain, R. Venkatesh Babu


[Noise Injection Node Regularization for Robust Learning. (2%)](http://arxiv.org/abs/2210.15764)

Noam Levi, Itay M. Bloch, Marat Freytsis, Tomer Volansky


[Domain Adaptive Object Detection for Autonomous Driving under Foggy Weather. (1%)](http://arxiv.org/abs/2210.15176)

Jinlong Li, Runsheng Xu, Jin Ma, Qin Zou, Jiaqi Ma, Hongkai Yu


## 2022-10-26

[Improving Adversarial Robustness with Self-Paced Hard-Class Pair Reweighting. (99%)](http://arxiv.org/abs/2210.15068)

Pengyue Hou, Jie Han, Xingyu Li


[There is more than one kind of robustness: Fooling Whisper with adversarial examples. (98%)](http://arxiv.org/abs/2210.17316)

Raphael Olivier, Bhiksha Raj


[Disentangled Text Representation Learning with Information-Theoretic Perspective for Adversarial Robustness. (86%)](http://arxiv.org/abs/2210.14957)

Jiahao Zhao, Wenji Mao


[BioNLI: Generating a Biomedical NLI Dataset Using Lexico-semantic Constraints for Adversarial Examples. (75%)](http://arxiv.org/abs/2210.14814)

Mohaddeseh Bastan, Mihai Surdeanu, Niranjan Balasubramanian


[EIPSIM: Modeling Secure IP Address Allocation at Cloud Scale. (11%)](http://arxiv.org/abs/2210.14999)

Eric University of Wisconsin-Madison Pauley, Kyle Pennsylvania State University Domico, Blaine University of Wisconsin-Madison Hoak, Ryan University of Wisconsin-Madison Sheatsley, Quinn University of Wisconsin-Madison Burke, Yohan University of Wisconsin-Madison Beugin, Patrick University of Wisconsin-Madison McDaniel


[V-Cloak: Intelligibility-, Naturalness- & Timbre-Preserving Real-Time Voice Anonymization. (10%)](http://arxiv.org/abs/2210.15140)

Jiangyi Zhejiang University Deng, Fei Zhejiang University Teng, Yanjiao Zhejiang University Chen, Xiaofu Wuhan University Chen, Zhaohui Wuhan University Wang, Wenyuan Zhejiang University Xu


[Rethinking the Reverse-engineering of Trojan Triggers. (5%)](http://arxiv.org/abs/2210.15127)

Zhenting Wang, Kai Mei, Hailun Ding, Juan Zhai, Shiqing Ma


[Cover Reproducible Steganography via Deep Generative Models. (1%)](http://arxiv.org/abs/2210.14632)

Kejiang Chen, Hang Zhou, Yaofei Wang, Menghan Li, Weiming Zhang, Nenghai Yu


[DEMIS: A Threat Model for Selectively Encrypted Visual Surveillance Data. (1%)](http://arxiv.org/abs/2210.14622)

Ifeoluwapo Aribilola, Mamoona Naveed Asghar, Brian Lee


[Privately Fine-Tuning Large Language Models with Differential Privacy. (1%)](http://arxiv.org/abs/2210.15042)

Rouzbeh Behnia, Mohamamdreza Ebrahimi, Jason Pacheco, Balaji Padmanabhan


## 2022-10-25

[LP-BFGS attack: An adversarial attack based on the Hessian with limited pixels. (99%)](http://arxiv.org/abs/2210.15446)

Jiebao Zhang, Wenhua Qian, Rencan Nie, Jinde Cao, Dan Xu


[Adversarially Robust Medical Classification via Attentive Convolutional Neural Networks. (99%)](http://arxiv.org/abs/2210.14405)

Isaac Wasserman


[A White-Box Adversarial Attack Against a Digital Twin. (99%)](http://arxiv.org/abs/2210.14018)

Wilson Patterson, Ivan Fernandez, Subash Neupane, Milan Parmar, Sudip Mittal, Shahram Rahimi


[Adaptive Test-Time Defense with the Manifold Hypothesis. (98%)](http://arxiv.org/abs/2210.14404)

Zhaoyuan Yang, Zhiwei Xu, Jing Zhang, Richard Hartley, Peter Tu


[Multi-view Representation Learning from Malware to Defend Against Adversarial Variants. (98%)](http://arxiv.org/abs/2210.15429)

James Lee Hu, Mohammadreza Ebrahimi, Weifeng Li, Xin Li, Hsinchun Chen


[Improving Adversarial Robustness via Joint Classification and Multiple Explicit Detection Classes. (98%)](http://arxiv.org/abs/2210.14410)

Sina Baharlouei, Fatemeh Sheikholeslami, Meisam Razaviyayn, Zico Kolter


[Accelerating Certified Robustness Training via Knowledge Transfer. (73%)](http://arxiv.org/abs/2210.14283)

Pratik Vaishnavi, Kevin Eykholt, Amir Rahmati


[Causal Information Bottleneck Boosts Adversarial Robustness of Deep Neural Network. (64%)](http://arxiv.org/abs/2210.14229)

Huan Hua, Jun Yan, Xi Fang, Weiquan Huang, Huilin Yin, Wancheng Ge


[Towards Robust Recommender Systems via Triple Cooperative Defense. (61%)](http://arxiv.org/abs/2210.13762)

Qingyang Wang, Defu Lian, Chenwang Wu, Enhong Chen


[Towards Formal Approximated Minimal Explanations of Neural Networks. (13%)](http://arxiv.org/abs/2210.13915)

Shahaf Bassan, Guy Katz


[FocusedCleaner: Sanitizing Poisoned Graphs for Robust GNN-based Node Classification. (13%)](http://arxiv.org/abs/2210.13815)

Yulin Zhu, Liang Tong, Kai Zhou


[A Streamlit-based Artificial Intelligence Trust Platform for Next-Generation Wireless Networks. (3%)](http://arxiv.org/abs/2211.12851)

M. Kuzlu, F. O. Catak, S. Sarp, U. Cali, O Gueler


[Robustness of Locally Differentially Private Graph Analysis Against Poisoning. (1%)](http://arxiv.org/abs/2210.14376)

Jacob Imola, Amrita Roy Chowdhury, Kamalika Chaudhuri


## 2022-10-24

[Ares: A System-Oriented Wargame Framework for Adversarial ML. (99%)](http://arxiv.org/abs/2210.12952)

Farhan Ahmed, Pratik Vaishnavi, Kevin Eykholt, Amir Rahmati


[SpacePhish: The Evasion-space of Adversarial Attacks against Phishing Website Detectors using Machine Learning. (99%)](http://arxiv.org/abs/2210.13660)

Giovanni Apruzzese, Mauro Conti, Ying Yuan


[Motif-Backdoor: Rethinking the Backdoor Attack on Graph Neural Networks via Motifs. (96%)](http://arxiv.org/abs/2210.13710)

Haibin Zheng, Haiyang Xiong, Jinyin Chen, Haonan Ma, Guohan Huang


[On the Robustness of Dataset Inference. (88%)](http://arxiv.org/abs/2210.13631)

Sebastian Szyller, Rui Zhang, Jian Liu, N. Asokan


[Flexible Android Malware Detection Model based on Generative Adversarial Networks with Code Tensor. (16%)](http://arxiv.org/abs/2210.14225)

Zhao Yang, Fengyang Deng, Linxi Han


[Revisiting Sparse Convolutional Model for Visual Recognition. (11%)](http://arxiv.org/abs/2210.12945)

Xili Dai, Mingyang Li, Pengyuan Zhai, Shengbang Tong, Xingjian Gao, Shao-Lun Huang, Zhihui Zhu, Chong You, Yi Ma


## 2022-10-23

[FLIP: A Provable Defense Framework for Backdoor Mitigation in Federated Learning. (68%)](http://arxiv.org/abs/2210.12873)

Kaiyuan Zhang, Guanhong Tao, Qiuling Xu, Siyuan Cheng, Shengwei An, Yingqi Liu, Shiwei Feng, Guangyu Shen, Pin-Yu Chen, Shiqing Ma, Xiangyu Zhang


[Adversarial Pretraining of Self-Supervised Deep Networks: Past, Present and Future. (45%)](http://arxiv.org/abs/2210.13463)

Guo-Jun Qi, Mubarak Shah


## 2022-10-22

[ADDMU: Detection of Far-Boundary Adversarial Examples with Data and Model Uncertainty Estimation. (99%)](http://arxiv.org/abs/2210.12396)

Fan Yin, Yao Li, Cho-Jui Hsieh, Kai-Wei Chang


[Hindering Adversarial Attacks with Implicit Neural Representations. (92%)](http://arxiv.org/abs/2210.13982)

Andrei A. Rusu, Dan A. Calian, Sven Gowal, Raia Hadsell


[GANI: Global Attacks on Graph Neural Networks via Imperceptible Node Injections. (81%)](http://arxiv.org/abs/2210.12598)

Junyuan Fang, Haixian Wen, Jiajing Wu, Qi Xuan, Zibin Zheng, Chi K. Tse


[Nash Equilibria and Pitfalls of Adversarial Training in Adversarial Robustness Games. (26%)](http://arxiv.org/abs/2210.12606)

Maria-Florina Balcan, Rattana Pukdee, Pradeep Ravikumar, Hongyang Zhang


[Precisely the Point: Adversarial Augmentations for Faithful and Informative Text Generation. (4%)](http://arxiv.org/abs/2210.12367)

Wenhao Wu, Wei Li, Jiachen Liu, Xinyan Xiao, Sujian Li, Yajuan Lyu


## 2022-10-21

[Evolution of Neural Tangent Kernels under Benign and Adversarial Training. (99%)](http://arxiv.org/abs/2210.12030)

Noel Loo, Ramin Hasani, Alexander Amini, Daniela Rus


[The Dark Side of AutoML: Towards Architectural Backdoor Search. (68%)](http://arxiv.org/abs/2210.12179)

Ren Pang, Changjiang Li, Zhaohan Xi, Shouling Ji, Ting Wang


[Diffusion Visual Counterfactual Explanations. (10%)](http://arxiv.org/abs/2210.11841)

Maximilian Augustin, Valentyn Boreiko, Francesco Croce, Matthias Hein


[TCAB: A Large-Scale Text Classification Attack Benchmark. (10%)](http://arxiv.org/abs/2210.12233)

Kalyani Asthana, Zhouhang Xie, Wencong You, Adam Noack, Jonathan Brophy, Sameer Singh, Daniel Lowd


[A critical review of cyber-physical security for building automation systems. (2%)](http://arxiv.org/abs/2210.11726)

Guowen Li, Lingyu Ren, Yangyang Fu, Zhiyao Yang, Veronica Adetola, Jin Wen, Qi Zhu, Teresa Wu, K. Selcuk Candanf, Zheng O'Neill


[Extracted BERT Model Leaks More Information than You Think! (1%)](http://arxiv.org/abs/2210.11735)

Xuanli He, Chen Chen, Lingjuan Lyu, Qiongkai Xu


## 2022-10-20

[Identifying Human Strategies for Generating Word-Level Adversarial Examples. (98%)](http://arxiv.org/abs/2210.11598)

Maximilian Mozes, Bennett Kleinberg, Lewis D. Griffin


[Are You Stealing My Model? Sample Correlation for Fingerprinting Deep Neural Networks. (98%)](http://arxiv.org/abs/2210.15427)

Jiyang Guan, Jian Liang, Ran He


[Balanced Adversarial Training: Balancing Tradeoffs between Fickleness and Obstinacy in NLP Models. (98%)](http://arxiv.org/abs/2210.11498)

Hannah Chen, Yangfeng Ji, David Evans


[Learning Sample Reweighting for Accuracy and Adversarial Robustness. (93%)](http://arxiv.org/abs/2210.11513)

Chester Holtz, Tsui-Wei Weng, Gal Mishne


[Similarity of Neural Architectures Based on Input Gradient Transferability. (86%)](http://arxiv.org/abs/2210.11407)

Jaehui Hwang, Dongyoon Han, Byeongho Heo, Song Park, Sanghyuk Chun, Jong-Seok Lee


[New data poison attacks on machine learning classifiers for mobile exfiltration. (80%)](http://arxiv.org/abs/2210.11592)

Miguel A. Ramirez, Sangyoung Yoon, Ernesto Damiani, Hussam Al Hamadi, Claudio Agostino Ardagna, Nicola Bena, Young-Ji Byon, Tae-Yeon Kim, Chung-Suk Cho, Chan Yeob Yeun


[Attacking Motion Estimation with Adversarial Snow. (16%)](http://arxiv.org/abs/2210.11242)

Jenny Schmalfuss, Lukas Mehl, Andrés Bruhn


[How Does a Deep Learning Model Architecture Impact Its Privacy? A Comprehensive Study of Privacy Attacks on CNNs and Transformers. (13%)](http://arxiv.org/abs/2210.11049)

Guangsheng Zhang, Bo Liu, Huan Tian, Tianqing Zhu, Ming Ding, Wanlei Zhou


[Analyzing the Robustness of Decentralized Horizontal and Vertical Federated Learning Architectures in a Non-IID Scenario. (4%)](http://arxiv.org/abs/2210.11061)

Pedro Miguel Sánchez Sánchez, Alberto Huertas Celdrán, Enrique Tomás Martínez Beltrán, Daniel Demeter, Gérôme Bovet, Gregorio Martínez Pérez, Burkhard Stiller


[Apple of Sodom: Hidden Backdoors in Superior Sentence Embeddings via Contrastive Learning. (3%)](http://arxiv.org/abs/2210.11082)

Xiaoyi Chen, Baisong Xin, Shengfang Zhai, Shiqing Ma, Qingni Shen, Zhonghai Wu


[LOT: Layer-wise Orthogonal Training on Improving $\ell_2$ Certified Robustness. (3%)](http://arxiv.org/abs/2210.11620)

Xiaojun Xu, Linyi Li, Bo Li


## 2022-10-19

[Few-shot Transferable Robust Representation Learning via Bilevel Attacks. (93%)](http://arxiv.org/abs/2210.10485)

Minseon Kim, Hyeonjeong Ha, Sung Ju Hwang


[Targeted Adversarial Self-Supervised Learning. (86%)](http://arxiv.org/abs/2210.10482)

Minseon Kim, Hyeonjeong Ha, Sooel Son, Sung Ju Hwang


[Backdoor Attack and Defense in Federated Generative Adversarial Network-based Medical Image Synthesis. (83%)](http://arxiv.org/abs/2210.10886)

Ruinan Jin, Xiaoxiao Li


[Chaos Theory and Adversarial Robustness. (73%)](http://arxiv.org/abs/2210.13235)

Jonathan S. Kent


[Emerging Threats in Deep Learning-Based Autonomous Driving: A Comprehensive Survey. (69%)](http://arxiv.org/abs/2210.11237)

Hui Cao, Wenlong Zou, Yinkun Wang, Ting Song, Mengjun Liu


[Why Should Adversarial Perturbations be Imperceptible? Rethink the Research Paradigm in Adversarial NLP. (64%)](http://arxiv.org/abs/2210.10683)

Yangyi Chen, Hongcheng Gao, Ganqu Cui, Fanchao Qi, Longtao Huang, Zhiyuan Liu, Maosong Sun


[Model-Free Prediction of Adversarial Drop Points in 3D Point Clouds. (54%)](http://arxiv.org/abs/2210.14164)

Hanieh Naderi, Chinthaka Dinesh, Ivan V. Bajic, Shohreh Kasaei


[FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information. (41%)](http://arxiv.org/abs/2210.10936)

Xiaoyu Cao, Jinyuan Jia, Zaixi Zhang, Neil Zhenqiang Gong


[Learning to Invert: Simple Adaptive Attacks for Gradient Inversion in Federated Learning. (16%)](http://arxiv.org/abs/2210.10880)

Ruihan Wu, Xiangyu Chen, Chuan Guo, Kilian Q. Weinberger


[Variational Model Perturbation for Source-Free Domain Adaptation. (1%)](http://arxiv.org/abs/2210.10378)

Mengmeng Jing, Xiantong Zhen, Jingjing Li, Cees G. M. Snoek


## 2022-10-18

[Scaling Adversarial Training to Large Perturbation Bounds. (98%)](http://arxiv.org/abs/2210.09852)

Sravanti Addepalli, Samyak Jain, Gaurang Sriramanan, R. Venkatesh Babu


[Not All Poisons are Created Equal: Robust Training against Data Poisoning. (97%)](http://arxiv.org/abs/2210.09671)

Yu Yang, Tian Yu Liu, Baharan Mirzasoleiman


[ROSE: Robust Selective Fine-tuning for Pre-trained Language Models. (73%)](http://arxiv.org/abs/2210.09658)

Lan Jiang, Hao Zhou, Yankai Lin, Peng Li, Jie Zhou, Rui Jiang


[Analysis of Master Vein Attacks on Finger Vein Recognition Systems. (56%)](http://arxiv.org/abs/2210.10667)

Huy H. Nguyen, Trung-Nghia Le, Junichi Yamagishi, Isao Echizen


[Training set cleansing of backdoor poisoning by self-supervised representation learning. (56%)](http://arxiv.org/abs/2210.10272)

H. Wang, S. Karami, O. Dia, H. Ritter, E. Emamjomeh-Zadeh, J. Chen, Z. Xiang, D. J. Miller, G. Kesidis


[On the Adversarial Robustness of Mixture of Experts. (13%)](http://arxiv.org/abs/2210.10253)

Joan Puigcerver, Rodolphe Jenatton, Carlos Riquelme, Pranjal Awasthi, Srinadh Bhojanapalli


[Transferable Unlearnable Examples. (8%)](http://arxiv.org/abs/2210.10114)

Jie Ren, Han Xu, Yuxuan Wan, Xingjun Ma, Lichao Sun, Jiliang Tang


[Automatic Detection of Fake Key Attacks in Secure Messaging. (8%)](http://arxiv.org/abs/2210.09940)

Tarun Kumar Yadav, Devashish Gosain, Amir Herzberg, Daniel Zappala, Kent Seamons


[Improving Adversarial Robustness by Contrastive Guided Diffusion Process. (2%)](http://arxiv.org/abs/2210.09643)

Yidong Ouyang, Liyan Xie, Guang Cheng


## 2022-10-17

[Towards Generating Adversarial Examples on Mixed-type Data. (99%)](http://arxiv.org/abs/2210.09405)

Han Xu, Menghai Pan, Zhimeng Jiang, Huiyuan Chen, Xiaoting Li, Mahashweta Das, Hao Yang


[Differential Evolution based Dual Adversarial Camouflage: Fooling Human Eyes and Object Detectors. (99%)](http://arxiv.org/abs/2210.08870)

Jialiang Sun, Tingsong Jiang, Wen Yao, Donghua Wang, Xiaoqian Chen


[Probabilistic Categorical Adversarial Attack & Adversarial Training. (99%)](http://arxiv.org/abs/2210.09364)

Pengfei He, Han Xu, Jie Ren, Yuxuan Wan, Zitao Liu, Jiliang Tang


[Marksman Backdoor: Backdoor Attacks with Arbitrary Target Class. (96%)](http://arxiv.org/abs/2210.09194)

Khoa D. Doan, Yingjie Lao, Ping Li


[DE-CROP: Data-efficient Certified Robustness for Pretrained Classifiers. (87%)](http://arxiv.org/abs/2210.08929)

Gaurav Kumar Nayak, Ruchit Rawal, Anirban Chakraborty


[Beyond Model Interpretability: On the Faithfulness and Adversarial Robustness of Contrastive Textual Explanations. (78%)](http://arxiv.org/abs/2210.08902)

Julia El Zini, Mariette Awad


[Towards Fair Classification against Poisoning Attacks. (76%)](http://arxiv.org/abs/2210.09503)

Han Xu, Xiaorui Liu, Yuxuan Wan, Jiliang Tang


[Deepfake Text Detection: Limitations and Opportunities. (41%)](http://arxiv.org/abs/2210.09421)

Jiameng Pu, Zain Sarwar, Sifat Muhammad Abdullah, Abdullah Rehman, Yoonjin Kim, Parantapa Bhattacharya, Mobin Javed, Bimal Viswanath


[You Can't See Me: Physical Removal Attacks on LiDAR-based Autonomous Vehicles Driving Frameworks. (15%)](http://arxiv.org/abs/2210.09482)

Yulong Cao, S. Hrushikesh Bhupathiraju, Pirouz Naghavi, Takeshi Sugawara, Z. Morley Mao, Sara Rampazzi


[Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models. (9%)](http://arxiv.org/abs/2210.09545)

Zhiyuan Zhang, Lingjuan Lyu, Xingjun Ma, Chenguang Wang, Xu Sun


[Understanding CNN Fragility When Learning With Imbalanced Data. (1%)](http://arxiv.org/abs/2210.09465)

Damien Dablain, Kristen N. Jacobson, Colin Bellinger, Mark Roberts, Nitesh Chawla


## 2022-10-16

[Object-Attentional Untargeted Adversarial Attack. (99%)](http://arxiv.org/abs/2210.08472)

Chao Zhou, Yuan-Gen Wang, Guopu Zhu


[Nowhere to Hide: A Lightweight Unsupervised Detector against Adversarial Examples. (99%)](http://arxiv.org/abs/2210.08579)

Hui Liu, Bo Zhao, Kehuan Zhang, Peng Liu


[ODG-Q: Robust Quantization via Online Domain Generalization. (83%)](http://arxiv.org/abs/2210.08701)

Chaofan Tao, Ngai Wong


[Interpretable Machine Learning for Detection and Classification of Ransomware Families Based on API Calls. (1%)](http://arxiv.org/abs/2210.11235)

Rawshan Ara Mowri, Madhuri Siddula, Kaushik Roy


## 2022-10-15

[RoS-KD: A Robust Stochastic Knowledge Distillation Approach for Noisy Medical Imaging. (2%)](http://arxiv.org/abs/2210.08388)

Ajay Jaiswal, Kumar Ashutosh, Justin F Rousseau, Yifan Peng, Zhangyang Wang, Ying Ding


## 2022-10-14

[When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture. (87%)](http://arxiv.org/abs/2210.07540)

Yichuan Mo, Dongxian Wu, Yifei Wang, Yiwen Guo, Yisen Wang


[Dynamics-aware Adversarial Attack of Adaptive Neural Networks. (86%)](http://arxiv.org/abs/2210.08159)

An Tao, Yueqi Duan, Yingqi Wang, Jiwen Lu, Jie Zhou


[Is Face Recognition Safe from Realizable Attacks? (84%)](http://arxiv.org/abs/2210.08178)

Sanjay Saha, Terence Sim


[Expose Backdoors on the Way: A Feature-Based Efficient Defense against Textual Backdoor Attacks. (76%)](http://arxiv.org/abs/2210.07907)

Sishuo Chen, Wenkai Yang, Zhiyuan Zhang, Xiaohan Bi, Xu Sun


[Close the Gate: Detecting Backdoored Models in Federated Learning based on Client-Side Deep Layer Output Analysis. (67%)](http://arxiv.org/abs/2210.07714)

Phillip Technical University Darmstadt Rieger, Torsten University of Würzburg Krauß, Markus Technical University Darmstadt Miettinen, Alexandra University of Würzburg Dmitrienko, Ahmad-Reza Technical University Darmstadt Sadeghi


## 2022-10-13

[Adv-Attribute: Inconspicuous and Transferable Adversarial Attack on Face Recognition. (99%)](http://arxiv.org/abs/2210.06871)

Shuai Jia, Bangjie Yin, Taiping Yao, Shouhong Ding, Chunhua Shen, Xiaokang Yang, Chao Ma


[AccelAT: A Framework for Accelerating the Adversarial Training of Deep Neural Networks through Accuracy Gradient. (99%)](http://arxiv.org/abs/2210.06888)

Farzad Nikfam, Alberto Marchisio, Maurizio Martina, Muhammad Shafique


[Demystifying Self-supervised Trojan Attacks. (95%)](http://arxiv.org/abs/2210.07346)

Changjiang Li, Ren Pang, Zhaohan Xi, Tianyu Du, Shouling Ji, Yuan Yao, Ting Wang


[Improving Out-of-Distribution Generalization by Adversarial Training with Structured Priors. (81%)](http://arxiv.org/abs/2210.06807)

Qixun Wang, Yifei Wang, Hong Zhu, Yisen Wang


[Efficiently Computing Local Lipschitz Constants of Neural Networks via Bound Propagation. (13%)](http://arxiv.org/abs/2210.07394)

Zhouxing Shi, Yihan Wang, Huan Zhang, Zico Kolter, Cho-Jui Hsieh


[Large-Scale Open-Set Classification Protocols for ImageNet. (2%)](http://arxiv.org/abs/2210.06789)

Jesus Andres Palechor Anacona, Annesha Bhoumik, Manuel Günther


[SoK: How Not to Architect Your Next-Generation TEE Malware? (1%)](http://arxiv.org/abs/2210.06792)

Kubilay Ahmet Küçük, Steve Moyle, Andrew Martin, Alexandru Mereacre, Nicholas Allott


[Feature Reconstruction Attacks and Countermeasures of DNN training in Vertical Federated Learning. (1%)](http://arxiv.org/abs/2210.06771)

Peng Ye, Zhifeng Jiang, Wei Wang, Bo Li, Baochun Li


[Characterizing the Influence of Graph Elements. (1%)](http://arxiv.org/abs/2210.07441)

Zizhang Chen, Peizhao Li, Hongfu Liu, Pengyu Hong


## 2022-10-12

[A Game Theoretical vulnerability analysis of Adversarial Attack. (99%)](http://arxiv.org/abs/2210.06670)

Khondker Fariha Hossain, Alireza Tavakkoli, Shamik Sengupta


[Boosting the Transferability of Adversarial Attacks with Reverse Adversarial Perturbation. (99%)](http://arxiv.org/abs/2210.05968)

Zeyu Qin, Yanbo Fan, Yi Liu, Li Shen, Yong Zhang, Jue Wang, Baoyuan Wu


[Visual Prompting for Adversarial Robustness. (99%)](http://arxiv.org/abs/2210.06284)

Aochuan Chen, Peter Lorenz, Yuguang Yao, Pin-Yu Chen, Sijia Liu


[Robust Models are less Over-Confident. (96%)](http://arxiv.org/abs/2210.05938)

Julia Grabinski, Paul Gavrikov, Janis Keuper, Margret Keuper


[Double Bubble, Toil and Trouble: Enhancing Certified Robustness through Transitivity. (86%)](http://arxiv.org/abs/2210.06077)

Andrew C. Cullen, Paul Montague, Shijie Liu, Sarah M. Erfani, Benjamin I. P. Rubinstein


[Efficient Adversarial Training without Attacking: Worst-Case-Aware Robust Reinforcement Learning. (82%)](http://arxiv.org/abs/2210.05927)

Yongyuan Liang, Yanchao Sun, Ruijie Zheng, Furong Huang


[COLLIDER: A Robust Training Framework for Backdoor Data. (81%)](http://arxiv.org/abs/2210.06704)

Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie


[Trap and Replace: Defending Backdoor Attacks by Trapping Them into an Easy-to-Replace Subnetwork. (76%)](http://arxiv.org/abs/2210.06428)

Haotao Wang, Junyuan Hong, Aston Zhang, Jiayu Zhou, Zhangyang Wang


[Few-shot Backdoor Attacks via Neural Tangent Kernels. (62%)](http://arxiv.org/abs/2210.05929)

Jonathan Hayase, Sewoong Oh


[How to Sift Out a Clean Data Subset in the Presence of Data Poisoning? (9%)](http://arxiv.org/abs/2210.06516)

Yi Zeng, Minzhou Pan, Himanshu Jahagirdar, Ming Jin, Lingjuan Lyu, Ruoxi Jia


[Understanding Impacts of Task Similarity on Backdoor Attack and Detection. (2%)](http://arxiv.org/abs/2210.06509)

Di Tang, Rui Zhu, XiaoFeng Wang, Haixu Tang, Yi Chen


[When are Local Queries Useful for Robust Learning? (1%)](http://arxiv.org/abs/2210.06089)

Pascale Gourdeau, Varun Kanade, Marta Kwiatkowska, James Worrell


## 2022-10-11

[What Can the Neural Tangent Kernel Tell Us About Adversarial Robustness? (99%)](http://arxiv.org/abs/2210.05577)

Nikolaos Tsilivis, Julia Kempe


[Stable and Efficient Adversarial Training through Local Linearization. (91%)](http://arxiv.org/abs/2210.05373)

Zhuorong Li, Daiwei Yu


[RoHNAS: A Neural Architecture Search Framework with Conjoint Optimization for Adversarial Robustness and Hardware Efficiency of Convolutional and Capsule Networks. (86%)](http://arxiv.org/abs/2210.05276)

Alberto Marchisio, Vojtech Mrazek, Andrea Massa, Beatrice Bussolino, Maurizio Martina, Muhammad Shafique


[Adversarial Attack Against Image-Based Localization Neural Networks. (78%)](http://arxiv.org/abs/2210.06589)

Meir Brand, Itay Naeh, Daniel Teitelman


[Detecting Backdoors in Deep Text Classifiers. (76%)](http://arxiv.org/abs/2210.11264)

You Guo, Jun Wang, Trevor Cohn


[Human Body Measurement Estimation with Adversarial Augmentation. (33%)](http://arxiv.org/abs/2210.05667)

Nataniel Ruiz, Miriam Bellver, Timo Bolkart, Ambuj Arora, Ming C. Lin, Javier Romero, Raja Bala


[Curved Representation Space of Vision Transformers. (10%)](http://arxiv.org/abs/2210.05742)

Juyeop Kim, Junha Park, Songkuk Kim, Jong-Seok Lee


[Zeroth-Order Hard-Thresholding: Gradient Error vs. Expansivity. (1%)](http://arxiv.org/abs/2210.05279)

Vazelhes William de, Hualin Zhang, Huimin Wu, Xiao-Tong Yuan, Bin Gu


[Make Sharpness-Aware Minimization Stronger: A Sparsified Perturbation Approach. (1%)](http://arxiv.org/abs/2210.05177)

Peng Mi, Li Shen, Tianhe Ren, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji, Dacheng Tao


## 2022-10-10

[Boosting Adversarial Robustness From The Perspective of Effective Margin Regularization. (92%)](http://arxiv.org/abs/2210.05118)

Ziquan Liu, Antoni B. Chan


[Revisiting adapters with adversarial training. (88%)](http://arxiv.org/abs/2210.04886)

Sylvestre-Alvise Rebuffi, Francesco Croce, Sven Gowal


[Universal Adversarial Perturbations: Efficiency on a small image dataset. (81%)](http://arxiv.org/abs/2210.04591)

Waris ENSEIRB-MATMECA, UB Radji


[Certified Training: Small Boxes are All You Need. (22%)](http://arxiv.org/abs/2210.04871)

Mark Niklas Müller, Franziska Eckert, Marc Fischer, Martin Vechev


[Denoising Masked AutoEncoders Help Robust Classification. (1%)](http://arxiv.org/abs/2210.06983)

Quanlin Wu, Hang Ye, Yuntian Gu, Huishuai Zhang, Liwei Wang, Di He


## 2022-10-09

[Pruning Adversarially Robust Neural Networks without Adversarial Examples. (99%)](http://arxiv.org/abs/2210.04311)

Tong Jian, Zifeng Wang, Yanzhi Wang, Jennifer Dy, Stratis Ioannidis


[Towards Understanding and Boosting Adversarial Transferability from a Distribution Perspective. (99%)](http://arxiv.org/abs/2210.04213)

Yao Zhu, Yuefeng Chen, Xiaodan Li, Kejiang Chen, Yuan He, Xiang Tian, Bolun Zheng, Yaowu Chen, Qingming Huang


[Online Training Through Time for Spiking Neural Networks. (1%)](http://arxiv.org/abs/2210.04195)

Mingqing Xiao, Qingyan Meng, Zongpeng Zhang, Di He, Zhouchen Lin


## 2022-10-08

[FedDef: Defense Against Gradient Leakage in Federated Learning-based Network Intrusion Detection Systems. (99%)](http://arxiv.org/abs/2210.04052)

Jiahui Chen, Yi Zhao, Qi Li, Xuewei Feng, Ke Xu


[Symmetry Defense Against CNN Adversarial Perturbation Attacks. (99%)](http://arxiv.org/abs/2210.04087)

Blerta Lindqvist


[Robustness of Unsupervised Representation Learning without Labels. (54%)](http://arxiv.org/abs/2210.04076)

Aleksandar Petrov, Marta Kwiatkowska


## 2022-10-07

[Adversarially Robust Prototypical Few-shot Segmentation with Neural-ODEs. (99%)](http://arxiv.org/abs/2210.03429)

Prashant Pandey, Aleti Vardhan, Mustafa Chasmai, Tanuj Sur, Brejesh Lall


[Pre-trained Adversarial Perturbations. (99%)](http://arxiv.org/abs/2210.03372)

Yuanhao Ban, Yinpeng Dong


[ViewFool: Evaluating the Robustness of Visual Recognition to Adversarial Viewpoints. (93%)](http://arxiv.org/abs/2210.03895)

Yinpeng Dong, Shouwei Ruan, Hang Su, Caixin Kang, Xingxing Wei, Jun Zhu


[Game-Theoretic Understanding of Misclassification. (47%)](http://arxiv.org/abs/2210.03349)

Kosuke Sumiyasu, Kazuhiko Kawamoto, Hiroshi Kera


[A2: Efficient Automated Attacker for Boosting Adversarial Training. (41%)](http://arxiv.org/abs/2210.03543)

Zhuoer Xu, Guanghui Zhu, Changhua Meng, Shiwen Cui, Zhenzhe Ying, Weiqiang Wang, Ming GU, Yihua Huang


[NMTSloth: Understanding and Testing Efficiency Degradation of Neural Machine Translation Systems. (13%)](http://arxiv.org/abs/2210.03696)

Simin Chen, Cong Liu, Mirazul Haque, Zihe Song, Wei Yang


[A Wolf in Sheep's Clothing: Spreading Deadly Pathogens Under the Disguise of Popular Music. (2%)](http://arxiv.org/abs/2210.03688)

Anomadarshi Barua, Yonatan Gizachew Achamyeleh, Mohammad Abdullah Al Faruque


[Improving Fine-Grain Segmentation via Interpretable Modifications: A Case Study in Fossil Segmentation. (1%)](http://arxiv.org/abs/2210.03879)

Indu Panigrahi, Ryan Manzuk, Adam Maloof, Ruth Fong


[Mind Your Data! Hiding Backdoors in Offline Reinforcement Learning Datasets. (1%)](http://arxiv.org/abs/2210.04688)

Chen Gong, Zhou Yang, Yunpeng Bai, Junda He, Jieke Shi, Arunesh Sinha, Bowen Xu, Xinwen Hou, Guoliang Fan, David Lo


## 2022-10-06

[Preprocessors Matter! Realistic Decision-Based Attacks on Machine Learning Systems. (99%)](http://arxiv.org/abs/2210.03297)

Chawin Sitawarin, Florian Tramèr, Nicholas Carlini


[Enhancing Code Classification by Mixup-Based Data Augmentation. (96%)](http://arxiv.org/abs/2210.03003)

Zeming Dong, Qiang Hu, Yuejun Guo, Maxime Cordy, Mike Papadakis, Yves Le Traon, Jianjun Zhao


[Deep Reinforcement Learning based Evasion Generative Adversarial Network for Botnet Detection. (92%)](http://arxiv.org/abs/2210.02840)

Rizwan Hamid Randhawa, Nauman Aslam, Mohammad Alauthman, Muhammad Khalid, Husnain Rafiq


[On Optimal Learning Under Targeted Data Poisoning. (82%)](http://arxiv.org/abs/2210.02713)

Steve Hanneke, Amin Karbasi, Mohammad Mahmoody, Idan Mehalel, Shay Moran


[Towards Out-of-Distribution Adversarial Robustness. (73%)](http://arxiv.org/abs/2210.03150)

Adam Ibrahim, Charles Guille-Escuret, Ioannis Mitliagkas, Irina Rish, David Krueger, Pouya Bashivan


[InferES : A Natural Language Inference Corpus for Spanish Featuring Negation-Based Contrastive and Adversarial Examples. (61%)](http://arxiv.org/abs/2210.03068)

Venelin Kovatchev, Mariona Taulé


[Unsupervised Domain Adaptation for COVID-19 Information Service with Contrastive Adversarial Domain Mixup. (41%)](http://arxiv.org/abs/2210.03250)

Huimin Zeng, Zhenrui Yue, Ziyi Kou, Lanyu Shang, Yang Zhang, Dong Wang


[Synthetic Dataset Generation for Privacy-Preserving Machine Learning. (2%)](http://arxiv.org/abs/2210.03205)

Efstathia Soufleri, Gobinda Saha, Kaushik Roy


[Enhancing Mixup-Based Graph Learning for Language Processing via Hybrid Pooling. (1%)](http://arxiv.org/abs/2210.03123)

Zeming Dong, Qiang Hu, Yuejun Guo, Maxime Cordy, Mike Papadakis, Yves Le Traon, Jianjun Zhao


[Bad Citrus: Reducing Adversarial Costs with Model Distances. (1%)](http://arxiv.org/abs/2210.03239)

Giorgio Severi, Will Pearce, Alina Oprea


## 2022-10-05

[Natural Color Fool: Towards Boosting Black-box Unrestricted Attacks. (99%)](http://arxiv.org/abs/2210.02041)

Shengming Yuan, Qilong Zhang, Lianli Gao, Yaya Cheng, Jingkuan Song


[Dynamic Stochastic Ensemble with Adversarial Robust Lottery Ticket Subnetworks. (98%)](http://arxiv.org/abs/2210.02618)

Qi Peng, Wenlin Liu, Ruoxi Qin, Libin Hou, Bin Yan, Linyuan Wang


[On Adversarial Robustness of Deep Image Deblurring. (83%)](http://arxiv.org/abs/2210.02502)

Kanchana Vaishnavi Gandikota, Paramanand Chandramouli, Michael Moeller


[A Closer Look at Robustness to L-infinity and Spatial Perturbations and their Composition. (81%)](http://arxiv.org/abs/2210.02577)

Luke Rowe, Benjamin Thérien, Krzysztof Czarnecki, Hongyang Zhang


[Jitter Does Matter: Adapting Gaze Estimation to New Domains. (78%)](http://arxiv.org/abs/2210.02082)

Ruicong Liu, Yiwei Bao, Mingjie Xu, Haofei Wang, Yunfei Liu, Feng Lu


[Image Masking for Robust Self-Supervised Monocular Depth Estimation. (38%)](http://arxiv.org/abs/2210.02357)

Hemang Chawla, Kishaan Jeeveswaran, Elahe Arani, Bahram Zonooz


[Over-the-Air Federated Learning with Privacy Protection via Correlated Additive Perturbations. (38%)](http://arxiv.org/abs/2210.02235)

Jialing Liao, Zheng Chen, Erik G. Larsson


## 2022-10-04

[Rethinking Lipschitz Neural Networks and Certified Robustness: A Boolean Function Perspective. (97%)](http://arxiv.org/abs/2210.01787)

Bohang Zhang, Du Jiang, Di He, Liwei Wang


[Robust Fair Clustering: A Novel Fairness Attack and Defense Framework. (93%)](http://arxiv.org/abs/2210.01953)

Anshuman Chhabra, Peizhao Li, Prasant Mohapatra, Hongfu Liu


[A Study on the Efficiency and Generalization of Light Hybrid Retrievers. (86%)](http://arxiv.org/abs/2210.01371)

Man Luo, Shashank Jain, Anchit Gupta, Arash Einolghozati, Barlas Oguz, Debojeet Chatterjee, Xilun Chen, Chitta Baral, Peyman Heidari


[Practical Adversarial Attacks on Spatiotemporal Traffic Forecasting Models. (81%)](http://arxiv.org/abs/2210.02447)

Fan Liu, Hao Liu, Wenzhao Jiang


[Invariant Aggregator for Defending Federated Backdoor Attacks. (80%)](http://arxiv.org/abs/2210.01834)

Xiaoyang Wang, Dimitrios Dimitriadis, Sanmi Koyejo, Shruti Tople


[On the Robustness of Deep Clustering Models: Adversarial Attacks and Defenses. (75%)](http://arxiv.org/abs/2210.01940)

Anshuman Chhabra, Ashwin Sekhari, Prasant Mohapatra


[Robustness Certification of Visual Perception Models via Camera Motion Smoothing. (70%)](http://arxiv.org/abs/2210.04625)

Hanjiang Hu, Zuxin Liu, Linyi Li, Jiacheng Zhu, Ding Zhao


[Backdoor Attacks in the Supply Chain of Masked Image Modeling. (68%)](http://arxiv.org/abs/2210.01632)

Xinyue Shen, Xinlei He, Zheng Li, Yun Shen, Michael Backes, Yang Zhang


[CADet: Fully Self-Supervised Anomaly Detection With Contrastive Learning. (67%)](http://arxiv.org/abs/2210.01742)

Charles Guille-Escuret, Pau Rodriguez, David Vazquez, Ioannis Mitliagkas, Joao Monteiro


## 2022-10-03

[MultiGuard: Provably Robust Multi-label Classification against Adversarial Examples. (99%)](http://arxiv.org/abs/2210.01111)

Jinyuan Jia, Wenjie Qu, Neil Zhenqiang Gong


[Push-Pull: Characterizing the Adversarial Robustness for Audio-Visual Active Speaker Detection. (97%)](http://arxiv.org/abs/2210.00753)

Xuanjun Chen, Haibin Wu, Helen Meng, Hung-yi Lee, Jyh-Shing Roger Jang


[Stability Analysis and Generalization Bounds of Adversarial Training. (96%)](http://arxiv.org/abs/2210.00960)

Jiancong Xiao, Yanbo Fan, Ruoyu Sun, Jue Wang, Zhi-Quan Luo


[On Attacking Out-Domain Uncertainty Estimation in Deep Neural Networks. (92%)](http://arxiv.org/abs/2210.02191)

Huimin Zeng, Zhenrui Yue, Yang Zhang, Ziyi Kou, Lanyu Shang, Dong Wang


[Decompiling x86 Deep Neural Network Executables. (83%)](http://arxiv.org/abs/2210.01075)

Zhibo Liu, Yuanyuan Yuan, Shuai Wang, Xiaofei Xie, Lei Ma


[Strength-Adaptive Adversarial Training. (80%)](http://arxiv.org/abs/2210.01288)

Chaojian Yu, Dawei Zhou, Li Shen, Jun Yu, Bo Han, Mingming Gong, Nannan Wang, Tongliang Liu


[ASGNN: Graph Neural Networks with Adaptive Structure. (68%)](http://arxiv.org/abs/2210.01002)

Zepeng Zhang, Songtao Lu, Zengfeng Huang, Ziping Zhao


[UnGANable: Defending Against GAN-based Face Manipulation. (2%)](http://arxiv.org/abs/2210.00957)

Zheng Li, Ning Yu, Ahmed Salem, Michael Backes, Mario Fritz, Yang Zhang


## 2022-10-02

[Adaptive Smoothness-weighted Adversarial Training for Multiple Perturbations with Its Stability Analysis. (99%)](http://arxiv.org/abs/2210.00557)

Jiancong Xiao, Zeyu Qin, Yanbo Fan, Baoyuan Wu, Jue Wang, Zhi-Quan Luo


[Understanding Adversarial Robustness Against On-manifold Adversarial Examples. (99%)](http://arxiv.org/abs/2210.00430)

Jiancong Xiao, Liusha Yang, Yanbo Fan, Jue Wang, Zhi-Quan Luo


[FLCert: Provably Secure Federated Learning against Poisoning Attacks. (74%)](http://arxiv.org/abs/2210.00584)

Xiaoyu Cao, Zaixi Zhang, Jinyuan Jia, Neil Zhenqiang Gong


[Optimization for Robustness Evaluation beyond $\ell_p$ Metrics. (16%)](http://arxiv.org/abs/2210.00621)

Hengyue Liang, Buyun Liang, Ying Cui, Tim Mitchell, Ju Sun


[Automated Security Analysis of Exposure Notification Systems. (1%)](http://arxiv.org/abs/2210.00649)

Kevin Morio, Ilkan Esiyok, Dennis Jackson, Robert Künnemann


## 2022-10-01

[DeltaBound Attack: Efficient decision-based attack in low queries regime. (96%)](http://arxiv.org/abs/2210.00292)

Lorenzo Rossi


[Adversarial Attacks on Transformers-Based Malware Detectors. (91%)](http://arxiv.org/abs/2210.00008)

Yash Jakhotiya, Heramb Patil, Jugal Rawlani, Dr. Sunil B. Mane


[Voice Spoofing Countermeasures: Taxonomy, State-of-the-art, experimental analysis of generalizability, open challenges, and the way forward. (5%)](http://arxiv.org/abs/2210.00417)

Awais Khan, Khalid Mahmood Malik, James Ryan, Mikul Saravanan


## 2022-09-30

[Your Out-of-Distribution Detection Method is Not Robust! (99%)](http://arxiv.org/abs/2209.15246)

Mohammad Azizmalayeri, Arshia Soltani Moakhar, Arman Zarei, Reihaneh Zohrabi, Mohammad Taghi Manzuri, Mohammad Hossein Rohban


[Learning Robust Kernel Ensembles with Kernel Average Pooling. (99%)](http://arxiv.org/abs/2210.00062)

Pouya Bashivan, Adam Ibrahim, Amirozhan Dehghani, Yifei Ren


[Adversarial Robustness of Representation Learning for Knowledge Graphs. (95%)](http://arxiv.org/abs/2210.00122)

Peru Bhardwaj


[Hiding Visual Information via Obfuscating Adversarial Perturbations. (92%)](http://arxiv.org/abs/2209.15304)

Zhigang Su, Dawei Zhou, Nannan Wangu, Decheng Li, Zhen Wang, Xinbo Gao


[On the tightness of linear relaxation based robustness certification methods. (78%)](http://arxiv.org/abs/2210.00178)

Cheng Tang


[Data Poisoning Attacks Against Multimodal Encoders. (73%)](http://arxiv.org/abs/2209.15266)

Ziqing Yang, Xinlei He, Zheng Li, Michael Backes, Mathias Humbert, Pascal Berrang, Yang Zhang


[ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled neural networks. (70%)](http://arxiv.org/abs/2210.00108)

Tim Clifford, Ilia Shumailov, Yiren Zhao, Ross Anderson, Robert Mullins


## 2022-09-29

[Physical Adversarial Attack meets Computer Vision: A Decade Survey. (99%)](http://arxiv.org/abs/2209.15179)

Hui Wei, Hao Tang, Xuemei Jia, Hanxun Yu, Zhubo Li, Zhixiang Wang, Shin'ichi Satoh, Zheng Wang


[Towards Lightweight Black-Box Attacks against Deep Neural Networks. (99%)](http://arxiv.org/abs/2209.14826)

Chenghao Sun, Yonggang Zhang, Wan Chaoqun, Qizhou Wang, Ya Li, Tongliang Liu, Bo Han, Xinmei Tian


[Generalizability of Adversarial Robustness Under Distribution Shifts. (83%)](http://arxiv.org/abs/2209.15042)

Kumail Alhamoud, Hasan Abed Al Kader Hammoud, Motasem Alfarra, Bernard Ghanem


[Digital and Physical Face Attacks: Reviewing and One Step Further. (2%)](http://arxiv.org/abs/2209.14692)

Chenqi Kong, Shiqi Wang, Haoliang Li


[Chameleon Cache: Approximating Fully Associative Caches with Random Replacement to Prevent Contention-Based Cache Attacks. (1%)](http://arxiv.org/abs/2209.14673)

Thomas Unterluggauer, Austin Harris, Scott Constable, Fangfei Liu, Carlos Rozas


## 2022-09-28

[A Survey on Physical Adversarial Attack in Computer Vision. (99%)](http://arxiv.org/abs/2209.14262)

Donghua Wang, Wen Yao, Tingsong Jiang, Guijiang Tang, Xiaoqian Chen


[Exploring the Relationship between Architecture and Adversarially Robust Generalization. (99%)](http://arxiv.org/abs/2209.14105)

Aishan Liu, Shiyu Tang, Siyuan Liang, Ruihao Gong, Boxi Wu, Xianglong Liu, Dacheng Tao


[A Closer Look at Evaluating the Bit-Flip Attack Against Deep Neural Networks. (67%)](http://arxiv.org/abs/2209.14243)

Kevin Hector, Mathieu Dumont, Pierre-Alain Moellic, Jean-Max Dutertre


[Supervised Contrastive Learning as Multi-Objective Optimization for Fine-Tuning Large Pre-trained Language Models. (47%)](http://arxiv.org/abs/2209.14161)

Youness Moukafih, Mounir Ghogho, Kamel Smaili


[On the Robustness of Random Forest Against Untargeted Data Poisoning: An Ensemble-Based Approach. (31%)](http://arxiv.org/abs/2209.14013)

Marco Anisetti, Claudio A. Ardagna, Alessandro Balestrucci, Nicola Bena, Ernesto Damiani, Chan Yeob Yeun


[CALIP: Zero-Shot Enhancement of CLIP with Parameter-free Attention. (1%)](http://arxiv.org/abs/2209.14169)

Ziyu Guo, Renrui Zhang, Longtian Qiu, Xianzheng Ma, Xupeng Miao, Xuming He, Bin Cui


[Improving alignment of dialogue agents via targeted human judgements. (1%)](http://arxiv.org/abs/2209.14375)

Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Soňa Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, Geoffrey Irving


## 2022-09-27

[Suppress with a Patch: Revisiting Universal Adversarial Patch Attacks against Object Detection. (74%)](http://arxiv.org/abs/2209.13353)

Svetlana Pavlitskaya, Jonas Hendl, Sebastian Kleim, Leopold Müller, Fabian Wylczoch, J. Marius Zöllner


[Inducing Data Amplification Using Auxiliary Datasets in Adversarial Training. (33%)](http://arxiv.org/abs/2209.14053)

Saehyung Lee, Hyungyu Lee


[Attacking Compressed Vision Transformers. (33%)](http://arxiv.org/abs/2209.13785)

Swapnil Parekh, Devansh Shah, Pratyush Shukla


[Mitigating Attacks on Artificial Intelligence-based Spectrum Sensing for Cellular Network Signals. (8%)](http://arxiv.org/abs/2209.13007)

Ferhat Ozgur Catak, Murat Kuzlu, Salih Sarp, Evren Catak, Umit Cali


[Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection. (5%)](http://arxiv.org/abs/2210.00875)

Yiming Li, Yang Bai, Yong Jiang, Yong Yang, Shu-Tao Xia, Bo Li


[Reconstruction-guided attention improves the robustness and shape processing of neural networks. (2%)](http://arxiv.org/abs/2209.13620)

Seoyoung Ahn, Hossein Adeli, Gregory J. Zelinsky


[A Learning-based Honeypot Game for Collaborative Defense in UAV Networks. (1%)](http://arxiv.org/abs/2209.13815)

Yuntao Wang, Zhou Su, Abderrahim Benslimane, Qichao Xu, Minghui Dai, Ruidong Li


[Stability Via Adversarial Training of Neural Network Stochastic Control of Mean-Field Type. (1%)](http://arxiv.org/abs/2210.00874)

Julian Barreiro-Gomez, Salah Eddine Choutri, Boualem Djehiche


[Measuring Overfitting in Convolutional Neural Networks using Adversarial Perturbations and Label Noise. (1%)](http://arxiv.org/abs/2209.13382)

Svetlana Pavlitskaya, Joël Oswald, J. Marius Zöllner


## 2022-09-26

[FG-UAP: Feature-Gathering Universal Adversarial Perturbation. (99%)](http://arxiv.org/abs/2209.13113)

Zhixing Ye, Xinwen Cheng, Xiaolin Huang


[Activation Learning by Local Competitions. (64%)](http://arxiv.org/abs/2209.13400)

Hongchao Zhou


[Multi-Task Adversarial Training Algorithm for Multi-Speaker Neural Text-to-Speech. (1%)](http://arxiv.org/abs/2209.12549)

Yusuke Nakai, Yuki Saito, Kenta Udagawa, Hiroshi Saruwatari


[Greybox XAI: a Neural-Symbolic learning framework to produce interpretable predictions for image classification. (1%)](http://arxiv.org/abs/2209.14974)

Adrien Bennetot, Gianni Franchi, Ser Javier Del, Raja Chatila, Natalia Diaz-Rodriguez


## 2022-09-25

[SPRITZ-1.5C: Employing Deep Ensemble Learning for Improving the Security of Computer Networks against Adversarial Attacks. (81%)](http://arxiv.org/abs/2209.12195)

Ehsan Nowroozi, Mohammadreza Mohammadi, Erkay Savas, Mauro Conti, Yassine Mekdad


## 2022-09-24

[Approximate better, Attack stronger: Adversarial Example Generation via Asymptotically Gaussian Mixture Distribution. (99%)](http://arxiv.org/abs/2209.11964)

Zhengwei Fang, Rui Wang, Tao Huang, Liping Jing


## 2022-09-23

[The "Beatrix'' Resurrections: Robust Backdoor Detection via Gram Matrices. (13%)](http://arxiv.org/abs/2209.11715)

Wanlun Ma, Derui Wang, Ruoxi Sun, Minhui Xue, Sheng Wen, Yang Xiang


## 2022-09-22

[Privacy Attacks Against Biometric Models with Fewer Samples: Incorporating the Output of Multiple Models. (50%)](http://arxiv.org/abs/2209.11020)

Sohaib Ahmad, Benjamin Fuller, Kaleel Mahmood


## 2022-09-21

[Fair Robust Active Learning by Joint Inconsistency. (99%)](http://arxiv.org/abs/2209.10729)

Tsung-Han Wu, Shang-Tse Chen, Winston H. Hsu


[Toy Models of Superposition. (45%)](http://arxiv.org/abs/2209.10652)

Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, Christopher Olah


[DARTSRepair: Core-failure-set Guided DARTS for Network Robustness to Common Corruptions. (13%)](http://arxiv.org/abs/2209.10381)

Xuhong Ren, Jianlang Chen, Felix Juefei-Xu, Wanli Xue, Qing Guo, Lei Ma, Jianjun Zhao, Shengyong Chen


[Fairness Reprogramming. (1%)](http://arxiv.org/abs/2209.10222)

Guanhua Zhang, Yihua Zhang, Yang Zhang, Wenqi Fan, Qing Li, Sijia Liu, Shiyu Chang


## 2022-09-20

[Understanding Real-world Threats to Deep Learning Models in Android Apps. (99%)](http://arxiv.org/abs/2209.09577)

Zizhuang Deng, Kai Chen, Guozhu Meng, Xiaodong Zhang, Ke Xu, Yao Cheng


[Audit and Improve Robustness of Private Neural Networks on Encrypted Data. (99%)](http://arxiv.org/abs/2209.09996)

Jiaqi Xue, Lei Xu, Lin Chen, Weidong Shi, Kaidi Xu, Qian Lou


[GAMA: Generative Adversarial Multi-Object Scene Attacks. (99%)](http://arxiv.org/abs/2209.09502)

Abhishek Aich, Calvin-Khang Ta, Akash Gupta, Chengyu Song, Srikanth V. Krishnamurthy, M. Salman Asif, Amit K. Roy-Chowdhury


[Sparse Vicious Attacks on Graph Neural Networks. (98%)](http://arxiv.org/abs/2209.09688)

Giovanni Trappolini, Valentino Maiorca, Silvio Severino, Emanuele Rodolà, Fabrizio Silvestri, Gabriele Tolomei


[Leveraging Local Patch Differences in Multi-Object Scenes for Generative Adversarial Attacks. (98%)](http://arxiv.org/abs/2209.09883)

Abhishek Aich, Shasha Li, Chengyu Song, M. Salman Asif, Srikanth V. Krishnamurthy, Amit K. Roy-Chowdhury


[Rethinking Data Augmentation in Knowledge Distillation for Object Detection. (68%)](http://arxiv.org/abs/2209.09841)

Jiawei Liang, Siyuan Liang, Aishan Liu, Mingli Zhu, Danni Yuan, Chenye Xu, Xiaochun Cao


[CANflict: Exploiting Peripheral Conflicts for Data-Link Layer Attacks on Automotive Networks. (1%)](http://arxiv.org/abs/2209.09557)

Alvise de Faveri Tron, Stefano Longari, Michele Carminati, Mario Polino, Stefano Zanero


[EM-Fault It Yourself: Building a Replicable EMFI Setup for Desktop and Server Hardware. (1%)](http://arxiv.org/abs/2209.09835)

Niclas Kühnapfel, Robert Buhren, Hans Niklas Jacob, Thilo Krachenfels, Christian Werling, Jean-Pierre Seifert


## 2022-09-19

[Adversarial Catoptric Light: An Effective, Stealthy and Robust Physical-World Attack to DNNs. (99%)](http://arxiv.org/abs/2209.11739)

Chengyin Hu, Weiwen Shi


[Adversarial Color Projection: A Projector-Based Physical Attack to DNNs. (99%)](http://arxiv.org/abs/2209.09652)

Chengyin Hu, Weiwen Shi


## 2022-09-18

[On the Adversarial Transferability of ConvMixer Models. (99%)](http://arxiv.org/abs/2209.08724)

Ryota Iijima, Miki Tanaka, Isao Echizen, Hitoshi Kiya


[AdvDO: Realistic Adversarial Attacks for Trajectory Prediction. (96%)](http://arxiv.org/abs/2209.08744)

Yulong Cao, Chaowei Xiao, Anima Anandkumar, Danfei Xu, Marco Pavone


[Distribution inference risks: Identifying and mitigating sources of leakage. (1%)](http://arxiv.org/abs/2209.08541)

Valentin Hartmann, Léo Meynent, Maxime Peyrard, Dimitrios Dimitriadis, Shruti Tople, Robert West


## 2022-09-17

[Watch What You Pretrain For: Targeted, Transferable Adversarial Examples on Self-Supervised Speech Recognition models. (99%)](http://arxiv.org/abs/2209.13523)

Raphael Olivier, Hadi Abdullah, Bhiksha Raj


[pFedDef: Defending Grey-Box Attacks for Personalized Federated Learning. (98%)](http://arxiv.org/abs/2209.08412)

Taejin Kim, Shubhranshu Singh, Nikhil Madaan, Carlee Joe-Wong


[A study on the deviations in performance of FNNs and CNNs in the realm of grayscale adversarial images. (4%)](http://arxiv.org/abs/2209.08262)

Durga Shree Nagabushanam, Steve Mathew, Chiranji Lal Chowdhary


## 2022-09-16

[Robust Ensemble Morph Detection with Domain Generalization. (99%)](http://arxiv.org/abs/2209.08130)

Hossein Kashiani, Shoaib Meraj Sami, Sobhan Soleymani, Nasser M. Nasrabadi


[A Large-scale Multiple-objective Method for Black-box Attack against Object Detection. (99%)](http://arxiv.org/abs/2209.07790)

Siyuan Liang, Longkang Li, Yanbo Fan, Xiaojun Jia, Jingzhi Li, Baoyuan Wu, Xiaochun Cao


[Enhance the Visual Representation via Discrete Adversarial Training. (97%)](http://arxiv.org/abs/2209.07735)

Xiaofeng Mao, Yuefeng Chen, Ranjie Duan, Yao Zhu, Gege Qi, Shaokai Ye, Xiaodan Li, Rong Zhang, Hui Xue


[Model Inversion Attacks against Graph Neural Networks. (92%)](http://arxiv.org/abs/2209.07807)

Zaixi Zhang, Qi Liu, Zhenya Huang, Hao Wang, Chee-Kong Lee, Enhong Chen


[PointCAT: Contrastive Adversarial Training for Robust Point Cloud Recognition. (62%)](http://arxiv.org/abs/2209.07788)

Qidong Huang, Xiaoyi Dong, Dongdong Chen, Hang Zhou, Weiming Zhang, Kui Zhang, Gang Hua, Nenghai Yu


[Cascading Failures in Power Grids. (33%)](http://arxiv.org/abs/2209.08116)

Rounak Meyur


[Dataset Inference for Self-Supervised Models. (16%)](http://arxiv.org/abs/2209.09024)

Adam Dziedzic, Haonan Duan, Muhammad Ahmad Kaleem, Nikita Dhawan, Jonas Guan, Yannis Cattan, Franziska Boenisch, Nicolas Papernot


[On the Robustness of Graph Neural Diffusion to Topology Perturbations. (15%)](http://arxiv.org/abs/2209.07754)

Yang Song, Qiyu Kang, Sijie Wang, Zhao Kai, Wee Peng Tay


[A Systematic Evaluation of Node Embedding Robustness. (11%)](http://arxiv.org/abs/2209.08064)

Alexandru Mara, Jefrey Lijffijt, Stephan Günnemann, Bie Tijl De


## 2022-09-15

[Improving Robust Fairness via Balance Adversarial Training. (99%)](http://arxiv.org/abs/2209.07534)

Chunyu Sun, Chenye Xu, Chengyuan Yao, Siyuan Liang, Yichao Wu, Ding Liang, XiangLong Liu, Aishan Liu


[A Light Recipe to Train Robust Vision Transformers. (98%)](http://arxiv.org/abs/2209.07399)

Edoardo Debenedetti, Vikash Sehwag, Prateek Mittal


[Part-Based Models Improve Adversarial Robustness. (92%)](http://arxiv.org/abs/2209.09117)

Chawin Sitawarin, Kornrapat Pongmala, Yizheng Chen, Nicholas Carlini, David Wagner


[Explicit Tradeoffs between Adversarial and Natural Distributional Robustness. (80%)](http://arxiv.org/abs/2209.07592)

Mazda Moayeri, Kiarash Banihashem, Soheil Feizi


[Adversarially Robust Learning: A Generic Minimax Optimal Learner and Characterization. (80%)](http://arxiv.org/abs/2209.07369)

Omar Montasser, Steve Hanneke, Nathan Srebro


[Defending Root DNS Servers Against DDoS Using Layered Defenses. (15%)](http://arxiv.org/abs/2209.07491)

A S M Rizvi, Jelena Mirkovic, John Heidemann, Wesley Hardaker, Robert Story


[BadRes: Reveal the Backdoors through Residual Connection. (2%)](http://arxiv.org/abs/2209.07125)

Mingrui He, Tianyu Chen, Haoyi Zhou, Shanghang Zhang, Jianxin Li


[Adversarial Cross-View Disentangled Graph Contrastive Learning. (1%)](http://arxiv.org/abs/2209.07699)

Qianlong Wen, Zhongyu Ouyang, Chunhui Zhang, Yiyue Qian, Yanfang Ye, Chuxu Zhang


[Towards Improving Calibration in Object Detection Under Domain Shift. (1%)](http://arxiv.org/abs/2209.07601)

Muhammad Akhtar Munir, Muhammad Haris Khan, M. Saquib Sarfraz, Mohsen Ali


## 2022-09-14

[Robust Transferable Feature Extractors: Learning to Defend Pre-Trained Networks Against White Box Adversaries. (99%)](http://arxiv.org/abs/2209.06931)

Alexander Cann, Ian Colbert, Ihab Amer


[PointACL:Adversarial Contrastive Learning for Robust Point Clouds Representation under Adversarial Attack. (99%)](http://arxiv.org/abs/2209.06971)

Junxuan Huang, Yatong An, Lu cheng, Bai Chen, Junsong Yuan, Chunming Qiao


[Certified Robustness to Word Substitution Ranking Attack for Neural Ranking Models. (99%)](http://arxiv.org/abs/2209.06691)

Chen Wu, Ruqing Zhang, Jiafeng Guo, Wei Chen, Yixing Fan, Rijke Maarten de, Xueqi Cheng


[Order-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models. (97%)](http://arxiv.org/abs/2209.06506)

Jiawei Liu, Yangyang Kang, Di Tang, Kaisong Song, Changlong Sun, Xiaofeng Wang, Wei Lu, Xiaozhong Liu


[On the interplay of adversarial robustness and architecture components: patches, convolution and attention. (67%)](http://arxiv.org/abs/2209.06953)

Francesco Croce, Matthias Hein


[M^4I: Multi-modal Models Membership Inference. (54%)](http://arxiv.org/abs/2209.06997)

Pingyi Hu, Zihan Wang, Ruoxi Sun, Hu Wang, Minhui Xue


[Finetuning Pretrained Vision-Language Models with Correlation Information Bottleneck for Robust Visual Question Answering. (12%)](http://arxiv.org/abs/2209.06954)

Jingjing Jiang, Ziyi Liu, Nanning Zheng


[Robust Constrained Reinforcement Learning. (9%)](http://arxiv.org/abs/2209.06866)

Yue Wang, Fei Miao, Shaofeng Zou


## 2022-09-13

[Adversarial Coreset Selection for Efficient Robust Training. (99%)](http://arxiv.org/abs/2209.05785)

Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie


[TSFool: Crafting High-quality Adversarial Time Series through Multi-objective Optimization to Fool Recurrent Neural Network Classifiers. (99%)](http://arxiv.org/abs/2209.06388)

Yanyun Wang, Dehui Du, Yuanhao Liu


[PINCH: An Adversarial Extraction Attack Framework for Deep Learning Models. (92%)](http://arxiv.org/abs/2209.06300)

William Hackett, Stefan Trawicki, Zhengxin Yu, Neeraj Suri, Peter Garraghan


[Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation. (78%)](http://arxiv.org/abs/2209.05980)

Maksym Yatsura, Kaspar Sakmann, N. Grace Hua, Matthias Hein, Jan Hendrik Metzen


[Adversarial Inter-Group Link Injection Degrades the Fairness of Graph Neural Networks. (68%)](http://arxiv.org/abs/2209.05957)

Hussain Hussain, Meng Cao, Sandipan Sikdar, Denis Helic, Elisabeth Lex, Markus Strohmaier, Roman Kern


[ADMM based Distributed State Observer Design under Sparse Sensor Attacks. (22%)](http://arxiv.org/abs/2209.06292)

Vinaya Mary Prinse, Rachel Kalpana Kalaimani


[A Tale of HodgeRank and Spectral Method: Target Attack Against Rank Aggregation Is the Fixed Point of Adversarial Game. (15%)](http://arxiv.org/abs/2209.05742)

Ke Ma, Qianqian Xu, Jinshan Zeng, Guorong Li, Xiaochun Cao, Qingming Huang


[Defense against Privacy Leakage in Federated Learning. (12%)](http://arxiv.org/abs/2209.05724)

Jing Wu, Munawar Hayat, Mingyi Zhou, Mehrtash Harandi


[Federated Learning based on Defending Against Data Poisoning Attacks in IoT. (1%)](http://arxiv.org/abs/2209.06397)

Jiayin Li, Wenzhong Guo, Xingshuo Han, Jianping Cai, Ximeng Liu


## 2022-09-12

[Adaptive Perturbation Generation for Multiple Backdoors Detection. (95%)](http://arxiv.org/abs/2209.05244)

Yuhang Wang, Huafeng Shi, Rui Min, Ruijia Wu, Siyuan Liang, Yichao Wu, Ding Liang, Aishan Liu


[CARE: Certifiably Robust Learning with Reasoning via Variational Inference. (75%)](http://arxiv.org/abs/2209.05055)

Jiawei Zhang, Linyi Li, Ce Zhang, Bo Li


[Sample Complexity of an Adversarial Attack on UCB-based Best-arm Identification Policy. (69%)](http://arxiv.org/abs/2209.05692)

Varsha Pendyala


[Boosting Robustness Verification of Semantic Feature Neighborhoods. (54%)](http://arxiv.org/abs/2209.05446)

Anan Kabaha, Dana Drachsler-Cohen


[Semantic-Preserving Adversarial Code Comprehension. (1%)](http://arxiv.org/abs/2209.05130)

Yiyang Li, Hongqiu Wu, Hai Zhao


[Holistic Segmentation. (1%)](http://arxiv.org/abs/2209.05407)

Stefano Gasperini, Alvaro Marcos-Ramiro, Michael Schmidt, Nassir Navab, Benjamin Busam, Federico Tombari


[Class-Level Logit Perturbation. (1%)](http://arxiv.org/abs/2209.05668)

Mengyang Li, Fengguang Su, Ou Wu, Ji Zhang


## 2022-09-11

[Resisting Deep Learning Models Against Adversarial Attack Transferability via Feature Randomization. (99%)](http://arxiv.org/abs/2209.04930)

Ehsan Nowroozi, Mohammadreza Mohammadi, Pargol Golmohammadi, Yassine Mekdad, Mauro Conti, Selcuk Uluagac


[Generate novel and robust samples from data: accessible sharing without privacy concerns. (5%)](http://arxiv.org/abs/2209.06113)

David Banh, Alan Huang


## 2022-09-10

[Scattering Model Guided Adversarial Examples for SAR Target Recognition: Attack and Defense. (99%)](http://arxiv.org/abs/2209.04779)

Bowen Peng, Bo Peng, Jie Zhou, Jianyue Xie, Li Liu


## 2022-09-09

[The Space of Adversarial Strategies. (99%)](http://arxiv.org/abs/2209.04521)

Ryan Sheatsley, Blaine Hoak, Eric Pauley, Patrick McDaniel


[Defend Data Poisoning Attacks on Voice Authentication. (54%)](http://arxiv.org/abs/2209.04547)

Ke Li, Cameron Baird, Dan Lin


[Robust-by-Design Classification via Unitary-Gradient Neural Networks. (41%)](http://arxiv.org/abs/2209.04293)

Fabio Brau, Giulio Rossolini, Alessandro Biondi, Giorgio Buttazzo


[Robust and Lossless Fingerprinting of Deep Neural Networks via Pooled Membership Inference. (10%)](http://arxiv.org/abs/2209.04113)

Hanzhou Wu


[Saliency Guided Adversarial Training for Learning Generalizable Features with Applications to Medical Imaging Classification System. (1%)](http://arxiv.org/abs/2209.04326)

Xin Li, Yao Qiang, Chengyin Li, Sijia Liu, Dongxiao Zhu


## 2022-09-08

[Incorporating Locality of Images to Generate Targeted Transferable Adversarial Examples. (99%)](http://arxiv.org/abs/2209.03716)

Zhipeng Wei, Jingjing Chen, Zuxuan Wu, Yu-Gang Jiang


[Evaluating the Security of Aircraft Systems. (92%)](http://arxiv.org/abs/2209.04028)

Edan Habler, Ron Bitton, Asaf Shabtai


[Uncovering the Connection Between Differential Privacy and Certified Robustness of Federated Learning against Poisoning Attacks. (62%)](http://arxiv.org/abs/2209.04030)

Chulin Xie, Yunhui Long, Pin-Yu Chen, Bo Li


[A Survey of Recent Advances in Deep Learning Models for Detecting Malware in Desktop and Mobile Platforms. (1%)](http://arxiv.org/abs/2209.03622)

Pascal Maniriho, Abdun Naser Mahmood, Mohammad Jabed Morshed Chowdhury


[FADE: Enabling Large-Scale Federated Adversarial Training on Resource-Constrained Edge Devices. (1%)](http://arxiv.org/abs/2209.03839)

Minxue Tang, Jianyi Zhang, Mingyuan Ma, Louis DiValentin, Aolin Ding, Amin Hassanzadeh, Hai Li, Yiran Chen


## 2022-09-07

[On the Transferability of Adversarial Examples between Encrypted Models. (99%)](http://arxiv.org/abs/2209.02997)

Miki Tanaka, Isao Echizen, Hitoshi Kiya


[Securing the Spike: On the Transferabilty and Security of Spiking Neural Networks to Adversarial Examples. (99%)](http://arxiv.org/abs/2209.03358)

Nuo Xu, Kaleel Mahmood, Haowen Fang, Ethan Rathbun, Caiwen Ding, Wujie Wen


[Reward Delay Attacks on Deep Reinforcement Learning. (70%)](http://arxiv.org/abs/2209.03540)

Anindya Sarkar, Jiarui Feng, Yevgeniy Vorobeychik, Christopher Gill, Ning Zhang


[Fact-Saboteurs: A Taxonomy of Evidence Manipulation Attacks against Fact-Verification Systems. (47%)](http://arxiv.org/abs/2209.03755)

Sahar Abdelnabi, Mario Fritz


[Why So Toxic? Measuring and Triggering Toxic Behavior in Open-Domain Chatbots. (15%)](http://arxiv.org/abs/2209.03463)

Wai Man Si, Michael Backes, Jeremy Blackburn, Cristofaro Emiliano De, Gianluca Stringhini, Savvas Zannettou, Yand Zhang


[Physics-Guided Adversarial Machine Learning for Aircraft Systems Simulation. (1%)](http://arxiv.org/abs/2209.03431)

Houssem Ben Braiek, Thomas Reid, Foutse Khomh


[Hardware faults that matter: Understanding and Estimating the safety impact of hardware faults on object detection DNNs. (1%)](http://arxiv.org/abs/2209.03225)

Syed Qutub, Florian Geissler, Yang Peng, Ralf Grafe, Michael Paulitsch, Gereon Hinz, Alois Knoll


[MalDetConv: Automated Behaviour-based Malware Detection Framework Based on Natural Language Processing and Deep Learning Techniques. (1%)](http://arxiv.org/abs/2209.03547)

Pascal Maniriho, Abdun Naser Mahmood, Mohammad Jabed Morshed Chowdhury


## 2022-09-06

[Instance Attack:An Explanation-based Vulnerability Analysis Framework Against DNNs for Malware Detection. (99%)](http://arxiv.org/abs/2209.02453)

Sun RuiJin, Guo ShiZe, Guo JinHong, Xing ChangYou, Yang LuMing, Guo Xi, Pan ZhiSong


[Bag of Tricks for FGSM Adversarial Training. (96%)](http://arxiv.org/abs/2209.02684)

Zichao Li, Li Liu, Zeyu Wang, Yuyin Zhou, Cihang Xie


[Improving Robustness to Out-of-Distribution Data by Frequency-based Augmentation. (82%)](http://arxiv.org/abs/2209.02369)

Koki Mukai, Soichiro Kumano, Toshihiko Yamasaki


[Defending Against Backdoor Attack on Graph Nerual Network by Explainability. (80%)](http://arxiv.org/abs/2209.02902)

Bingchen Jiang, Zhao Li


[MACAB: Model-Agnostic Clean-Annotation Backdoor to Object Detection with Natural Trigger in Real-World. (54%)](http://arxiv.org/abs/2209.02339)

Hua Ma, Yinshan Li, Yansong Gao, Zhi Zhang, Alsharif Abuadbba, Anmin Fu, Said F. Al-Sarawi, Nepal Surya, Derek Abbott


[Multimodal contrastive learning for remote sensing tasks. (1%)](http://arxiv.org/abs/2209.02329)

Umangi Jain, Alex Wilson, Varun Gulshan


[Annealing Optimization for Progressive Learning with Stochastic Approximation. (1%)](http://arxiv.org/abs/2209.02826)

Christos Mavridis, John Baras


[Interpretations Steered Network Pruning via Amortized Inferred Saliency Maps. (1%)](http://arxiv.org/abs/2209.02869)

Alireza Ganjdanesh, Shangqian Gao, Heng Huang


[A Survey of Machine Unlearning. (1%)](http://arxiv.org/abs/2209.02299)

Thanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, Quoc Viet Hung Nguyen


## 2022-09-05

[Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples. (98%)](http://arxiv.org/abs/2209.02128)

Hezekiah J. Branch, Jonathan Rodriguez Cefalu, Jeremy McHugh, Leyla Hujer, Aditya Bahl, Daniel del Castillo Iglesias, Ron Heichman, Ramesh Darwishi


[White-Box Adversarial Policies in Deep Reinforcement Learning. (98%)](http://arxiv.org/abs/2209.02167)

Stephen Casper, Taylor Killian, Gabriel Kreiman, Dylan Hadfield-Menell


["Is your explanation stable?": A Robustness Evaluation Framework for Feature Attribution. (69%)](http://arxiv.org/abs/2209.01782)

Yuyou Gan, Yuhao Mao, Xuhong Zhang, Shouling Ji, Yuwen Pu, Meng Han, Jianwei Yin, Ting Wang


[Adversarial Detection: Attacking Object Detection in Real Time. (64%)](http://arxiv.org/abs/2209.01962)

Han Wu, Syed Yunas, Sareh Rowlands, Wenjie Ruan, Johan Wahlstrom


[PromptAttack: Prompt-based Attack for Language Models via Gradient Search. (16%)](http://arxiv.org/abs/2209.01882)

Yundi Shi, Piji Li, Changchun Yin, Zhaoyang Han, Lu Zhou, Zhe Liu


[Federated Zero-Shot Learning for Visual Recognition. (2%)](http://arxiv.org/abs/2209.01994)

Zhi Chen, Yadan Luo, Sen Wang, Jingjing Li, Zi Huang


[Improving Out-of-Distribution Detection via Epistemic Uncertainty Adversarial Training. (2%)](http://arxiv.org/abs/2209.03148)

Derek Everett, Andre T. Nguyen, Luke E. Richards, Edward Raff


## 2022-09-04

[An Adaptive Black-box Defense against Trojan Attacks (TrojDef). (98%)](http://arxiv.org/abs/2209.01721)

Guanxiong Liu, Abdallah Khreishah, Fatima Sharadgah, Issa Khalil


[Hide & Seek: Seeking the (Un)-Hidden key in Provably-Secure Logic Locking Techniques. (11%)](http://arxiv.org/abs/2209.01711)

Satwik Patnaik, Nimisha Limaye, Ozgur Sinanoglu


[Synergistic Redundancy: Towards Verifiable Safety for Autonomous Vehicles. (1%)](http://arxiv.org/abs/2209.01710)

Ayoosh Bansal, Simon Yu, Hunmin Kim, Bo Li, Naira Hovakimyan, Marco Caccamo, Lui Sha


## 2022-09-02

[Adversarial Color Film: Effective Physical-World Attack to DNNs. (98%)](http://arxiv.org/abs/2209.02430)

Chengyin Hu, Weiwen Shi


[Impact of Scaled Image on Robustness of Deep Neural Networks. (98%)](http://arxiv.org/abs/2209.02132)

Chengyin Hu, Weiwen Shi


[Property inference attack; Graph neural networks; Privacy attacks and defense; Trustworthy machine learning. (95%)](http://arxiv.org/abs/2209.01100)

Xiuling Wang, Wendy Hui Wang


[Impact of Colour Variation on Robustness of Deep Neural Networks. (92%)](http://arxiv.org/abs/2209.02832)

Chengyin Hu, Weiwen Shi


[Scalable Adversarial Attack Algorithms on Influence Maximization. (68%)](http://arxiv.org/abs/2209.00892)

Lichao Sun, Xiaobin Rui, Wei Chen


[Are Attribute Inference Attacks Just Imputation? (31%)](http://arxiv.org/abs/2209.01292)

Bargav Jayaraman, David Evans


[Explainable AI for Android Malware Detection: Towards Understanding Why the Models Perform So Well? (9%)](http://arxiv.org/abs/2209.00812)

Yue Liu, Chakkrit Tantithamthavorn, Li Li, Yepang Liu


[Revisiting Outer Optimization in Adversarial Training. (5%)](http://arxiv.org/abs/2209.01199)

Ali Dabouei, Fariborz Taherkhani, Sobhan Soleymani, Nasser M. Nasrabadi


## 2022-09-01

[Adversarial for Social Privacy: A Poisoning Strategy to Degrade User Identity Linkage. (98%)](http://arxiv.org/abs/2209.00269)

Jiangli Shao, Yongqing Wang, Boshen Shi, Hao Gao, Huawei Shen, Xueqi Cheng


[Universal Fourier Attack for Time Series. (12%)](http://arxiv.org/abs/2209.00757)

Elizabeth Coda, Brad Clymer, Chance DeSmet, Yijing Watkins, Michael Girard


## 2022-08-31

[Be Your Own Neighborhood: Detecting Adversarial Example by the Neighborhood Relations Built on Self-Supervised Learning. (99%)](http://arxiv.org/abs/2209.00005)

Zhiyuan He, Yijun Yang, Pin-Yu Chen, Qiang Xu, Tsung-Yi Ho


[Unrestricted Adversarial Samples Based on Non-semantic Feature Clusters Substitution. (99%)](http://arxiv.org/abs/2209.02406)

MingWei Zhou, Xiaobing Pei


[Membership Inference Attacks by Exploiting Loss Trajectory. (70%)](http://arxiv.org/abs/2208.14933)

Yiyong Liu, Zhengyu Zhao, Michael Backes, Yang Zhang


[Explainable Artificial Intelligence Applications in Cyber Security: State-of-the-Art in Research. (13%)](http://arxiv.org/abs/2208.14937)

Zhibo Zhang, Hussam Al Hamadi, Ernesto Damiani, Chan Yeob Yeun, Fatma Taher


[Feature Alignment by Uncertainty and Self-Training for Source-Free Unsupervised Domain Adaptation. (1%)](http://arxiv.org/abs/2208.14888)

JoonHo Lee, Gyemin Lee


[Vulnerability of Distributed Inverter VAR Control in PV Distributed Energy System. (1%)](http://arxiv.org/abs/2208.14672)

Bo Tu, Wen-Tai Li, Chau Yuen


## 2022-08-30

[A Black-Box Attack on Optical Character Recognition Systems. (99%)](http://arxiv.org/abs/2208.14302)

Samet Bayram, Kenneth Barner


[Robustness and invariance properties of image classifiers. (99%)](http://arxiv.org/abs/2209.02408)

Apostolos Modas


[Solving the Capsulation Attack against Backdoor-based Deep Neural Network Watermarks by Reversing Triggers. (1%)](http://arxiv.org/abs/2208.14127)

Fangqi Li, Shilin Wang, Yun Zhu


[Constraining Representations Yields Models That Know What They Don't Know. (1%)](http://arxiv.org/abs/2208.14488)

Joao Monteiro, Pau Rodriguez, Pierre-Andre Noel, Issam Laradji, David Vazquez


## 2022-08-29

[Towards Adversarial Purification using Denoising AutoEncoders. (99%)](http://arxiv.org/abs/2208.13838)

Dvij Kalaria, Aritra Hazra, Partha Pratim Chakrabarti


[Reducing Certified Regression to Certified Classification for General Poisoning Attacks. (54%)](http://arxiv.org/abs/2208.13904)

Zayd Hammoudeh, Daniel Lowd


[Interpreting Black-box Machine Learning Models for High Dimensional Datasets. (1%)](http://arxiv.org/abs/2208.13405)

Md. Rezaul Karim, Md. Shajalal, Alex Graß, Till Döhmen, Sisay Adugna Chala, Christian Beecks, Stefan Decker


## 2022-08-28

[Cross-domain Cross-architecture Black-box Attacks on Fine-tuned Models with Transferred Evolutionary Strategies. (99%)](http://arxiv.org/abs/2208.13182)

Yinghua Zhang, Yangqiu Song, Kun Bai, Qiang Yang


## 2022-08-27

[Adversarial Robustness for Tabular Data through Cost and Utility Awareness. (99%)](http://arxiv.org/abs/2208.13058)

Klim Kireev, Bogdan Kulynych, Carmela Troncoso


[SA: Sliding attack for synthetic speech detection with resistance to clipping and self-splicing. (99%)](http://arxiv.org/abs/2208.13066)

Deng JiaCheng, Dong Li, Yan Diqun, Wang Rangding, Zeng Jiaming


[Overparameterized (robust) models from computational constraints. (13%)](http://arxiv.org/abs/2208.12926)

Sanjam Garg, Somesh Jha, Saeed Mahloujifar, Mohammad Mahmoody, Mingyuan Wang


[TrojViT: Trojan Insertion in Vision Transformers. (11%)](http://arxiv.org/abs/2208.13049)

Mengxin Zheng, Qian Lou, Lei Jiang


[RL-DistPrivacy: Privacy-Aware Distributed Deep Inference for low latency IoT systems. (1%)](http://arxiv.org/abs/2208.13032)

Emna Baccour, Aiman Erbad, Amr Mohamed, Mounir Hamdi, Mohsen Guizani


## 2022-08-26

[What Does the Gradient Tell When Attacking the Graph Structure. (69%)](http://arxiv.org/abs/2208.12815)

Zihan Liu, Ge Wang, Yun Luo, Stan Z. Li


[Network-Level Adversaries in Federated Learning. (54%)](http://arxiv.org/abs/2208.12911)

Giorgio Severi, Matthew Jagielski, Gökberk Yar, Yuxuan Wang, Alina Oprea, Cristina Nita-Rotaru


[ATTRITION: Attacking Static Hardware Trojan Detection Techniques Using Reinforcement Learning. (45%)](http://arxiv.org/abs/2208.12897)

Vasudev JV Gohil, Hao JV Guo, Satwik JV Patnaik, JV Jeyavijayan, Rajendran


[Lower Difficulty and Better Robustness: A Bregman Divergence Perspective for Adversarial Training. (4%)](http://arxiv.org/abs/2208.12511)

Zihui Wu, Haichang Gao, Bingqian Zhou, Xiaoyan Guo, Shudong Zhang


## 2022-08-25

[Semantic Preserving Adversarial Attack Generation with Autoencoder and Genetic Algorithm. (99%)](http://arxiv.org/abs/2208.12230)

Xinyi Wang, Simon Yusuf Enoch, Dong Seong Kim


[SNAP: Efficient Extraction of Private Properties with Poisoning. (89%)](http://arxiv.org/abs/2208.12348)

Harsh Chaudhari, John Abascal, Alina Oprea, Matthew Jagielski, Florian Tramèr, Jonathan Ullman


[FuncFooler: A Practical Black-box Attack Against Learning-based Binary Code Similarity Detection Methods. (78%)](http://arxiv.org/abs/2208.14191)

Lichen Jia, Bowen Tang, Chenggang Wu, Zhe Wang, Zihan Jiang, Yuanming Lai, Yan Kang, Ning Liu, Jingfeng Zhang


[Robust Prototypical Few-Shot Organ Segmentation with Regularized Neural-ODEs. (31%)](http://arxiv.org/abs/2208.12428)

Prashant Pandey, Mustafa Chasmai, Tanuj Sur, Brejesh Lall


[Calibrated Selective Classification. (15%)](http://arxiv.org/abs/2208.12084)

Adam Fisch, Tommi Jaakkola, Regina Barzilay


[XDRI Attacks - and - How to Enhance Resilience of Residential Routers. (4%)](http://arxiv.org/abs/2208.12003)

Philipp Jeitner, Haya Shulman, Lucas Teichmann, Michael Waidner


[FedPrompt: Communication-Efficient and Privacy Preserving Prompt Tuning in Federated Learning. (1%)](http://arxiv.org/abs/2208.12268)

Haodong Zhao, Wei Du, Fangqi Li, Peixuan Li, Gongshen Liu


## 2022-08-24

[Attacking Neural Binary Function Detection. (99%)](http://arxiv.org/abs/2208.11667)

Joshua Bundt, Michael Davinroy, Ioannis Agadakos, Alina Oprea, William Robertson


[Unrestricted Black-box Adversarial Attack Using GAN with Limited Queries. (99%)](http://arxiv.org/abs/2208.11613)

Dongbin Na, Sangwoo Ji, Jong Kim


[Trace and Detect Adversarial Attacks on CNNs using Feature Response Maps. (98%)](http://arxiv.org/abs/2208.11436)

Mohammadreza Amirian, Friedhelm Schwenker, Thilo Stadelmann


[A Perturbation Resistant Transformation and Classification System for Deep Neural Networks. (98%)](http://arxiv.org/abs/2208.11839)

Nathaniel Dean, Dilip Sarkar


[Rethinking Cost-sensitive Classification in Deep Learning via Adversarial Data Augmentation. (92%)](http://arxiv.org/abs/2208.11739)

Qiyuan Chen, Raed Al Kontar, Maher Nouiehed, Jessie Yang, Corey Lester


[Bidirectional Contrastive Split Learning for Visual Question Answering. (38%)](http://arxiv.org/abs/2208.11435)

Yuwei Sun, Hideya Ochiai


## 2022-08-23

[Towards an Awareness of Time Series Anomaly Detection Models' Adversarial Vulnerability. (99%)](http://arxiv.org/abs/2208.11264)

Shahroz Tariq, Binh M. Le, Simon S. Woo


[Adversarial Vulnerability of Temporal Feature Networks for Object Detection. (99%)](http://arxiv.org/abs/2208.10773)

Svetlana Pavlitskaya, Nikolai Polley, Michael Weber, J. Marius Zöllner


[Transferability Ranking of Adversarial Examples. (99%)](http://arxiv.org/abs/2208.10878)

Mosh Levy, Yuval Elovici, Yisroel Mirsky


[Auditing Membership Leakages of Multi-Exit Networks. (76%)](http://arxiv.org/abs/2208.11180)

Zheng Li, Yiyong Liu, Xinlei He, Ning Yu, Michael Backes, Yang Zhang


[A Comprehensive Study of Real-Time Object Detection Networks Across Multiple Domains: A Survey. (13%)](http://arxiv.org/abs/2208.10895)

Elahe Arani, Shruthi Gowda, Ratnajit Mukherjee, Omar Magdy, Senthilkumar Kathiresan, Bahram Zonooz


[Robust DNN Watermarking via Fixed Embedding Weights with Optimized Distribution. (10%)](http://arxiv.org/abs/2208.10973)

Benedetta Tondi, Andrea Costanzo, Mauro Barni


## 2022-08-22

[Fight Fire With Fire: Reversing Skin Adversarial Examples by Multiscale Diffusive and Denoising Aggregation Mechanism. (99%)](http://arxiv.org/abs/2208.10373)

Yongwei Wang, Yuan Li, Zhiqi Shen


[Hierarchical Perceptual Noise Injection for Social Media Fingerprint Privacy Protection. (98%)](http://arxiv.org/abs/2208.10688)

Simin Li, Huangxinxin Xu, Jiakai Wang, Aishan Liu, Fazhi He, Xianglong Liu, Dacheng Tao


[Different Spectral Representations in Optimized Artificial Neural Networks and Brains. (93%)](http://arxiv.org/abs/2208.10576)

Richard C. Gerum, Cassidy Pirlot, Alona Fyshe, Joel Zylberberg


[Membership-Doctor: Comprehensive Assessment of Membership Inference Against Machine Learning Models. (87%)](http://arxiv.org/abs/2208.10445)

Xinlei He, Zheng Li, Weilin Xu, Cory Cornelius, Yang Zhang


[BARReL: Bottleneck Attention for Adversarial Robustness in Vision-Based Reinforcement Learning. (86%)](http://arxiv.org/abs/2208.10481)

Eugene Bykovets, Yannick Metz, Mennatallah El-Assady, Daniel A. Keim, Joachim M. Buhmann


[RIBAC: Towards Robust and Imperceptible Backdoor Attack against Compact DNN. (62%)](http://arxiv.org/abs/2208.10608)

Huy Phan, Cong Shi, Yi Xie, Tianfang Zhang, Zhuohang Li, Tianming Zhao, Jian Liu, Yan Wang, Yingying Chen, Bo Yuan


[Toward Better Target Representation for Source-Free and Black-Box Domain Adaptation. (31%)](http://arxiv.org/abs/2208.10531)

Qucheng Peng, Zhengming Ding, Lingjuan Lyu, Lichao Sun, Chen Chen


[Optimal Bootstrapping of PoW Blockchains. (1%)](http://arxiv.org/abs/2208.10618)

Ranvir Rana, Dimitris Karakostas, Sreeram Kannan, Aggelos Kiayias, Pramod Viswanath


## 2022-08-21

[PointDP: Diffusion-driven Purification against Adversarial Attacks on 3D Point Cloud Recognition. (99%)](http://arxiv.org/abs/2208.09801)

Jiachen Sun, Weili Nie, Zhiding Yu, Z. Morley Mao, Chaowei Xiao


[Inferring Sensitive Attributes from Model Explanations. (56%)](http://arxiv.org/abs/2208.09967)

Vasisht Duddu, Antoine Boutet


[Byzantines can also Learn from History: Fall of Centered Clipping in Federated Learning. (10%)](http://arxiv.org/abs/2208.09894)

Kerem Ozfatura, Emre Ozfatura, Alptekin Kupcu, Deniz Gunduz


[MockingBERT: A Method for Retroactively Adding Resilience to NLP Models. (4%)](http://arxiv.org/abs/2208.09915)

Jan Jezabek, Akash Singh


[NOSMOG: Learning Noise-robust and Structure-aware MLPs on Graphs. (1%)](http://arxiv.org/abs/2208.10010)

Yijun Tian, Chuxu Zhang, Zhichun Guo, Xiangliang Zhang, Nitesh V. Chawla


[A Unified Analysis of Mixed Sample Data Augmentation: A Loss Function Perspective. (1%)](http://arxiv.org/abs/2208.09913)

Chanwoo Park, Sangdoo Yun, Sanghyuk Chun


## 2022-08-20

[Analyzing Adversarial Robustness of Vision Transformers against Spatial and Spectral Attacks. (86%)](http://arxiv.org/abs/2208.09602)

Gihyun Kim, Jong-Seok Lee


[GAIROSCOPE: Injecting Data from Air-Gapped Computers to Nearby Gyroscopes. (33%)](http://arxiv.org/abs/2208.09764)

Mordechai Guri


[Sensor Security: Current Progress, Research Challenges, and Future Roadmap. (10%)](http://arxiv.org/abs/2208.09741)

Anomadarshi Barua, Mohammad Abdullah Al Faruque


[Evaluating Out-of-Distribution Detectors Through Adversarial Generation of Outliers. (5%)](http://arxiv.org/abs/2208.10940)

Sangwoong Yoon, Jinwon Choi, Yonghyeon Lee, Yung-Kyun Noh, Frank Chongwoo Park


[Adversarial contamination of networks in the setting of vertex nomination: a new trimming method. (1%)](http://arxiv.org/abs/2208.09710)

Sheyda Peyman, Minh Tang, Vince Lyzinski


## 2022-08-19

[Real-Time Robust Video Object Detection System Against Physical-World Adversarial Attacks. (99%)](http://arxiv.org/abs/2208.09195)

Husheng Han, Xing Hu, Kaidi Xu, Pucheng Dang, Ying Wang, Yongwei Zhao, Zidong Du, Qi Guo, Yanzhi Yang, Tianshi Chen


[Gender Bias and Universal Substitution Adversarial Attacks on Grammatical Error Correction Systems for Automated Assessment. (92%)](http://arxiv.org/abs/2208.09466)

Vyas Raina, Mark Gales


[Dispersed Pixel Perturbation-based Imperceptible Backdoor Trigger for Image Classifier Models. (76%)](http://arxiv.org/abs/2208.09336)

Yulong Wang, Minghui Zhao, Shenghong Li, Xin Yuan, Wei Ni


[A Novel Plug-and-Play Approach for Adversarially Robust Generalization. (54%)](http://arxiv.org/abs/2208.09449)

Deepak Maurya, Adarsh Barik, Jean Honorio


[SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability. (8%)](http://arxiv.org/abs/2208.09418)

Wei Huang, Xingyu Zhao, Gaojie Jin, Xiaowei Huang


[UKP-SQuARE v2 Explainability and Adversarial Attacks for Trustworthy QA. (1%)](http://arxiv.org/abs/2208.09316)

Rachneet Sachdeva, Haritz Puerto, Tim Baumgärtner, Sewin Tariverdian, Hao Zhang, Kexin Wang, Hossain Shaikh Saadi, Leonardo F. R. Ribeiro, Iryna Gurevych


## 2022-08-18

[Resisting Adversarial Attacks in Deep Neural Networks using Diverse Decision Boundaries. (99%)](http://arxiv.org/abs/2208.08697)

Manaar Alam, Shubhajit Datta, Debdeep Mukhopadhyay, Arijit Mondal, Partha Pratim Chakrabarti


[Enhancing Targeted Attack Transferability via Diversified Weight Pruning. (99%)](http://arxiv.org/abs/2208.08677)

Hung-Jui Wang, Yu-Yu Wu, Shang-Tse Chen


[Enhancing Diffusion-Based Image Synthesis with Robust Classifier Guidance. (45%)](http://arxiv.org/abs/2208.08664)

Bahjat Kawar, Roy Ganz, Michael Elad


[Reverse Engineering of Integrated Circuits: Tools and Techniques. (33%)](http://arxiv.org/abs/2208.08689)

Abhijitt Dhavlle


[DAFT: Distilling Adversarially Fine-tuned Models for Better OOD Generalization. (10%)](http://arxiv.org/abs/2208.09139)

Anshul Nasery, Sravanti Addepalli, Praneeth Netrapalli, Prateek Jain


[Discovering Bugs in Vision Models using Off-the-shelf Image Generation and Captioning. (3%)](http://arxiv.org/abs/2208.08831)

Olivia Wiles, Isabela Albuquerque, Sven Gowal


[Private, Efficient, and Accurate: Protecting Models Trained by Multi-party Learning with Differential Privacy. (2%)](http://arxiv.org/abs/2208.08662)

Wenqiang Ruan, Mingxin Xu, Wenjing Fang, Li Wang, Lei Wang, Weili Han


[Profiler: Profile-Based Model to Detect Phishing Emails. (1%)](http://arxiv.org/abs/2208.08745)

Mariya Shmalko, Alsharif Abuadbba, Raj Gaire, Tingmin Wu, Hye-Young Paik, Surya Nepal


## 2022-08-17

[Two Heads are Better than One: Robust Learning Meets Multi-branch Models. (99%)](http://arxiv.org/abs/2208.08083)

Dong Huang, Qingwen Bu, Yuhao Qing, Haowen Pi, Sen Wang, Heming Cui


[An Evolutionary, Gradient-Free, Query-Efficient, Black-Box Algorithm for Generating Adversarial Instances in Deep Networks. (99%)](http://arxiv.org/abs/2208.08297)

Raz Lapid, Zvika Haramaty, Moshe Sipper


[Shadows Aren't So Dangerous After All: A Fast and Robust Defense Against Shadow-Based Adversarial Attacks. (98%)](http://arxiv.org/abs/2208.09285)

Andrew Wang, Wyatt Mayor, Ryan Smith, Gopal Nookula, Gregory Ditzler


[Label Flipping Data Poisoning Attack Against Wearable Human Activity Recognition System. (70%)](http://arxiv.org/abs/2208.08433)

Abdur R. Shahid, Ahmed Imteaj, Peter Y. Wu, Diane A. Igoche, Tauhidul Alam


[An Efficient Multi-Step Framework for Malware Packing Identification. (41%)](http://arxiv.org/abs/2208.08071)

Jong-Wouk Kim, Yang-Sae Moon, Mi-Jung Choi


[An Empirical Study on the Membership Inference Attack against Tabular Data Synthesis Models. (26%)](http://arxiv.org/abs/2208.08114)

Jihyeon Hyeong, Jayoung Kim, Noseong Park, Sushil Jajodia


[Efficient Detection and Filtering Systems for Distributed Training. (26%)](http://arxiv.org/abs/2208.08085)

Konstantinos Konstantinidis, Aditya Ramamoorthy


[On the Privacy Effect of Data Enhancement via the Lens of Memorization. (10%)](http://arxiv.org/abs/2208.08270)

Xiao Li, Qiongxiu Li, Zhanhao Hu, Xiaolin Hu


[ObfuNAS: A Neural Architecture Search-based DNN Obfuscation Approach. (2%)](http://arxiv.org/abs/2208.08569)

Tong Zhou, Shaolei Ren, Xiaolin Xu


[DF-Captcha: A Deepfake Captcha for Preventing Fake Calls. (1%)](http://arxiv.org/abs/2208.08524)

Yisroel Mirsky


[Analyzing Robustness of End-to-End Neural Models for Automatic Speech Recognition. (1%)](http://arxiv.org/abs/2208.08509)

Goutham Rajendran, Wei Zou


## 2022-08-16

[A Context-Aware Approach for Textual Adversarial Attack through Probability Difference Guided Beam Search. (82%)](http://arxiv.org/abs/2208.08029)

Huijun Liu, Jie Yu, Shasha Li, Jun Ma, Bin Ji


[Imperceptible and Robust Backdoor Attack in 3D Point Cloud. (68%)](http://arxiv.org/abs/2208.08052)

Kuofeng Gao, Jiawang Bai, Baoyuan Wu, Mengxi Ya, Shu-Tao Xia


[AutoCAT: Reinforcement Learning for Automated Exploration of Cache-Timing Attacks. (13%)](http://arxiv.org/abs/2208.08025)

Mulong Luo, Wenjie Xiong, Geunbae Lee, Yueying Li, Xiaomeng Yang, Amy Zhang, Yuandong Tian, Hsien-Hsin S. Lee, G. Edward Suh


[Investigating the Impact of Model Width and Density on Generalization in Presence of Label Noise. (1%)](http://arxiv.org/abs/2208.08003)

Yihao Xue, Kyle Whitecross, Baharan Mirzasoleiman


## 2022-08-15

[Man-in-the-Middle Attack against Object Detection Systems. (96%)](http://arxiv.org/abs/2208.07174)

Han Wu, Sareh Rowlands, Johan Wahlstrom


[MENLI: Robust Evaluation Metrics from Natural Language Inference. (92%)](http://arxiv.org/abs/2208.07316)

Yanran Chen, Steffen Eger


[Training-Time Attacks against k-Nearest Neighbors. (2%)](http://arxiv.org/abs/2208.07272)

Ara Vartanian, Will Rosenbaum, Scott Alfeld


[CTI4AI: Threat Intelligence Generation and Sharing after Red Teaming AI Models. (1%)](http://arxiv.org/abs/2208.07476)

Chuyen Nguyen, Caleb Morgan, Sudip Mittal


## 2022-08-14

[A Multi-objective Memetic Algorithm for Auto Adversarial Attack Optimization Design. (99%)](http://arxiv.org/abs/2208.06984)

Jialiang Sun, Wen Yao, Tingsong Jiang, Xiaoqian Chen


[Link-Backdoor: Backdoor Attack on Link Prediction via Node Injection. (92%)](http://arxiv.org/abs/2208.06776)

Haibin Zheng, Haiyang Xiong, Haonan Ma, Guohan Huang, Jinyin Chen


[InvisibiliTee: Angle-agnostic Cloaking from Person-Tracking Systems with a Tee. (92%)](http://arxiv.org/abs/2208.06962)

Yaxian Li, Bingqing Zhang, Guoping Zhao, Mingyu Zhang, Jiajun Liu, Ziwei Wang, Jirong Wen


[Long-Short History of Gradients is All You Need: Detecting Malicious and Unreliable Clients in Federated Learning. (67%)](http://arxiv.org/abs/2208.10273)

Ashish Gupta, Tie Luo, Mao V. Ngo, Sajal K. Das


## 2022-08-13

[Friendly Noise against Adversarial Noise: A Powerful Defense against Data Poisoning Attacks. (99%)](http://arxiv.org/abs/2208.10224)

Tian Yu Liu, Yu Yang, Baharan Mirzasoleiman


[Revisiting Adversarial Attacks on Graph Neural Networks for Graph Classification. (95%)](http://arxiv.org/abs/2208.06651)

Beini Xie, Heng Chang, Xin Wang, Tian Bian, Shiji Zhou, Daixin Wang, Zhiqiang Zhang, Wenwu Zhu


[Confidence Matters: Inspecting Backdoors in Deep Neural Networks via Distribution Transfer. (62%)](http://arxiv.org/abs/2208.06592)

Tong Wang, Yuan Yao, Feng Xu, Miao Xu, Shengwei An, Ting Wang


## 2022-08-12

[Scale-free and Task-agnostic Attack: Generating Photo-realistic Adversarial Patterns with Patch Quilting Generator. (99%)](http://arxiv.org/abs/2208.06222)

Xiangbo Gao, Cheng Luo, Qinliang Lin, Weicheng Xie, Minmin Liu, Linlin Shen, Keerthy Kusumam, Siyang Song


[MaskBlock: Transferable Adversarial Examples with Bayes Approach. (99%)](http://arxiv.org/abs/2208.06538)

Mingyuan Fan, Cen Chen, Ximeng Liu, Wenzhong Guo


[Defensive Distillation based Adversarial Attacks Mitigation Method for Channel Estimation using Deep Learning Models in Next-Generation Wireless Networks. (98%)](http://arxiv.org/abs/2208.10279)

Ferhat Ozgur Catak, Murat Kuzlu, Evren Catak, Umit Cali, Ozgur Guler


[Unifying Gradients to Improve Real-world Robustness for Deep Networks. (96%)](http://arxiv.org/abs/2208.06228)

Yingwen Wu, Sizhe Chen, Kun Fang, Xiaolin Huang


[A Knowledge Distillation-Based Backdoor Attack in Federated Learning. (93%)](http://arxiv.org/abs/2208.06176)

Yifan Wang, Wei Fan, Keke Yang, Naji Alhusaini, Jing Li


[Dropout is NOT All You Need to Prevent Gradient Leakage. (62%)](http://arxiv.org/abs/2208.06163)

Daniel Scheliga, Patrick Mäder, Marco Seeland


[Defense against Backdoor Attacks via Identifying and Purifying Bad Neurons. (2%)](http://arxiv.org/abs/2208.06537)

Mingyuan Fan, Yang Liu, Cen Chen, Ximeng Liu, Wenzhong Guo


[PRIVEE: A Visual Analytic Workflow for Proactive Privacy Risk Inspection of Open Data. (2%)](http://arxiv.org/abs/2208.06481)

Kaustav Bhattacharjee, Akm Islam, Jaideep Vaidya, Aritra Dasgupta


## 2022-08-11

[Diverse Generative Perturbations on Attention Space for Transferable Adversarial Attacks. (99%)](http://arxiv.org/abs/2208.05650)

Woo Jae Kim, Seunghoon Hong, Sung-Eui Yoon


[General Cutting Planes for Bound-Propagation-Based Neural Network Verification. (68%)](http://arxiv.org/abs/2208.05740)

Huan Zhang, Shiqi Wang, Kaidi Xu, Linyi Li, Bo Li, Suman Jana, Cho-Jui Hsieh, J. Zico Kolter


[On deceiving malware classification with section injection. (5%)](http://arxiv.org/abs/2208.06092)

Silva Adeilson Antonio da, Mauricio Pamplona Segundo


[A Probabilistic Framework for Mutation Testing in Deep Neural Networks. (1%)](http://arxiv.org/abs/2208.06018)

Florian Tambon, Foutse Khomh, Giuliano Antoniol


[Safety and Performance, Why not Both? Bi-Objective Optimized Model Compression toward AI Software Deployment. (1%)](http://arxiv.org/abs/2208.05969)

Jie Zhu, Leye Wang, Xiao Han


[Shielding Federated Learning Systems against Inference Attacks with ARM TrustZone. (1%)](http://arxiv.org/abs/2208.05895)

Aghiles Ait Messaoud, Sonia Ben Mokhtar, Vlad Nitu, Valerio Schiavoni


## 2022-08-10

[Explaining Machine Learning DGA Detectors from DNS Traffic Data. (13%)](http://arxiv.org/abs/2208.05285)

Giorgio Piras, Maura Pintor, Luca Demetrio, Battista Biggio


[A Sublinear Adversarial Training Algorithm. (3%)](http://arxiv.org/abs/2208.05395)

Yeqi Gao, Lianke Qin, Zhao Song, Yitan Wang


[DVR: Micro-Video Recommendation Optimizing Watch-Time-Gain under Duration Bias. (1%)](http://arxiv.org/abs/2208.05190)

Yu Zheng, Chen Gao, Jingtao Ding, Lingling Yi, Depeng Jin, Yong Li, Meng Wang


## 2022-08-09

[Adversarial Machine Learning-Based Anticipation of Threats Against Vehicle-to-Microgrid Services. (98%)](http://arxiv.org/abs/2208.05073)

Ahmed Omara, Burak Kantarci


[Reducing Exploitability with Population Based Training. (67%)](http://arxiv.org/abs/2208.05083)

Pavel Czempin, Adam Gleave


[Robust Machine Learning for Malware Detection over Time. (9%)](http://arxiv.org/abs/2208.04838)

Daniele Angioni, Luca Demetrio, Maura Pintor, Battista Biggio


## 2022-08-08

[Robust and Imperceptible Black-box DNN Watermarking Based on Fourier Perturbation Analysis and Frequency Sensitivity Clustering. (75%)](http://arxiv.org/abs/2208.03944)

Yong Liu, Hanzhou Wu, Xinpeng Zhang


[PerD: Perturbation Sensitivity-based Neural Trojan Detection Framework on NLP Applications. (67%)](http://arxiv.org/abs/2208.04943)

Diego Garcia-soto, Huili Chen, Farinaz Koushanfar


[Adversarial robustness of VAEs through the lens of local geometry. (47%)](http://arxiv.org/abs/2208.03923)

Asif Khan, Amos Storkey


[AWEncoder: Adversarial Watermarking Pre-trained Encoders in Contrastive Learning. (26%)](http://arxiv.org/abs/2208.03948)

Tianxing Zhang, Hanzhou Wu, Xiaofeng Lu, Guangling Sun


[Abutting Grating Illusion: Cognitive Challenge to Neural Network Models. (1%)](http://arxiv.org/abs/2208.03958)

Jinyu Fan, Yi Zeng


[Testing of Machine Learning Models with Limited Samples: An Industrial Vacuum Pumping Application. (1%)](http://arxiv.org/abs/2208.04062)

Ayan Chatterjee, Bestoun S. Ahmed, Erik Hallin, Anton Engman


## 2022-08-07

[Federated Adversarial Learning: A Framework with Convergence Analysis. (80%)](http://arxiv.org/abs/2208.03635)

Xiaoxiao Li, Zhao Song, Jiaming Yang


[Are Gradients on Graph Structure Reliable in Gray-box Attacks? (13%)](http://arxiv.org/abs/2208.05514)

Zihan Liu, Yun Luo, Lirong Wu, Siyuan Li, Zicheng Liu, Stan Z. Li


## 2022-08-06

[Blackbox Attacks via Surrogate Ensemble Search. (99%)](http://arxiv.org/abs/2208.03610)

Zikui Cai, Chengyu Song, Srikanth Krishnamurthy, Amit Roy-Chowdhury, M. Salman Asif


[On the Fundamental Limits of Formally (Dis)Proving Robustness in Proof-of-Learning. (22%)](http://arxiv.org/abs/2208.03567)

Congyu Fang, Hengrui Jia, Anvith Thudi, Mohammad Yaghini, Christopher A. Choquette-Choo, Natalie Dullerud, Varun Chandrasekaran, Nicolas Papernot


[Preventing or Mitigating Adversarial Supply Chain Attacks; a legal analysis. (3%)](http://arxiv.org/abs/2208.03466)

Kaspar Rosager Ludvigsen, Shishir Nagaraja, Angela Daly


## 2022-08-05

[Adversarial Robustness of MR Image Reconstruction under Realistic Perturbations. (73%)](http://arxiv.org/abs/2208.03161)

Jan Nikolas Morshuis, Sergios Gatidis, Matthias Hein, Christian F. Baumgartner


[Data-free Backdoor Removal based on Channel Lipschitzness. (64%)](http://arxiv.org/abs/2208.03111)

Runkai Zheng, Rongjun Tang, Jianze Li, Li Liu


[Lethal Dose Conjecture on Data Poisoning. (2%)](http://arxiv.org/abs/2208.03309)

Wenxiao Wang, Alexander Levine, Soheil Feizi


[LCCDE: A Decision-Based Ensemble Framework for Intrusion Detection in The Internet of Vehicles. (1%)](http://arxiv.org/abs/2208.03399)

Li Yang, Abdallah Shami, Gary Stevens, Rusett Stephen De


[Almost-Orthogonal Layers for Efficient General-Purpose Lipschitz Networks. (1%)](http://arxiv.org/abs/2208.03160)

Bernd Prach, Christoph H. Lampert


## 2022-08-04

[Self-Ensembling Vision Transformer (SEViT) for Robust Medical Image Classification. (99%)](http://arxiv.org/abs/2208.02851)

Faris Almalik, Mohammad Yaqub, Karthik Nandakumar


## 2022-08-03

[Spectrum Focused Frequency Adversarial Attacks for Automatic Modulation Classification. (99%)](http://arxiv.org/abs/2208.01919)

Sicheng College of Information and Communication Engineering, Harbin Engineering University, Harbin Zhang, Jiarun College of Information and Communication Engineering, Harbin Engineering University, Harbin Yu, Zhida College of Information and Communication Engineering, Harbin Engineering University, Harbin Bao, Shiwen Department of Electrical & Computer Engineering, Auburn University, Auburn Mao, Yun College of Information and Communication Engineering, Harbin Engineering University, Harbin Lin


[Design of secure and robust cognitive system for malware detection. (99%)](http://arxiv.org/abs/2208.02310)

Sanket Shukla


[A New Kind of Adversarial Example. (99%)](http://arxiv.org/abs/2208.02430)

Ali Borji


[Adversarial Attacks on ASR Systems: An Overview. (98%)](http://arxiv.org/abs/2208.02250)

Xiao Zhang, Hao Tan, Xuan Huang, Denghui Zhang, Keke Tang, Zhaoquan Gu


[Multiclass ASMA vs Targeted PGD Attack in Image Segmentation. (96%)](http://arxiv.org/abs/2208.01844)

Johnson University of Toronto Vo, Jiabao University of Toronto Xie, Sahil University of Toronto Patel


[MOVE: Effective and Harmless Ownership Verification via Embedded External Features. (84%)](http://arxiv.org/abs/2208.02820)

Yiming Li, Linghui Zhu, Xiaojun Jia, Yang Bai, Yong Jiang, Shu-Tao Xia, Xiaochun Cao


[Robust Graph Neural Networks using Weighted Graph Laplacian. (13%)](http://arxiv.org/abs/2208.01853)

Bharat Runwal, Vivek, Sandeep Kumar


## 2022-08-02

[Adversarial Camouflage for Node Injection Attack on Graphs. (81%)](http://arxiv.org/abs/2208.01819)

Shuchang Tao, Qi Cao, Huawei Shen, Yunfan Wu, Liang Hou, Xueqi Cheng


[Success of Uncertainty-Aware Deep Models Depends on Data Manifold Geometry. (2%)](http://arxiv.org/abs/2208.01705)

Mark Penrod, Harrison Termotto, Varshini Reddy, Jiayu Yao, Finale Doshi-Velez, Weiwei Pan


[SCFI: State Machine Control-Flow Hardening Against Fault Attacks. (1%)](http://arxiv.org/abs/2208.01356)

Pascal Nasahl, Martin Unterguggenberger, Rishub Nagpal, Robert Schilling, David Schrammel, Stefan Mangard


## 2022-08-01

[GeoECG: Data Augmentation via Wasserstein Geodesic Perturbation for Robust Electrocardiogram Prediction. (98%)](http://arxiv.org/abs/2208.01220)

Jiacheng Zhu, Jielin Qiu, Zhuolin Yang, Douglas Weber, Michael A. Rosenberg, Emerson Liu, Bo Li, Ding Zhao


[Understanding Adversarial Robustness of Vision Transformers via Cauchy Problem. (81%)](http://arxiv.org/abs/2208.00906)

Zheng Wang, Wenjie Ruan


[On the Evaluation of User Privacy in Deep Neural Networks using Timing Side Channel. (75%)](http://arxiv.org/abs/2208.01113)

Shubhi Shukla, Manaar Alam, Sarani Bhattacharya, Debdeep Mukhopadhyay, Pabitra Mitra


[Attacking Adversarial Defences by Smoothing the Loss Landscape. (26%)](http://arxiv.org/abs/2208.00862)

Panagiotis Eustratiadis, Henry Gouk, Da Li, Timothy Hospedales


## 2022-07-31

[DNNShield: Dynamic Randomized Model Sparsification, A Defense Against Adversarial Machine Learning. (99%)](http://arxiv.org/abs/2208.00498)

Mohammad Hossein Samavatian, Saikat Majumdar, Kristin Barber, Radu Teodorescu


[Robust Real-World Image Super-Resolution against Adversarial Attacks. (99%)](http://arxiv.org/abs/2208.00428)

Jiutao Yue, Haofeng Li, Pengxu Wei, Guanbin Li, Liang Lin


[Is current research on adversarial robustness addressing the right problem? (97%)](http://arxiv.org/abs/2208.00539)

Ali Borji


## 2022-07-30

[enpheeph: A Fault Injection Framework for Spiking and Compressed Deep Neural Networks. (5%)](http://arxiv.org/abs/2208.00328)

Alessio Colucci, Andreas Steininger, Muhammad Shafique


[CoNLoCNN: Exploiting Correlation and Non-Uniform Quantization for Energy-Efficient Low-precision Deep Convolutional Neural Networks. (2%)](http://arxiv.org/abs/2208.00331)

Muhammad Abdullah Hanif, Giuseppe Maria Sarda, Alberto Marchisio, Guido Masera, Maurizio Martina, Muhammad Shafique


## 2022-07-29

[Robust Trajectory Prediction against Adversarial Attacks. (99%)](http://arxiv.org/abs/2208.00094)

Yulong Cao, Danfei Xu, Xinshuo Weng, Zhuoqing Mao, Anima Anandkumar, Chaowei Xiao, Marco Pavone


[Sampling Attacks on Meta Reinforcement Learning: A Minimax Formulation and Complexity Analysis. (56%)](http://arxiv.org/abs/2208.00081)

Tao Li, Haozhe Lei, Quanyan Zhu


## 2022-07-28

[Pro-tuning: Unified Prompt Tuning for Vision Tasks. (1%)](http://arxiv.org/abs/2207.14381)

Xing Nie, Bolin Ni, Jianlong Chang, Gaomeng Meng, Chunlei Huo, Zhaoxiang Zhang, Shiming Xiang, Qi Tian, Chunhong Pan


## 2022-07-27

[Point Cloud Attacks in Graph Spectral Domain: When 3D Geometry Meets Graph Signal Processing. (96%)](http://arxiv.org/abs/2207.13326)

Daizong Liu, Wei Hu, Xin Li


[Look Closer to Your Enemy: Learning to Attack via Teacher-student Mimicking. (91%)](http://arxiv.org/abs/2207.13381)

Mingejie Wang, Zhiqing Tang, Sirui Li, Dingwen Xiao


[Membership Inference Attacks via Adversarial Examples. (73%)](http://arxiv.org/abs/2207.13572)

Hamid Jalalzai, Elie Kadoche, Rémi Leluc, Vincent Plassier


[Hardly Perceptible Trojan Attack against Neural Networks with Bit Flips. (69%)](http://arxiv.org/abs/2207.13417)

Jiawang Bai, Kuofeng Gao, Dihong Gong, Shu-Tao Xia, Zhifeng Li, Wei Liu


[DynaMarks: Defending Against Deep Learning Model Extraction Using Dynamic Watermarking. (47%)](http://arxiv.org/abs/2207.13321)

Abhishek Chakraborty, Daniel Xing, Yuntao Liu, Ankur Srivastava


[Label-Only Membership Inference Attack against Node-Level Graph Neural Networks. (22%)](http://arxiv.org/abs/2207.13766)

Mauro Conti, Jiaxin Li, Stjepan Picek, Jing Xu


[Generative Steganography Network. (1%)](http://arxiv.org/abs/2207.13867)

Ping Wei, Sheng Li, Xinpeng Zhang, Ge Luo, Zhenxing Qian, Qing Zhou


## 2022-07-26

[LGV: Boosting Adversarial Example Transferability from Large Geometric Vicinity. (99%)](http://arxiv.org/abs/2207.13129)

Martin Gubri, Maxime Cordy, Mike Papadakis, Yves Le Traon, Koushik Sen


[Perception-Aware Attack: Creating Adversarial Music via Reverse-Engineering Human Perception. (99%)](http://arxiv.org/abs/2207.13192)

Rui Duan, Zhe Qu, Shangqing Zhao, Leah Ding, Yao Liu, Zhuo Lu


[Generative Extraction of Audio Classifiers for Speaker Identification. (73%)](http://arxiv.org/abs/2207.12816)

Tejumade Afonja, Lucas Bourtoule, Varun Chandrasekaran, Sageev Oore, Nicolas Papernot


[Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks. (8%)](http://arxiv.org/abs/2207.13243)

Tilman Räuker, Anson Ho, Stephen Casper, Dylan Hadfield-Menell


## 2022-07-25

[$p$-DkNN: Out-of-Distribution Detection Through Statistical Testing of Deep Representations. (99%)](http://arxiv.org/abs/2207.12545)

Adam Dziedzic, Stephan Rabanser, Mohammad Yaghini, Armin Ale, Murat A. Erdogdu, Nicolas Papernot


[Improving Adversarial Robustness via Mutual Information Estimation. (99%)](http://arxiv.org/abs/2207.12203)

Dawei Zhou, Nannan Wang, Xinbo Gao, Bo Han, Xiaoyu Wang, Yibing Zhan, Tongliang Liu


[SegPGD: An Effective and Efficient Adversarial Attack for Evaluating and Boosting Segmentation Robustness. (99%)](http://arxiv.org/abs/2207.12391)

Jindong Gu, Hengshuang Zhao, Volker Tresp, Philip Torr


[Jigsaw-ViT: Learning Jigsaw Puzzles in Vision Transformer. (75%)](http://arxiv.org/abs/2207.11971)

Yingyi Chen, Xi Shen, Yahui Liu, Qinghua Tao, Johan A. K. Suykens


[Technical Report: Assisting Backdoor Federated Learning with Whole Population Knowledge Alignment. (9%)](http://arxiv.org/abs/2207.12327)

Tian Liu, Xueyang Hu, Tao Shu


[Semi-Leak: Membership Inference Attacks Against Semi-supervised Learning. (2%)](http://arxiv.org/abs/2207.12535)

Xinlei He, Hongbin Liu, Neil Zhenqiang Gong, Yang Zhang


## 2022-07-24

[Versatile Weight Attack via Flipping Limited Bits. (86%)](http://arxiv.org/abs/2207.12405)

Jiawang Bai, Baoyuan Wu, Zhifeng Li, Shu-tao Xia


[Can we achieve robustness from data alone? (82%)](http://arxiv.org/abs/2207.11727)

Nikolaos Tsilivis, Jingtong Su, Julia Kempe


[Proving Common Mechanisms Shared by Twelve Methods of Boosting Adversarial Transferability. (69%)](http://arxiv.org/abs/2207.11694)

Quanshi Zhang, Xin Wang, Jie Ren, Xu Cheng, Shuyun Lin, Yisen Wang, Xiangming Zhu


[Privacy Against Inference Attacks in Vertical Federated Learning. (2%)](http://arxiv.org/abs/2207.11788)

Borzoo Rassouli, Morteza Varasteh, Deniz Gunduz


[Semantic-guided Multi-Mask Image Harmonization. (1%)](http://arxiv.org/abs/2207.11722)

Xuqian Ren, Yifan Liu


## 2022-07-22

[Do Perceptually Aligned Gradients Imply Adversarial Robustness? (99%)](http://arxiv.org/abs/2207.11378)

Roy Ganz, Bahjat Kawar, Michael Elad


[Provable Defense Against Geometric Transformations. (47%)](http://arxiv.org/abs/2207.11177)

Rem Yang, Jacob Laurel, Sasa Misailovic, Gagandeep Singh


[Aries: Efficient Testing of Deep Neural Networks via Labeling-Free Accuracy Estimation. (41%)](http://arxiv.org/abs/2207.10942)

Qiang Hu, Yuejun Guo, Xiaofei Xie, Maxime Cordy, Lei Ma, Mike Papadakis, Yves Le Traon


[Learning from Multiple Annotator Noisy Labels via Sample-wise Label Fusion. (1%)](http://arxiv.org/abs/2207.11327)

Zhengqi Gao, Fan-Keng Sun, Mingran Yang, Sucheng Ren, Zikai Xiong, Marc Engeler, Antonio Burazer, Linda Wildling, Luca Daniel, Duane S. Boning


## 2022-07-21

[Synthetic Dataset Generation for Adversarial Machine Learning Research. (99%)](http://arxiv.org/abs/2207.10719)

Xiruo Liu, Shibani Singh, Cory Cornelius, Colin Busho, Mike Tan, Anindya Paul, Jason Martin


[Careful What You Wish For: on the Extraction of Adversarially Trained Models. (99%)](http://arxiv.org/abs/2207.10561)

Kacem Khaled, Gabriela Nicolescu, Magalhães Felipe Gohring de


[Rethinking Textual Adversarial Defense for Pre-trained Language Models. (99%)](http://arxiv.org/abs/2208.10251)

Jiayi Wang, Rongzhou Bao, Zhuosheng Zhang, Hai Zhao


[AugRmixAT: A Data Processing and Training Method for Improving Multiple Robustness and Generalization Performance. (98%)](http://arxiv.org/abs/2207.10290)

Xiaoliang Liu, Furao Shen, Jian Zhao, Changhai Nie


[Knowledge-enhanced Black-box Attacks for Recommendations. (92%)](http://arxiv.org/abs/2207.10307)

Jingfan Chen, Wenqi Fan, Guanghui Zhu, Xiangyu Zhao, Chunfeng Yuan, Qing Li, Yihua Huang


[Towards Efficient Adversarial Training on Vision Transformers. (92%)](http://arxiv.org/abs/2207.10498)

Boxi Wu, Jindong Gu, Zhifeng Li, Deng Cai, Xiaofei He, Wei Liu


[Just Rotate it: Deploying Backdoor Attacks via Rotation Transformation. (87%)](http://arxiv.org/abs/2207.10825)

Tong Wu, Tianhao Wang, Vikash Sehwag, Saeed Mahloujifar, Prateek Mittal


[Contrastive Self-Supervised Learning Leads to Higher Adversarial Susceptibility. (83%)](http://arxiv.org/abs/2207.10862)

Rohit Gupta, Naveed Akhtar, Ajmal Mian, Mubarak Shah


[A Forgotten Danger in DNN Supervision Testing: Generating and Detecting True Ambiguity. (1%)](http://arxiv.org/abs/2207.10495)

Michael Weiss, André García Gómez, Paolo Tonella


## 2022-07-20

[Switching One-Versus-the-Rest Loss to Increase the Margin of Logits for Adversarial Robustness. (99%)](http://arxiv.org/abs/2207.10283)

Sekitoshi Kanai, Shin'ya Yamaguchi, Masanori Yamada, Hiroshi Takahashi, Kentaro Ohno, Yasutoshi Ida


[Illusory Attacks: Detectability Matters in Adversarial Attacks on Sequential Decision-Makers. (98%)](http://arxiv.org/abs/2207.10170)

Tim Franzmeyer, Stephen McAleer, João F. Henriques, Jakob N. Foerster, Philip H. S. Torr, Adel Bibi, Witt Christian Schroeder de


[Test-Time Adaptation via Conjugate Pseudo-labels. (10%)](http://arxiv.org/abs/2207.09640)

Sachin Goyal, Mingjie Sun, Aditi Raghunathan, Zico Kolter


[Malware Triage Approach using a Task Memory based on Meta-Transfer Learning Framework. (9%)](http://arxiv.org/abs/2207.10242)

Jinting Zhu, Julian Jang-Jaccard, Ian Welch, Harith Al-Sahaf, Seyit Camtepe


[A temporally and spatially local spike-based backpropagation algorithm to enable training in hardware. (1%)](http://arxiv.org/abs/2207.09755)

Anmol Biswas, Vivek Saraswat, Udayan Ganguly


## 2022-07-19

[Robust Multivariate Time-Series Forecasting: Adversarial Attacks and Defense Mechanisms. (99%)](http://arxiv.org/abs/2207.09572)

Linbo Liu, Youngsuk Park, Trong Nghia Hoang, Hilaf Hasson, Jun Huan


[FLDetector: Defending Federated Learning Against Model Poisoning Attacks via Detecting Malicious Clients. (41%)](http://arxiv.org/abs/2207.09209)

Zaixi Zhang, Xiaoyu Cao, Jinyuan Jia, Neil Zhenqiang Gong


[Is Vertical Logistic Regression Privacy-Preserving? A Comprehensive Privacy Analysis and Beyond. (26%)](http://arxiv.org/abs/2207.09087)

Yuzheng Hu, Tianle Cai, Jinyong Shan, Shange Tang, Chaochao Cai, Ethan Song, Bo Li, Dawn Song


[Assaying Out-Of-Distribution Generalization in Transfer Learning. (1%)](http://arxiv.org/abs/2207.09239)

Florian Wenzel, Andrea Dittadi, Peter Vincent Gehler, Carl-Johann Simon-Gabriel, Max Horn, Dominik Zietlow, David Kernert, Chris Russell, Thomas Brox, Bernt Schiele, Bernhard Schölkopf, Francesco Locatello


## 2022-07-18

[Defending Substitution-Based Profile Pollution Attacks on Sequential Recommenders. (99%)](http://arxiv.org/abs/2207.11237)

Zhenrui Yue, Huimin Zeng, Ziyi Kou, Lanyu Shang, Dong Wang


[Prior-Guided Adversarial Initialization for Fast Adversarial Training. (99%)](http://arxiv.org/abs/2207.08859)

Xiaojun Jia, Yong Zhang, Xingxing Wei, Baoyuan Wu, Ke Ma, Jue Wang, Xiaochun Cao


[Decorrelative Network Architecture for Robust Electrocardiogram Classification. (99%)](http://arxiv.org/abs/2207.09031)

Christopher Wiedeman, Ge Wang


[Multi-step domain adaptation by adversarial attack to $\mathcal{H} \Delta \mathcal{H}$-divergence. (96%)](http://arxiv.org/abs/2207.08948)

Arip Asadulaev, Alexander Panfilov, Andrey Filchenkov


[Adversarial Pixel Restoration as a Pretext Task for Transferable Perturbations. (91%)](http://arxiv.org/abs/2207.08803)

Hashmat Shadab Malik, Shahina K Kunhimon, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan


[Easy Batch Normalization. (69%)](http://arxiv.org/abs/2207.08940)

Arip Asadulaev, Alexander Panfilov, Andrey Filchenkov


[Adversarial Contrastive Learning via Asymmetric InfoNCE. (61%)](http://arxiv.org/abs/2207.08374)

Qiying Yu, Jieming Lou, Xianyuan Zhan, Qizhang Li, Wangmeng Zuo, Yang Liu, Jingjing Liu


[Using Anomaly Detection to Detect Poisoning Attacks in Federated Learning Applications. (22%)](http://arxiv.org/abs/2207.08486)

Ali Raza, Shujun Li, Kim-Phuc Tran, Ludovic Koehl


[A Certifiable Security Patch for Object Tracking in Self-Driving Systems via Historical Deviation Modeling. (10%)](http://arxiv.org/abs/2207.08556)

Xudong Pan, Qifan Xiao, Mi Zhang, Min Yang


[Benchmarking Machine Learning Robustness in Covid-19 Genome Sequence Classification. (2%)](http://arxiv.org/abs/2207.08898)

Sarwan Ali, Bikram Sahoo, Alexander Zelikovskiy, Pin-Yu Chen, Murray Patterson


## 2022-07-17

[Watermark Vaccine: Adversarial Attacks to Prevent Watermark Removal. (99%)](http://arxiv.org/abs/2207.08178)

Xinwei Liu, Jian Liu, Yang Bai, Jindong Gu, Tao Chen, Xiaojun Jia, Xiaochun Cao


[Threat Model-Agnostic Adversarial Defense using Diffusion Models. (99%)](http://arxiv.org/abs/2207.08089)

Tsachi Blau, Roy Ganz, Bahjat Kawar, Alex Bronstein, Michael Elad


[Achieve Optimal Adversarial Accuracy for Adversarial Deep Learning using Stackelberg Game. (96%)](http://arxiv.org/abs/2207.08137)

Xiao-Shan Gao, Shuang Liu, Lijia Yu


[Automated Repair of Neural Networks. (16%)](http://arxiv.org/abs/2207.08157)

Dor Cohen, Ofer Strichman


## 2022-07-16

[DIMBA: Discretely Masked Black-Box Attack in Single Object Tracking. (99%)](http://arxiv.org/abs/2207.08044)

Xiangyu Yin, Wenjie Ruan, Jonathan Fieldsend


[Certified Neural Network Watermarks with Randomized Smoothing. (1%)](http://arxiv.org/abs/2207.07972)

Arpit Bansal, Ping-yeh Chiang, Michael Curry, Rajiv Jain, Curtis Wigington, Varun Manjunatha, John P Dickerson, Tom Goldstein


[Progress and limitations of deep networks to recognize objects in unusual poses. (1%)](http://arxiv.org/abs/2207.08034)

Amro Abbas, Stéphane Deny


[Exploring The Resilience of Control Execution Skips against False Data Injection Attacks. (1%)](http://arxiv.org/abs/2207.08005)

Ipsita Koley, Sunandan Adhikary, Soumyajit Dey


[MixTailor: Mixed Gradient Aggregation for Robust Learning Against Tailored Attacks. (1%)](http://arxiv.org/abs/2207.07941)

Ali Ramezani-Kebrya, Iman Tabrizian, Fartash Faghri, Petar Popovski


## 2022-07-15

[Towards the Desirable Decision Boundary by Moderate-Margin Adversarial Training. (99%)](http://arxiv.org/abs/2207.07793)

Xiaoyu Liang, Yaguan Qian, Jianchang Huang, Xiang Ling, Bin Wang, Chunming Wu, Wassim Swaileh


[CARBEN: Composite Adversarial Robustness Benchmark. (98%)](http://arxiv.org/abs/2207.07797)

Lei Hsiung, Yun-Yun Tsai, Pin-Yu Chen, Tsung-Yi Ho


[Masked Spatial-Spectral Autoencoders Are Excellent Hyperspectral Defenders. (68%)](http://arxiv.org/abs/2207.07803)

Jiahao Qi, Zhiqiang Gong, Xingyue Liu, Kangcheng Bin, Chen Chen, Yongqian Li, Wei Xue, Yu Zhang, Ping Zhong


[Feasibility of Inconspicuous GAN-generated Adversarial Patches against Object Detection. (10%)](http://arxiv.org/abs/2207.07347)

Svetlana Pavlitskaya, Bianca-Marina Codău, J. Marius Zöllner


[PASS: Parameters Audit-based Secure and Fair Federated Learning Scheme against Free Rider. (5%)](http://arxiv.org/abs/2207.07292)

Jianhua Wang


[3DVerifier: Efficient Robustness Verification for 3D Point Cloud Models. (1%)](http://arxiv.org/abs/2207.07539)

Ronghui Mu, Wenjie Ruan, Leandro S. Marcolino, Qiang Ni


## 2022-07-14

[Adversarial Examples for Model-Based Control: A Sensitivity Analysis. (98%)](http://arxiv.org/abs/2207.06982)

Po-han Department of Electrical and Computer Engineering, The University of Texas at Austin Li, Ufuk Oden Institute for Computational Engineering and Sciences, The University of Texas at Austin Topcu, Sandeep P. Department of Electrical and Computer Engineering, The University of Texas at Austin Chinchali


[Adversarial Attacks on Monocular Pose Estimation. (98%)](http://arxiv.org/abs/2207.07032)

Hemang Chawla, Arnav Varma, Elahe Arani, Bahram Zonooz


[Provably Adversarially Robust Nearest Prototype Classifiers. (83%)](http://arxiv.org/abs/2207.07208)

Václav Voráček, Matthias Hein


[Improving Task-free Continual Learning by Distributionally Robust Memory Evolution. (70%)](http://arxiv.org/abs/2207.07256)

Zhenyi Wang, Li Shen, Le Fang, Qiuling Suo, Tiehang Duan, Mingchen Gao


[RSD-GAN: Regularized Sobolev Defense GAN Against Speech-to-Text Adversarial Attacks. (67%)](http://arxiv.org/abs/2207.06858)

Mohammad Esmaeilpour, Nourhene Chaalia, Patrick Cardinal


[Sound Randomized Smoothing in Floating-Point Arithmetics. (50%)](http://arxiv.org/abs/2207.07209)

Václav Voráček, Matthias Hein


[Audio-guided Album Cover Art Generation with Genetic Algorithms. (38%)](http://arxiv.org/abs/2207.07162)

James Marien, Sam Leroux, Bart Dhoedt, Boom Cedric De


[Distance Learner: Incorporating Manifold Prior to Model Training. (16%)](http://arxiv.org/abs/2207.06888)

Aditya Chetan, Nipun Kwatra


[Active Data Pattern Extraction Attacks on Generative Language Models. (11%)](http://arxiv.org/abs/2207.10802)

Bargav Jayaraman, Esha Ghosh, Huseyin Inan, Melissa Chase, Sambuddha Roy, Wei Dai


[Contrastive Adapters for Foundation Model Group Robustness. (1%)](http://arxiv.org/abs/2207.07180)

Michael Zhang, Christopher Ré


[Lipschitz Bound Analysis of Neural Networks. (1%)](http://arxiv.org/abs/2207.07232)

Sarosij Bose


## 2022-07-13

[Perturbation Inactivation Based Adversarial Defense for Face Recognition. (99%)](http://arxiv.org/abs/2207.06035)

Min Ren, Yuhao Zhu, Yunlong Wang, Zhenan Sun


[On the Robustness of Bayesian Neural Networks to Adversarial Attacks. (93%)](http://arxiv.org/abs/2207.06154)

Luca Bortolussi, Ginevra Carbone, Luca Laurenti, Andrea Patane, Guido Sanguinetti, Matthew Wicker


[Adversarially-Aware Robust Object Detector. (91%)](http://arxiv.org/abs/2207.06202)

Ziyi Dong, Pengxu Wei, Liang Lin


[PIAT: Physics Informed Adversarial Training for Solving Partial Differential Equations. (15%)](http://arxiv.org/abs/2207.06647)

Simin Shekarpaz, Mohammad Azizmalayeri, Mohammad Hossein Rohban


[Explainable Intrusion Detection Systems (X-IDS): A Survey of Current Methods, Challenges, and Opportunities. (10%)](http://arxiv.org/abs/2207.06236)

Subash Neupane, Jesse Ables, William Anderson, Sudip Mittal, Shahram Rahimi, Ioana Banicescu, Maria Seale


[Interactive Machine Learning: A State of the Art Review. (4%)](http://arxiv.org/abs/2207.06196)

Natnael A. Wondimu, Cédric Buche, Ubbo Visser


[Sample-dependent Adaptive Temperature Scaling for Improved Calibration. (2%)](http://arxiv.org/abs/2207.06211)

Tom Joy, Francesco Pinto, Ser-Nam Lim, Philip H. S. Torr, Puneet K. Dokania


[DiverGet: A Search-Based Software Testing Approach for Deep Neural Network Quantization Assessment. (1%)](http://arxiv.org/abs/2207.06282)

Ahmed Haj Yahmed, Houssem Ben Braiek, Foutse Khomh, Sonia Bouzidi, Rania Zaatour


## 2022-07-12

[Exploring Adversarial Examples and Adversarial Robustness of Convolutional Neural Networks by Mutual Information. (99%)](http://arxiv.org/abs/2207.05756)

Jiebao Zhang, Wenhua Qian, Rencan Nie, Jinde Cao, Dan Xu


[Adversarial Robustness Assessment of NeuroEvolution Approaches. (99%)](http://arxiv.org/abs/2207.05451)

Inês Valentim, Nuno Lourenço, Nuno Antunes


[Frequency Domain Model Augmentation for Adversarial Attack. (99%)](http://arxiv.org/abs/2207.05382)

Yuyang Long, Qilong Zhang, Boheng Zeng, Lianli Gao, Xianglong Liu, Jian Zhang, Jingkuan Song


[Practical Attacks on Machine Learning: A Case Study on Adversarial Windows Malware. (92%)](http://arxiv.org/abs/2207.05548)

Luca Demetrio, Battista Biggio, Fabio Roli


[Game of Trojans: A Submodular Byzantine Approach. (87%)](http://arxiv.org/abs/2207.05937)

Dinuka Sahabandu, Arezoo Rajabi, Luyao Niu, Bo Li, Bhaskar Ramasubramanian, Radha Poovendran


[Bi-fidelity Evolutionary Multiobjective Search for Adversarially Robust Deep Neural Architectures. (84%)](http://arxiv.org/abs/2207.05321)

Jia Liu, Ran Cheng, Yaochu Jin


[Certified Adversarial Robustness via Anisotropic Randomized Smoothing. (76%)](http://arxiv.org/abs/2207.05327)

Hanbin Hong, Yuan Hong


[RelaxLoss: Defending Membership Inference Attacks without Losing Utility. (26%)](http://arxiv.org/abs/2207.05801)

Dingfan Chen, Ning Yu, Mario Fritz


[Verifying Attention Robustness of Deep Neural Networks against Semantic Perturbations. (5%)](http://arxiv.org/abs/2207.05902)

Satoshi Munakata, Caterina Urban, Haruki Yokoyama, Koji Yamamoto, Kazuki Munakata


[Markov Decision Process For Automatic Cyber Defense. (4%)](http://arxiv.org/abs/2207.05436)

Simon Yusuf Enoch, Simon Yusuf Enoch, Dong Seong Kim


[Estimating Test Performance for AI Medical Devices under Distribution Shift with Conformal Prediction. (1%)](http://arxiv.org/abs/2207.05796)

Charles Lu, Syed Rakin Ahmed, Praveer Singh, Jayashree Kalpathy-Cramer


[Backdoor Attacks on Crowd Counting. (1%)](http://arxiv.org/abs/2207.05641)

Yuhua Sun, Tailai Zhang, Xingjun Ma, Pan Zhou, Jian Lou, Zichuan Xu, Xing Di, Yu Cheng, Lichao


## 2022-07-11

[Statistical Detection of Adversarial examples in Blockchain-based Federated Forest In-vehicle Network Intrusion Detection Systems. (99%)](http://arxiv.org/abs/2207.04843)

Ibrahim Aliyu, Engelenburg Selinde van, Muhammed Bashir Muazu, Jinsul Kim, Chang Gyoon Lim


[RUSH: Robust Contrastive Learning via Randomized Smoothing. (98%)](http://arxiv.org/abs/2207.05127)

Yijiang Pang, Boyang Liu, Jiayu Zhou


[Physical Passive Patch Adversarial Attacks on Visual Odometry Systems. (98%)](http://arxiv.org/abs/2207.05729)

Yaniv Nemcovsky, Matan Yaakoby, Alex M. Bronstein, Chaim Baskin


[Towards Effective Multi-Label Recognition Attacks via Knowledge Graph Consistency. (83%)](http://arxiv.org/abs/2207.05137)

Hassan Mahmood, Ehsan Elhamifar


["Why do so?" -- A Practical Perspective on Machine Learning Security. (64%)](http://arxiv.org/abs/2207.05164)

Kathrin Grosse, Lukas Bieringer, Tarek Richard Besold, Battista Biggio, Katharina Krombholz


[Susceptibility of Continual Learning Against Adversarial Attacks. (45%)](http://arxiv.org/abs/2207.05225)

Hikmat Khan, Pir Masoom Shah, Syed Farhan Alam Zaidi, Saif ul Islam


[Physical Attack on Monocular Depth Estimation with Optimal Adversarial Patches. (22%)](http://arxiv.org/abs/2207.04718)

Zhiyuan Cheng, James Liang, Hongjun Choi, Guanhong Tao, Zhiwen Cao, Dongfang Liu, Xiangyu Zhang


[Adversarial Style Augmentation for Domain Generalized Urban-Scene Segmentation. (1%)](http://arxiv.org/abs/2207.04892)

Zhun Zhong, Yuyang Zhao, Gim Hee Lee, Nicu Sebe


## 2022-07-10

[One-shot Neural Backdoor Erasing via Adversarial Weight Masking. (33%)](http://arxiv.org/abs/2207.04497)

Shuwen Chai, Jinghui Chen


[Hiding Your Signals: A Security Analysis of PPG-based Biometric Authentication. (4%)](http://arxiv.org/abs/2207.04434)

Lin Li, Chao Chen, Lei Pan, Yonghang Tai, Jun Zhang, Yang Xiang


## 2022-07-09

[Adversarial Framework with Certified Robustness for Time-Series Domain via Statistical Features. (98%)](http://arxiv.org/abs/2207.04307)

Taha Belkhouja, Janardhan Rao Doppa


[Invisible Backdoor Attacks Using Data Poisoning in the Frequency Domain. (98%)](http://arxiv.org/abs/2207.04209)

Chang Yue, Peizhuo Lv, Ruigang Liang, Kai Chen


[Dynamic Time Warping based Adversarial Framework for Time-Series Domain. (97%)](http://arxiv.org/abs/2207.04308)

Taha Belkhouja, Yan Yan, Janardhan Rao Doppa


[Training Robust Deep Models for Time-Series Domain: Novel Algorithms and Theoretical Analysis. (67%)](http://arxiv.org/abs/2207.04305)

Taha Belkhouja, Yan Yan, Janardhan Rao Doppa


## 2022-07-08

[Not all broken defenses are equal: The dead angles of adversarial accuracy. (99%)](http://arxiv.org/abs/2207.04129)

Raphael Olivier, Bhiksha Raj


[Improved and Interpretable Defense to Transferred Adversarial Examples by Jacobian Norm with Selective Input Gradient Regularization. (99%)](http://arxiv.org/abs/2207.13036)

Deyin Liu, Lin Wu, Lingqiao Liu, Haifeng Zhao, Farid Boussaid, Mohammed Bennamoun


[Defense Against Multi-target Trojan Attacks. (80%)](http://arxiv.org/abs/2207.03895)

Haripriya Harikumar, Santu Rana, Kien Do, Sunil Gupta, Wei Zong, Willy Susilo, Svetha Venkastesh


[Guiding the retraining of convolutional neural networks against adversarial inputs. (80%)](http://arxiv.org/abs/2207.03689)

Francisco Durán López, Silverio Martínez-Fernández, Michael Felderer, Xavier Franch


[Online Evasion Attacks on Recurrent Models:The Power of Hallucinating the Future. (68%)](http://arxiv.org/abs/2207.09912)

Byunggill Joe, Insik Shin, Jihun Hamm


[Models Out of Line: A Fourier Lens on Distribution Shift Robustness. (10%)](http://arxiv.org/abs/2207.04075)

Sara Fridovich-Keil, Brian R. Bartoldson, James Diffenderfer, Bhavya Kailkhura, Peer-Timo Bremer


[A law of adversarial risk, interpolation, and label noise. (1%)](http://arxiv.org/abs/2207.03933)

Daniel Paleka, Amartya Sanyal


## 2022-07-07

[On the Relationship Between Adversarial Robustness and Decision Region in Deep Neural Network. (99%)](http://arxiv.org/abs/2207.03400)

Seongjin Park, Haedong Jeong, Giyoung Jeon, Jaesik Choi


[Harnessing Out-Of-Distribution Examples via Augmenting Content and Style. (11%)](http://arxiv.org/abs/2207.03162)

Zhuo Huang, Xiaobo Xia, Li Shen, Bo Han, Mingming Gong, Chen Gong, Tongliang Liu


[CausalAgents: A Robustness Benchmark for Motion Forecasting using Causal Relationships. (5%)](http://arxiv.org/abs/2207.03586)

Rebecca Roelofs, Liting Sun, Ben Caine, Khaled S. Refaat, Ben Sapp, Scott Ettinger, Wei Chai


## 2022-07-06

[The Weaknesses of Adversarial Camouflage in Overhead Imagery. (83%)](http://arxiv.org/abs/2207.02963)

Etten Adam Van


[Adversarial Robustness of Visual Dialog. (64%)](http://arxiv.org/abs/2207.02639)

Lu Yu, Verena Rieser


[Enhancing Adversarial Attacks on Single-Layer NVM Crossbar-Based Neural Networks with Power Consumption Information. (54%)](http://arxiv.org/abs/2207.02764)

Cory Merkel


[When does Bias Transfer in Transfer Learning? (10%)](http://arxiv.org/abs/2207.02842)

Hadi Salman, Saachi Jain, Andrew Ilyas, Logan Engstrom, Eric Wong, Aleksander Madry


[Privacy-preserving Reflection Rendering for Augmented Reality. (2%)](http://arxiv.org/abs/2207.03056)

Yiqin Zhao, Sheng Wei, Tian Guo


[Not All Models Are Equal: Predicting Model Transferability in a Self-challenging Fisher Space. (1%)](http://arxiv.org/abs/2207.03036)

Wenqi Shao, Xun Zhao, Yixiao Ge, Zhaoyang Zhang, Lei Yang, Xiaogang Wang, Ying Shan, Ping Luo


## 2022-07-05

[Query-Efficient Adversarial Attack Based on Latin Hypercube Sampling. (99%)](http://arxiv.org/abs/2207.02391)

Dan Wang, Jiayu Lin, Yuan-Gen Wang


[Defending against the Label-flipping Attack in Federated Learning. (98%)](http://arxiv.org/abs/2207.01982)

Najeeb Moharram Jebreel, Josep Domingo-Ferrer, David Sánchez, Alberto Blanco-Justicia


[UniCR: Universally Approximated Certified Robustness via Randomized Smoothing. (93%)](http://arxiv.org/abs/2207.02152)

Hanbin Hong, Binghui Wang, Yuan Hong


[PRoA: A Probabilistic Robustness Assessment against Functional Perturbations. (92%)](http://arxiv.org/abs/2207.02036)

Tianle Zhang, Wenjie Ruan, Jonathan E. Fieldsend


[Learning to Accelerate Approximate Methods for Solving Integer Programming via Early Fixing. (38%)](http://arxiv.org/abs/2207.02087)

Longkang Li, Baoyuan Wu


[Robustness Analysis of Video-Language Models Against Visual and Language Perturbations. (1%)](http://arxiv.org/abs/2207.02159)

Madeline C. Schiappa, Shruti Vyas, Hamid Palangi, Yogesh S. Rawat, Vibhav Vineet


[Conflicting Interactions Among Protection Mechanisms for Machine Learning Models. (1%)](http://arxiv.org/abs/2207.01991)

Sebastian Szyller, N. Asokan


[PoF: Post-Training of Feature Extractor for Improving Generalization. (1%)](http://arxiv.org/abs/2207.01847)

Ikuro Sato, Ryota Yamada, Masayuki Tanaka, Nakamasa Inoue, Rei Kawakami


[Class-Specific Semantic Reconstruction for Open Set Recognition. (1%)](http://arxiv.org/abs/2207.02158)

Hongzhi Huang, Yu Wang, Qinghua Hu, Ming-Ming Cheng


## 2022-07-04

[Hessian-Free Second-Order Adversarial Examples for Adversarial Learning. (99%)](http://arxiv.org/abs/2207.01396)

Yaguan Qian, Yuqi Wang, Bin Wang, Zhaoquan Gu, Yuhan Guo, Wassim Swaileh


[Wild Networks: Exposure of 5G Network Infrastructures to Adversarial Examples. (98%)](http://arxiv.org/abs/2207.01531)

Giovanni Apruzzese, Rodion Vladimirov, Aliya Tastemirova, Pavel Laskov


[Task-agnostic Defense against Adversarial Patch Attacks. (98%)](http://arxiv.org/abs/2207.01795)

Ke Xu, Yao Xiao, Zhaoheng Zheng, Kaijie Cai, Ram Nevatia


[Large-scale Robustness Analysis of Video Action Recognition Models. (70%)](http://arxiv.org/abs/2207.01398)

Madeline C. Schiappa, Naman Biyani, Shruti Vyas, Hamid Palangi, Vibhav Vineet, Yogesh Rawat


[Counterbalancing Teacher: Regularizing Batch Normalized Models for Robustness. (1%)](http://arxiv.org/abs/2207.01548)

Saeid Asgari Taghanaki, Ali Gholami, Fereshte Khani, Kristy Choi, Linh Tran, Ran Zhang, Aliasghar Khani


## 2022-07-03

[RAF: Recursive Adversarial Attacks on Face Recognition Using Extremely Limited Queries. (99%)](http://arxiv.org/abs/2207.01149)

Keshav Kasichainula, Hadi Mansourifar, Weidong Shi


[Removing Batch Normalization Boosts Adversarial Training. (98%)](http://arxiv.org/abs/2207.01156)

Haotao Wang, Aston Zhang, Shuai Zheng, Xingjian Shi, Mu Li, Zhangyang Wang


[Anomaly Detection with Adversarially Learned Perturbations of Latent Space. (13%)](http://arxiv.org/abs/2207.01106)

Vahid Reza Khazaie, Anthony Wong, John Taylor Jewell, Yalda Mohsenzadeh


[Identifying the Context Shift between Test Benchmarks and Production Data. (1%)](http://arxiv.org/abs/2207.01059)

Matthew Groh


## 2022-07-02

[FL-Defender: Combating Targeted Attacks in Federated Learning. (80%)](http://arxiv.org/abs/2207.00872)

Najeeb Jebreel, Josep Domingo-Ferrer


[Backdoor Attack is a Devil in Federated GAN-based Medical Image Synthesis. (11%)](http://arxiv.org/abs/2207.00762)

Ruinan Jin, Xiaoxiao Li


[PhilaeX: Explaining the Failure and Success of AI Models in Malware Detection. (1%)](http://arxiv.org/abs/2207.00740)

Zhi Lu, Vrizlynn L. L. Thing


## 2022-07-01

[Efficient Adversarial Training With Data Pruning. (99%)](http://arxiv.org/abs/2207.00694)

Maximilian Kaufmann, Yiren Zhao, Ilia Shumailov, Robert Mullins, Nicolas Papernot


[BadHash: Invisible Backdoor Attacks against Deep Hashing with Clean Label. (99%)](http://arxiv.org/abs/2207.00278)

Shengshan Hu, Ziqi Zhou, Yechao Zhang, Leo Yu Zhang, Yifeng Zheng, Yuanyuan HE, Hai Jin


## 2022-06-30

[Detecting and Recovering Adversarial Examples from Extracting Non-robust and Highly Predictive Adversarial Perturbations. (99%)](http://arxiv.org/abs/2206.15128)

Mingyu Dong, Jiahao Chen, Diqun Yan, Jingxing Gao, Li Dong, Rangding Wang


[Measuring Forgetting of Memorized Training Examples. (83%)](http://arxiv.org/abs/2207.00099)

Matthew Jagielski, Om Thakkar, Florian Tramèr, Daphne Ippolito, Katherine Lee, Nicholas Carlini, Eric Wallace, Shuang Song, Abhradeep Thakurta, Nicolas Papernot, Chiyuan Zhang


[MEAD: A Multi-Armed Approach for Evaluation of Adversarial Examples Detectors. (80%)](http://arxiv.org/abs/2206.15415)

Federica Granese, Marine Picot, Marco Romanelli, Francisco Messina, Pablo Piantanida


[Reliable Representations Make A Stronger Defender: Unsupervised Structure Refinement for Robust GNN. (16%)](http://arxiv.org/abs/2207.00012)

Kuan Li, Yang Liu, Xiang Ao, Jianfeng Chi, Jinghua Feng, Hao Yang, Qing He


[Threat Assessment in Machine Learning based Systems. (13%)](http://arxiv.org/abs/2207.00091)

Lionel Nganyewou Tidjon, Foutse Khomh


[Robustness of Epinets against Distributional Shifts. (1%)](http://arxiv.org/abs/2207.00137)

Xiuyuan Lu, Ian Osband, Seyed Mohammad Asghari, Sven Gowal, Vikranth Dwaracherla, Zheng Wen, Roy Benjamin Van


[ProSelfLC: Progressive Self Label Correction Towards A Low-Temperature Entropy State. (1%)](http://arxiv.org/abs/2207.00118)

Xinshao Wang, Yang Hua, Elyor Kodirov, Sankha Subhra Mukherjee, David A. Clifton, Neil M. Robertson


[No Reason for No Supervision: Improved Generalization in Supervised Models. (1%)](http://arxiv.org/abs/2206.15369)

Mert Bulent Sariyildiz, Yannis Kalantidis, Karteek Alahari, Diane Larlus


[Augment like there's no tomorrow: Consistently performing neural networks for medical imaging. (1%)](http://arxiv.org/abs/2206.15274)

Joona Pohjonen, Carolin Stürenberg, Atte Föhr, Reija Randen-Brady, Lassi Luomala, Jouni Lohi, Esa Pitkänen, Antti Rannikko, Tuomas Mirtti


## 2022-06-29

[IBP Regularization for Verified Adversarial Robustness via Branch-and-Bound. (92%)](http://arxiv.org/abs/2206.14772)

Palma Alessandro De, Rudy Bunel, Krishnamurthy Dvijotham, M. Pawan Kumar, Robert Stanforth


[Adversarial Ensemble Training by Jointly Learning Label Dependencies and Member Models. (33%)](http://arxiv.org/abs/2206.14477)

Lele Wang, Bin Liu


[longhorns at DADC 2022: How many linguists does it take to fool a Question Answering model? A systematic approach to adversarial attacks. (10%)](http://arxiv.org/abs/2206.14729)

Venelin Kovatchev, Trina Chatterjee, Venkata S Govindarajan, Jifan Chen, Eunsol Choi, Gabriella Chronis, Anubrata Das, Katrin Erk, Matthew Lease, Junyi Jessy Li, Yating Wu, Kyle Mahowald


[Private Graph Extraction via Feature Explanations. (4%)](http://arxiv.org/abs/2206.14724)

Iyiola E. Olatunji, Mandeep Rathee, Thorben Funke, Megha Khosla


[RegMixup: Mixup as a Regularizer Can Surprisingly Improve Accuracy and Out Distribution Robustness. (2%)](http://arxiv.org/abs/2206.14502)

Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip H. S. Torr, Puneet K. Dokania


## 2022-06-28

[Increasing Confidence in Adversarial Robustness Evaluations. (99%)](http://arxiv.org/abs/2206.13991)

Roland S. Zimmermann, Wieland Brendel, Florian Tramer, Nicholas Carlini


[Rethinking Adversarial Examples for Location Privacy Protection. (93%)](http://arxiv.org/abs/2206.14020)

Trung-Nghia Le, Ta Gu, Huy H. Nguyen, Isao Echizen


[A Deep Learning Approach to Create DNS Amplification Attacks. (92%)](http://arxiv.org/abs/2206.14346)

Jared Mathews, Prosenjit Chatterjee, Shankar Banik, Cory Nance


[On the amplification of security and privacy risks by post-hoc explanations in machine learning models. (31%)](http://arxiv.org/abs/2206.14004)

Pengrui Quan, Supriyo Chakraborty, Jeya Vikranth Jeyakumar, Mani Srivastava


[How to Steer Your Adversary: Targeted and Efficient Model Stealing Defenses with Gradient Redirection. (12%)](http://arxiv.org/abs/2206.14157)

Mantas Mazeika, Bo Li, David Forsyth


[An Empirical Study of Challenges in Converting Deep Learning Models. (5%)](http://arxiv.org/abs/2206.14322)

Moses Jack Openja, Amin Jack Nikanjam, Ahmed Haj Jack Yahmed, Foutse Jack Khomh, Zhen Jack Ming, Jiang


[Reasoning about Moving Target Defense in Attack Modeling Formalisms. (2%)](http://arxiv.org/abs/2206.14076)

Gabriel Ballot, Vadim Malvone, Jean Leneutre, Etienne Borde


[AS-IntroVAE: Adversarial Similarity Distance Makes Robust IntroVAE. (1%)](http://arxiv.org/abs/2206.13903)

Changjie Lu, Shen Zheng, Zirui Wang, Omar Dib, Gaurav Gupta


## 2022-06-27

[Adversarial Example Detection in Deployed Tree Ensembles. (99%)](http://arxiv.org/abs/2206.13083)

Laurens Devos, Wannes Meert, Jesse Davis


[Towards Secrecy-Aware Attacks Against Trust Prediction in Signed Graphs. (38%)](http://arxiv.org/abs/2206.13104)

Yulin Zhu, Tomasz Michalak, Xiapu Luo, Kai Zhou


[Utilizing Class Separation Distance for the Evaluation of Corruption Robustness of Machine Learning Classifiers. (15%)](http://arxiv.org/abs/2206.13405)

Georg Siedel, Silvia Vock, Andrey Morozov, Stefan Voß


[Cyber Network Resilience against Self-Propagating Malware Attacks. (13%)](http://arxiv.org/abs/2206.13594)

Alesia Chernikova, Nicolò Gozzi, Simona Boboila, Priyanka Angadi, John Loughner, Matthew Wilden, Nicola Perra, Tina Eliassi-Rad, Alina Oprea


[Quantification of Deep Neural Network Prediction Uncertainties for VVUQ of Machine Learning Models. (4%)](http://arxiv.org/abs/2206.14615)

Mahmoud Yaseen, Xu Wu


## 2022-06-26

[Self-Healing Robust Neural Networks via Closed-Loop Control. (45%)](http://arxiv.org/abs/2206.12963)

Zhuotong Chen, Qianxiao Li, Zheng Zhang


[De-END: Decoder-driven Watermarking Network. (1%)](http://arxiv.org/abs/2206.13032)

Han Fang, Zhaoyang Jia, Yupeng Qiu, Jiyi Zhang, Weiming Zhang, Ee-Chien Chang


## 2022-06-25

[Empirical Evaluation of Physical Adversarial Patch Attacks Against Overhead Object Detection Models. (99%)](http://arxiv.org/abs/2206.12725)

Gavin S. Hartnett, Li Ang Zhang, Caolionn O'Connell, Andrew J. Lohn, Jair Aguirre


[Defense against adversarial attacks on deep convolutional neural networks through nonlocal denoising. (99%)](http://arxiv.org/abs/2206.12685)

Sandhya Aneja, Nagender Aneja, Pg Emeroylariffion Abas, Abdul Ghani Naim


[RSTAM: An Effective Black-Box Impersonation Attack on Face Recognition using a Mobile and Compact Printer. (99%)](http://arxiv.org/abs/2206.12590)

Xiaoliang Liu, Furao Shen, Jian Zhao, Changhai Nie


[Defending Multimodal Fusion Models against Single-Source Adversaries. (81%)](http://arxiv.org/abs/2206.12714)

Karren Yang, Wan-Yi Lin, Manash Barman, Filipe Condessa, Zico Kolter


[BackdoorBench: A Comprehensive Benchmark of Backdoor Learning. (12%)](http://arxiv.org/abs/2206.12654)

Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, Chao Shen, Hongyuan Zha


[Cascading Failures in Smart Grids under Random, Targeted and Adaptive Attacks. (1%)](http://arxiv.org/abs/2206.12735)

Sushmita Ruj, Arindam Pal


## 2022-06-24

[Defending Backdoor Attacks on Vision Transformer via Patch Processing. (99%)](http://arxiv.org/abs/2206.12381)

Khoa D. Doan, Yingjie Lao, Peng Yang, Ping Li


[AdAUC: End-to-end Adversarial AUC Optimization Against Long-tail Problems. (96%)](http://arxiv.org/abs/2206.12169)

Wenzheng Hou, Qianqian Xu, Zhiyong Yang, Shilong Bao, Yuan He, Qingming Huang


[Adversarial Robustness of Deep Neural Networks: A Survey from a Formal Verification Perspective. (92%)](http://arxiv.org/abs/2206.12227)

Mark Huasong Meng, Guangdong Bai, Sin Gee Teo, Zhe Hou, Yan Xiao, Yun Lin, Jin Song Dong


[Robustness of Explanation Methods for NLP Models. (82%)](http://arxiv.org/abs/2206.12284)

Shriya Atmakuri, Tejas Chheda, Dinesh Kandula, Nishant Yadav, Taesung Lee, Hessel Tuinhof


[zPROBE: Zero Peek Robustness Checks for Federated Learning. (4%)](http://arxiv.org/abs/2206.12100)

Zahra Ghodsi, Mojan Javaheripi, Nojan Sheybani, Xinqiao Zhang, Ke Huang, Farinaz Koushanfar


[Robustness Evaluation of Deep Unsupervised Learning Algorithms for Intrusion Detection Systems. (2%)](http://arxiv.org/abs/2207.03576)

D'Jeff Kanda Nkashama, Arian Soltani, Jean-Charles Verdier, Marc Frappier, Pierre-Marting Tardif, Froduald Kabanza


## 2022-06-23

[Adversarial Zoom Lens: A Novel Physical-World Attack to DNNs. (99%)](http://arxiv.org/abs/2206.12251)

Chengyin Hu, Weiwen Shi


[A Framework for Understanding Model Extraction Attack and Defense. (98%)](http://arxiv.org/abs/2206.11480)

Xun Xian, Mingyi Hong, Jie Ding


[Towards End-to-End Private Automatic Speaker Recognition. (76%)](http://arxiv.org/abs/2206.11750)

Francisco Teixeira, Alberto Abad, Bhiksha Raj, Isabel Trancoso


[BERT Rankers are Brittle: a Study using Adversarial Document Perturbations. (75%)](http://arxiv.org/abs/2206.11724)

Yumeng Wang, Lijun Lyu, Avishek Anand


[Never trust, always verify : a roadmap for Trustworthy AI? (1%)](http://arxiv.org/abs/2206.11981)

Lionel Nganyewou Tidjon, Foutse Khomh


[Measuring Representational Robustness of Neural Networks Through Shared Invariances. (1%)](http://arxiv.org/abs/2206.11939)

Vedant Nanda, Till Speicher, Camila Kolling, John P. Dickerson, Krishna P. Gummadi, Adrian Weller


## 2022-06-22

[AdvSmo: Black-box Adversarial Attack by Smoothing Linear Structure of Texture. (99%)](http://arxiv.org/abs/2206.10988)

Hui Xia, Rui Zhang, Shuliang Jiang, Zi Kang


[InfoAT: Improving Adversarial Training Using the Information Bottleneck Principle. (98%)](http://arxiv.org/abs/2206.12292)

Mengting Xu, Tao Zhang, Zhongnian Li, Daoqiang Zhang


[Robust Universal Adversarial Perturbations. (97%)](http://arxiv.org/abs/2206.10858)

Changming Xu, Gagandeep Singh


[Guided Diffusion Model for Adversarial Purification from Random Noise. (68%)](http://arxiv.org/abs/2206.10875)

Quanlin Wu, Hang Ye, Yuntian Gu


[Understanding the effect of sparsity on neural networks robustness. (61%)](http://arxiv.org/abs/2206.10915)

Lukas Timpl, Rahim Entezari, Hanie Sedghi, Behnam Neyshabur, Olga Saukh


[Shilling Black-box Recommender Systems by Learning to Generate Fake User Profiles. (41%)](http://arxiv.org/abs/2206.11433)

Chen Lin, Si Chen, Meifang Zeng, Sheng Zhang, Min Gao, Hui Li


## 2022-06-21

[SSMI: How to Make Objects of Interest Disappear without Accessing Object Detectors? (99%)](http://arxiv.org/abs/2206.10809)

Hui Xia, Rui Zhang, Zi Kang, Shuliang Jiang


[Transferable Graph Backdoor Attack. (99%)](http://arxiv.org/abs/2207.00425)

Shuiqiao Yang, Bao Gia Doan, Paul Montague, Vel Olivier De, Tamas Abraham, Seyit Camtepe, Damith C. Ranasinghe, Salil S. Kanhere


[(Certified!!) Adversarial Robustness for Free! (84%)](http://arxiv.org/abs/2206.10550)

Nicholas Dj Carlini, Florian Dj Tramer, Dj Krishnamurthy, Dvijotham, J. Zico Kolter


[Certifiably Robust Policy Learning against Adversarial Communication in Multi-agent Systems. (81%)](http://arxiv.org/abs/2206.10158)

Yanchao Sun, Ruijie Zheng, Parisa Hassanzadeh, Yongyuan Liang, Soheil Feizi, Sumitra Ganesh, Furong Huang


[FlashSyn: Flash Loan Attack Synthesis via Counter Example Driven Approximation. (68%)](http://arxiv.org/abs/2206.10708)

Zhiyang Chen, Sidi Mohamed Beillahi, Fan Long


[Natural Backdoor Datasets. (33%)](http://arxiv.org/abs/2206.10673)

Emily Wenger, Roma Bhattacharjee, Arjun Nitin Bhagoji, Josephine Passananti, Emilio Andere, Haitao Zheng, Ben Y. Zhao


[The Privacy Onion Effect: Memorization is Relative. (22%)](http://arxiv.org/abs/2206.10469)

Nicholas Carlini, Matthew Jagielski, Nicolas Papernot, Andreas Terzis, Florian Tramer, Chiyuan Zhang


[ProML: A Decentralised Platform for Provenance Management of Machine Learning Software Systems. (1%)](http://arxiv.org/abs/2206.10110)

Nguyen Khoi Tran, Bushra Sabir, M. Ali Babar, Nini Cui, Mehran Abolhasan, Justin Lipman


## 2022-06-20

[Understanding Robust Learning through the Lens of Representation Similarities. (99%)](http://arxiv.org/abs/2206.09868)

Christian Cianfarani, Arjun Nitin Bhagoji, Vikash Sehwag, Ben Zhao, Prateek Mittal


[Diversified Adversarial Attacks based on Conjugate Gradient Method. (98%)](http://arxiv.org/abs/2206.09628)

Keiichiro Yamamura, Haruki Sato, Nariaki Tateiwa, Nozomi Hata, Toru Mitsutake, Issa Oe, Hiroki Ishikura, Katsuki Fujisawa


[Robust Deep Reinforcement Learning through Bootstrapped Opportunistic Curriculum. (76%)](http://arxiv.org/abs/2206.10057)

Junlin Wu, Yevgeniy Vorobeychik


[SafeBench: A Benchmarking Platform for Safety Evaluation of Autonomous Vehicles. (5%)](http://arxiv.org/abs/2206.09682)

Chejian Xu, Wenhao Ding, Weijie Lyu, Zuxin Liu, Shuai Wang, Yihan He, Hanjiang Hu, Ding Zhao, Bo Li


[Breaking Down Out-of-Distribution Detection: Many Methods Based on OOD Training Data Estimate a Combination of the Same Core Quantities. (1%)](http://arxiv.org/abs/2206.09880)

Julian Bitterwolf, Alexander Meinke, Maximilian Augustin, Matthias Hein


## 2022-06-19

[On the Limitations of Stochastic Pre-processing Defenses. (99%)](http://arxiv.org/abs/2206.09491)

Yue Gao, Ilia Shumailov, Kassem Fawaz, Nicolas Papernot


[Towards Adversarial Attack on Vision-Language Pre-training Models. (98%)](http://arxiv.org/abs/2206.09391)

Jiaming Zhang, Qi Yi, Jitao Sang


[A Universal Adversarial Policy for Text Classifiers. (98%)](http://arxiv.org/abs/2206.09458)

Gallil Maimon, Lior Rokach


[JPEG Compression-Resistant Low-Mid Adversarial Perturbation against Unauthorized Face Recognition System. (68%)](http://arxiv.org/abs/2206.09410)

Jiaming Zhang, Qi Yi, Jitao Sang


[Adversarially trained neural representations may already be as robust as corresponding biological neural representations. (31%)](http://arxiv.org/abs/2206.11228)

Chong Guo, Michael J. Lee, Guillaume Leclerc, Joel Dapello, Yug Rao, Aleksander Madry, James J. DiCarlo


## 2022-06-18

[Demystifying the Adversarial Robustness of Random Transformation Defenses. (99%)](http://arxiv.org/abs/2207.03574)

Chawin Sitawarin, Zachary Golan-Strieb, David Wagner


[On the Role of Generalization in Transferability of Adversarial Examples. (99%)](http://arxiv.org/abs/2206.09238)

Yilin Wang, Farzan Farnia


[DECK: Model Hardening for Defending Pervasive Backdoors. (98%)](http://arxiv.org/abs/2206.09272)

Guanhong Tao, Yingqi Liu, Siyuan Cheng, Shengwei An, Zhuo Zhang, Qiuling Xu, Guangyu Shen, Xiangyu Zhang


[Measuring Lower Bounds of Local Differential Privacy via Adversary Instantiations in Federated Learning. (10%)](http://arxiv.org/abs/2206.09122)

Marin Matsumoto, Tsubasa Takahashi, Seng Pei Liew, Masato Oguchi


[Adversarial Scrutiny of Evidentiary Statistical Software. (2%)](http://arxiv.org/abs/2206.09305)

Rediet Abebe, Moritz Hardt, Angela Jin, John Miller, Ludwig Schmidt, Rebecca Wexler


## 2022-06-17

[Detecting Adversarial Examples in Batches -- a geometrical approach. (99%)](http://arxiv.org/abs/2206.08738)

Danush Kumar Venkatesh, Peter Steinbach


[Minimum Noticeable Difference based Adversarial Privacy Preserving Image Generation. (99%)](http://arxiv.org/abs/2206.08638)

Wen Sun, Jian Jin, Weisi Lin


[Query-Efficient and Scalable Black-Box Adversarial Attacks on Discrete Sequential Data via Bayesian Optimization. (99%)](http://arxiv.org/abs/2206.08575)

Deokjae Lee, Seungyong Moon, Junhyeok Lee, Hyun Oh Song


[Comment on Transferability and Input Transformation with Additive Noise. (99%)](http://arxiv.org/abs/2206.09075)

Hoki Kim, Jinseong Park, Jaewook Lee


[Adversarial Robustness is at Odds with Lazy Training. (98%)](http://arxiv.org/abs/2207.00411)

Yunjuan Wang, Enayat Ullah, Poorya Mianjy, Raman Arora


[Is Multi-Modal Necessarily Better? Robustness Evaluation of Multi-modal Fake News Detection. (83%)](http://arxiv.org/abs/2206.08788)

Jinyin Chen, Chengyu Jia, Haibin Zheng, Ruoxi Chen, Chenbo Fu


[RetrievalGuard: Provably Robust 1-Nearest Neighbor Image Retrieval. (81%)](http://arxiv.org/abs/2206.11225)

Yihan Wu, Hongyang Zhang, Heng Huang


[The Consistency of Adversarial Training for Binary Classification. (26%)](http://arxiv.org/abs/2206.09099)

Natalie S. Frank, Jonathan Niles-Weed


[Existence and Minimax Theorems for Adversarial Surrogate Risks in Binary Classification. (15%)](http://arxiv.org/abs/2206.09098)

Natalie S. Frank


[Understanding Robust Overfitting of Adversarial Training and Beyond. (8%)](http://arxiv.org/abs/2206.08675)

Chaojian Yu, Bo Han, Li Shen, Jun Yu, Chen Gong, Mingming Gong, Tongliang Liu


## 2022-06-16

[Adversarial Privacy Protection on Speech Enhancement. (99%)](http://arxiv.org/abs/2206.08170)

Mingyu Dong, Diqun Yan, Rangding Wang


[Boosting the Adversarial Transferability of Surrogate Model with Dark Knowledge. (99%)](http://arxiv.org/abs/2206.08316)

Dingcheng Yang, Zihao Xiao, Wenjian Yu


[Analysis and Extensions of Adversarial Training for Video Classification. (93%)](http://arxiv.org/abs/2206.07953)

Kaleab A. Kinfu, René Vidal


[Double Sampling Randomized Smoothing. (89%)](http://arxiv.org/abs/2206.07912)

Linyi Li, Jiawei Zhang, Tao Xie, Bo Li


[Adversarial Robustness of Graph-based Anomaly Detection. (76%)](http://arxiv.org/abs/2206.08260)

Yulin Zhu, Yuni Lai, Kaifa Zhao, Xiapu Luo, Mingquan Yuan, Jian Ren, Kai Zhou


[A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks. (68%)](http://arxiv.org/abs/2206.08514)

Ganqu Cui, Lifan Yuan, Bingxiang He, Yangyi Chen, Zhiyuan Liu, Maosong Sun


[Backdoor Attacks on Vision Transformers. (31%)](http://arxiv.org/abs/2206.08477)

Akshayvarun Subramanya, Aniruddha Saha, Soroush Abbasi Koohpayegani, Ajinkya Tejankar, Hamed Pirsiavash


[Adversarial Patch Attacks and Defences in Vision-Based Tasks: A Survey. (22%)](http://arxiv.org/abs/2206.08304)

Abhijith Sharma, Yijun Bian, Phil Munz, Apurva Narayan


[Catastrophic overfitting is a bug but also a feature. (16%)](http://arxiv.org/abs/2206.08242)

Guillermo Ortiz-Jiménez, Jorge Pau de, Amartya Sanyal, Adel Bibi, Puneet K. Dokania, Pascal Frossard, Gregory Rogéz, Philip H. S. Torr


[I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences. (5%)](http://arxiv.org/abs/2206.08451)

Daryna Oliynyk, Rudolf Mayer, Andreas Rauber


[Gradient-Based Adversarial and Out-of-Distribution Detection. (2%)](http://arxiv.org/abs/2206.08255)

Jinsol Lee, Mohit Prabhushankar, Ghassan AlRegib


["Understanding Robustness Lottery": A Comparative Visual Analysis of Neural Network Pruning Approaches. (1%)](http://arxiv.org/abs/2206.07918)

Zhimin Li, Shusen Liu, Xin Yu, Kailkhura Bhavya, Jie Cao, Diffenderfer James Daniel, Peer-Timo Bremer, Valerio Pascucci


## 2022-06-15

[Fast and Reliable Evaluation of Adversarial Robustness with Minimum-Margin Attack. (99%)](http://arxiv.org/abs/2206.07314)

Ruize Gao, Jiongxiao Wang, Kaiwen Zhou, Feng Liu, Binghui Xie, Gang Niu, Bo Han, James Cheng


[Morphence-2.0: Evasion-Resilient Moving Target Defense Powered by Out-of-Distribution Detection. (99%)](http://arxiv.org/abs/2206.07321)

Abderrahmen Amich, Ata Kaboudi, Birhanu Eshete


[Architectural Backdoors in Neural Networks. (83%)](http://arxiv.org/abs/2206.07840)

Mikel Bober-Irizar, Ilia Shumailov, Yiren Zhao, Robert Mullins, Nicolas Papernot


[Hardening DNNs against Transfer Attacks during Network Compression using Greedy Adversarial Pruning. (75%)](http://arxiv.org/abs/2206.07406)

Jonah O'Brien Weiss, Tiago Alves, Sandip Kundu


[Linearity Grafting: Relaxed Neuron Pruning Helps Certifiable Robustness. (74%)](http://arxiv.org/abs/2206.07839)

Tianlong Chen, Huan Zhang, Zhenyu Zhang, Shiyu Chang, Sijia Liu, Pin-Yu Chen, Zhangyang Wang


[A Search-Based Testing Approach for Deep Reinforcement Learning Agents. (62%)](http://arxiv.org/abs/2206.07813)

Amirhossein Zolfagharian, Manel Abdellatif, Lionel Briand, Mojtaba Bagherzadeh, Ramesh S


[Can pruning improve certified robustness of neural networks? (56%)](http://arxiv.org/abs/2206.07311)

Zhangheng Li, Tianlong Chen, Linyi Li, Bo Li, Zhangyang Wang


[Improving Diversity with Adversarially Learned Transformations for Domain Generalization. (33%)](http://arxiv.org/abs/2206.07736)

Tejas Gokhale, Rushil Anirudh, Jayaraman J. Thiagarajan, Bhavya Kailkhura, Chitta Baral, Yezhou Yang


[Queried Unlabeled Data Improves and Robustifies Class-Incremental Learning. (11%)](http://arxiv.org/abs/2206.07842)

Tianlong Chen, Sijia Liu, Shiyu Chang, Lisa Amini, Zhangyang Wang


[The Manifold Hypothesis for Gradient-Based Explanations. (2%)](http://arxiv.org/abs/2206.07387)

Sebastian Bordt, Uddeshya Upadhyay, Zeynep Akata, Luxburg Ulrike von


[READ: Aggregating Reconstruction Error into Out-of-distribution Detection. (1%)](http://arxiv.org/abs/2206.07459)

Wenyu Jiang, Hao Cheng, Mingcai Chen, Shuai Feng, Yuxin Ge, Chongjun Wang


## 2022-06-14

[Adversarial Vulnerability of Randomized Ensembles. (99%)](http://arxiv.org/abs/2206.06737)

Hassan Dbouk, Naresh R. Shanbhag


[Downlink Power Allocation in Massive MIMO via Deep Learning: Adversarial Attacks and Training. (99%)](http://arxiv.org/abs/2206.06592)

B. R. Manoj, Meysam Sadeghi, Erik G. Larsson


[Efficiently Training Low-Curvature Neural Networks. (92%)](http://arxiv.org/abs/2206.07144)

Suraj Srinivas, Kyle Matoba, Himabindu Lakkaraju, Francois Fleuret


[Proximal Splitting Adversarial Attacks for Semantic Segmentation. (92%)](http://arxiv.org/abs/2206.07179)

Jérôme Rony, Jean-Christophe Pesquet, Ismail Ben Ayed


[Defending Observation Attacks in Deep Reinforcement Learning via Detection and Denoising. (88%)](http://arxiv.org/abs/2206.07188)

Zikang Xiong, Joe Eappen, He Zhu, Suresh Jagannathan


[On the explainable properties of 1-Lipschitz Neural Networks: An Optimal Transport Perspective. (88%)](http://arxiv.org/abs/2206.06854)

Mathieu IRIT, UT Serrurier, Franck UT Mamalet, Thomas UT Fel, Louis UT3, UT, IRIT Béthune, Thibaut UT Boissin


[Exploring Adversarial Attacks and Defenses in Vision Transformers trained with DINO. (86%)](http://arxiv.org/abs/2206.06761)

Javier Rando, Nasib Naimi, Thomas Baumann, Max Mathys


[Turning a Curse Into a Blessing: Enabling Clean-Data-Free Defenses by Model Inversion. (68%)](http://arxiv.org/abs/2206.07018)

Si Chen, Yi Zeng, Won Park, Ruoxi Jia


[Human Eyes Inspired Recurrent Neural Networks are More Robust Against Adversarial Noises. (62%)](http://arxiv.org/abs/2206.07282)

Minkyu Choi, Yizhen Zhang, Kuan Han, Xiaokai Wang, Zhongming Liu


[Attacks on Perception-Based Control Systems: Modeling and Fundamental Limits. (2%)](http://arxiv.org/abs/2206.07150)

Amir Khazraei, Henry Pfister, Miroslav Pajic


[A Gift from Label Smoothing: Robust Training with Adaptive Label Smoothing via Auxiliary Classifier under Label Noise. (1%)](http://arxiv.org/abs/2206.07277)

Jongwoo Ko, Bongsoo Yi, Se-Young Yun


[A Survey on Gradient Inversion: Attacks, Defenses and Future Directions. (1%)](http://arxiv.org/abs/2206.07284)

Rui Zhang, Song Guo, Junxiao Wang, Xin Xie, Dacheng Tao


## 2022-06-13

[Towards Alternative Techniques for Improving Adversarial Robustness: Analysis of Adversarial Training at a Spectrum of Perturbations. (99%)](http://arxiv.org/abs/2206.06496)

Kaustubh Sridhar, Souradeep Dutta, Ramneet Kaur, James Weimer, Oleg Sokolsky, Insup Lee


[Distributed Adversarial Training to Robustify Deep Neural Networks at Scale. (99%)](http://arxiv.org/abs/2206.06257)

Gaoyuan Zhang, Songtao Lu, Yihua Zhang, Xiangyi Chen, Pin-Yu Chen, Quanfu Fan, Lee Martie, Lior Horesh, Mingyi Hong, Sijia Liu


[Pixel to Binary Embedding Towards Robustness for CNNs. (47%)](http://arxiv.org/abs/2206.05898)

Ikki Kishida, Hideki Nakayama


[Towards Understanding Sharpness-Aware Minimization. (1%)](http://arxiv.org/abs/2206.06232)

Maksym Andriushchenko, Nicolas Flammarion


[Efficient Human-in-the-loop System for Guiding DNNs Attention. (1%)](http://arxiv.org/abs/2206.05981)

Yi He, Xi Yang, Chia-Ming Chang, Haoran Xie, Takeo Igarashi


## 2022-06-12

[Consistent Attack: Universal Adversarial Perturbation on Embodied Vision Navigation. (98%)](http://arxiv.org/abs/2206.05751)

Chengyang Ying, You Qiaoben, Xinning Zhou, Hang Su, Wenbo Ding, Jianyong Ai


[Security of Machine Learning-Based Anomaly Detection in Cyber Physical Systems. (92%)](http://arxiv.org/abs/2206.05678)

Zahra Jadidi, Shantanu Pal, Nithesh Nayak K, Arawinkumaar Selvakkumar, Chih-Chia Chang, Maedeh Beheshti, Alireza Jolfaei


[Darknet Traffic Classification and Adversarial Attacks. (81%)](http://arxiv.org/abs/2206.06371)

Nhien Rust-Nguyen, Mark Stamp


[InBiaseD: Inductive Bias Distillation to Improve Generalization and Robustness through Shape-awareness. (26%)](http://arxiv.org/abs/2206.05846)

Shruthi Gowda, Bahram Zonooz, Elahe Arani


[RSSD: Defend against Ransomware with Hardware-Isolated Network-Storage Codesign and Post-Attack Analysis. (9%)](http://arxiv.org/abs/2206.05821)

Benjamin Reidys, Peng Liu, Jian Huang


[Neurotoxin: Durable Backdoors in Federated Learning. (5%)](http://arxiv.org/abs/2206.10341)

Zhengming Zhang, Ashwinee Panda, Linyue Song, Yaoqing Yang, Michael W. Mahoney, Joseph E. Gonzalez, Kannan Ramchandran, Prateek Mittal


[An Efficient Method for Sample Adversarial Perturbations against Nonlinear Support Vector Machines. (4%)](http://arxiv.org/abs/2206.05664)

Wen Su, Qingna Li


## 2022-06-11

[Improving the Adversarial Robustness of NLP Models by Information Bottleneck. (99%)](http://arxiv.org/abs/2206.05511)

Cenyuan Zhang, Xiang Zhou, Yixin Wan, Xiaoqing Zheng, Kai-Wei Chang, Cho-Jui Hsieh


[Defending Adversarial Examples by Negative Correlation Ensemble. (99%)](http://arxiv.org/abs/2206.10334)

Wenjian Luo, Hongwei Zhang, Linghao Kong, Zhijian Chen, Ke Tang


[NeuGuard: Lightweight Neuron-Guided Defense against Membership Inference Attacks. (81%)](http://arxiv.org/abs/2206.05565)

Nuo Xu, Binghui Wang, Ran Ran, Wujie Wen, Parv Venkitasubramaniam


[Bilateral Dependency Optimization: Defending Against Model-inversion Attacks. (69%)](http://arxiv.org/abs/2206.05483)

Xiong Peng, Feng Liu, Jingfen Zhang, Long Lan, Junjie Ye, Tongliang Liu, Bo Han


## 2022-06-10

[Localized adversarial artifacts for compressed sensing MRI. (76%)](http://arxiv.org/abs/2206.05289)

Rima Alaifari, Giovanni S. Alberti, Tandri Gauksson


[Rethinking the Defense Against Free-rider Attack From the Perspective of Model Weight Evolving Frequency. (70%)](http://arxiv.org/abs/2206.05406)

Jinyin Chen, Mingjun Li, Tao Liu, Haibin Zheng, Yao Cheng, Changting Lin


[Blades: A Simulator for Attacks and Defenses in Federated Learning. (33%)](http://arxiv.org/abs/2206.05359)

Shenghui Li, Li Ju, Tianru Zhang, Edith Ngai, Thiemo Voigt


[Enhancing Clean Label Backdoor Attack with Two-phase Specific Triggers. (9%)](http://arxiv.org/abs/2206.04881)

Nan Luo, Yuanzhang Li, Yajie Wang, Shangbo Wu, Yu-an Tan, Quanxin Zhang


[Deep Leakage from Model in Federated Learning. (3%)](http://arxiv.org/abs/2206.04887)

Zihao Zhao, Mengen Luo, Wenbo Ding


[Adversarial Counterfactual Environment Model Learning. (1%)](http://arxiv.org/abs/2206.04890)

Xiong-Hui Chen, Yang Yu, Zheng-Mao Zhu, Zhihua Yu, Zhenjun Chen, Chenghe Wang, Yinan Wu, Hongqiu Wu, Rong-Jun Qin, Ruijin Ding, Fangsheng Huang


## 2022-06-09

[CARLA-GeAR: a Dataset Generator for a Systematic Evaluation of Adversarial Robustness of Vision Models. (99%)](http://arxiv.org/abs/2206.04365)

Federico Nesti, Giulio Rossolini, Gianluca D'Amico, Alessandro Biondi, Giorgio Buttazzo


[ReFace: Real-time Adversarial Attacks on Face Recognition Systems. (99%)](http://arxiv.org/abs/2206.04783)

Shehzeen Hussain, Todd Huster, Chris Mesterharm, Paarth Neekhara, Kevin An, Malhar Jere, Harshvardhan Sikka, Farinaz Koushanfar


[Adversarial Noises Are Linearly Separable for (Nearly) Random Neural Networks. (98%)](http://arxiv.org/abs/2206.04316)

Huishuai Zhang, Da Yu, Yiping Lu, Di He


[Meet You Halfway: Explaining Deep Learning Mysteries. (92%)](http://arxiv.org/abs/2206.04463)

Oriel BenShmuel


[Early Transferability of Adversarial Examples in Deep Neural Networks. (86%)](http://arxiv.org/abs/2206.04472)

Oriel BenShmuel


[GSmooth: Certified Robustness against Semantic Transformations via Generalized Randomized Smoothing. (86%)](http://arxiv.org/abs/2206.04310)

Zhongkai Hao, Chengyang Ying, Yinpeng Dong, Hang Su, Jun Zhu, Jian Song


[Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. (84%)](http://arxiv.org/abs/2206.04615)

Aarohi Shammie Srivastava, Abhinav Shammie Rastogi, Abhishek Shammie Rao, Abu Awal Md Shammie Shoeb, Abubakar Shammie Abid, Adam Shammie Fisch, Adam R. Shammie Brown, Adam Shammie Santoro, Aditya Shammie Gupta, Adrià Shammie Garriga-Alonso, Agnieszka Shammie Kluska, Aitor Shammie Lewkowycz, Akshat Shammie Agarwal, Alethea Shammie Power, Alex Shammie Ray, Alex Shammie Warstadt, Alexander W. Shammie Kocurek, Ali Shammie Safaya, Ali Shammie Tazarv, Alice Shammie Xiang, Alicia Shammie Parrish, Allen Shammie Nie, Aman Shammie Hussain, Amanda Shammie Askell, Amanda Shammie Dsouza, Ambrose Shammie Slone, Ameet Shammie Rahane, Anantharaman S. Shammie Iyer, Anders Shammie Andreassen, Andrea Shammie Madotto, Andrea Shammie Santilli, Andreas Shammie Stuhlmüller, Andrew Shammie Dai, Andrew Shammie La, Andrew Shammie Lampinen, Andy Shammie Zou, Angela Shammie Jiang, Angelica Shammie Chen, Anh Shammie Vuong, Animesh Shammie Gupta, Anna Shammie Gottardi, Antonio Shammie Norelli, Anu Shammie Venkatesh, Arash Shammie Gholamidavoodi, Arfa Shammie Tabassum, Arul Shammie Menezes, Arun Shammie Kirubarajan, Asher Shammie Mullokandov, Ashish Shammie Sabharwal, Austin Shammie Herrick, Avia Shammie Efrat, Aykut Shammie Erdem, Ayla Shammie Karakaş, B. Ryan Shammie Roberts, Bao Sheng Shammie Loe, Barret Shammie Zoph, Bartłomiej Shammie Bojanowski, Batuhan Shammie Özyurt, Behnam Shammie Hedayatnia, Behnam Shammie Neyshabur, Benjamin Shammie Inden, Benno Shammie Stein, Berk Shammie Ekmekci, Bill Yuchen Shammie Lin, Blake Shammie Howald, Cameron Shammie Diao, Cameron Shammie Dour, Catherine Shammie Stinson, Cedrick Shammie Argueta, César Ferri Shammie Ramírez, Chandan Shammie Singh, Charles Shammie Rathkopf, Chenlin Shammie Meng, Chitta Shammie Baral, Chiyu Shammie Wu, Chris Shammie Callison-Burch, Chris Shammie Waites, Christian Shammie Voigt, Christopher D. Shammie Manning, Christopher Shammie Potts, Cindy Shammie Ramirez, Clara E. Shammie Rivera, Clemencia Shammie Siro, Colin Shammie Raffel, Courtney Shammie Ashcraft, Cristina Shammie Garbacea, Damien Shammie Sileo, Dan Shammie Garrette, Dan Shammie Hendrycks, Dan Shammie Kilman, Dan Shammie Roth, Daniel Shammie Freeman, Daniel Shammie Khashabi, Daniel Shammie Levy, Daniel Moseguí Shammie González, Danielle Shammie Perszyk, Danny Shammie Hernandez, Danqi Shammie Chen, Daphne Shammie Ippolito, Dar Shammie Gilboa, David Shammie Dohan, David Shammie Drakard, David Shammie Jurgens, Debajyoti Shammie Datta, Deep Shammie Ganguli, Denis Shammie Emelin, Denis Shammie Kleyko, Deniz Shammie Yuret, Derek Shammie Chen, Derek Shammie Tam, Dieuwke Shammie Hupkes, Diganta Shammie Misra, Dilyar Shammie Buzan, Dimitri Coelho Shammie Mollo, Diyi Shammie Yang, Dong-Ho Shammie Lee, Ekaterina Shammie Shutova, Ekin Dogus Shammie Cubuk, Elad Shammie Segal, Eleanor Shammie Hagerman, Elizabeth Shammie Barnes, Elizabeth Shammie Donoway, Ellie Shammie Pavlick, Emanuele Shammie Rodola, Emma Shammie Lam, Eric Shammie Chu, Eric Shammie Tang, Erkut Shammie Erdem, Ernie Shammie Chang, Ethan A. Shammie Chi, Ethan Shammie Dyer, Ethan Shammie Jerzak, Ethan Shammie Kim, Eunice Engefu Shammie Manyasi, Evgenii Shammie Zheltonozhskii, Fanyue Shammie Xia, Fatemeh Shammie Siar, Fernando Shammie Martínez-Plumed, Francesca Shammie Happé, Francois Shammie Chollet, Frieda Shammie Rong, Gaurav Shammie Mishra, Genta Indra Shammie Winata, Melo Gerard Shammie de, Germán Shammie Kruszewski, Giambattista Shammie Parascandolo, Giorgio Shammie Mariani, Gloria Shammie Wang, Gonzalo Shammie Jaimovitch-López, Gregor Shammie Betz, Guy Shammie Gur-Ari, Hana Shammie Galijasevic, Hannah Shammie Kim, Hannah Shammie Rashkin, Hannaneh Shammie Hajishirzi, Harsh Shammie Mehta, Hayden Shammie Bogar, Henry Shammie Shevlin, Hinrich Shammie Schütze, Hiromu Shammie Yakura, Hongming Shammie Zhang, Hugh Mee Shammie Wong, Ian Shammie Ng, Isaac Shammie Noble, Jaap Shammie Jumelet, Jack Shammie Geissinger, Jackson Shammie Kernion, Jacob Shammie Hilton, Jaehoon Shammie Lee, Jaime Fernández Shammie Fisac, James B. Shammie Simon, James Shammie Koppel, James Shammie Zheng, James Shammie Zou, Jan Shammie Kocoń, Jana Shammie Thompson, Jared Shammie Kaplan, Jarema Shammie Radom, Jascha Shammie Sohl-Dickstein, Jason Shammie Phang, Jason Shammie Wei, Jason Shammie Yosinski, Jekaterina Shammie Novikova, Jelle Shammie Bosscher, Jennifer Shammie Marsh, Jeremy Shammie Kim, Jeroen Shammie Taal, Jesse Shammie Engel, Jesujoba Shammie Alabi, Jiacheng Shammie Xu, Jiaming Shammie Song, Jillian Shammie Tang, Joan Shammie Waweru, John Shammie Burden, John Shammie Miller, John U. Shammie Balis, Jonathan Shammie Berant, Jörg Shammie Frohberg, Jos Shammie Rozen, Jose Shammie Hernandez-Orallo, Joseph Shammie Boudeman, Joseph Shammie Jones, Joshua B. Shammie Tenenbaum, Joshua S. Shammie Rule, Joyce Shammie Chua, Kamil Shammie Kanclerz, Karen Shammie Livescu, Karl Shammie Krauth, Karthik Shammie Gopalakrishnan, Katerina Shammie Ignatyeva, Katja Shammie Markert, Kaustubh D. Shammie Dhole, Kevin Shammie Gimpel, Kevin Shammie Omondi, Kory Shammie Mathewson, Kristen Shammie Chiafullo, Ksenia Shammie Shkaruta, Kumar Shammie Shridhar, Kyle Shammie McDonell, Kyle Shammie Richardson, Laria Shammie Reynolds, Leo Shammie Gao, Li Shammie Zhang, Liam Shammie Dugan, Lianhui Shammie Qin, Lidia Shammie Contreras-Ochando, Louis-Philippe Shammie Morency, Luca Shammie Moschella, Lucas Shammie Lam, Lucy Shammie Noble, Ludwig Shammie Schmidt, Luheng Shammie He, Luis Oliveros Shammie Colón, Luke Shammie Metz, Lütfi Kerem Shammie Şenel, Maarten Shammie Bosma, Maarten Shammie Sap, Hoeve Maartje Shammie ter, Maheen Shammie Farooqi, Manaal Shammie Faruqui, Mantas Shammie Mazeika, Marco Shammie Baturan, Marco Shammie Marelli, Marco Shammie Maru, Maria Jose Ramírez Shammie Quintana, Marie Shammie Tolkiehn, Mario Shammie Giulianelli, Martha Shammie Lewis, Martin Shammie Potthast, Matthew L. Shammie Leavitt, Matthias Shammie Hagen, Mátyás Shammie Schubert, Medina Orduna Shammie Baitemirova, Melody Shammie Arnaud, Melvin Shammie McElrath, Michael A. Shammie Yee, Michael Shammie Cohen, Michael Shammie Gu, Michael Shammie Ivanitskiy, Michael Shammie Starritt, Michael Shammie Strube, Michał Shammie Swędrowski, Michele Shammie Bevilacqua, Michihiro Shammie Yasunaga, Mihir Shammie Kale, Mike Shammie Cain, Mimee Shammie Xu, Mirac Shammie Suzgun, Mo Shammie Tiwari, Mohit Shammie Bansal, Moin Shammie Aminnaseri, Mor Shammie Geva, Mozhdeh Shammie Gheini, Mukund Varma Shammie T, Nanyun Shammie Peng, Nathan Shammie Chi, Nayeon Shammie Lee, Neta Gur-Ari Shammie Krakover, Nicholas Shammie Cameron, Nicholas Shammie Roberts, Nick Shammie Doiron, Nikita Shammie Nangia, Niklas Shammie Deckers, Niklas Shammie Muennighoff, Nitish Shirish Shammie Keskar, Niveditha S. Shammie Iyer, Noah Shammie Constant, Noah Shammie Fiedel, Nuan Shammie Wen, Oliver Shammie Zhang, Omar Shammie Agha, Omar Shammie Elbaghdadi, Omer Shammie Levy, Owain Shammie Evans, Pablo Antonio Moreno Shammie Casares, Parth Shammie Doshi, Pascale Shammie Fung, Paul Pu Shammie Liang, Paul Shammie Vicol, Pegah Shammie Alipoormolabashi, Peiyuan Shammie Liao, Percy Shammie Liang, Peter Shammie Chang, Peter Shammie Eckersley, Phu Mon Shammie Htut, Pinyu Shammie Hwang, Piotr Shammie Miłkowski, Piyush Shammie Patil, Pouya Shammie Pezeshkpour, Priti Shammie Oli, Qiaozhu Shammie Mei, Qing Shammie Lyu, Qinlang Shammie Chen, Rabin Shammie Banjade, Rachel Etta Shammie Rudolph, Raefer Shammie Gabriel, Rahel Shammie Habacker, Ramón Risco Shammie Delgado, Raphaël Shammie Millière, Rhythm Shammie Garg, Richard Shammie Barnes, Rif A. Shammie Saurous, Riku Shammie Arakawa, Robbe Shammie Raymaekers, Robert Shammie Frank, Rohan Shammie Sikand, Roman Shammie Novak, Roman Shammie Sitelew, Ronan Shammie LeBras, Rosanne Shammie Liu, Rowan Shammie Jacobs, Rui Shammie Zhang, Ruslan Shammie Salakhutdinov, Ryan Shammie Chi, Ryan Shammie Lee, Ryan Shammie Stovall, Ryan Shammie Teehan, Rylan Shammie Yang, Sahib Shammie Singh, Saif M. Shammie Mohammad, Sajant Shammie Anand, Sam Shammie Dillavou, Sam Shammie Shleifer, Sam Shammie Wiseman, Samuel Shammie Gruetter, Samuel R. Shammie Bowman, Samuel S. Shammie Schoenholz, Sanghyun Shammie Han, Sanjeev Shammie Kwatra, Sarah A. Shammie Rous, Sarik Shammie Ghazarian, Sayan Shammie Ghosh, Sean Shammie Casey, Sebastian Shammie Bischoff, Sebastian Shammie Gehrmann, Sebastian Shammie Schuster, Sepideh Shammie Sadeghi, Shadi Shammie Hamdan, Sharon Shammie Zhou, Shashank Shammie Srivastava, Sherry Shammie Shi, Shikhar Shammie Singh, Shima Shammie Asaadi, Shixiang Shane Shammie Gu, Shubh Shammie Pachchigar, Shubham Shammie Toshniwal, Shyam Shammie Upadhyay, Shammie Shyamolima, Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven T. Piantadosi, Stuart M. Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto, Te-Lin Wu, Théo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Timothy Telleen-Lawton, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, Ziyi Wu


[Data-Efficient Double-Win Lottery Tickets from Robust Pre-training. (41%)](http://arxiv.org/abs/2206.04762)

Tianlong Chen, Zhenyu Zhang, Sijia Liu, Yang Zhang, Shiyu Chang, Zhangyang Wang


[DORA: Exploring outlier representations in Deep Neural Networks. (1%)](http://arxiv.org/abs/2206.04530)

Kirill Bykov, Mayukh Deb, Dennis Grinwald, Klaus-Robert Müller, Marina M. -C. Höhne


[Membership Inference via Backdooring. (1%)](http://arxiv.org/abs/2206.04823)

Hongsheng Hu, Zoran Salcic, Gillian Dobbie, Jinjun Chen, Lichao Sun, Xuyun Zhang


## 2022-06-08

[Wavelet Regularization Benefits Adversarial Training. (99%)](http://arxiv.org/abs/2206.03727)

Jun Yan, Huilin Yin, Xiaoyang Deng, Ziming Zhao, Wancheng Ge, Hao Zhang, Gerhard Rigoll


[Latent Boundary-guided Adversarial Training. (99%)](http://arxiv.org/abs/2206.03717)

Xiaowei Zhou, Ivor W. Tsang, Jie Yin


[Adversarial Text Normalization. (73%)](http://arxiv.org/abs/2206.04137)

Joanna Bitton, Maya Pavlova, Ivan Evtimov


[Autoregressive Perturbations for Data Poisoning. (70%)](http://arxiv.org/abs/2206.03693)

Pedro Sandoval-Segura, Vasu Singla, Jonas Geiping, Micah Goldblum, Tom Goldstein, David W. Jacobs


[Toward Certified Robustness Against Real-World Distribution Shifts. (5%)](http://arxiv.org/abs/2206.03669)

Haoze Wu, Teruhiro Tagomori, Alexander Robey, Fengjun Yang, Nikolai Matni, George Pappas, Hamed Hassani, Corina Pasareanu, Clark Barrett


[Generative Adversarial Networks and Image-Based Malware Classification. (1%)](http://arxiv.org/abs/2207.00421)

Huy Nguyen, Troia Fabio Di, Genya Ishigaki, Mark Stamp


[Robust Deep Ensemble Method for Real-world Image Denoising. (1%)](http://arxiv.org/abs/2206.03691)

Pengju Liu, Hongzhi Zhang, Jinghui Wang, Yuzhi Wang, Dongwei Ren, Wangmeng Zuo


## 2022-06-07

[Fooling Explanations in Text Classifiers. (99%)](http://arxiv.org/abs/2206.03178)

Adam Ivankay, Ivan Girardi, Chiara Marchiori, Pascal Frossard


[AS2T: Arbitrary Source-To-Target Adversarial Attack on Speaker Recognition Systems. (99%)](http://arxiv.org/abs/2206.03351)

Guangke Chen, Zhe Zhao, Fu Song, Sen Chen, Lingling Fan, Yang Liu


[Towards Understanding and Mitigating Audio Adversarial Examples for Speaker Recognition. (99%)](http://arxiv.org/abs/2206.03393)

Guangke Chen, Zhe Zhao, Fu Song, Sen Chen, Lingling Fan, Feng Wang, Jiashui Wang


[Adaptive Regularization for Adversarial Training. (98%)](http://arxiv.org/abs/2206.03353)

Dongyoon Yang, Insung Kong, Yongdai Kim


[Building Robust Ensembles via Margin Boosting. (83%)](http://arxiv.org/abs/2206.03362)

Dinghuai Zhang, Hongyang Zhang, Aaron Courville, Yoshua Bengio, Pradeep Ravikumar, Arun Sai Suggala


[On the Permanence of Backdoors in Evolving Models. (67%)](http://arxiv.org/abs/2206.04677)

Huiying Li, Arjun Nitin Bhagoji, Yuxin Chen, Haitao Zheng, Ben Y. Zhao


[Subject Membership Inference Attacks in Federated Learning. (4%)](http://arxiv.org/abs/2206.03317)

Anshuman Suri, Pallika Kanani, Virendra J. Marathe, Daniel W. Peterson


[Adversarial Reprogramming Revisited. (3%)](http://arxiv.org/abs/2206.03466)

Matthias Englert, Ranko Lazic


[Certifying Data-Bias Robustness in Linear Regression. (1%)](http://arxiv.org/abs/2206.03575)

Anna P. Meyer, Aws Albarghouthi, Loris D'Antoni


[Parametric Chordal Sparsity for SDP-based Neural Network Verification. (1%)](http://arxiv.org/abs/2206.03482)

Anton Xue, Lars Lindemann, Rajeev Alur


[Can CNNs Be More Robust Than Transformers? (1%)](http://arxiv.org/abs/2206.03452)

Zeyu Wang, Yutong Bai, Yuyin Zhou, Cihang Xie


## 2022-06-06

[Robust Adversarial Attacks Detection based on Explainable Deep Reinforcement Learning For UAV Guidance and Planning. (99%)](http://arxiv.org/abs/2206.02670)

Thomas Hickling, Nabil Aouf, Phillippa Spencer


[Fast Adversarial Training with Adaptive Step Size. (98%)](http://arxiv.org/abs/2206.02417)

Zhichao Huang, Yanbo Fan, Chen Liu, Weizhong Zhang, Yong Zhang, Mathieu Salzmann, Sabine Süsstrunk, Jue Wang


[Certified Robustness in Federated Learning. (87%)](http://arxiv.org/abs/2206.02535)

Motasem Alfarra, Juan C. Pérez, Egor Shulgin, Peter Richtárik, Bernard Ghanem


[Robust Image Protection Countering Cropping Manipulation. (12%)](http://arxiv.org/abs/2206.02405)

Qichao Ying, Hang Zhou, Zhenxing Qian, Sheng Li, Xinpeng Zhang


[PCPT and ACPT: Copyright Protection and Traceability Scheme for DNN Model. (3%)](http://arxiv.org/abs/2206.02541)

Xuefeng Fan, Hangyu Gui, Xiaoyi Zhou


[Tackling covariate shift with node-based Bayesian neural networks. (1%)](http://arxiv.org/abs/2206.02435)

Trung Trinh, Markus Heinonen, Luigi Acerbi, Samuel Kaski


[Anomaly Detection with Test Time Augmentation and Consistency Evaluation. (1%)](http://arxiv.org/abs/2206.02345)

Haowei He, Jiaye Teng, Yang Yuan


## 2022-06-05

[Federated Adversarial Training with Transformers. (98%)](http://arxiv.org/abs/2206.02131)

Ahmed Aldahdooh, Wassim Hamidouche, Olivier Déforges


[Vanilla Feature Distillation for Improving the Accuracy-Robustness Trade-Off in Adversarial Training. (98%)](http://arxiv.org/abs/2206.02158)

Guodong Cao, Zhibo Wang, Xiaowei Dong, Zhifei Zhang, Hengchang Guo, Zhan Qin, Kui Ren


[Which models are innately best at uncertainty estimation? (1%)](http://arxiv.org/abs/2206.02152)

Ido Galil, Mohammed Dabbah, Ran El-Yaniv


## 2022-06-04

[Soft Adversarial Training Can Retain Natural Accuracy. (76%)](http://arxiv.org/abs/2206.01904)

Abhijith Sharma, Apurva Narayan


## 2022-06-03

[Saliency Attack: Towards Imperceptible Black-box Adversarial Attack. (99%)](http://arxiv.org/abs/2206.01898)

Zeyu Dai, Shengcai Liu, Ke Tang, Qing Li


[Towards Evading the Limits of Randomized Smoothing: A Theoretical Analysis. (96%)](http://arxiv.org/abs/2206.01715)

Raphael Ettedgui, Alexandre Araujo, Rafael Pinot, Yann Chevaleyre, Jamal Atif


[Evaluating Transfer-based Targeted Adversarial Perturbations against Real-World Computer Vision Systems based on Human Judgments. (92%)](http://arxiv.org/abs/2206.01467)

Zhengyu Zhao, Nga Dang, Martha Larson


[A Robust Backpropagation-Free Framework for Images. (80%)](http://arxiv.org/abs/2206.01820)

Timothy Zee, Alexander G. Ororbia, Ankur Mali, Ifeoma Nwogu


[Gradient Obfuscation Checklist Test Gives a False Sense of Security. (73%)](http://arxiv.org/abs/2206.01705)

Nikola Popovic, Danda Pani Paudel, Thomas Probst, Gool Luc Van


[Kallima: A Clean-label Framework for Textual Backdoor Attacks. (26%)](http://arxiv.org/abs/2206.01832)

Xiaoyi Chen, Yinpeng Dong, Zeyu Sun, Shengfang Zhai, Qingni Shen, Zhonghai Wu


## 2022-06-02

[Improving the Robustness and Generalization of Deep Neural Network with Confidence Threshold Reduction. (99%)](http://arxiv.org/abs/2206.00913)

Xiangyuan Yang, Jie Lin, Hanlin Zhang, Xinyu Yang, Peng Zhao


[FACM: Intermediate Layer Still Retain Effective Features against Adversarial Examples. (99%)](http://arxiv.org/abs/2206.00924)

Xiangyuan Yang, Jie Lin, Hanlin Zhang, Xinyu Yang, Peng Zhao


[Adaptive Adversarial Training to Improve Adversarial Robustness of DNNs for Medical Image Segmentation and Detection. (99%)](http://arxiv.org/abs/2206.01736)

Linhai Ma, Liang Liang


[Adversarial RAW: Image-Scaling Attack Against Imaging Pipeline. (99%)](http://arxiv.org/abs/2206.01733)

Junjian Li, Honglong Chen


[Adversarial Laser Spot: Robust and Covert Physical Adversarial Attack to DNNs. (98%)](http://arxiv.org/abs/2206.01034)

Chengyin Hu


[Adversarial Unlearning: Reducing Confidence Along Adversarial Directions. (31%)](http://arxiv.org/abs/2206.01367)

Amrith Setlur, Benjamin Eysenbach, Virginia Smith, Sergey Levine


[MaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation. (8%)](http://arxiv.org/abs/2206.01737)

Chen Chen, Zeju Li, Cheng Ouyang, Matt Sinclair, Wenjia Bai, Daniel Rueckert


[A temporal chrominance trigger for clean-label backdoor attack against anti-spoof rebroadcast detection. (4%)](http://arxiv.org/abs/2206.01102)

Wei Guo, Benedetta Tondi, Mauro Barni


[Learning Unbiased Transferability for Domain Adaptation by Uncertainty Modeling. (1%)](http://arxiv.org/abs/2206.01319)

Jian Hu, Haowen Zhong, Junchi Yan, Shaogang Gong, Guile Wu, Fei Yang


## 2022-06-01

[On the reversibility of adversarial attacks. (99%)](http://arxiv.org/abs/2206.00772)

Chau Yi Li, Ricardo Sánchez-Matilla, Ali Shahin Shamsabadi, Riccardo Mazzon, Andrea Cavallaro


[NeuroUnlock: Unlocking the Architecture of Obfuscated Deep Neural Networks. (99%)](http://arxiv.org/abs/2206.00402)

Mahya Morid Ahmadi, Lilas Alrahis, Alessio Colucci, Ozgur Sinanoglu, Muhammad Shafique


[Attack-Agnostic Adversarial Detection. (99%)](http://arxiv.org/abs/2206.00489)

Jiaxin Cheng, Mohamed Hussein, Jay Billa, Wael AbdAlmageed


[On the Perils of Cascading Robust Classifiers. (98%)](http://arxiv.org/abs/2206.00278)

Ravi Mangal, Zifan Wang, Chi Zhang, Klas Leino, Corina Pasareanu, Matt Fredrikson


[Anti-Forgery: Towards a Stealthy and Robust DeepFake Disruption Attack via Adversarial Perceptual-aware Perturbations. (98%)](http://arxiv.org/abs/2206.00477)

Run Wang, Ziheng Huang, Zhikai Chen, Li Liu, Jing Chen, Lina Wang


[Support Vector Machines under Adversarial Label Contamination. (97%)](http://arxiv.org/abs/2206.00352)

Huang Xiao, Battista Biggio, Blaine Nelson, Han Xiao, Claudia Eckert, Fabio Roli


[Defense Against Gradient Leakage Attacks via Learning to Obscure Data. (80%)](http://arxiv.org/abs/2206.00769)

Yuxuan Wan, Han Xu, Xiaorui Liu, Jie Ren, Wenqi Fan, Jiliang Tang


[The robust way to stack and bag: the local Lipschitz way. (70%)](http://arxiv.org/abs/2206.00513)

Thulasi Tholeti, Sheetal Kalyani


[Robustness Evaluation and Adversarial Training of an Instance Segmentation Model. (54%)](http://arxiv.org/abs/2206.02539)

Jacob Bond, Andrew Lingg


[RoCourseNet: Distributionally Robust Training of a Prediction Aware Recourse Model. (1%)](http://arxiv.org/abs/2206.00700)

Hangzhi Guo, Feiran Jia, Jinghui Chen, Anna Squicciarini, Amulya Yadav


## 2022-05-31

[Hide and Seek: on the Stealthiness of Attacks against Deep Learning Systems. (99%)](http://arxiv.org/abs/2205.15944)

Zeyan Liu, Fengjun Li, Jingqiang Lin, Zhu Li, Bo Luo


[Exact Feature Collisions in Neural Networks. (95%)](http://arxiv.org/abs/2205.15763)

Utku Ozbulak, Manvel Gasparyan, Shodhan Rao, Neve Wesley De, Messem Arnout Van


[CodeAttack: Code-based Adversarial Attacks for Pre-Trained Programming Language Models. (93%)](http://arxiv.org/abs/2206.00052)

Akshita Jha, Chandan K. Reddy


[CASSOCK: Viable Backdoor Attacks against DNN in The Wall of Source-Specific Backdoor Defences. (83%)](http://arxiv.org/abs/2206.00145)

Shang Wang, Yansong Gao, Anmin Fu, Zhi Zhang, Yuqing Zhang, Willy Susilo


[Semantic Autoencoder and Its Potential Usage for Adversarial Attack. (81%)](http://arxiv.org/abs/2205.15592)

Yurui Ming, Cuihuan Du, Chin-Teng Lin


[An Effective Fusion Method to Enhance the Robustness of CNN. (80%)](http://arxiv.org/abs/2205.15582)

Yating Ma, Zhichao Lian


[Order-sensitive Shapley Values for Evaluating Conceptual Soundness of NLP Models. (64%)](http://arxiv.org/abs/2206.00192)

Kaiji Lu, Anupam Datta


[Generative Models with Information-Theoretic Protection Against Membership Inference Attacks. (10%)](http://arxiv.org/abs/2206.00071)

Parisa Hassanzadeh, Robert E. Tillman


[Likelihood-Free Inference with Generative Neural Networks via Scoring Rule Minimization. (1%)](http://arxiv.org/abs/2205.15784)

Lorenzo Pacchiardi, Ritabrata Dutta


## 2022-05-30

[Domain Constraints in Feature Space: Strengthening Robustness of Android Malware Detection against Realizable Adversarial Examples. (99%)](http://arxiv.org/abs/2205.15128)

Hamid Bostani, Zhuoran Liu, Zhengyu Zhao, Veelasha Moonsamy


[Searching for the Essence of Adversarial Perturbations. (99%)](http://arxiv.org/abs/2205.15357)

Dennis Y. Menn, Tzu-hsun Feng, Hung-yi Lee


[Exposing Fine-Grained Adversarial Vulnerability of Face Anti-Spoofing Models. (99%)](http://arxiv.org/abs/2205.14851)

Songlin Yang, Wei Wang, Chenye Xu, Ziwen He, Bo Peng, Jing Dong


[Guided Diffusion Model for Adversarial Purification. (99%)](http://arxiv.org/abs/2205.14969)

Jinyi Wang, Zhaoyang Lyu, Dahua Lin, Bo Dai, Hongfei Fu


[Why Adversarial Training of ReLU Networks Is Difficult? (68%)](http://arxiv.org/abs/2205.15130)

Xu Cheng, Hao Zhang, Yue Xin, Wen Shen, Jie Ren, Quanshi Zhang


[CalFAT: Calibrated Federated Adversarial Training with Label Skewness. (67%)](http://arxiv.org/abs/2205.14926)

Chen Chen, Yuchen Liu, Xingjun Ma, Lingjuan Lyu


[Securing AI-based Healthcare Systems using Blockchain Technology: A State-of-the-Art Systematic Literature Review and Future Research Directions. (15%)](http://arxiv.org/abs/2206.04793)

Rucha Shinde, Shruti Patil, Ketan Kotecha, Vidyasagar Potdar, Ganeshsree Selvachandran, Ajith Abraham


[Efficient Reward Poisoning Attacks on Online Deep Reinforcement Learning. (13%)](http://arxiv.org/abs/2205.14842)

Yinglun Xu, Qi Zeng, Gagandeep Singh


[White-box Membership Attack Against Machine Learning Based Retinopathy Classification. (10%)](http://arxiv.org/abs/2206.03584)

Mounia Hamidouche, Reda Bellafqira, Gwenolé Quellec, Gouenou Coatrieux


[Fool SHAP with Stealthily Biased Sampling. (2%)](http://arxiv.org/abs/2205.15419)

Gabriel Laberge, Ulrich Aïvodji, Satoshi Hara, Mario Marchand., Foutse Khomh


[Snoopy: A Webpage Fingerprinting Framework with Finite Query Model for Mass-Surveillance. (2%)](http://arxiv.org/abs/2205.15037)

Gargi Mitra, Prasanna Karthik Vairam, Sandip Saha, Nitin Chandrachoodan, V. Kamakoti


## 2022-05-29

[Robust Weight Perturbation for Adversarial Training. (99%)](http://arxiv.org/abs/2205.14826)

Chaojian Yu, Bo Han, Mingming Gong, Li Shen, Shiming Ge, Bo Du, Tongliang Liu


[Mixture GAN For Modulation Classification Resiliency Against Adversarial Attacks. (99%)](http://arxiv.org/abs/2205.15743)

Eyad Shtaiwi, Ahmed El Ouadrhiri, Majid Moradikia, Salma Sultana, Ahmed Abdelhadi, Zhu Han


[Unfooling Perturbation-Based Post Hoc Explainers. (98%)](http://arxiv.org/abs/2205.14772)

Zachariah Carmichael, Walter J Scheirer


[On the Robustness of Safe Reinforcement Learning under Observational Perturbations. (93%)](http://arxiv.org/abs/2205.14691)

Zuxin Liu, Zijian Guo, Zhepeng Cen, Huan Zhang, Jie Tan, Bo Li, Ding Zhao


[Superclass Adversarial Attack. (80%)](http://arxiv.org/abs/2205.14629)

Soichiro Kumano, Hiroshi Kera, Toshihiko Yamasaki


[Problem-Space Evasion Attacks in the Android OS: a Survey. (50%)](http://arxiv.org/abs/2205.14576)

Harel Berger, Chen Hajaj, Amit Dvir


[Context-based Virtual Adversarial Training for Text Classification with Noisy Labels. (11%)](http://arxiv.org/abs/2206.11851)

Do-Myoung Lee, Yeachan Kim, Chang-gyun Seo


[A General Multiple Data Augmentation Based Framework for Training Deep Neural Networks. (1%)](http://arxiv.org/abs/2205.14606)

Binyan Hu, Yu Sun, A. K. Qin


## 2022-05-28

[Contributor-Aware Defenses Against Adversarial Backdoor Attacks. (98%)](http://arxiv.org/abs/2206.03583)

Glenn Dawson, Muhammad Umer, Robi Polikar


[BadDet: Backdoor Attacks on Object Detection. (92%)](http://arxiv.org/abs/2205.14497)

Shih-Han Chan, Yinpeng Dong, Jun Zhu, Xiaolu Zhang, Jun Zhou


[Syntax-Guided Program Reduction for Understanding Neural Code Intelligence Models. (62%)](http://arxiv.org/abs/2205.14374)

Md Rafiqul Islam Rabin, Aftab Hussain, Mohammad Amin Alipour


## 2022-05-27

[fakeWeather: Adversarial Attacks for Deep Neural Networks Emulating Weather Conditions on the Camera Lens of Autonomous Systems. (96%)](http://arxiv.org/abs/2205.13807)

Alberto Marchisio, Giovanni Caramia, Maurizio Martina, Muhammad Shafique


[Why Robust Generalization in Deep Learning is Difficult: Perspective of Expressive Power. (95%)](http://arxiv.org/abs/2205.13863)

Binghui Li, Jikai Jin, Han Zhong, John E. Hopcroft, Liwei Wang


[Semi-supervised Semantics-guided Adversarial Training for Trajectory Prediction. (93%)](http://arxiv.org/abs/2205.14230)

Ruochen Jiao, Xiangguo Liu, Takami Sato, Qi Alfred Chen, Qi Zhu


[Defending Against Stealthy Backdoor Attacks. (73%)](http://arxiv.org/abs/2205.14246)

Sangeet Sagar, Abhinav Bhatt, Abhijith Srinivas Bidaralli


[EvenNet: Ignoring Odd-Hop Neighbors Improves Robustness of Graph Neural Networks. (13%)](http://arxiv.org/abs/2205.13892)

Runlin Lei, Zhen Wang, Yaliang Li, Bolin Ding, Zhewei Wei


## 2022-05-26

[A Physical-World Adversarial Attack Against 3D Face Recognition. (99%)](http://arxiv.org/abs/2205.13412)

Yanjie Li, Yiquan Li, Bin Xiao


[Transferable Adversarial Attack based on Integrated Gradients. (99%)](http://arxiv.org/abs/2205.13152)

Yi Huang, Adams Wai-Kin Kong


[MALICE: Manipulation Attacks on Learned Image ComprEssion. (99%)](http://arxiv.org/abs/2205.13253)

Kang Liu, Di Wu, Yiru Wang, Dan Feng, Benjamin Tan, Siddharth Garg


[Phantom Sponges: Exploiting Non-Maximum Suppression to Attack Deep Object Detectors. (98%)](http://arxiv.org/abs/2205.13618)

Avishag Shapira, Alon Zolfi, Luca Demetrio, Battista Biggio, Asaf Shabtai


[Circumventing Backdoor Defenses That Are Based on Latent Separability. (96%)](http://arxiv.org/abs/2205.13613)

Xiangyu Qi, Tinghao Xie, Yiming Li, Saeed Mahloujifar, Prateek Mittal


[An Analytic Framework for Robust Training of Artificial Neural Networks. (93%)](http://arxiv.org/abs/2205.13502)

Ramin Barati, Reza Safabakhsh, Mohammad Rahmati


[Adversarial attacks and defenses in Speaker Recognition Systems: A survey. (81%)](http://arxiv.org/abs/2205.13685)

Jiahe Lan, Rui Zhang, Zheng Yan, Jie Wang, Yu Chen, Ronghui Hou


[PerDoor: Persistent Non-Uniform Backdoors in Federated Learning using Adversarial Perturbations. (81%)](http://arxiv.org/abs/2205.13523)

Manaar Alam, Esha Sarkar, Michail Maniatakos


[BppAttack: Stealthy and Efficient Trojan Attacks against Deep Neural Networks via Image Quantization and Contrastive Adversarial Learning. (81%)](http://arxiv.org/abs/2205.13383)

Zhenting Wang, Juan Zhai, Shiqing Ma


[R-HTDetector: Robust Hardware-Trojan Detection Based on Adversarial Training. (80%)](http://arxiv.org/abs/2205.13702)

Kento Hasegawa, Seira Hidano, Kohei Nozawa, Shinsaku Kiyomoto, Nozomu Togawa


[BagFlip: A Certified Defense against Data Poisoning. (75%)](http://arxiv.org/abs/2205.13634)

Yuhao Zhang, Aws Albarghouthi, Loris D'Antoni


[Towards A Proactive ML Approach for Detecting Backdoor Poison Samples. (67%)](http://arxiv.org/abs/2205.13616)

Xiangyu Qi, Tinghao Xie, Jiachen T. Wang, Tong Wu, Saeed Mahloujifar, Prateek Mittal


[Membership Inference Attack Using Self Influence Functions. (45%)](http://arxiv.org/abs/2205.13680)

Gilad Cohen, Raja Giryes


[MemeTector: Enforcing deep focus for meme detection. (1%)](http://arxiv.org/abs/2205.13268)

Christos Koutlis, Manos Schinas, Symeon Papadopoulos


[ES-GNN: Generalizing Graph Neural Networks Beyond Homophily with Edge Splitting. (1%)](http://arxiv.org/abs/2205.13700)

Jingwei Guo, Kaizhu Huang, Rui Zhang, Xinping Yi


## 2022-05-25

[Surprises in adversarially-trained linear regression. (87%)](http://arxiv.org/abs/2205.12695)

Antônio H. Ribeiro, Dave Zachariah, Thomas B. Schön


[BITE: Textual Backdoor Attacks with Iterative Trigger Injection. (75%)](http://arxiv.org/abs/2205.12700)

Jun Yan, Vansh Gupta, Xiang Ren


[How explainable are adversarially-robust CNNs? (8%)](http://arxiv.org/abs/2205.13042)

Mehdi Nourelahi, Lars Kotthoff, Peijie Chen, Anh Nguyen


## 2022-05-24

[Defending a Music Recommender Against Hubness-Based Adversarial Attacks. (99%)](http://arxiv.org/abs/2205.12032)

Katharina Hoedt, Arthur Flexer, Gerhard Widmer


[Adversarial Attack on Attackers: Post-Process to Mitigate Black-Box Score-Based Query Attacks. (99%)](http://arxiv.org/abs/2205.12134)

Sizhe Chen, Zhehao Huang, Qinghua Tao, Yingwen Wu, Cihang Xie, Xiaolin Huang


[Certified Robustness Against Natural Language Attacks by Causal Intervention. (98%)](http://arxiv.org/abs/2205.12331)

Haiteng Zhao, Chang Ma, Xinshuai Dong, Anh Tuan Luu, Zhi-Hong Deng, Hanwang Zhang


[One-Pixel Shortcut: on the Learning Preference of Deep Neural Networks. (92%)](http://arxiv.org/abs/2205.12141)

Shutong Wu, Sizhe Chen, Cihang Xie, Xiaolin Huang


[Fine-grained Poisoning Attacks to Local Differential Privacy Protocols for Mean and Variance Estimation. (64%)](http://arxiv.org/abs/2205.11782)

Xiaoguang Li, Neil Zhenqiang Gong, Ninghui Li, Wenhai Sun, Hui Li


[WeDef: Weakly Supervised Backdoor Defense for Text Classification. (56%)](http://arxiv.org/abs/2205.11803)

Lesheng Jin, Zihan Wang, Jingbo Shang


[Recipe2Vec: Multi-modal Recipe Representation Learning with Graph Neural Networks. (50%)](http://arxiv.org/abs/2205.12396)

Yijun Tian, Chuxu Zhang, Zhichun Guo, Yihong Ma, Ronald Metoyer, Nitesh V. Chawla


[EBM Life Cycle: MCMC Strategies for Synthesis, Defense, and Density Modeling. (10%)](http://arxiv.org/abs/2205.12243)

Mitch Hill, Jonathan Mitchell, Chu Chen, Yuan Du, Mubarak Shah, Song-Chun Zhu


[Comprehensive Privacy Analysis on Federated Recommender System against Attribute Inference Attacks. (9%)](http://arxiv.org/abs/2205.11857)

Shijie Zhang, Hongzhi Yin


[Fast & Furious: Modelling Malware Detection as Evolving Data Streams. (2%)](http://arxiv.org/abs/2205.12311)

Fabrício Ceschin, Marcus Botacin, Heitor Murilo Gomes, Felipe Pinagé, Luiz S. Oliveira, André Grégio


[Quarantine: Sparsity Can Uncover the Trojan Attack Trigger for Free. (2%)](http://arxiv.org/abs/2205.11819)

Tianlong Chen, Zhenyu Zhang, Yihua Zhang, Shiyu Chang, Sijia Liu, Zhangyang Wang


[CDFKD-MFS: Collaborative Data-free Knowledge Distillation via Multi-level Feature Sharing. (1%)](http://arxiv.org/abs/2205.11845)

Zhiwei Hao, Yong Luo, Zhi Wang, Han Hu, Jianping An


## 2022-05-23

[Collaborative Adversarial Training. (98%)](http://arxiv.org/abs/2205.11156)

Qizhang Li, Yiwen Guo, Wangmeng Zuo, Hao Chen


[Alleviating Robust Overfitting of Adversarial Training With Consistency Regularization. (98%)](http://arxiv.org/abs/2205.11744)

Shudong Zhang, Haichang Gao, Tianwei Zhang, Yunyi Zhou, Zihui Wu


[Learning to Ignore Adversarial Attacks. (95%)](http://arxiv.org/abs/2205.11551)

Yiming Zhang, Yangqiaoyu Zhou, Samuel Carton, Chenhao Tan


[Towards a Defense against Backdoor Attacks in Continual Federated Learning. (50%)](http://arxiv.org/abs/2205.11736)

Shuaiqi Wang, Jonathan Hayase, Giulia Fanti, Sewoong Oh


[Compressing Deep Graph Neural Networks via Adversarial Knowledge Distillation. (10%)](http://arxiv.org/abs/2205.11678)

Huarui He, Jie Wang, Zhanqiu Zhang, Feng Wu


[RCC-GAN: Regularized Compound Conditional GAN for Large-Scale Tabular Data Synthesis. (1%)](http://arxiv.org/abs/2205.11693)

Mohammad Esmaeilpour, Nourhene Chaalia, Adel Abusitta, Francois-Xavier Devailly, Wissem Maazoun, Patrick Cardinal


## 2022-05-22

[AutoJoin: Efficient Adversarial Training for Robust Maneuvering via Denoising Autoencoder and Joint Learning. (26%)](http://arxiv.org/abs/2205.10933)

Michael Villarreal, Bibek Poudel, Ryan Wickman, Yu Shen, Weizi Li


[Robust Quantity-Aware Aggregation for Federated Learning. (13%)](http://arxiv.org/abs/2205.10848)

Jingwei Yi, Fangzhao Wu, Huishuai Zhang, Bin Zhu, Tao Qi, Guangzhong Sun, Xing Xie


[Analysis of functional neural codes of deep learning models. (10%)](http://arxiv.org/abs/2205.10952)

Jung Hoon Lee, Sujith Vijayan


## 2022-05-21

[Post-breach Recovery: Protection against White-box Adversarial Examples for Leaked DNN Models. (99%)](http://arxiv.org/abs/2205.10686)

Shawn Shan, Wenxin Ding, Emily Wenger, Haitao Zheng, Ben Y. Zhao


[Gradient Concealment: Free Lunch for Defending Adversarial Attacks. (99%)](http://arxiv.org/abs/2205.10617)

Sen Pei, Jiaxi Sun, Xiaopeng Zhang, Gaofeng Meng


[Phrase-level Textual Adversarial Attack with Label Preservation. (99%)](http://arxiv.org/abs/2205.10710)

Yibin Lei, Yu Cao, Dianqi Li, Tianyi Zhou, Meng Fang, Mykola Pechenizkiy


[On the Feasibility and Generality of Patch-based Adversarial Attacks on Semantic Segmentation Problems. (16%)](http://arxiv.org/abs/2205.10539)

Soma Kontar, Andras Horvath


## 2022-05-20

[Getting a-Round Guarantees: Floating-Point Attacks on Certified Robustness. (99%)](http://arxiv.org/abs/2205.10159)

Jiankai Jin, Olga Ohrimenko, Benjamin I. P. Rubinstein


[Robust Sensible Adversarial Learning of Deep Neural Networks for Image Classification. (98%)](http://arxiv.org/abs/2205.10457)

Jungeum Kim, Xiao Wang


[Adversarial joint attacks on legged robots. (86%)](http://arxiv.org/abs/2205.10098)

Takuto Otomo, Hiroshi Kera, Kazuhiko Kawamoto


[Towards Consistency in Adversarial Classification. (82%)](http://arxiv.org/abs/2205.10022)

Laurent Meunier, Raphaël Ettedgui, Rafael Pinot, Yann Chevaleyre, Jamal Atif


[Adversarial Body Shape Search for Legged Robots. (80%)](http://arxiv.org/abs/2205.10187)

Takaaki Azakami, Hiroshi Kera, Kazuhiko Kawamoto


[SafeNet: Mitigating Data Poisoning Attacks on Private Machine Learning. (64%)](http://arxiv.org/abs/2205.09986)

Harsh Chaudhari, Matthew Jagielski, Alina Oprea


[The developmental trajectory of object recognition robustness: children are like small adults but unlike big deep neural networks. (11%)](http://arxiv.org/abs/2205.10144)

Lukas S. Huber, Robert Geirhos, Felix A. Wichmann


[Vulnerability Analysis and Performance Enhancement of Authentication Protocol in Dynamic Wireless Power Transfer Systems. (10%)](http://arxiv.org/abs/2205.10292)

Tommaso Bianchi, Surudhi Asokraj, Alessandro Brighente, Mauro Conti, Radha Poovendran


[Exploring the Trade-off between Plausibility, Change Intensity and Adversarial Power in Counterfactual Explanations using Multi-objective Optimization. (4%)](http://arxiv.org/abs/2205.10232)

Ser Javier Del, Alejandro Barredo-Arrieta, Natalia Díaz-Rodríguez, Francisco Herrera, Andreas Holzinger


## 2022-05-19

[Focused Adversarial Attacks. (99%)](http://arxiv.org/abs/2205.09624)

Thomas Cilloni, Charles Walter, Charles Fleming


[Transferable Physical Attack against Object Detection with Separable Attention. (99%)](http://arxiv.org/abs/2205.09592)

Yu Zhang, Zhiqiang Gong, Yichuang Zhang, YongQian Li, Kangcheng Bin, Jiahao Qi, Wei Xue, Ping Zhong


[Gradient Aligned Attacks via a Few Queries. (99%)](http://arxiv.org/abs/2205.09518)

Xiangyuan Yang, Jie Lin, Hanlin Zhang, Xinyu Yang, Peng Zhao


[On Trace of PGD-Like Adversarial Attacks. (99%)](http://arxiv.org/abs/2205.09586)

Mo Zhou, Vishal M. Patel


[Improving Robustness against Real-World and Worst-Case Distribution Shifts through Decision Region Quantification. (98%)](http://arxiv.org/abs/2205.09619)

Leo Schwinn, Leon Bungert, An Nguyen, René Raab, Falk Pulsmeyer, Doina Precup, Björn Eskofier, Dario Zanca


[Defending Against Adversarial Attacks by Energy Storage Facility. (96%)](http://arxiv.org/abs/2205.09522)

Jiawei Li, Jianxiao Wang, Lin Chen, Yang Yu


[Sparse Adversarial Attack in Multi-agent Reinforcement Learning. (82%)](http://arxiv.org/abs/2205.09362)

Yizheng Hu, Zhihua Zhang


[Data Valuation for Offline Reinforcement Learning. (1%)](http://arxiv.org/abs/2205.09550)

Amir Abolfazli, Gregory Palmer, Daniel Kudenko


## 2022-05-18

[Passive Defense Against 3D Adversarial Point Clouds Through the Lens of 3D Steganalysis. (99%)](http://arxiv.org/abs/2205.08738)

Jiahao Zhu


[Property Unlearning: A Defense Strategy Against Property Inference Attacks. (84%)](http://arxiv.org/abs/2205.08821)

Joshua Universität Hamburg Stock, Jens Universität Hamburg Wettlaufer, Daniel Universität Hamburg Demmler, Hannes Universität Hamburg Federrath


[Backdoor Attacks on Bayesian Neural Networks using Reverse Distribution. (56%)](http://arxiv.org/abs/2205.09167)

Zhixin Pan, Prabhat Mishra


[Empirical Advocacy of Bio-inspired Models for Robust Image Recognition. (38%)](http://arxiv.org/abs/2205.09037)

Harshitha Machiraju, Oh-Hyeon Choung, Michael H. Herzog, Pascal Frossard


[Constraining the Attack Space of Machine Learning Models with Distribution Clamping Preprocessing. (1%)](http://arxiv.org/abs/2205.08989)

Ryan Feng, Somesh Jha, Atul Prakash


[Mitigating Neural Network Overconfidence with Logit Normalization. (1%)](http://arxiv.org/abs/2205.09310)

Hongxin Wei, Renchunzi Xie, Hao Cheng, Lei Feng, Bo An, Yixuan Li


## 2022-05-17

[Hierarchical Distribution-Aware Testing of Deep Learning. (98%)](http://arxiv.org/abs/2205.08589)

Wei Huang, Xingyu Zhao, Alec Banks, Victoria Cox, Xiaowei Huang


[Bankrupting DoS Attackers Despite Uncertainty. (12%)](http://arxiv.org/abs/2205.08287)

Trisha Chakraborty, Abir Islam, Valerie King, Daniel Rayborn, Jared Saia, Maxwell Young


[A two-steps approach to improve the performance of Android malware detectors. (10%)](http://arxiv.org/abs/2205.08265)

Nadia Daoudi, Kevin Allix, Tegawendé F. Bissyandé, Jacques Klein


[Policy Distillation with Selective Input Gradient Regularization for Efficient Interpretability. (2%)](http://arxiv.org/abs/2205.08685)

Jinwei Xing, Takashi Nagata, Xinyun Zou, Emre Neftci, Jeffrey L. Krichmar


[Recovering Private Text in Federated Learning of Language Models. (2%)](http://arxiv.org/abs/2205.08514)

Samyak Gupta, Yangsibo Huang, Zexuan Zhong, Tianyu Gao, Kai Li, Danqi Chen


[Semi-Supervised Building Footprint Generation with Feature and Output Consistency Training. (1%)](http://arxiv.org/abs/2205.08416)

Qingyu Li, Yilei Shi, Xiao Xiang Zhu


## 2022-05-16

[Attacking and Defending Deep Reinforcement Learning Policies. (99%)](http://arxiv.org/abs/2205.07626)

Chao Wang


[Diffusion Models for Adversarial Purification. (99%)](http://arxiv.org/abs/2205.07460)

Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, Anima Anandkumar


[Robust Representation via Dynamic Feature Aggregation. (84%)](http://arxiv.org/abs/2205.07466)

Haozhe Liu, Haoqin Ji, Yuexiang Li, Nanjun He, Haoqian Wu, Feng Liu, Linlin Shen, Yefeng Zheng


[Sparse Visual Counterfactual Explanations in Image Space. (83%)](http://arxiv.org/abs/2205.07972)

Valentyn Boreiko, Maximilian Augustin, Francesco Croce, Philipp Berens, Matthias Hein


[On the Difficulty of Defending Self-Supervised Learning against Model Extraction. (67%)](http://arxiv.org/abs/2205.07890)

Adam Dziedzic, Nikita Dhawan, Muhammad Ahmad Kaleem, Jonas Guan, Nicolas Papernot


[Transferability of Adversarial Attacks on Synthetic Speech Detection. (47%)](http://arxiv.org/abs/2205.07711)

Jiacheng Deng, Shunyi Chen, Li Dong, Diqun Yan, Rangding Wang


## 2022-05-15

[Learn2Weight: Parameter Adaptation against Similar-domain Adversarial Attacks. (99%)](http://arxiv.org/abs/2205.07315)

Siddhartha Datta


[Exploiting the Relationship Between Kendall's Rank Correlation and Cosine Similarity for Attribution Protection. (64%)](http://arxiv.org/abs/2205.07279)

Fan Wang, Adams Wai-Kin Kong


[RoMFAC: A robust mean-field actor-critic reinforcement learning against adversarial perturbations on states. (62%)](http://arxiv.org/abs/2205.07229)

Ziyuan Zhou, Guanjun Liu


[Automation Slicing and Testing for in-App Deep Learning Models. (1%)](http://arxiv.org/abs/2205.07228)

Hao Wu, Yuhang Gong, Xiaopeng Ke, Hanzhong Liang, Minghao Li, Fengyuan Xu, Yunxin Liu, Sheng Zhong


## 2022-05-14

[Evaluating Membership Inference Through Adversarial Robustness. (98%)](http://arxiv.org/abs/2205.06986)

Zhaoxi Zhang, Leo Yu Zhang, Xufei Zheng, Bilal Hussain Abbasi, Shengshan Hu


[Verifying Neural Networks Against Backdoor Attacks. (2%)](http://arxiv.org/abs/2205.06992)

Long H. Pham, Jun Sun


## 2022-05-13

[MM-BD: Post-Training Detection of Backdoor Attacks with Arbitrary Backdoor Pattern Types Using a Maximum Margin Statistic. (98%)](http://arxiv.org/abs/2205.06900)

Hang Wang, Zhen Xiang, David J. Miller, George Kesidis


[l-Leaks: Membership Inference Attacks with Logits. (41%)](http://arxiv.org/abs/2205.06469)

Shuhao Li, Yajie Wang, Yuanzhang Li, Yu-an Tan


[DualCF: Efficient Model Extraction Attack from Counterfactual Explanations. (26%)](http://arxiv.org/abs/2205.06504)

Yongjie Wang, Hangwei Qian, Chunyan Miao


[Millimeter-Wave Automotive Radar Spoofing. (2%)](http://arxiv.org/abs/2205.06567)

Mihai Ordean, Flavio D. Garcia


## 2022-05-12

[Sample Complexity Bounds for Robustly Learning Decision Lists against Evasion Attacks. (75%)](http://arxiv.org/abs/2205.06127)

Pascale Gourdeau, Varun Kanade, Marta Kwiatkowska, James Worrell


[PoisonedEncoder: Poisoning the Unlabeled Pre-training Data in Contrastive Learning. (61%)](http://arxiv.org/abs/2205.06401)

Hongbin Liu, Jinyuan Jia, Neil Zhenqiang Gong


[How to Combine Membership-Inference Attacks on Multiple Updated Models. (11%)](http://arxiv.org/abs/2205.06369)

Matthew Jagielski, Stanley Wu, Alina Oprea, Jonathan Ullman, Roxana Geambasu


[Infrared Invisible Clothing:Hiding from Infrared Detectors at Multiple Angles in Real World. (4%)](http://arxiv.org/abs/2205.05909)

Xiaopei Zhu, Zhanhao Hu, Siyuan Huang, Jianmin Li, Xiaolin Hu


[Smooth-Reduce: Leveraging Patches for Improved Certified Robustness. (2%)](http://arxiv.org/abs/2205.06154)

Ameya Joshi, Minh Pham, Minsu Cho, Leonid Boytsov, Filipe Condessa, J. Zico Kolter, Chinmay Hegde


[Stalloris: RPKI Downgrade Attack. (1%)](http://arxiv.org/abs/2205.06064)

Tomas Hlavacek, Philipp Jeitner, Donika Mirdita, Haya Shulman, Michael Waidner


## 2022-05-11

[Injection Attacks Reloaded: Tunnelling Malicious Payloads over DNS. (1%)](http://arxiv.org/abs/2205.05439)

Philipp Jeitner, Haya Shulman


[The Hijackers Guide To The Galaxy: Off-Path Taking Over Internet Resources. (1%)](http://arxiv.org/abs/2205.05473)

Tianxiang Dai, Philipp Jeitner, Haya Shulman, Michael Waidner


[A Longitudinal Study of Cryptographic API: a Decade of Android Malware. (1%)](http://arxiv.org/abs/2205.05573)

Adam Janovsky, Davide Maiorca, Dominik Macko, Vashek Matyas, Giorgio Giacinto


## 2022-05-10

[Robust Medical Image Classification from Noisy Labeled Data with Global and Local Representation Guided Co-training. (1%)](http://arxiv.org/abs/2205.04723)

Cheng Xue, Lequan Yu, Pengfei Chen, Qi Dou, Pheng-Ann Heng


[White-box Testing of NLP models with Mask Neuron Coverage. (1%)](http://arxiv.org/abs/2205.05050)

Arshdeep Sekhon, Yangfeng Ji, Matthew B. Dwyer, Yanjun Qi


## 2022-05-09

[Btech thesis report on adversarial attack detection and purification of adverserially attacked images. (99%)](http://arxiv.org/abs/2205.07859)

Dvij Kalaria


[Using Frequency Attention to Make Adversarial Patch Powerful Against Person Detector. (98%)](http://arxiv.org/abs/2205.04638)

Xiaochun Lei, Chang Lu, Zetao Jiang, Zhaoting Gong, Xiang Cai, Linjun Lu


[Do You Think You Can Hold Me? The Real Challenge of Problem-Space Evasion Attacks. (97%)](http://arxiv.org/abs/2205.04293)

Harel Berger, Amit Dvir, Chen Hajaj, Rony Ronen


[Model-Contrastive Learning for Backdoor Defense. (87%)](http://arxiv.org/abs/2205.04411)

Zhihao Yue, Jun Xia, Zhiwei Ling, Ming Hu, Ting Wang, Xian Wei, Mingsong Chen


[How Does Frequency Bias Affect the Robustness of Neural Image Classifiers against Common Corruption and Adversarial Perturbations? (61%)](http://arxiv.org/abs/2205.04533)

Alvin Chan, Yew-Soon Ong, Clement Tan


[Federated Multi-Armed Bandits Under Byzantine Attacks. (2%)](http://arxiv.org/abs/2205.04134)

Ilker Demirel, Yigit Yildirim, Cem Tekin


[Verifying Integrity of Deep Ensemble Models by Lossless Black-box Watermarking with Sensitive Samples. (2%)](http://arxiv.org/abs/2205.04145)

Lina Lin, Hanzhou Wu


## 2022-05-08

[Fingerprint Template Invertibility: Minutiae vs. Deep Templates. (68%)](http://arxiv.org/abs/2205.03809)

Kanishka P. Wijewardena, Steven A. Grosz, Kai Cao, Anil K. Jain


[ResSFL: A Resistance Transfer Framework for Defending Model Inversion Attack in Split Federated Learning. (22%)](http://arxiv.org/abs/2205.04007)

Jingtao Li, Adnan Siraj Rakin, Xing Chen, Zhezhi He, Deliang Fan, Chaitali Chakrabarti


[VPN: Verification of Poisoning in Neural Networks. (9%)](http://arxiv.org/abs/2205.03894)

Youcheng Sun, Muhammad Usman, Divya Gopinath, Corina S. Păsăreanu


[FOLPETTI: A Novel Multi-Armed Bandit Smart Attack for Wireless Networks. (4%)](http://arxiv.org/abs/2205.03915)

Emilie Bout, Alessandro Brighente, Mauro Conti, Valeria Loscri


[PGADA: Perturbation-Guided Adversarial Alignment for Few-shot Learning Under the Support-Query Shift. (1%)](http://arxiv.org/abs/2205.03817)

Siyang Jiang, Wei Ding, Hsi-Wen Chen, Ming-Syan Chen


## 2022-05-07

[A Simple Yet Efficient Method for Adversarial Word-Substitute Attack. (99%)](http://arxiv.org/abs/2206.05015)

Tianle Li, Yi Yang


[Bandits for Structure Perturbation-based Black-box Attacks to Graph Neural Networks with Theoretical Guarantees. (92%)](http://arxiv.org/abs/2205.03546)

Binghui Wang, Youqi Li, Pan Zhou


## 2022-05-06

[Imperceptible Backdoor Attack: From Input Space to Feature Representation. (68%)](http://arxiv.org/abs/2205.03190)

Nan Zhong, Zhenxing Qian, Xinpeng Zhang


[Defending against Reconstruction Attacks through Differentially Private Federated Learning for Classification of Heterogeneous Chest X-Ray Data. (26%)](http://arxiv.org/abs/2205.03168)

Joceline Ziegler, Bjarne Pfitzner, Heinrich Schulz, Axel Saalbach, Bert Arnrich


[LPGNet: Link Private Graph Networks for Node Classification. (1%)](http://arxiv.org/abs/2205.03105)

Aashish Kolluri, Teodora Baluta, Bryan Hooi, Prateek Saxena


[Unlimited Lives: Secure In-Process Rollback with Isolated Domains. (1%)](http://arxiv.org/abs/2205.03205)

Merve Gülmez, Thomas Nyman, Christoph Baumann, Jan Tobias Mühlberg


## 2022-05-05

[Holistic Approach to Measure Sample-level Adversarial Vulnerability and its Utility in Building Trustworthy Systems. (99%)](http://arxiv.org/abs/2205.02604)

Gaurav Kumar Nayak, Ruchit Rawal, Rohit Lal, Himanshu Patil, Anirban Chakraborty


[Structural Extensions of Basis Pursuit: Guarantees on Adversarial Robustness. (78%)](http://arxiv.org/abs/2205.08955)

Dávid Szeghy, Mahmoud Aslan, Áron Fóthi, Balázs Mészáros, Zoltán Ádám Milacski, András Lőrincz


[Can collaborative learning be private, robust and scalable? (61%)](http://arxiv.org/abs/2205.02652)

Dmitrii Usynin, Helena Klause, Daniel Rueckert, Georgios Kaissis


[Large Scale Transfer Learning for Differentially Private Image Classification. (2%)](http://arxiv.org/abs/2205.02973)

Harsh Mehta, Abhradeep Thakurta, Alexey Kurakin, Ashok Cutkosky


[Are GAN-based Morphs Threatening Face Recognition? (1%)](http://arxiv.org/abs/2205.02496)

Eklavya Sarkar, Pavel Korshunov, Laurent Colbois, Sébastien Marcel


[Heterogeneous Domain Adaptation with Adversarial Neural Representation Learning: Experiments on E-Commerce and Cybersecurity. (1%)](http://arxiv.org/abs/2205.07853)

Mohammadreza Ebrahimi, Yidong Chai, Hao Helen Zhang, Hsinchun Chen


## 2022-05-04

[Based-CE white-box adversarial attack will not work using super-fitting. (99%)](http://arxiv.org/abs/2205.02741)

Youhuan Yang, Lei Sun, Leyu Dai, Song Guo, Xiuqing Mao, Xiaoqin Wang, Bayi Xu


[Rethinking Classifier And Adversarial Attack. (98%)](http://arxiv.org/abs/2205.02743)

Youhuan Yang, Lei Sun, Leyu Dai, Song Guo, Xiuqing Mao, Xiaoqin Wang, Bayi Xu


[Wild Patterns Reloaded: A Survey of Machine Learning Security against Training Data Poisoning. (98%)](http://arxiv.org/abs/2205.01992)

Antonio Emanuele Cinà, Kathrin Grosse, Ambra Demontis, Sebastiano Vascon, Werner Zellinger, Bernhard A. Moser, Alina Oprea, Battista Biggio, Marcello Pelillo, Fabio Roli


[Robust Conversational Agents against Imperceptible Toxicity Triggers. (92%)](http://arxiv.org/abs/2205.02392)

Ninareh Mehrabi, Ahmad Beirami, Fred Morstatter, Aram Galstyan


[Subverting Fair Image Search with Generative Adversarial Perturbations. (83%)](http://arxiv.org/abs/2205.02414)

Avijit Ghosh, Matthew Jagielski, Christo Wilson


## 2022-05-03

[Adversarial Training for High-Stakes Reliability. (98%)](http://arxiv.org/abs/2205.01663)

Daniel M. Ziegler, Seraphina Nix, Lawrence Chan, Tim Bauman, Peter Schmidt-Nielsen, Tao Lin, Adam Scherlis, Noa Nabeshima, Ben Weinstein-Raun, Haas Daniel de, Buck Shlegeris, Nate Thomas


[Don't sweat the small stuff, classify the rest: Sample Shielding to protect text classifiers against adversarial attacks. (96%)](http://arxiv.org/abs/2205.01714)

Jonathan Rusert, Padmini Srinivasan


[On the uncertainty principle of neural networks. (3%)](http://arxiv.org/abs/2205.01493)

Jun-Jie Zhang, Dong-Xiao Zhang, Jian-Nan Chen, Long-Gang Pang


[Meta-Cognition. An Inverse-Inverse Reinforcement Learning Approach for Cognitive Radars. (1%)](http://arxiv.org/abs/2205.01794)

Kunal Pattanayak, Vikram Krishnamurthy, Christopher Berry


## 2022-05-02

[SemAttack: Natural Textual Attacks via Different Semantic Spaces. (96%)](http://arxiv.org/abs/2205.01287)

Boxin Wang, Chejian Xu, Xiangyu Liu, Yu Cheng, Bo Li


[Deep-Attack over the Deep Reinforcement Learning. (93%)](http://arxiv.org/abs/2205.00807)

Yang Li, Quan Pan, Erik Cambria


[Enhancing Adversarial Training with Feature Separability. (92%)](http://arxiv.org/abs/2205.00637)

Yaxin Li, Xiaorui Liu, Han Xu, Wentao Wang, Jiliang Tang


[BERTops: Studying BERT Representations under a Topological Lens. (92%)](http://arxiv.org/abs/2205.00953)

Jatin Chauhan, Manohar Kaul


[MIRST-DM: Multi-Instance RST with Drop-Max Layer for Robust Classification of Breast Cancer. (83%)](http://arxiv.org/abs/2205.01674)

Shoukun Sun, Min Xian, Aleksandar Vakanski, Hossny Ghanem


[Revisiting Gaussian Neurons for Online Clustering with Unknown Number of Clusters. (1%)](http://arxiv.org/abs/2205.00920)

Ole Christian Eidheim


## 2022-05-01

[A Word is Worth A Thousand Dollars: Adversarial Attack on Tweets Fools Stock Prediction. (98%)](http://arxiv.org/abs/2205.01094)

Yong Xie, Dakuo Wang, Pin-Yu Chen, Jinjun Xiong, Sijia Liu, Sanmi Koyejo


[DDDM: a Brain-Inspired Framework for Robust Classification. (76%)](http://arxiv.org/abs/2205.10117)

Xiyuan Chen, Xingyu Li, Yi Zhou, Tianming Yang


[Robust Fine-tuning via Perturbation and Interpolation from In-batch Instances. (9%)](http://arxiv.org/abs/2205.00633)

Shoujie Tong, Qingxiu Dong, Damai Dai, Yifan song, Tianyu Liu, Baobao Chang, Zhifang Sui


[A Simple Approach to Improve Single-Model Deep Uncertainty via Distance-Awareness. (3%)](http://arxiv.org/abs/2205.00403)

Jeremiah Zhe Liu, Shreyas Padhy, Jie Ren, Zi Lin, Yeming Wen, Ghassen Jerfel, Zack Nado, Jasper Snoek, Dustin Tran, Balaji Lakshminarayanan


[Adversarial Plannning. (2%)](http://arxiv.org/abs/2205.00566)

Valentin Vie, Ryan Sheatsley, Sophia Beyda, Sushrut Shringarputale, Kevin Chan, Trent Jaeger, Patrick McDaniel


## 2022-04-30

[Optimizing One-pixel Black-box Adversarial Attacks. (82%)](http://arxiv.org/abs/2205.02116)

Tianxun Zhou, Shubhankar Agrawal, Prateek Manocha


[Cracking White-box DNN Watermarks via Invariant Neuron Transforms. (26%)](http://arxiv.org/abs/2205.00199)

Yifan Yan, Xudong Pan, Yining Wang, Mi Zhang, Min Yang


[Loss Function Entropy Regularization for Diverse Decision Boundaries. (1%)](http://arxiv.org/abs/2205.00224)

Chong Sue Sin


[Adapting and Evaluating Influence-Estimation Methods for Gradient-Boosted Decision Trees. (1%)](http://arxiv.org/abs/2205.00359)

Jonathan Brophy, Zayd Hammoudeh, Daniel Lowd


## 2022-04-29

[Adversarial attacks on an optical neural network. (92%)](http://arxiv.org/abs/2205.01226)

Shuming Jiao, Ziwei Song, Shuiying Xiang


[Logically Consistent Adversarial Attacks for Soft Theorem Provers. (2%)](http://arxiv.org/abs/2205.00047)

Alexander Gaskell, Yishu Miao, Lucia Specia, Francesca Toni


[Bridging Differential Privacy and Byzantine-Robustness via Model Aggregation. (1%)](http://arxiv.org/abs/2205.00107)

Heng Zhu, Qing Ling


## 2022-04-28

[Detecting Textual Adversarial Examples Based on Distributional Characteristics of Data Representations. (99%)](http://arxiv.org/abs/2204.13853)

Na Liu, Mark Dras, Wei Emma Zhang


[Formulating Robustness Against Unforeseen Attacks. (99%)](http://arxiv.org/abs/2204.13779)

Sihui Dai, Saeed Mahloujifar, Prateek Mittal


[Randomized Smoothing under Attack: How Good is it in Pratice? (84%)](http://arxiv.org/abs/2204.14187)

Thibault Maho, Teddy Furon, Erwan Le Merrer


[Improving robustness of language models from a geometry-aware perspective. (68%)](http://arxiv.org/abs/2204.13309)

Bin Zhu, Zhaoquan Gu, Le Wang, Jinyin Chen, Qi Xuan


[Mixup-based Deep Metric Learning Approaches for Incomplete Supervision. (50%)](http://arxiv.org/abs/2204.13572)

Luiz H. Buris, Daniel C. G. Pedronette, Joao P. Papa, Jurandy Almeida, Gustavo Carneiro, Fabio A. Faria


[AGIC: Approximate Gradient Inversion Attack on Federated Learning. (16%)](http://arxiv.org/abs/2204.13784)

Jin Xu, Chi Hong, Jiyue Huang, Lydia Y. Chen, Jérémie Decouchant


[An Online Ensemble Learning Model for Detecting Attacks in Wireless Sensor Networks. (1%)](http://arxiv.org/abs/2204.13814)

Hiba Tabbaa, Samir Ifzarne, Imad Hafidi


## 2022-04-27

[Adversarial Fine-tune with Dynamically Regulated Adversary. (99%)](http://arxiv.org/abs/2204.13232)

Pengyue Hou, Ming Zhou, Jie Han, Petr Musilek, Xingyu Li


[Defending Against Person Hiding Adversarial Patch Attack with a Universal White Frame. (98%)](http://arxiv.org/abs/2204.13004)

Youngjoon Yu, Hong Joo Lee, Hakmin Lee, Yong Man Ro


[An Adversarial Attack Analysis on Malicious Advertisement URL Detection Framework. (81%)](http://arxiv.org/abs/2204.13172)

Ehsan Nowroozi, Abhishek, Mohammadreza Mohammadi, Mauro Conti


## 2022-04-26

[Boosting Adversarial Transferability of MLP-Mixer. (99%)](http://arxiv.org/abs/2204.12204)

Haoran Lyu, Yajie Wang, Yu-an Tan, Huipeng Zhou, Yuhang Zhao, Quanxin Zhang


[Restricted Black-box Adversarial Attack Against DeepFake Face Swapping. (99%)](http://arxiv.org/abs/2204.12347)

Junhao Dong, Yuan Wang, Jianhuang Lai, Xiaohua Xie


[Improving the Transferability of Adversarial Examples with Restructure Embedded Patches. (99%)](http://arxiv.org/abs/2204.12680)

Huipeng Zhou, Yu-an Tan, Yajie Wang, Haoran Lyu, Shangbo Wu, Yuanzhang Li


[On Fragile Features and Batch Normalization in Adversarial Training. (97%)](http://arxiv.org/abs/2204.12393)

Nils Philipp Walter, David Stutz, Bernt Schiele


[Mixed Strategies for Security Games with General Defending Requirements. (75%)](http://arxiv.org/abs/2204.12158)

Rufan Bai, Haoxing Lin, Xinyu Yang, Xiaowei Wu, Minming Li, Weijia Jia


[Poisoning Deep Learning based Recommender Model in Federated Learning Scenarios. (26%)](http://arxiv.org/abs/2204.13594)

Dazhong Rong, Qinming He, Jianhai Chen


[Designing Perceptual Puzzles by Differentiating Probabilistic Programs. (13%)](http://arxiv.org/abs/2204.12301)

Kartik Chandra, Tzu-Mao Li, Joshua Tenenbaum, Jonathan Ragan-Kelley


[Enhancing Privacy against Inversion Attacks in Federated Learning by using Mixing Gradients Strategies. (8%)](http://arxiv.org/abs/2204.12495)

Shaltiel Eloul, Fran Silavong, Sanket Kamthe, Antonios Georgiadis, Sean J. Moran


[Performance Analysis of Out-of-Distribution Detection on Trained Neural Networks. (4%)](http://arxiv.org/abs/2204.12378)

Jens Henriksson, Christian Berger, Markus Borg, Lars Tornberg, Sankar Raman Sathyamoorthy, Cristofer Englund


## 2022-04-25

[Self-recoverable Adversarial Examples: A New Effective Protection Mechanism in Social Networks. (99%)](http://arxiv.org/abs/2204.12050)

Jiawei Zhang, Jinwei Wang, Hao Wang, Xiangyang Luo


[When adversarial examples are excusable. (89%)](http://arxiv.org/abs/2204.11985)

Pieter-Jan Kindermans, Charles Staats


[A Simple Structure For Building A Robust Model. (81%)](http://arxiv.org/abs/2204.11596)

Xiao Tan, JingBo Gao, Ruolin Li


[Real or Virtual: A Video Conferencing Background Manipulation-Detection System. (67%)](http://arxiv.org/abs/2204.11853)

Ehsan Nowroozi, Yassine Mekdad, Mauro Conti, Simone Milani, Selcuk Uluagac, Berrin Yanikoglu


[Can Rationalization Improve Robustness? (12%)](http://arxiv.org/abs/2204.11790)

Howard Chen, Jacqueline He, Karthik Narasimhan, Danqi Chen


[PhysioGAN: Training High Fidelity Generative Model for Physiological Sensor Readings. (1%)](http://arxiv.org/abs/2204.13597)

Moustafa Alzantot, Luis Garcia, Mani Srivastava


[VITA: A Multi-Source Vicinal Transfer Augmentation Method for Out-of-Distribution Generalization. (1%)](http://arxiv.org/abs/2204.11531)

Minghui Chen, Cheng Wen, Feng Zheng, Fengxiang He, Ling Shao


[Enable Deep Learning on Mobile Devices: Methods, Systems, and Applications. (1%)](http://arxiv.org/abs/2204.11786)

Han Cai, Ji Lin, Yujun Lin, Zhijian Liu, Haotian Tang, Hanrui Wang, Ligeng Zhu, Song Han


## 2022-04-24

[A Hybrid Defense Method against Adversarial Attacks on Traffic Sign Classifiers in Autonomous Vehicles. (99%)](http://arxiv.org/abs/2205.01225)

Zadid Khan, Mashrur Chowdhury, Sakib Mahmud Khan


[Improving Deep Learning Model Robustness Against Adversarial Attack by Increasing the Network Capacity. (81%)](http://arxiv.org/abs/2204.11357)

Marco Marchetti, Edmond S. L. Ho


## 2022-04-23

[Smart App Attack: Hacking Deep Learning Models in Android Apps. (98%)](http://arxiv.org/abs/2204.11075)

Yujin Huang, Chunyang Chen


[Towards Data-Free Model Stealing in a Hard Label Setting. (13%)](http://arxiv.org/abs/2204.11022)

Sunandini Sanyal, Sravanti Addepalli, R. Venkatesh Babu


[Reinforced Causal Explainer for Graph Neural Networks. (1%)](http://arxiv.org/abs/2204.11028)

Xiang Wang, Yingxin Wu, An Zhang, Fuli Feng, Xiangnan He, Tat-Seng Chua


## 2022-04-22

[How Sampling Impacts the Robustness of Stochastic Neural Networks. (99%)](http://arxiv.org/abs/2204.10839)

Sina Däubener, Asja Fischer


[A Tale of Two Models: Constructing Evasive Attacks on Edge Models. (83%)](http://arxiv.org/abs/2204.10933)

Wei Hao, Aahil Awatramani, Jiayang Hu, Chengzhi Mao, Pin-Chun Chen, Eyal Cidon, Asaf Cidon, Junfeng Yang


[Enhancing the Transferability via Feature-Momentum Adversarial Attack. (82%)](http://arxiv.org/abs/2204.10606)

Xianglong, Yuezun Li, Haipeng Qu, Junyu Dong


[Data-Efficient Backdoor Attacks. (76%)](http://arxiv.org/abs/2204.12281)

Pengfei Xia, Ziqiang Li, Wei Zhang, Bin Li


## 2022-04-21

[A Mask-Based Adversarial Defense Scheme. (99%)](http://arxiv.org/abs/2204.11837)

Weizhen Xu, Chenyi Zhang, Fangzhen Zhao, Liangda Fang


[Is Neuron Coverage Needed to Make Person Detection More Robust? (98%)](http://arxiv.org/abs/2204.10027)

Svetlana Pavlitskaya, Şiyar Yıkmış, J. Marius Zöllner


[Testing robustness of predictions of trained classifiers against naturally occurring perturbations. (98%)](http://arxiv.org/abs/2204.10046)

Sebastian Scher, Andreas Trügler


[Adversarial Contrastive Learning by Permuting Cluster Assignments. (15%)](http://arxiv.org/abs/2204.10314)

Muntasir Wahed, Afrina Tabassum, Ismini Lourentzou


[Eliminating Backdoor Triggers for Deep Neural Networks Using Attention Relation Graph Distillation. (4%)](http://arxiv.org/abs/2204.09975)

Jun Xia, Ting Wang, Jiepin Ding, Xian Wei, Mingsong Chen


[Detecting Topology Attacks against Graph Neural Networks. (1%)](http://arxiv.org/abs/2204.10072)

Senrong Xu, Yuan Yao, Liangyue Li, Wei Yang, Feng Xu, Hanghang Tong


## 2022-04-20

[Adversarial Scratches: Deployable Attacks to CNN Classifiers. (99%)](http://arxiv.org/abs/2204.09397)

Loris Giulivi, Malhar Jere, Loris Rossi, Farinaz Koushanfar, Gabriela Ciocarlie, Briland Hitaj, Giacomo Boracchi


[GUARD: Graph Universal Adversarial Defense. (99%)](http://arxiv.org/abs/2204.09803)

Jintang Li, Jie Liao, Ruofan Wu, Liang Chen, Zibin Zheng, Jiawang Dan, Changhua Meng, Weiqiang Wang


[Fast AdvProp. (98%)](http://arxiv.org/abs/2204.09838)

Jieru Mei, Yucheng Han, Yutong Bai, Yixiao Zhang, Yingwei Li, Xianhang Li, Alan Yuille, Cihang Xie


[Case-Aware Adversarial Training. (98%)](http://arxiv.org/abs/2204.09398)

Mingyuan Fan, Yang Liu, Wenzhong Guo, Ximeng Liu, Jianhua Li


[Improved Worst-Group Robustness via Classifier Retraining on Independent Splits. (1%)](http://arxiv.org/abs/2204.09583)

Thien Hang Nguyen, Hongyang R. Zhang, Huy Le Nguyen


## 2022-04-19

[Jacobian Ensembles Improve Robustness Trade-offs to Adversarial Attacks. (99%)](http://arxiv.org/abs/2204.08726)

Kenneth T. Co, David Martinez-Rego, Zhongyuan Hau, Emil C. Lupu


[Robustness Testing of Data and Knowledge Driven Anomaly Detection in Cyber-Physical Systems. (86%)](http://arxiv.org/abs/2204.09183)

Xugui Zhou, Maxfield Kouzel, Homa Alemzadeh


[Generating Authentic Adversarial Examples beyond Meaning-preserving with Doubly Round-trip Translation. (83%)](http://arxiv.org/abs/2204.08689)

Siyu Lai, Zhen Yang, Fandong Meng, Xue Zhang, Yufeng Chen, Jinan Xu, Jie Zhou


## 2022-04-18

[UNBUS: Uncertainty-aware Deep Botnet Detection System in Presence of Perturbed Samples. (99%)](http://arxiv.org/abs/2204.09502)

Rahim Taheri


[Sardino: Ultra-Fast Dynamic Ensemble for Secure Visual Sensing at Mobile Edge. (99%)](http://arxiv.org/abs/2204.08189)

Qun Song, Zhenyu Yan, Wenjie Luo, Rui Tan


[CgAT: Center-Guided Adversarial Training for Deep Hashing-Based Retrieval. (99%)](http://arxiv.org/abs/2204.10779)

Xunguang Wang, Yiqun Lin, Xiaomeng Li


[Metamorphic Testing-based Adversarial Attack to Fool Deepfake Detectors. (98%)](http://arxiv.org/abs/2204.08612)

Nyee Thoang Lim, Meng Yi Kuan, Muxin Pu, Mei Kuan Lim, Chun Yong Chong


[A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability. (75%)](http://arxiv.org/abs/2204.08570)

Enyan Dai, Tianxiang Zhao, Huaisheng Zhu, Junjie Xu, Zhimeng Guo, Hui Liu, Jiliang Tang, Suhang Wang


[CorrGAN: Input Transformation Technique Against Natural Corruptions. (70%)](http://arxiv.org/abs/2204.08623)

Mirazul Haque, Christof J. Budnik, Wei Yang


[Poisons that are learned faster are more effective. (64%)](http://arxiv.org/abs/2204.08615)

Pedro Sandoval-Segura, Vasu Singla, Liam Fowl, Jonas Geiping, Micah Goldblum, David Jacobs, Tom Goldstein


## 2022-04-17

[Residue-Based Natural Language Adversarial Attack Detection. (99%)](http://arxiv.org/abs/2204.10192)

Vyas Raina, Mark Gales


[Towards Comprehensive Testing on the Robustness of Cooperative Multi-agent Reinforcement Learning. (95%)](http://arxiv.org/abs/2204.07932)

Jun Guo, Yonghong Chen, Yihang Hao, Zixin Yin, Yin Yu, Simin Li


## 2022-04-16

[SETTI: A Self-supervised Adversarial Malware Detection Architecture in an IoT Environment. (95%)](http://arxiv.org/abs/2204.07772)

Marjan Golmaryami, Rahim Taheri, Zahra Pooranian, Mohammad Shojafar, Pei Xiao


[Homomorphic Encryption and Federated Learning based Privacy-Preserving CNN Training: COVID-19 Detection Use-Case. (67%)](http://arxiv.org/abs/2204.07752)

Febrianti Wibawa, Ferhat Ozgur Catak, Salih Sarp, Murat Kuzlu, Umit Cali


## 2022-04-15

[Revisiting the Adversarial Robustness-Accuracy Tradeoff in Robot Learning. (92%)](http://arxiv.org/abs/2204.07373)

Mathias Lechner, Alexander Amini, Daniela Rus, Thomas A. Henzinger


## 2022-04-14

[From Environmental Sound Representation to Robustness of 2D CNN Models Against Adversarial Attacks. (99%)](http://arxiv.org/abs/2204.07018)

Mohammad Esmaeilpour, Patrick Cardinal, Alessandro Lameiras Koerich


[Planting Undetectable Backdoors in Machine Learning Models. (99%)](http://arxiv.org/abs/2204.06974)

Shafi Goldwasser, Michael P. Kim, Vinod Vaikuntanathan, Or Zamir


[Q-TART: Quickly Training for Adversarial Robustness and in-Transferability. (50%)](http://arxiv.org/abs/2204.07024)

Madan Ravi Ganesh, Salimeh Yasaei Sekeh, Jason J. Corso


[Robotic and Generative Adversarial Attacks in Offline Writer-independent Signature Verification. (41%)](http://arxiv.org/abs/2204.07246)

Jordan J. Bird


## 2022-04-13

[Task-Driven Data Augmentation for Vision-Based Robotic Control. (96%)](http://arxiv.org/abs/2204.06173)

Shubhankar Agarwal, Sandeep P. Chinchali


[Stealing and Evading Malware Classifiers and Antivirus at Low False Positive Conditions. (87%)](http://arxiv.org/abs/2204.06241)

Maria Rigaki, Sebastian Garcia


[Defensive Patches for Robust Recognition in the Physical World. (80%)](http://arxiv.org/abs/2204.06213)

Jiakai Wang, Zixin Yin, Pengfei Hu, Aishan Liu, Renshuai Tao, Haotong Qin, Xianglong Liu, Dacheng Tao


[A Novel Approach to Train Diverse Types of Language Models for Health Mention Classification of Tweets. (78%)](http://arxiv.org/abs/2204.06337)

Pervaiz Iqbal Khan, Imran Razzak, Andreas Dengel, Sheraz Ahmed


[Overparameterized Linear Regression under Adversarial Attacks. (76%)](http://arxiv.org/abs/2204.06274)

Antônio H. Ribeiro, Thomas B. Schön


[Towards A Critical Evaluation of Robustness for Deep Learning Backdoor Countermeasures. (38%)](http://arxiv.org/abs/2204.06273)

Huming Qiu, Hua Ma, Zhi Zhang, Alsharif Abuadbba, Wei Kang, Anmin Fu, Yansong Gao


[A Natural Language Processing Approach for Instruction Set Architecture Identification. (1%)](http://arxiv.org/abs/2204.06624)

Dinuka Sahabandu, Sukarno Mertoguno, Radha Poovendran


## 2022-04-12

[Liuer Mihou: A Practical Framework for Generating and Evaluating Grey-box Adversarial Attacks against NIDS. (99%)](http://arxiv.org/abs/2204.06113)

Ke He, Dan Dongseong Kim, Jing Sun, Jeong Do Yoo, Young Hun Lee, Huy Kang Kim


[Examining the Proximity of Adversarial Examples to Class Manifolds in Deep Networks. (98%)](http://arxiv.org/abs/2204.05764)

Štefan Pócoš, Iveta Bečková, Igor Farkaš


[Toward Robust Spiking Neural Network Against Adversarial Perturbation. (98%)](http://arxiv.org/abs/2205.01625)

Ling Liang, Kaidi Xu, Xing Hu, Lei Deng, Yuan Xie


[Machine Learning Security against Data Poisoning: Are We There Yet? (92%)](http://arxiv.org/abs/2204.05986)

Antonio Emanuele Cinà, Kathrin Grosse, Ambra Demontis, Battista Biggio, Fabio Roli, Marcello Pelillo


[Optimal Membership Inference Bounds for Adaptive Composition of Sampled Gaussian Mechanisms. (11%)](http://arxiv.org/abs/2204.06106)

Saeed Mahloujifar, Alexandre Sablayrolles, Graham Cormode, Somesh Jha


[3DeformRS: Certifying Spatial Deformations on Point Clouds. (9%)](http://arxiv.org/abs/2204.05687)

Gabriel Pérez S., Juan C. Pérez, Motasem Alfarra, Silvio Giancola, Bernard Ghanem


## 2022-04-11

[A Simple Approach to Adversarial Robustness in Few-shot Image Classification. (98%)](http://arxiv.org/abs/2204.05432)

Akshayvarun Subramanya, Hamed Pirsiavash


[Narcissus: A Practical Clean-Label Backdoor Attack with Limited Information. (92%)](http://arxiv.org/abs/2204.05255)

Yi Zeng, Minzhou Pan, Hoang Anh Just, Lingjuan Lyu, Meikang Qiu, Ruoxi Jia


[Generalizing Adversarial Explanations with Grad-CAM. (84%)](http://arxiv.org/abs/2204.05427)

Tanmay Chakraborty, Utkarsh Trehan, Khawla Mallat, Jean-Luc Dugelay


[Anti-Adversarially Manipulated Attributions for Weakly Supervised Semantic Segmentation and Object Localization. (83%)](http://arxiv.org/abs/2204.04890)

Jungbeom Lee, Eunji Kim, Jisoo Mok, Sungroh Yoon


[Exploring the Universal Vulnerability of Prompt-based Learning Paradigm. (47%)](http://arxiv.org/abs/2204.05239)

Lei Xu, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Zhiyuan Liu


[medXGAN: Visual Explanations for Medical Classifiers through a Generative Latent Space. (1%)](http://arxiv.org/abs/2204.05376)

Amil Dravid, Florian Schiffers, Boqing Gong, Aggelos K. Katsaggelos


## 2022-04-10

["That Is a Suspicious Reaction!": Interpreting Logits Variation to Detect NLP Adversarial Attacks. (88%)](http://arxiv.org/abs/2204.04636)

Edoardo Mosca, Shreyash Agarwal, Javier Rando-Ramirez, Georg Groh


[Analysis of Power-Oriented Fault Injection Attacks on Spiking Neural Networks. (54%)](http://arxiv.org/abs/2204.04768)

Karthikeyan Nagarajan, Junde Li, Sina Sayyah Ensan, Mohammad Nasim Imtiaz Khan, Sachhidh Kannan, Swaroop Ghosh


[Measuring the False Sense of Security. (26%)](http://arxiv.org/abs/2204.04778)

Carlos Gomes


## 2022-04-08

[Defense against Adversarial Attacks on Hybrid Speech Recognition using Joint Adversarial Fine-tuning with Denoiser. (99%)](http://arxiv.org/abs/2204.03851)

Sonal Joshi, Saurabh Kataria, Yiwen Shao, Piotr Zelasko, Jesus Villalba, Sanjeev Khudanpur, Najim Dehak


[AdvEst: Adversarial Perturbation Estimation to Classify and Detect Adversarial Attacks against Speaker Identification. (99%)](http://arxiv.org/abs/2204.03848)

Sonal Joshi, Saurabh Kataria, Jesus Villalba, Najim Dehak


[Evaluating the Adversarial Robustness for Fourier Neural Operators. (92%)](http://arxiv.org/abs/2204.04259)

Abolaji D. Adesoji, Pin-Yu Chen


[Backdoor Attack against NLP models with Robustness-Aware Perturbation defense. (87%)](http://arxiv.org/abs/2204.05758)

Shaik Mohammed Maqsood, Viveros Manuela Ceron, Addluri GowthamKrishna


[An Adaptive Black-box Backdoor Detection Method for Deep Neural Networks. (45%)](http://arxiv.org/abs/2204.04329)

Xinqiao Zhang, Huili Chen, Ke Huang, Farinaz Koushanfar


[Characterizing and Understanding the Behavior of Quantized Models for Reliable Deployment. (13%)](http://arxiv.org/abs/2204.04220)

Qiang Hu, Yuejun Guo, Maxime Cordy, Xiaofei Xie, Wei Ma, Mike Papadakis, Yves Le Traon


[Neural Tangent Generalization Attacks. (12%)](http://arxiv.org/abs/2204.04090)

Chia-Hung Yuan, Shan-Hung Wu


[Labeling-Free Comparison Testing of Deep Learning Models. (11%)](http://arxiv.org/abs/2204.03994)

Yuejun Guo, Qiang Hu, Maxime Cordy, Xiaofei Xie, Mike Papadakis, Yves Le Traon


[Does Robustness on ImageNet Transfer to Downstream Tasks? (2%)](http://arxiv.org/abs/2204.03934)

Yutaro Yamada, Mayu Otani


[The self-learning AI controller for adaptive power beaming with fiber-array laser transmitter system. (1%)](http://arxiv.org/abs/2204.05227)

A. M. Vorontsov, G. A. Filimonov


## 2022-04-07

[Transfer Attacks Revisited: A Large-Scale Empirical Study in Real Computer Vision Settings. (99%)](http://arxiv.org/abs/2204.04063)

Yuhao Mao, Chong Fu, Saizhuo Wang, Shouling Ji, Xuhong Zhang, Zhenguang Liu, Jun Zhou, Alex X. Liu, Raheem Beyah, Ting Wang


[Adaptive-Gravity: A Defense Against Adversarial Samples. (99%)](http://arxiv.org/abs/2204.03694)

Ali Mirzaeian, Zhi Tian, Sai Manoj P D, Banafsheh S. Latibari, Ioannis Savidis, Houman Homayoun, Avesta Sasan


[Using Multiple Self-Supervised Tasks Improves Model Robustness. (81%)](http://arxiv.org/abs/2204.03714)

Matthew Lawhon, Chengzhi Mao, Junfeng Yang


[Transformer-Based Language Models for Software Vulnerability Detection: Performance, Model's Security and Platforms. (69%)](http://arxiv.org/abs/2204.03214)

Chandra Thapa, Seung Ick Jang, Muhammad Ejaz Ahmed, Seyit Camtepe, Josef Pieprzyk, Surya Nepal


[Defending Active Directory by Combining Neural Network based Dynamic Program and Evolutionary Diversity Optimisation. (1%)](http://arxiv.org/abs/2204.03397)

Diksha Goel, Max Hector Ward-Graham, Aneta Neumann, Frank Neumann, Hung Nguyen, Mingyu Guo


## 2022-04-06

[Sampling-based Fast Gradient Rescaling Method for Highly Transferable Adversarial Attacks. (99%)](http://arxiv.org/abs/2204.02887)

Xu Han, Anmin Liu, Yifeng Xiong, Yanbo Fan, Kun He


[Masking Adversarial Damage: Finding Adversarial Saliency for Robust and Sparse Network. (95%)](http://arxiv.org/abs/2204.02738)

Byung-Kwan Lee, Junho Kim, Yong Man Ro


[Distilling Robust and Non-Robust Features in Adversarial Examples by Information Bottleneck. (93%)](http://arxiv.org/abs/2204.02735)

Junho Kim, Byung-Kwan Lee, Yong Man Ro


[Optimization Models and Interpretations for Three Types of Adversarial Perturbations against Support Vector Machines. (68%)](http://arxiv.org/abs/2204.03154)

Wen Su, Qingna Li, Chunfeng Cui


[Adversarial Machine Learning Attacks Against Video Anomaly Detection Systems. (62%)](http://arxiv.org/abs/2204.03141)

Furkan Mumcu, Keval Doshi, Yasin Yilmaz


[Adversarial Analysis of the Differentially-Private Federated Learning in Cyber-Physical Critical Infrastructures. (33%)](http://arxiv.org/abs/2204.02654)

Md Tamjid Jim Hossain, Shahriar Jim Badsha, Jim Hung, La, Haoting Shen, Shafkat Islam, Ibrahim Khalil, Xun Yi


## 2022-04-05

[Hear No Evil: Towards Adversarial Robustness of Automatic Speech Recognition via Multi-Task Learning. (98%)](http://arxiv.org/abs/2204.02381)

Nilaksh Das, Duen Horng Chau


[Adversarial Robustness through the Lens of Convolutional Filters. (87%)](http://arxiv.org/abs/2204.02481)

Paul Gavrikov, Janis Keuper


[User-Level Differential Privacy against Attribute Inference Attack of Speech Emotion Recognition in Federated Learning. (2%)](http://arxiv.org/abs/2204.02500)

Tiantian Feng, Raghuveer Peri, Shrikanth Narayanan


[SwapMix: Diagnosing and Regularizing the Over-Reliance on Visual Context in Visual Question Answering. (1%)](http://arxiv.org/abs/2204.02285)

Vipul Gupta, Zhuowan Li, Adam Kortylewski, Chenyu Zhang, Yingwei Li, Alan Yuille


[GAIL-PT: A Generic Intelligent Penetration Testing Framework with Generative Adversarial Imitation Learning. (1%)](http://arxiv.org/abs/2204.01975)

Jinyin Chen, Shulong Hu, Haibin Zheng, Changyou Xing, Guomin Zhang


## 2022-04-04

[DAD: Data-free Adversarial Defense at Test Time. (99%)](http://arxiv.org/abs/2204.01568)

Gaurav Kumar Nayak, Ruchit Rawal, Anirban Chakraborty


[SecureSense: Defending Adversarial Attack for Secure Device-Free Human Activity Recognition. (99%)](http://arxiv.org/abs/2204.01560)

Jianfei Yang, Han Zou, Lihua Xie


[Experimental quantum adversarial learning with programmable superconducting qubits. (99%)](http://arxiv.org/abs/2204.01738)

Wenhui Ren, Weikang Li, Shibo Xu, Ke Wang, Wenjie Jiang, Feitong Jin, Xuhao Zhu, Jiachen Chen, Zixuan Song, Pengfei Zhang, Hang Dong, Xu Zhang, Jinfeng Deng, Yu Gao, Chuanyu Zhang, Yaozu Wu, Bing Zhang, Qiujiang Guo, Hekang Li, Zhen Wang, Jacob Biamonte, Chao Song, Dong-Ling Deng, H. Wang


[PRADA: Practical Black-Box Adversarial Attacks against Neural Ranking Models. (99%)](http://arxiv.org/abs/2204.01321)

Chen Wu, Ruqing Zhang, Jiafeng Guo, Rijke Maarten de, Yixing Fan, Xueqi Cheng


[FaceSigns: Semi-Fragile Neural Watermarks for Media Authentication and Countering Deepfakes. (98%)](http://arxiv.org/abs/2204.01960)

Paarth Neekhara, Shehzeen Hussain, Xinqiao Zhang, Ke Huang, Julian McAuley, Farinaz Koushanfar


## 2022-04-03

[Breaking the De-Pois Poisoning Defense. (98%)](http://arxiv.org/abs/2204.01090)

Alaa Anani, Mohamed Ghanem, Lotfy Abdel Khaliq


[Adversarially robust segmentation models learn perceptually-aligned gradients. (16%)](http://arxiv.org/abs/2204.01099)

Pedro Sandoval-Segura


[Detecting In-vehicle Intrusion via Semi-supervised Learning-based Convolutional Adversarial Autoencoders. (1%)](http://arxiv.org/abs/2204.01193)

Thien-Nu Hoang, Daehee Kim


[Improving Vision Transformers by Revisiting High-frequency Components. (1%)](http://arxiv.org/abs/2204.00993)

Jiawang Bai, Li Yuan, Shu-Tao Xia, Shuicheng Yan, Zhifeng Li, Wei Liu


## 2022-04-02

[DST: Dynamic Substitute Training for Data-free Black-box Attack. (98%)](http://arxiv.org/abs/2204.00972)

Wenxuan Wang, Xuelin Qian, Yanwei Fu, Xiangyang Xue


[Adversarial Neon Beam: Robust Physical-World Adversarial Attack to DNNs. (98%)](http://arxiv.org/abs/2204.00853)

Chengyin Hu, Kalibinuer Tiliwalidi


## 2022-04-01

[SkeleVision: Towards Adversarial Resiliency of Person Tracking with Multi-Task Learning. (47%)](http://arxiv.org/abs/2204.00734)

Nilaksh Das, Sheng-Yun Peng, Duen Horng Chau


[Robust and Accurate -- Compositional Architectures for Randomized Smoothing. (31%)](http://arxiv.org/abs/2204.00487)

Miklós Z. Horváth, Mark Niklas Müller, Marc Fischer, Martin Vechev


[FrequencyLowCut Pooling -- Plug & Play against Catastrophic Overfitting. (16%)](http://arxiv.org/abs/2204.00491)

Julia Grabinski, Steffen Jung, Janis Keuper, Margret Keuper


[Preventing Distillation-based Attacks on Neural Network IP. (2%)](http://arxiv.org/abs/2204.00292)

Mahdieh Grailoo, Zain Ul Abideen, Mairo Leier, Samuel Pagliarini


[FedRecAttack: Model Poisoning Attack to Federated Recommendation. (1%)](http://arxiv.org/abs/2204.01499)

Dazhong Rong, Shuai Ye, Ruoyan Zhao, Hon Ning Yuen, Jianhai Chen, Qinming He


## 2022-03-31

[Improving Adversarial Transferability via Neuron Attribution-Based Attacks. (99%)](http://arxiv.org/abs/2204.00008)

Jianping Zhang, Weibin Wu, Jen-tse Huang, Yizhan Huang, Wenxuan Wang, Yuxin Su, Michael R. Lyu


[Adversarial Examples in Random Neural Networks with General Activations. (98%)](http://arxiv.org/abs/2203.17209)

Andrea Montanari, Yuchen Wu


[Scalable Whitebox Attacks on Tree-based Models. (96%)](http://arxiv.org/abs/2204.00103)

Giuseppe Castiglione, Gavin Ding, Masoud Hashemi, Christopher Srinivasa, Ga Wu


[Towards Robust Rain Removal Against Adversarial Attacks: A Comprehensive Benchmark Analysis and Beyond. (86%)](http://arxiv.org/abs/2203.16931)

Yi Yu, Wenhan Yang, Yap-Peng Tan, Alex C. Kot


[Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets. (81%)](http://arxiv.org/abs/2204.00032)

Florian Tramèr, Reza Shokri, Ayrton San Joaquin, Hoang Le, Matthew Jagielski, Sanghyun Hong, Nicholas Carlini


## 2022-03-30

[Investigating Top-$k$ White-Box and Transferable Black-box Attack. (87%)](http://arxiv.org/abs/2204.00089)

Chaoning Zhang, Philipp Benz, Adil Karjauv, Jae Won Cho, Kang Zhang, In So Kweon


[Sensor Data Validation and Driving Safety in Autonomous Driving Systems. (83%)](http://arxiv.org/abs/2203.16130)

Jindi Zhang


[Example-based Explanations with Adversarial Attacks for Respiratory Sound Analysis. (56%)](http://arxiv.org/abs/2203.16141)

Yi Chang, Zhao Ren, Thanh Tam Nguyen, Wolfgang Nejdl, Björn W. Schuller


## 2022-03-29

[Mel Frequency Spectral Domain Defenses against Adversarial Attacks on Speech Recognition Systems. (99%)](http://arxiv.org/abs/2203.15283)

Nicholas Mehlman, Anirudh Sreeram, Raghuveer Peri, Shrikanth Narayanan


[Zero-Query Transfer Attacks on Context-Aware Object Detectors. (99%)](http://arxiv.org/abs/2203.15230)

Zikui Cai, Shantanu Rane, Alejandro E. Brito, Chengyu Song, Srikanth V. Krishnamurthy, Amit K. Roy-Chowdhury, M. Salman Asif


[Exploring Frequency Adversarial Attacks for Face Forgery Detection. (99%)](http://arxiv.org/abs/2203.15674)

Shuai Jia, Chao Ma, Taiping Yao, Bangjie Yin, Shouhong Ding, Xiaokang Yang


[StyleFool: Fooling Video Classification Systems via Style Transfer. (99%)](http://arxiv.org/abs/2203.16000)

Yuxin Cao, Xi Xiao, Ruoxi Sun, Derui Wang, Minhui Xue, Sheng Wen


[Recent improvements of ASR models in the face of adversarial attacks. (98%)](http://arxiv.org/abs/2203.16536)

Raphael Olivier, Bhiksha Raj


[Robust Structured Declarative Classifiers for 3D Point Clouds: Defending Adversarial Attacks with Implicit Gradients. (83%)](http://arxiv.org/abs/2203.15245)

Kaidong Li, Ziming Zhang, Cuncong Zhong, Guanghui Wang


[Treatment Learning Transformer for Noisy Image Classification. (26%)](http://arxiv.org/abs/2203.15529)

Chao-Han Huck Yang, I-Te Danny Hung, Yi-Chieh Liu, Pin-Yu Chen


[Can NMT Understand Me? Towards Perturbation-based Evaluation of NMT Models for Code Generation. (11%)](http://arxiv.org/abs/2203.15319)

Pietro Liguori, Cristina Improta, Vivo Simona De, Roberto Natella, Bojan Cukic, Domenico Cotroneo


## 2022-03-28

[Boosting Black-Box Adversarial Attacks with Meta Learning. (99%)](http://arxiv.org/abs/2203.14607)

Junjie the State Key Lab of Intelligent Control and Decision of Complex Systems and the School of Automation, Beijing Institute of Technology, Beijing, China Beijing Institute of Technology Chongqing Innovation Center, Chongqing, China Fu, Jian the State Key Lab of Intelligent Control and Decision of Complex Systems and the School of Automation, Beijing Institute of Technology, Beijing, China Beijing Institute of Technology Chongqing Innovation Center, Chongqing, China Sun, Gang the State Key Lab of Intelligent Control and Decision of Complex Systems and the School of Automation, Beijing Institute of Technology, Beijing, China Beijing Institute of Technology Chongqing Innovation Center, Chongqing, China Wang


[A Fast and Efficient Conditional Learning for Tunable Trade-Off between Accuracy and Robustness. (62%)](http://arxiv.org/abs/2204.00426)

Souvik Kundu, Sairam Sundaresan, Massoud Pedram, Peter A. Beerel


[Robust Unlearnable Examples: Protecting Data Against Adversarial Learning. (16%)](http://arxiv.org/abs/2203.14533)

Shaopeng Fu, Fengxiang He, Yang Liu, Li Shen, Dacheng Tao


[Neurosymbolic hybrid approach to driver collision warning. (15%)](http://arxiv.org/abs/2203.15076)

Kyongsik Yun, Thomas Lu, Alexander Huyen, Patrick Hammer, Pei Wang


[Attacker Attribution of Audio Deepfakes. (1%)](http://arxiv.org/abs/2203.15563)

Nicolas M. Müller, Franziska Dieckmann, Jennifer Williams


## 2022-03-27

[Text Adversarial Purification as Defense against Adversarial Attacks. (99%)](http://arxiv.org/abs/2203.14207)

Linyang Li, Demin Song, Xipeng Qiu


[Adversarial Representation Sharing: A Quantitative and Secure Collaborative Learning Framework. (8%)](http://arxiv.org/abs/2203.14299)

Jikun Chen, Feng Qiang, Na Ruan


## 2022-03-26

[How to Robustify Black-Box ML Models? A Zeroth-Order Optimization Perspective. (99%)](http://arxiv.org/abs/2203.14195)

Yimeng Zhang, Yuguang Yao, Jinghan Jia, Jinfeng Yi, Mingyi Hong, Shiyu Chang, Sijia Liu


[A Survey of Robust Adversarial Training in Pattern Recognition: Fundamental, Theory, and Methodologies. (99%)](http://arxiv.org/abs/2203.14046)

Zhuang Qian, Kaizhu Huang, Qiu-Feng Wang, Xu-Yao Zhang


[Reverse Engineering of Imperceptible Adversarial Image Perturbations. (99%)](http://arxiv.org/abs/2203.14145)

Yifan Gong, Yuguang Yao, Yize Li, Yimeng Zhang, Xiaoming Liu, Xue Lin, Sijia Liu


[Efficient Global Robustness Certification of Neural Networks via Interleaving Twin-Network Encoding. (33%)](http://arxiv.org/abs/2203.14141)

Zhilu Wang, Chao Huang, Qi Zhu


[A Systematic Survey of Attack Detection and Prevention in Connected and Autonomous Vehicles. (1%)](http://arxiv.org/abs/2203.14965)

Trupil Limbasiya, Ko Zheng Teng, Sudipta Chattopadhyay, Jianying Zhou


[A Roadmap for Big Model. (1%)](http://arxiv.org/abs/2203.14101)

Sha Yuan, Hanyu Zhao, Shuai Zhao, Jiahong Leng, Yangxiao Liang, Xiaozhi Wang, Jifan Yu, Xin Lv, Zhou Shao, Jiaao He, Yankai Lin, Xu Han, Zhenghao Liu, Ning Ding, Yongming Rao, Yizhao Gao, Liang Zhang, Ming Ding, Cong Fang, Yisen Wang, Mingsheng Long, Jing Zhang, Yinpeng Dong, Tianyu Pang, Peng Cui, Lingxiao Huang, Zheng Liang, Huawei Shen, Hui Zhang, Quanshi Zhang, Qingxiu Dong, Zhixing Tan, Mingxuan Wang, Shuo Wang, Long Zhou, Haoran Li, Junwei Bao, Yingwei Pan, Weinan Zhang, Zhou Yu, Rui Yan, Chence Shi, Minghao Xu, Zuobai Zhang, Guoqiang Wang, Xiang Pan, Mengjie Li, Xiaoyu Chu, Zijun Yao, Fangwei Zhu, Shulin Cao, Weicheng Xue, Zixuan Ma, Zhengyan Zhang, Shengding Hu, Yujia Qin, Chaojun Xiao, Zheni Zeng, Ganqu Cui, Weize Chen, Weilin Zhao, Yuan Yao, Peng Li, Wenzhao Zheng, Wenliang Zhao, Ziyi Wang, Borui Zhang, Nanyi Fei, Anwen Hu, Zenan Ling, Haoyang Li, Boxi Cao, Xianpei Han, Weidong Zhan, Baobao Chang, Hao Sun, Jiawen Deng, Chujie Zheng, Juanzi Li, Lei Hou, Xigang Cao, Jidong Zhai, Zhiyuan Liu, Maosong Sun, Jiwen Lu, Zhiwu Lu, Qin Jin, Ruihua Song, Ji-Rong Wen, Zhouchen Lin, Liwei Wang, Hang Su, Jun Zhu, Zhifang Sui, Jiajun Zhang, Yang Liu, Xiaodong He, Minlie Huang, Jian Tang, Jie Tang


## 2022-03-25

[Enhancing Transferability of Adversarial Examples with Spatial Momentum. (99%)](http://arxiv.org/abs/2203.13479)

Guoqiu Wang, Huanqian Yan, Xingxing Wei


[Origins of Low-dimensional Adversarial Perturbations. (98%)](http://arxiv.org/abs/2203.13779)

Elvis Dohmatob, Chuan Guo, Morgane Goibert


[Give Me Your Attention: Dot-Product Attention Considered Harmful for Adversarial Patch Robustness. (89%)](http://arxiv.org/abs/2203.13639)

Giulio Lovisotto, Nicole Finnie, Mauricio Munoz, Chaithanya Kumar Mummadi, Jan Hendrik Metzen


[Improving Robustness of Jet Tagging Algorithms with Adversarial Training. (10%)](http://arxiv.org/abs/2203.13890)

Annika Stein, Xavier Coubez, Spandan Mondal, Andrzej Novak, Alexander Schmidt


[A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training. (5%)](http://arxiv.org/abs/2203.13455)

Yifei Wang, Yisen Wang, Jiansheng Yang, Zhouchen Lin


[A Stitch in Time Saves Nine: A Train-Time Regularizing Loss for Improved Neural Network Calibration. (1%)](http://arxiv.org/abs/2203.13834)

Ramya Hebbalaguppe, Jatin Prakash, Neelabh Madan, Chetan Arora


## 2022-03-24

[Trojan Horse Training for Breaking Defenses against Backdoor Attacks in Deep Learning. (99%)](http://arxiv.org/abs/2203.15506)

Arezoo Rajabi, Bhaskar Ramasubramanian, Radha Poovendran


[A Perturbation Constrained Adversarial Attack for Evaluating the Robustness of Optical Flow. (99%)](http://arxiv.org/abs/2203.13214)

Jenny Schmalfuss, Philipp Scholze, Andrés Bruhn


[NPC: Neuron Path Coverage via Characterizing Decision Logic of Deep Neural Networks. (93%)](http://arxiv.org/abs/2203.12915)

Xiaofei Xie, Tianlin Li, Jian Wang, Lei Ma, Qing Guo, Felix Juefei-Xu, Yang Liu


[MERLIN -- Malware Evasion with Reinforcement LearnINg. (56%)](http://arxiv.org/abs/2203.12980)

Tony Quertier, Benjamin Marais, Stéphane Morucci, Bertrand Fournel


[Repairing Group-Level Errors for DNNs Using Weighted Regularization. (13%)](http://arxiv.org/abs/2203.13612)

Ziyuan Zhong, Yuchi Tian, Conor J. Sweeney, Vicente Ordonez-Roman, Baishakhi Ray


[A Manifold View of Adversarial Risk. (11%)](http://arxiv.org/abs/2203.13277)

Wenjia Zhang, Yikai Zhang, Xiaoling Hu, Mayank Goswami, Chao Chen, Dimitris Metaxas


## 2022-03-23

[Powerful Physical Adversarial Examples Against Practical Face Recognition Systems. (99%)](http://arxiv.org/abs/2203.15498)

Inderjeet Singh, Toshinori Araki, Kazuya Kakizaki


[Adversarial Training for Improving Model Robustness? Look at Both Prediction and Interpretation. (99%)](http://arxiv.org/abs/2203.12709)

Hanjie Chen, Yangfeng Ji


[Input-specific Attention Subnetworks for Adversarial Detection. (99%)](http://arxiv.org/abs/2203.12298)

Emil Biju, Anirudh Sriram, Pratyush Kumar, Mitesh M Khapra


[Self-supervised Learning of Adversarial Example: Towards Good Generalizations for Deepfake Detection. (69%)](http://arxiv.org/abs/2203.12208)

Liang Chen, Yong Zhang, Yibing Song, Lingqiao Liu, Jue Wang


[Distort to Detect, not Affect: Detecting Stealthy Sensor Attacks with Micro-distortion. (3%)](http://arxiv.org/abs/2203.12249)

Suman Sourav, Binbin Chen


[On the (Limited) Generalization of MasterFace Attacks and Its Relation to the Capacity of Face Representations. (3%)](http://arxiv.org/abs/2203.12387)

Philipp Terhörst, Florian Bierbaum, Marco Huber, Naser Damer, Florian Kirchbuchner, Kiran Raja, Arjan Kuijper


## 2022-03-22

[Exploring High-Order Structure for Robust Graph Structure Learning. (99%)](http://arxiv.org/abs/2203.11492)

Guangqian Yang, Yibing Zhan, Jinlong Li, Baosheng Yu, Liu Liu, Fengxiang He


[On Adversarial Robustness of Large-scale Audio Visual Learning. (93%)](http://arxiv.org/abs/2203.12122)

Juncheng B Bernie Li, Shuhui Bernie Qu, Xinjian Bernie Li, Bernie Po-Yao, Huang, Florian Metze


[On the (Non-)Robustness of Two-Layer Neural Networks in Different Learning Regimes. (86%)](http://arxiv.org/abs/2203.11864)

Elvis Dohmatob, Alberto Bietti


[Semi-Targeted Model Poisoning Attack on Federated Learning via Backward Error Analysis. (78%)](http://arxiv.org/abs/2203.11633)

Yuwei Sun, Hideya Ochiai, Jun Sakuma


[A Girl Has A Name, And It's ... Adversarial Authorship Attribution for Deobfuscation. (2%)](http://arxiv.org/abs/2203.11849)

Wanyue Zhai, Jonathan Rusert, Zubair Shafiq, Padmini Srinivasan


[GradViT: Gradient Inversion of Vision Transformers. (1%)](http://arxiv.org/abs/2203.11894)

Ali Hatamizadeh, Hongxu Yin, Holger Roth, Wenqi Li, Jan Kautz, Daguang Xu, Pavlo Molchanov


[On Robust Classification using Contractive Hamiltonian Neural ODEs. (1%)](http://arxiv.org/abs/2203.11805)

Muhammad Zakwan, Liang Xu, Giancarlo Ferrari-Trecate


## 2022-03-21

[Making DeepFakes more spurious: evading deep face forgery detection via trace removal attack. (92%)](http://arxiv.org/abs/2203.11433)

Chi Liu, Huajie Chen, Tianqing Zhu, Jun Zhang, Wanlei Zhou


[Integrity Fingerprinting of DNN with Double Black-box Design and Verification. (10%)](http://arxiv.org/abs/2203.10902)

Shuo Wang, Sidharth Agarwal, Sharif Abuadbba, Kristen Moore, Surya Nepal, Salil Kanhere


[On The Robustness of Offensive Language Classifiers. (2%)](http://arxiv.org/abs/2203.11331)

Jonathan Rusert, Zubair Shafiq, Padmini Srinivasan


[Defending against Co-residence Attack in Energy-Efficient Cloud: An Optimization based Real-time Secure VM Allocation Strategy. (1%)](http://arxiv.org/abs/2203.10734)

Lu Cao, Ruiwen Li, Xiaojun Ruan, Yuhong Liu


## 2022-03-20

[An Intermediate-level Attack Framework on The Basis of Linear Regression. (99%)](http://arxiv.org/abs/2203.10723)

Yiwen Guo, Qizhang Li, Wangmeng Zuo, Hao Chen


[A Prompting-based Approach for Adversarial Example Generation and Robustness Enhancement. (99%)](http://arxiv.org/abs/2203.10714)

Yuting Yang, Pei Huang, Juan Cao, Jintao Li, Yun Lin, Jin Song Dong, Feifei Ma, Jian Zhang


[Leveraging Expert Guided Adversarial Augmentation For Improving Generalization in Named Entity Recognition. (82%)](http://arxiv.org/abs/2203.10693)

Aaron Reich, Jiaao Chen, Aastha Agrawal, Yanzhe Zhang, Diyi Yang


[Adversarial Parameter Attack on Deep Neural Networks. (62%)](http://arxiv.org/abs/2203.10502)

Lijia Yu, Yihan Wang, Xiao-Shan Gao


## 2022-03-19

[Adversarial Defense via Image Denoising with Chaotic Encryption. (99%)](http://arxiv.org/abs/2203.10290)

Shi Hu, Eric Nalisnick, Max Welling


[Perturbations in the Wild: Leveraging Human-Written Text Perturbations for Realistic Adversarial Attack and Defense. (98%)](http://arxiv.org/abs/2203.10346)

Thai Le, Jooyoung Lee, Kevin Yen, Yifan Hu, Dongwon Lee


[Distinguishing Non-natural from Natural Adversarial Samples for More Robust Pre-trained Language Model. (84%)](http://arxiv.org/abs/2203.11199)

Jiayi Wang, Rongzhou Bao, Zhuosheng Zhang, Hai Zhao


[Efficient Neural Network Analysis with Sum-of-Infeasibilities. (74%)](http://arxiv.org/abs/2203.11201)

Haoze Wu, Aleksandar Zeljić, Guy Katz, Clark Barrett


[Deep Learning Generalization, Extrapolation, and Over-parameterization. (68%)](http://arxiv.org/abs/2203.10366)

Roozbeh Yousefzadeh


[On Robust Prefix-Tuning for Text Classification. (10%)](http://arxiv.org/abs/2203.10378)

Zonghan Yang, Yang Liu


## 2022-03-18

[Concept-based Adversarial Attacks: Tricking Humans and Classifiers Alike. (99%)](http://arxiv.org/abs/2203.10166)

Johannes Schneider, Giovanni Apruzzese


[Adversarial Attacks on Deep Learning-based Video Compression and Classification Systems. (99%)](http://arxiv.org/abs/2203.10183)

Jung-Woo Chang, Mojan Javaheripi, Seira Hidano, Farinaz Koushanfar


[Neural Predictor for Black-Box Adversarial Attacks on Speech Recognition. (99%)](http://arxiv.org/abs/2203.09849)

Marie Biolková, Bac Nguyen


[AutoAdversary: A Pixel Pruning Method for Sparse Adversarial Attack. (99%)](http://arxiv.org/abs/2203.09756)

Jinqiao Li, Xiaotao Liu, Jian Zhao, Furao Shen


[Alleviating Adversarial Attacks on Variational Autoencoders with MCMC. (96%)](http://arxiv.org/abs/2203.09940)

Anna Kuzina, Max Welling, Jakub M. Tomczak


[DTA: Physical Camouflage Attacks using Differentiable Transformation Network. (83%)](http://arxiv.org/abs/2203.09831)

Naufal Suryanto, Yongsu Kim, Hyoeun Kang, Harashta Tatimma Larasati, Youngyeo Yun, Thi-Thu-Huong Le, Hunmin Yang, Se-Yoon Oh, Howon Kim


[AdIoTack: Quantifying and Refining Resilience of Decision Tree Ensemble Inference Models against Adversarial Volumetric Attacks on IoT Networks. (78%)](http://arxiv.org/abs/2203.09792)

Arman Pashamokhtari, Gustavo Batista, Hassan Habibi Gharakheili


[Towards Robust 2D Convolution for Reliable Visual Recognition. (9%)](http://arxiv.org/abs/2203.09790)

Lida Li, Shuai Li, Kun Wang, Xiangchu Feng, Lei Zhang


## 2022-03-17

[Improving the Transferability of Targeted Adversarial Examples through Object-Based Diverse Input. (99%)](http://arxiv.org/abs/2203.09123)

Junyoung Byun, Seungju Cho, Myung-Joon Kwon, Hee-Seon Kim, Changick Kim


[Self-Ensemble Adversarial Training for Improved Robustness. (99%)](http://arxiv.org/abs/2203.09678)

Hongjun Wang, Yisen Wang


[Leveraging Adversarial Examples to Quantify Membership Information Leakage. (98%)](http://arxiv.org/abs/2203.09566)

Grosso Ganesh Del, Hamid Jalalzai, Georg Pichler, Catuscia Palamidessi, Pablo Piantanida


[On the Properties of Adversarially-Trained CNNs. (93%)](http://arxiv.org/abs/2203.09243)

Mattia Carletti, Matteo Terzi, Gian Antonio Susto


[PiDAn: A Coherence Optimization Approach for Backdoor Attack Detection and Mitigation in Deep Neural Networks. (89%)](http://arxiv.org/abs/2203.09289)

Yue Wang, Wenqing Li, Esha Sarkar, Muhammad Shafique, Michail Maniatakos, Saif Eddin Jabari


[HDLock: Exploiting Privileged Encoding to Protect Hyperdimensional Computing Models against IP Stealing. (1%)](http://arxiv.org/abs/2203.09681)

Shijin Duan, Shaolei Ren, Xiaolin Xu


## 2022-03-16

[Robustness through Cognitive Dissociation Mitigation in Contrastive Adversarial Training. (99%)](http://arxiv.org/abs/2203.08959)

Adir Rahamim, Itay Naeh


[Towards Practical Certifiable Patch Defense with Vision Transformer. (98%)](http://arxiv.org/abs/2203.08519)

Zhaoyu Chen, Bo Li, Jianghe Xu, Shuang Wu, Shouhong Ding, Wenqiang Zhang


[Patch-Fool: Are Vision Transformers Always Robust Against Adversarial Perturbations? (97%)](http://arxiv.org/abs/2203.08392)

Yonggan Fu, Shunyao Zhang, Shang Wu, Cheng Wan, Yingyan Lin


[Provable Adversarial Robustness for Fractional Lp Threat Models. (87%)](http://arxiv.org/abs/2203.08945)

Alexander Levine, Soheil Feizi


[What Do Adversarially trained Neural Networks Focus: A Fourier Domain-based Study. (83%)](http://arxiv.org/abs/2203.08739)

Binxiao Huang, Chaofan Tao, Rui Lin, Ngai Wong


[COPA: Certifying Robust Policies for Offline Reinforcement Learning against Poisoning Attacks. (82%)](http://arxiv.org/abs/2203.08398)

Fan Wu, Linyi Li, Chejian Xu, Huan Zhang, Bhavya Kailkhura, Krishnaram Kenthapadi, Ding Zhao, Bo Li


[Sniper Backdoor: Single Client Targeted Backdoor Attack in Federated Learning. (70%)](http://arxiv.org/abs/2203.08689)

Gorka Abad, Servio Paguada, Oguzhan Ersoy, Stjepan Picek, Víctor Julio Ramírez-Durán, Aitor Urbieta


[Reducing Flipping Errors in Deep Neural Networks. (68%)](http://arxiv.org/abs/2203.08390)

Xiang Deng, Yun Xiao, Bo Long, Zhongfei Zhang


[Attacking deep networks with surrogate-based adversarial black-box methods is easy. (45%)](http://arxiv.org/abs/2203.08725)

Nicholas A. Lord, Romain Mueller, Luca Bertinetto


[On the Convergence of Certified Robust Training with Interval Bound Propagation. (15%)](http://arxiv.org/abs/2203.08961)

Yihan Wang, Zhouxing Shi, Quanquan Gu, Cho-Jui Hsieh


[MPAF: Model Poisoning Attacks to Federated Learning based on Fake Clients. (15%)](http://arxiv.org/abs/2203.08669)

Xiaoyu Cao, Neil Zhenqiang Gong


[Understanding robustness and generalization of artificial neural networks through Fourier masks. (2%)](http://arxiv.org/abs/2203.08822)

Nikos Karantzas, Emma Besier, Josue Ortega Caro, Xaq Pitkow, Andreas S. Tolias, Ankit B. Patel, Fabio Anselmi


## 2022-03-15

[Generalized but not Robust? Comparing the Effects of Data Modification Methods on Out-of-Domain Generalization and Adversarial Robustness. (76%)](http://arxiv.org/abs/2203.07653)

Tejas Gokhale, Swaroop Mishra, Man Luo, Bhavdeep Singh Sachdeva, Chitta Baral


[Internet-based Social Engineering Attacks, Defenses and Psychology: A Survey. (13%)](http://arxiv.org/abs/2203.08302)

Theodore Longtchi, Rosana Montañez Rodriguez, Laith Al-Shawaf, Adham Atyabi, Shouhuai Xu


[Towards Adversarial Control Loops in Sensor Attacks: A Case Study to Control the Kinematics and Actuation of Embedded Systems. (10%)](http://arxiv.org/abs/2203.07670)

Yazhou Tu, Sara Rampazzi, Xiali Hei


[LDP: Learnable Dynamic Precision for Efficient Deep Neural Network Training and Inference. (1%)](http://arxiv.org/abs/2203.07713)

Zhongzhi Yu, Yonggan Fu, Shang Wu, Mengquan Li, Haoran You, Yingyan Lin


[Adversarial Counterfactual Augmentation: Application in Alzheimer's Disease Classification. (1%)](http://arxiv.org/abs/2203.07815)

Tian Xia, Pedro Sanchez, Chen Qin, Sotirios A. Tsaftaris


## 2022-03-14

[Efficient universal shuffle attack for visual object tracking. (99%)](http://arxiv.org/abs/2203.06898)

Siao Liu, Zhaoyu Chen, Wei Li, Jiwei Zhu, Jiafeng Wang, Wenqiang Zhang, Zhongxue Gan


[Defending Against Adversarial Attack in ECG Classification with Adversarial Distillation Training. (99%)](http://arxiv.org/abs/2203.09487)

Jiahao Shao, Shijia Geng, Zhaoji Fu, Weilun Xu, Tong Liu, Shenda Hong


[Task-Agnostic Robust Representation Learning. (98%)](http://arxiv.org/abs/2203.07596)

A. Tuan Nguyen, Ser Nam Lim, Philip Torr


[Energy-Latency Attacks via Sponge Poisoning. (91%)](http://arxiv.org/abs/2203.08147)

Antonio Emanuele Cinà, Ambra Demontis, Battista Biggio, Fabio Roli, Marcello Pelillo


[Adversarial amplitude swap towards robust image classifiers. (83%)](http://arxiv.org/abs/2203.07138)

Chun Yang Tan, Hiroshi Kera, Kazuhiko Kawamoto


[On the benefits of knowledge distillation for adversarial robustness. (82%)](http://arxiv.org/abs/2203.07159)

Javier Maroto, Guillermo Ortiz-Jiménez, Pascal Frossard


[RES-HD: Resilient Intelligent Fault Diagnosis Against Adversarial Attacks Using Hyper-Dimensional Computing. (82%)](http://arxiv.org/abs/2203.08148)

Onat Gungor, Tajana Rosing, Baris Aksanli


[Defending From Physically-Realizable Adversarial Attacks Through Internal Over-Activation Analysis. (54%)](http://arxiv.org/abs/2203.07341)

Giulio Rossolini, Federico Nesti, Fabio Brau, Alessandro Biondi, Giorgio Buttazzo


## 2022-03-13

[LAS-AT: Adversarial Training with Learnable Attack Strategy. (99%)](http://arxiv.org/abs/2203.06616)

Xiaojun Jia, Yong Zhang, Baoyuan Wu, Ke Ma, Jue Wang, Xiaochun Cao


[Generating Practical Adversarial Network Traffic Flows Using NIDSGAN. (99%)](http://arxiv.org/abs/2203.06694)

Bolor-Erdene Zolbayar, Ryan Sheatsley, Patrick McDaniel, Michael J. Weisman, Sencun Zhu, Shitong Zhu, Srikanth Krishnamurthy


[Model Inversion Attack against Transfer Learning: Inverting a Model without Accessing It. (92%)](http://arxiv.org/abs/2203.06570)

Dayong Ye, Huiqiang Chen, Shuai Zhou, Tianqing Zhu, Wanlei Zhou, Shouling Ji


[One Parameter Defense -- Defending against Data Inference Attacks via Differential Privacy. (67%)](http://arxiv.org/abs/2203.06580)

Dayong Ye, Sheng Shen, Tianqing Zhu, Bo Liu, Wanlei Zhou


[Policy Learning for Robust Markov Decision Process with a Mismatched Generative Model. (3%)](http://arxiv.org/abs/2203.06587)

Jialian Li, Tongzheng Ren, Dong Yan, Hang Su, Jun Zhu


## 2022-03-12

[Query-Efficient Black-box Adversarial Attacks Guided by a Transfer-based Prior. (99%)](http://arxiv.org/abs/2203.06560)

Yinpeng Dong, Shuyu Cheng, Tianyu Pang, Hang Su, Jun Zhu


[A Survey of Adversarial Defences and Robustness in NLP. (99%)](http://arxiv.org/abs/2203.06414)

Shreya Goyal, Sumanth Doddapaneni, Mitesh M. Khapra, Balaraman Ravindran


[Label-only Model Inversion Attack: The Attack that Requires the Least Information. (47%)](http://arxiv.org/abs/2203.06555)

Dayong Ye, Tianqing Zhu, Shuai Zhou, Bo Liu, Wanlei Zhou


## 2022-03-11

[Block-Sparse Adversarial Attack to Fool Transformer-Based Text Classifiers. (99%)](http://arxiv.org/abs/2203.05948)

Sahar Sadrizadeh, Ljiljana Dolamic, Pascal Frossard


[Learning from Attacks: Attacking Variational Autoencoder for Improving Image Classification. (98%)](http://arxiv.org/abs/2203.07027)

Jianzhang Zheng, Fan Yang, Hao Shen, Xuan Tang, Mingsong Chen, Liang Song, Xian Wei


[An integrated Auto Encoder-Block Switching defense approach to prevent adversarial attacks. (96%)](http://arxiv.org/abs/2203.10930)

Anirudh Yadav, Ashutosh Upadhyay, S. Sharanya


[Enhancing Adversarial Training with Second-Order Statistics of Weights. (38%)](http://arxiv.org/abs/2203.06020)

Gaojie Jin, Xinping Yi, Wei Huang, Sven Schewe, Xiaowei Huang


[ROOD-MRI: Benchmarking the robustness of deep learning segmentation models to out-of-distribution and corrupted data in MRI. (33%)](http://arxiv.org/abs/2203.06060)

Lyndon Boone, Mahdi Biparva, Parisa Mojiri Forooshani, Joel Ramirez, Mario Masellis, Robert Bartha, Sean Symons, Stephen Strother, Sandra E. Black, Chris Heyn, Anne L. Martel, Richard H. Swartz, Maged Goubran


[Perception Over Time: Temporal Dynamics for Robust Image Understanding. (16%)](http://arxiv.org/abs/2203.06254)

Maryam Daniali, Edward Kim


[Reinforcement Learning for Linear Quadratic Control is Vulnerable Under Cost Manipulation. (15%)](http://arxiv.org/abs/2203.05774)

Yunhan Huang, Quanyan Zhu


## 2022-03-10

[Exploiting the Potential of Datasets: A Data-Centric Approach for Model Robustness. (92%)](http://arxiv.org/abs/2203.05323)

Yiqi Zhong, Lei Wu, Xianming Liu, Junjun Jiang


[Membership Privacy Protection for Image Translation Models via Adversarial Knowledge Distillation. (75%)](http://arxiv.org/abs/2203.05212)

Saeed Ranjbar Alvar, Lanjun Wang, Jian Pei, Yong Zhang


[Attack Analysis of Face Recognition Authentication Systems Using Fast Gradient Sign Method. (69%)](http://arxiv.org/abs/2203.05653)

Arbena Musa, Kamer Vishi, Blerim Rexha


[Attacks as Defenses: Designing Robust Audio CAPTCHAs Using Attacks on Automatic Speech Recognition Systems. (64%)](http://arxiv.org/abs/2203.05408)

Hadi Abdullah, Aditya Karlekar, Saurabh Prasad, Muhammad Sajidur Rahman, Logan Blue, Luke A. Bauer, Vincent Bindschaedler, Patrick Traynor


[SoK: On the Semantic AI Security in Autonomous Driving. (10%)](http://arxiv.org/abs/2203.05314)

Junjie Shen, Ningfei Wang, Ziwen Wan, Yunpeng Luo, Takami Sato, Zhisheng Hu, Xinyang Zhang, Shengjian Guo, Zhenyu Zhong, Kang Li, Ziming Zhao, Chunming Qiao, Qi Alfred Chen


## 2022-03-09

[Practical No-box Adversarial Attacks with Training-free Hybrid Image Transformation. (99%)](http://arxiv.org/abs/2203.04607)

Qilong Zhang, Chaoning Zhang, Chaoqun Li, Jingkuan Song, Lianli Gao, Heng Tao Shen


[Practical Evaluation of Adversarial Robustness via Adaptive Auto Attack. (99%)](http://arxiv.org/abs/2203.05154)

Ye Liu, Yaya Cheng, Lianli Gao, Xianglong Liu, Qilong Zhang, Jingkuan Song


[Frequency-driven Imperceptible Adversarial Attack on Semantic Similarity. (99%)](http://arxiv.org/abs/2203.05151)

Cheng Luo, Qinliang Lin, Weicheng Xie, Bizhu Wu, Jinheng Xie, Linlin Shen


[Binary Classification Under $\ell_0$ Attacks for General Noise Distribution. (98%)](http://arxiv.org/abs/2203.04855)

Payam Delgosha, Hamed Hassani, Ramtin Pedarsani


[Controllable Evaluation and Generation of Physical Adversarial Patch on Face Recognition. (97%)](http://arxiv.org/abs/2203.04623)

Xiao Yang, Yinpeng Dong, Tianyu Pang, Zihao Xiao, Hang Su, Jun Zhu


[Reverse Engineering $\ell_p$ attacks: A block-sparse optimization approach with recovery guarantees. (92%)](http://arxiv.org/abs/2203.04886)

Darshan Thaker, Paris Giampouras, René Vidal


[Defending Black-box Skeleton-based Human Activity Classifiers. (92%)](http://arxiv.org/abs/2203.04713)

He Wang, Yunfeng Diao, Zichang Tan, Guodong Guo


[Robust Federated Learning Against Adversarial Attacks for Speech Emotion Recognition. (81%)](http://arxiv.org/abs/2203.04696)

Yi Chang, Sofiane Laridi, Zhao Ren, Gregory Palmer, Björn W. Schuller, Marco Fisichella


[Improving Neural ODEs via Knowledge Distillation. (80%)](http://arxiv.org/abs/2203.05103)

Haoyu Chu, Shikui Wei, Qiming Lu, Yao Zhao


[Physics-aware Complex-valued Adversarial Machine Learning in Reconfigurable Diffractive All-optical Neural Network. (22%)](http://arxiv.org/abs/2203.06055)

Ruiyang Chen, Yingjie Li, Minhan Lou, Jichao Fan, Yingheng Tang, Berardi Sensale-Rodriguez, Cunxi Yu, Weilu Gao


[On the surprising tradeoff between ImageNet accuracy and perceptual similarity. (1%)](http://arxiv.org/abs/2203.04946)

Manoj Kumar, Neil Houlsby, Nal Kalchbrenner, Ekin D. Cubuk


## 2022-03-08

[Adaptative Perturbation Patterns: Realistic Adversarial Learning for Robust NIDS. (99%)](http://arxiv.org/abs/2203.04234)

João Vitorino, Nuno Oliveira, Isabel Praça


[Shape-invariant 3D Adversarial Point Clouds. (99%)](http://arxiv.org/abs/2203.04041)

Qidong Huang, Xiaoyi Dong, Dongdong Chen, Hang Zhou, Weiming Zhang, Nenghai Yu


[ART-Point: Improving Rotation Robustness of Point Cloud Classifiers via Adversarial Rotation. (92%)](http://arxiv.org/abs/2203.03888)

Robin Wang, Yibo Yang, Dacheng Tao


[Robustly-reliable learners under poisoning attacks. (13%)](http://arxiv.org/abs/2203.04160)

Maria-Florina Balcan, Avrim Blum, Steve Hanneke, Dravyansh Sharma


[DeepSE-WF: Unified Security Estimation for Website Fingerprinting Defenses. (2%)](http://arxiv.org/abs/2203.04428)

Alexander Veicht, Cedric Renggli, Diogo Barradas


[Joint rotational invariance and adversarial training of a dual-stream Transformer yields state of the art Brain-Score for Area V4. (1%)](http://arxiv.org/abs/2203.06649)

William Berrios, Arturo Deza


[Harmonicity Plays a Critical Role in DNN Based Versus in Biologically-Inspired Monaural Speech Segregation Systems. (1%)](http://arxiv.org/abs/2203.04420)

Rahil Institute for Systems Research, University of Maryland Parikh, Ilya Google Inc Kavalerov, Carol Institute for Systems Research, University of Maryland Espy-Wilson, Shihab Institute for Systems Research, University of Maryland Shamma


## 2022-03-07

[ImageNet-Patch: A Dataset for Benchmarking Machine Learning Robustness against Adversarial Patches. (99%)](http://arxiv.org/abs/2203.04412)

Maura Pintor, Daniele Angioni, Angelo Sotgiu, Luca Demetrio, Ambra Demontis, Battista Biggio, Fabio Roli


[Art-Attack: Black-Box Adversarial Attack via Evolutionary Art. (99%)](http://arxiv.org/abs/2203.04405)

Phoenix Williams, Ke Li


[Shadows can be Dangerous: Stealthy and Effective Physical-world Adversarial Attack by Natural Phenomenon. (99%)](http://arxiv.org/abs/2203.03818)

Yiqi Zhong, Xianming Liu, Deming Zhai, Junjun Jiang, Xiangyang Ji


[Adversarial Texture for Fooling Person Detectors in the Physical World. (98%)](http://arxiv.org/abs/2203.03373)

Zhanhao Hu, Siyuan Huang, Xiaopei Zhu, Xiaolin Hu, Fuchun Sun, Bo Zhang


[Defending Graph Convolutional Networks against Dynamic Graph Perturbations via Bayesian Self-supervision. (83%)](http://arxiv.org/abs/2203.03762)

Jun Zhuang, Mohammad Al Hasan


[Towards Efficient Data-Centric Robust Machine Learning with Noise-based Augmentation. (31%)](http://arxiv.org/abs/2203.03810)

Xiaogeng Liu, Haoyu Wang, Yechao Zhang, Fangzhou Wu, Shengshan Hu


## 2022-03-06

[$A^{3}D$: A Platform of Searching for Robust Neural Architectures and Efficient Adversarial Attacks. (99%)](http://arxiv.org/abs/2203.03128)

Jialiang Sun, Wen Yao, Tingsong Jiang, Chao Li, Xiaoqian Chen


[Protecting Facial Privacy: Generating Adversarial Identity Masks via Style-robust Makeup Transfer. (98%)](http://arxiv.org/abs/2203.03121)

Shengshan Hu, Xiaogeng Liu, Yechao Zhang, Minghui Li, Leo Yu Zhang, Hai Jin, Libing Wu


[Scalable Uncertainty Quantification for Deep Operator Networks using Randomized Priors. (45%)](http://arxiv.org/abs/2203.03048)

Yibo Yang, Georgios Kissas, Paris Perdikaris


[Evaluation of Interpretability Methods and Perturbation Artifacts in Deep Neural Networks. (2%)](http://arxiv.org/abs/2203.02928)

Lennart Brocki, Neo Christopher Chung


## 2022-03-05

[aaeCAPTCHA: The Design and Implementation of Audio Adversarial CAPTCHA. (92%)](http://arxiv.org/abs/2203.02735)

Md Imran Hossen, Xiali Hei


## 2022-03-04

[Targeted Data Poisoning Attack on News Recommendation System by Content Perturbation. (82%)](http://arxiv.org/abs/2203.03560)

Xudong Zhang, Zan Wang, Jingke Zhao, Lanjun Wang


[Concept-based Explanations for Out-Of-Distribution Detectors. (1%)](http://arxiv.org/abs/2203.02586)

Jihye Choi, Jayaram Raghuram, Ryan Feng, Jiefeng Chen, Somesh Jha, Atul Prakash


## 2022-03-03

[Ad2Attack: Adaptive Adversarial Attack on Real-Time UAV Tracking. (99%)](http://arxiv.org/abs/2203.01516)

Changhong Fu, Sihang Li, Xinnan Yuan, Junjie Ye, Ziang Cao, Fangqiang Ding


[Detection of Word Adversarial Examples in Text Classification: Benchmark and Baseline via Robust Density Estimation. (98%)](http://arxiv.org/abs/2203.01677)

KiYoon Yoo, Jangho Kim, Jiho Jang, Nojun Kwak


[Adversarial Patterns: Building Robust Android Malware Classifiers. (98%)](http://arxiv.org/abs/2203.02121)

Dipkamal Bhusal, Nidhi Rastogi


[Improving Health Mentioning Classification of Tweets using Contrastive Adversarial Training. (84%)](http://arxiv.org/abs/2203.01895)

Pervaiz Iqbal Khan, Shoaib Ahmed Siddiqui, Imran Razzak, Andreas Dengel, Sheraz Ahmed


[Label-Only Model Inversion Attacks via Boundary Repulsion. (74%)](http://arxiv.org/abs/2203.01925)

Mostafa Kahla, Si Chen, Hoang Anh Just, Ruoxi Jia


[Fairness-aware Adversarial Perturbation Towards Bias Mitigation for Deployed Deep Models. (56%)](http://arxiv.org/abs/2203.01584)

Zhibo Wang, Xiaowei Dong, Henry Xue, Zhifei Zhang, Weifeng Chiu, Tao Wei, Kui Ren


[Why adversarial training can hurt robust accuracy. (22%)](http://arxiv.org/abs/2203.02006)

Jacob Clarysse, Julia Hörmann, Fanny Yang


[Understanding Failure Modes of Self-Supervised Learning. (4%)](http://arxiv.org/abs/2203.01881)

Neha Mukund Kalibhat, Kanika Narang, Liang Tan, Hamed Firooz, Maziar Sanjabi, Soheil Feizi


[Ensemble Methods for Robust Support Vector Machines using Integer Programming. (2%)](http://arxiv.org/abs/2203.01606)

Jannis Kurtz


[Autonomous and Resilient Control for Optimal LEO Satellite Constellation Coverage Against Space Threats. (1%)](http://arxiv.org/abs/2203.02050)

Yuhan Zhao, Quanyan Zhu


## 2022-03-02

[Enhancing Adversarial Robustness for Deep Metric Learning. (99%)](http://arxiv.org/abs/2203.01439)

Mo Zhou, Vishal M. Patel


[Canonical foliations of neural networks: application to robustness. (98%)](http://arxiv.org/abs/2203.00922)

Eliot Tron, Nicolas Couellan, Stéphane Puechmorel


[Detecting Adversarial Perturbations in Multi-Task Perception. (98%)](http://arxiv.org/abs/2203.01177)

Marvin Klingner, Varun Ravi Kumar, Senthil Yogamani, Andreas Bär, Tim Fingscheidt


[Adversarial Robustness of Neural-Statistical Features in Detection of Generative Transformers. (69%)](http://arxiv.org/abs/2203.07983)

Evan Crothers, Nathalie Japkowicz, Herna Viktor, Paula Branco


[Video is All You Need: Attacking PPG-based Biometric Authentication. (13%)](http://arxiv.org/abs/2203.00928)

Lin Li, Chao Chen, Lei Pan, Jun Zhang, Yang Xiang


[MIAShield: Defending Membership Inference Attacks via Preemptive Exclusion of Members. (2%)](http://arxiv.org/abs/2203.00915)

Ismat Jarin, Birhanu Eshete


[A Quantitative Geometric Approach to Neural-Network Smoothness. (2%)](http://arxiv.org/abs/2203.01212)

Zi Wang, Gautam Prakriya, Somesh Jha


## 2022-03-01

[Adversarial samples for deep monocular 6D object pose estimation. (99%)](http://arxiv.org/abs/2203.00302)

Jinlai Zhang, Weiming Li, Shuang Liang, Hao Wang, Jihong Zhu


[Physical Backdoor Attacks to Lane Detection Systems in Autonomous Driving. (87%)](http://arxiv.org/abs/2203.00858)

Xingshuo Han, Guowen Xu, Yuan Zhou, Xuehuan Yang, Jiwei Li, Tianwei Zhang


[Global-Local Regularization Via Distributional Robustness. (86%)](http://arxiv.org/abs/2203.00553)

Hoang Phan, Trung Le, Trung Phung, Tuan Anh Bui, Nhat Ho, Dinh Phung


[Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation. (11%)](http://arxiv.org/abs/2203.01323)

Wei Dai, Daniel Berleant


[Signature Correction Attack on Dilithium Signature Scheme. (1%)](http://arxiv.org/abs/2203.00637)

Saad Islam, Koksal Mus, Richa Singh, Patrick Schaumont, Berk Sunar


## 2022-02-28

[Enhance transferability of adversarial examples with model architecture. (99%)](http://arxiv.org/abs/2202.13625)

Mingyuan Fan, Wenzhong Guo, Shengxing Yu, Zuobin Ying, Ximeng Liu


[Towards Robust Stacked Capsule Autoencoder with Hybrid Adversarial Training. (99%)](http://arxiv.org/abs/2202.13755)

Jiazhu Dai, Siwei Xiong


[Evaluating the Adversarial Robustness of Adaptive Test-time Defenses. (98%)](http://arxiv.org/abs/2202.13711)

Francesco Croce, Sven Gowal, Thomas Brunner, Evan Shelhamer, Matthias Hein, Taylan Cemgil


[MaMaDroid2.0 -- The Holes of Control Flow Graphs. (88%)](http://arxiv.org/abs/2202.13922)

Harel Berger, Chen Hajaj, Enrico Mariconti, Amit Dvir


[Improving Lexical Embeddings for Robust Question Answering. (67%)](http://arxiv.org/abs/2202.13636)

Weiwen Xu, Bowei Zou, Wai Lam, Ai Ti Aw


[Robust Textual Embedding against Word-level Adversarial Attacks. (26%)](http://arxiv.org/abs/2202.13817)

Yichen Yang, Xiaosen Wang, Kun He


[Artificial Intelligence for Cyber Security (AICS). (1%)](http://arxiv.org/abs/2202.14010)

James Holt, Edward Raff, Ahmad Ridley, Dennis Ross, Arunesh Sinha, Diane Staheli, William Streilen, Milind Tambe, Yevgeniy Vorobeychik, Allan Wollaber


[Explaining RADAR features for detecting spoofing attacks in Connected Autonomous Vehicles. (1%)](http://arxiv.org/abs/2203.00150)

Nidhi Rastogi, Sara Rampazzi, Michael Clifford, Miriam Heller, Matthew Bishop, Karl Levitt


## 2022-02-27

[A Unified Wasserstein Distributional Robustness Framework for Adversarial Training. (99%)](http://arxiv.org/abs/2202.13437)

Tuan Anh Bui, Trung Le, Quan Tran, He Zhao, Dinh Phung


[Robust Control of Partially Specified Boolean Networks. (1%)](http://arxiv.org/abs/2202.13440)

Luboš Brim, Samuel Pastva, David Šafránek, Eva Šmijáková


## 2022-02-26

[Adversarial robustness of sparse local Lipschitz predictors. (87%)](http://arxiv.org/abs/2202.13216)

Ramchandran Muthukumar, Jeremias Sulam


[Neuro-Inspired Deep Neural Networks with Sparse, Strong Activations. (45%)](http://arxiv.org/abs/2202.13074)

Metehan Cekic, Can Bakiskan, Upamanyu Madhow


[Automation of reversible steganographic coding with nonlinear discrete optimisation. (1%)](http://arxiv.org/abs/2202.13133)

Ching-Chun Chang


## 2022-02-25

[ARIA: Adversarially Robust Image Attribution for Content Provenance. (99%)](http://arxiv.org/abs/2202.12860)

Maksym Andriushchenko, Xiaoyang Rebecca Li, Geoffrey Oxholm, Thomas Gittings, Tu Bui, Nicolas Flammarion, John Collomosse


[Projective Ranking-based GNN Evasion Attacks. (97%)](http://arxiv.org/abs/2202.12993)

He Zhang, Xingliang Yuan, Chuan Zhou, Shirui Pan


[On the Effectiveness of Dataset Watermarking in Adversarial Settings. (56%)](http://arxiv.org/abs/2202.12506)

Buse Gul Atli Tekgul, N. Asokan


## 2022-02-24

[Towards Effective and Robust Neural Trojan Defenses via Input Filtering. (92%)](http://arxiv.org/abs/2202.12154)

Kien Do, Haripriya Harikumar, Hung Le, Dung Nguyen, Truyen Tran, Santu Rana, Dang Nguyen, Willy Susilo, Svetha Venkatesh


[Robust Probabilistic Time Series Forecasting. (76%)](http://arxiv.org/abs/2202.11910)

TaeHo Yoon, Youngsuk Park, Ernest K. Ryu, Yuyang Wang


[Understanding Adversarial Robustness from Feature Maps of Convolutional Layers. (50%)](http://arxiv.org/abs/2202.12435)

Cong Xu, Min Yang


[Measuring CLEVRness: Blackbox testing of Visual Reasoning Models. (16%)](http://arxiv.org/abs/2202.12162)

Spyridon Mouselinos, Henryk Michalewski, Mateusz Malinowski


[Bounding Membership Inference. (11%)](http://arxiv.org/abs/2202.12232)

Anvith Thudi, Ilia Shumailov, Franziska Boenisch, Nicolas Papernot


[Fourier-Based Augmentations for Improved Robustness and Uncertainty Calibration. (3%)](http://arxiv.org/abs/2202.12412)

Ryan Soklaski, Michael Yee, Theodoros Tsiligkaridis


[Threading the Needle of On and Off-Manifold Value Functions for Shapley Explanations. (2%)](http://arxiv.org/abs/2202.11919)

Chih-Kuan Yeh, Kuan-Yun Lee, Frederick Liu, Pradeep Ravikumar


[Interpolation-based Contrastive Learning for Few-Label Semi-Supervised Learning. (1%)](http://arxiv.org/abs/2202.11915)

Xihong Yang, Xiaochang Hu, Sihang Zhou, Xinwang Liu, En Zhu


## 2022-02-23

[Improving Robustness of Convolutional Neural Networks Using Element-Wise Activation Scaling. (96%)](http://arxiv.org/abs/2202.11898)

Zhi-Yuan Zhang, Di Liu


[Using calibrator to improve robustness in Machine Reading Comprehension. (13%)](http://arxiv.org/abs/2202.11865)

Jing Jin, Houfeng Wang


## 2022-02-22

[LPF-Defense: 3D Adversarial Defense based on Frequency Analysis. (99%)](http://arxiv.org/abs/2202.11287)

Hanieh Naderi, Kimia Noorbakhsh, Arian Etemadi, Shohreh Kasaei


[Universal adversarial perturbation for remote sensing images. (95%)](http://arxiv.org/abs/2202.10693)

Zhaoxia Yin, Qingyu Wang, Jin Tang, Bin Luo


[Seeing is Living? Rethinking the Security of Facial Liveness Verification in the Deepfake Era. (84%)](http://arxiv.org/abs/2202.10673)

Changjiang Li, Li Wang, Shouling Ji, Xuhong Zhang, Zhaohan Xi, Shanqing Guo, Ting Wang


[Indiscriminate Poisoning Attacks on Unsupervised Contrastive Learning. (1%)](http://arxiv.org/abs/2202.11202)

Hao He, Kaiwen Zha, Dina Katabi


## 2022-02-21

[Adversarial Attacks on Speech Recognition Systems for Mission-Critical Applications: A Survey. (99%)](http://arxiv.org/abs/2202.10594)

Ngoc Dung Huynh, Mohamed Reda Bouadjenek, Imran Razzak, Kevin Lee, Chetan Arora, Ali Hassani, Arkady Zaslavsky


[Semi-Implicit Hybrid Gradient Methods with Application to Adversarial Robustness. (99%)](http://arxiv.org/abs/2202.10523)

Beomsu Kim, Junghoon Seo


[HoneyModels: Machine Learning Honeypots. (99%)](http://arxiv.org/abs/2202.10309)

Ahmed Abdou, Ryan Sheatsley, Yohan Beugin, Tyler Shipp, Patrick McDaniel


[Transferring Adversarial Robustness Through Robust Representation Matching. (99%)](http://arxiv.org/abs/2202.09994)

Pratik Vaishnavi, Kevin Eykholt, Amir Rahmati


[On the Effectiveness of Adversarial Training against Backdoor Attacks. (96%)](http://arxiv.org/abs/2202.10627)

Yinghua Gao, Dongxian Wu, Jingfeng Zhang, Guanhao Gan, Shu-Tao Xia, Gang Niu, Masashi Sugiyama


[Poisoning Attacks and Defenses on Artificial Intelligence: A Survey. (83%)](http://arxiv.org/abs/2202.10276)

Miguel A. Ramirez, Song-Kyoo Kim, Hussam Al Hamadi, Ernesto Damiani, Young-Ji Byon, Tae-Yeon Kim, Chung-Suk Cho, Chan Yeob Yeun


[A Tutorial on Adversarial Learning Attacks and Countermeasures. (75%)](http://arxiv.org/abs/2202.10377)

Cato Pauling, Michael Gimson, Muhammed Qaid, Ahmad Kida, Basel Halak


[Backdoor Defense in Federated Learning Using Differential Testing and Outlier Detection. (41%)](http://arxiv.org/abs/2202.11196)

Yein Kim, Huili Chen, Farinaz Koushanfar


[Privacy Leakage of Adversarial Training Models in Federated Learning Systems. (38%)](http://arxiv.org/abs/2202.10546)

Jingyang Zhang, Yiran Chen, Hai Li


[Robustness and Accuracy Could Be Reconcilable by (Proper) Definition. (11%)](http://arxiv.org/abs/2202.10103)

Tianyu Pang, Min Lin, Xiao Yang, Jun Zhu, Shuicheng Yan


[Cyber-Physical Defense in the Quantum Era. (2%)](http://arxiv.org/abs/2202.10354)

Michel Barbeau, Joaquin Garcia-Alfaro


## 2022-02-20

[Real-time Over-the-air Adversarial Perturbations for Digital Communications using Deep Neural Networks. (93%)](http://arxiv.org/abs/2202.11197)

Roman A. Sandler, Peter K. Relich, Cloud Cho, Sean Holloway


[Sparsity Winning Twice: Better Robust Generaliztion from More Efficient Training. (26%)](http://arxiv.org/abs/2202.09844)

Tianlong Chen, Zhenyu Zhang, Pengjun Wang, Santosh Balachandra, Haoyu Ma, Zehao Wang, Zhangyang Wang


[Overparametrization improves robustness against adversarial attacks: A replication study. (3%)](http://arxiv.org/abs/2202.09735)

Ali Borji


## 2022-02-18

[Exploring Adversarially Robust Training for Unsupervised Domain Adaptation. (99%)](http://arxiv.org/abs/2202.09300)

Shao-Yuan Lo, Vishal M. Patel


[Learning Representations Robust to Group Shifts and Adversarial Examples. (93%)](http://arxiv.org/abs/2202.09446)

Ming-Chang Chiu, Xuezhe Ma


[Critical Checkpoints for Evaluating Defence Models Against Adversarial Attack and Robustness. (92%)](http://arxiv.org/abs/2202.09039)

Kanak Tekwani, Manojkumar Parmar


[Resurrecting Trust in Facial Recognition: Mitigating Backdoor Attacks in Face Recognition to Prevent Potential Privacy Breaches. (80%)](http://arxiv.org/abs/2202.10320)

Reena Zelenkova, Jack Swallow, M. A. P. Chamikara, Dongxi Liu, Mohan Baruwal Chhetri, Seyit Camtepe, Marthie Grobler, Mahathir Almashor


[Data-Driven Mitigation of Adversarial Text Perturbation. (75%)](http://arxiv.org/abs/2202.09483)

Rasika Bhalerao, Mohammad Al-Rubaie, Anand Bhaskar, Igor Markov


[Debiasing Backdoor Attack: A Benign Application of Backdoor Attack in Eliminating Data Bias. (68%)](http://arxiv.org/abs/2202.10582)

Shangxi Wu, Qiuyang He, Yi Zhang, Jitao Sang


[Stochastic Perturbations of Tabular Features for Non-Deterministic Inference with Automunge. (38%)](http://arxiv.org/abs/2202.09248)

Nicholas J. Teague


[Label-Smoothed Backdoor Attack. (33%)](http://arxiv.org/abs/2202.11203)

Minlong Peng, Zidi Xiong, Mingming Sun, Ping Li


[Black-box Node Injection Attack for Graph Neural Networks. (33%)](http://arxiv.org/abs/2202.09389)

Mingxuan Ju, Yujie Fan, Yanfang Ye, Liang Zhao


[Robust Reinforcement Learning as a Stackelberg Game via Adaptively-Regularized Adversarial Training. (9%)](http://arxiv.org/abs/2202.09514)

Peide Huang, Mengdi Xu, Fei Fang, Ding Zhao


[Attacks, Defenses, And Tools: A Framework To Facilitate Robust AI/ML Systems. (4%)](http://arxiv.org/abs/2202.09465)

Mohamad Fazelnia, Igor Khokhlov, Mehdi Mirakhorli


[Synthetic Disinformation Attacks on Automated Fact Verification Systems. (1%)](http://arxiv.org/abs/2202.09381)

Yibing Du, Antoine Bosselut, Christopher D. Manning


## 2022-02-17

[Rethinking Machine Learning Robustness via its Link with the Out-of-Distribution Problem. (99%)](http://arxiv.org/abs/2202.08944)

Abderrahmen Amich, Birhanu Eshete


[Mitigating Closed-model Adversarial Examples with Bayesian Neural Modeling for Enhanced End-to-End Speech Recognition. (98%)](http://arxiv.org/abs/2202.08532)

Chao-Han Huck Yang, Zeeshan Ahmed, Yile Gu, Joseph Szurley, Roger Ren, Linda Liu, Andreas Stolcke, Ivan Bulyko


[Developing Imperceptible Adversarial Patches to Camouflage Military Assets From Computer Vision Enabled Technologies. (98%)](http://arxiv.org/abs/2202.08892)

Chris Wise, Jo Plested


[Fingerprinting Deep Neural Networks Globally via Universal Adversarial Perturbations. (78%)](http://arxiv.org/abs/2202.08602)

Zirui Peng, Shaofeng Li, Guoxing Chen, Cheng Zhang, Haojin Zhu, Minhui Xue


## 2022-02-16

[The Adversarial Security Mitigations of mmWave Beamforming Prediction Models using Defensive Distillation and Adversarial Retraining. (99%)](http://arxiv.org/abs/2202.08185)

Murat Kuzlu, Ferhat Ozgur Catak, Umit Cali, Evren Catak, Ozgur Guler


[Understanding and Improving Graph Injection Attack by Promoting Unnoticeability. (10%)](http://arxiv.org/abs/2202.08057)

Yongqiang Chen, Han Yang, Yonggang Zhang, Kaili Ma, Tongliang Liu, Bo Han, James Cheng


[Gradient Based Activations for Accurate Bias-Free Learning. (1%)](http://arxiv.org/abs/2202.10943)

Vinod K Kurmi, Rishabh Sharma, Yash Vardhan Sharma, Vinay P. Namboodiri


## 2022-02-15

[Unreasonable Effectiveness of Last Hidden Layer Activations. (99%)](http://arxiv.org/abs/2202.07342)

Omer Faruk Tuna, Ferhat Ozgur Catak, M. Taner Eskil


[Exploring the Devil in Graph Spectral Domain for 3D Point Cloud Attacks. (99%)](http://arxiv.org/abs/2202.07261)

Qianjiang Hu, Daizong Liu, Wei Hu


[StratDef: Strategic Defense Against Adversarial Attacks in ML-based Malware Detection. (99%)](http://arxiv.org/abs/2202.07568)

Aqib Rashid, Jose Such


[Random Walks for Adversarial Meshes. (97%)](http://arxiv.org/abs/2202.07453)

Amir Belder, Gal Yefet, Ran Ben Izhak, Ayellet Tal


[Generative Adversarial Network-Driven Detection of Adversarial Tasks in Mobile Crowdsensing. (93%)](http://arxiv.org/abs/2202.07802)

Zhiyan Chen, Burak Kantarci


[Applying adversarial networks to increase the data efficiency and reliability of Self-Driving Cars. (89%)](http://arxiv.org/abs/2202.07815)

Aakash Kumar


[Improving the repeatability of deep learning models with Monte Carlo dropout. (1%)](http://arxiv.org/abs/2202.07562)

Andreanne Lemay, Katharina Hoebel, Christopher P. Bridge, Brian Befano, Sanjosé Silvia De, Diden Egemen, Ana Cecilia Rodriguez, Mark Schiffman, John Peter Campbell, Jayashree Kalpathy-Cramer


[Holistic Adversarial Robustness of Deep Learning Models. (1%)](http://arxiv.org/abs/2202.07201)

Pin-Yu Chen, Sijia Liu


[Taking a Step Back with KCal: Multi-Class Kernel-Based Calibration for Deep Neural Networks. (1%)](http://arxiv.org/abs/2202.07679)

Zhen Lin, Shubhendu Trivedi, Jimeng Sun


## 2022-02-14

[Universal Adversarial Examples in Remote Sensing: Methodology and Benchmark. (99%)](http://arxiv.org/abs/2202.07054)

Yonghao Xu, Pedram Ghamisi


[Finding Dynamics Preserving Adversarial Winning Tickets. (86%)](http://arxiv.org/abs/2202.06488)

Xupeng Shi, Pengfei Zheng, A. Adam Ding, Yuan Gao, Weizhong Zhang


[Recent Advances in Reliable Deep Graph Learning: Inherent Noise, Distribution Shift, and Adversarial Attack. (83%)](http://arxiv.org/abs/2202.07114)

Jintang Li, Bingzhe Wu, Chengbin Hou, Guoji Fu, Yatao Bian, Liang Chen, Junzhou Huang, Zibin Zheng


[PFGE: Parsimonious Fast Geometric Ensembling of DNNs. (1%)](http://arxiv.org/abs/2202.06658)

Hao Guo, Jiyong Jin, Bin Liu


[UA-FedRec: Untargeted Attack on Federated News Recommendation. (1%)](http://arxiv.org/abs/2202.06701)

Jingwei Yi, Fangzhao Wu, Bin Zhu, Jing Yao, Zhulin Tao, Guangzhong Sun, Xing Xie


## 2022-02-13

[Progressive Backdoor Erasing via connecting Backdoor and Adversarial Attacks. (99%)](http://arxiv.org/abs/2202.06312)

Bingxu Mu, Zhenxing Niu, Le Wang, Xue Wang, Rong Jin, Gang Hua


[Training with More Confidence: Mitigating Injected and Natural Backdoors During Training. (92%)](http://arxiv.org/abs/2202.06382)

Zhenting Wang, Hailun Ding, Juan Zhai, Shiqing Ma


[Extracting Label-specific Key Input Features for Neural Code Intelligence Models. (9%)](http://arxiv.org/abs/2202.06474)

Md Rafiqul Islam Rabin


[Defense Strategies Toward Model Poisoning Attacks in Federated Learning: A Survey. (2%)](http://arxiv.org/abs/2202.06414)

Zhilin Wang, Qiao Kang, Xinyi Zhang, Qin Hu


[SQuant: On-the-Fly Data-Free Quantization via Diagonal Hessian Approximation. (1%)](http://arxiv.org/abs/2202.07471)

Cong Guo, Yuxian Qiu, Jingwen Leng, Xiaotian Gao, Chen Zhang, Yunxin Liu, Fan Yang, Yuhao Zhu, Minyi Guo


## 2022-02-12

[RoPGen: Towards Robust Code Authorship Attribution via Automatic Coding Style Transformation. (98%)](http://arxiv.org/abs/2202.06043)

Zhen Qian Li, Qian Guenevere, Chen, Chen Chen, Yayi Zou, Shouhuai Xu


[Excitement Surfeited Turns to Errors: Deep Learning Testing Framework Based on Excitable Neurons. (98%)](http://arxiv.org/abs/2202.07464)

Haibo Jin, Ruoxi Chen, Haibin Zheng, Jinyin Chen, Yao Cheng, Yue Yu, Xianglong Liu


## 2022-02-11

[Adversarial Attacks and Defense Methods for Power Quality Recognition. (99%)](http://arxiv.org/abs/2202.07421)

Jiwei Tian, Buhong Wang, Jing Li, Zhen Wang, Mete Ozay


[Towards Adversarially Robust Deepfake Detection: An Ensemble Approach. (99%)](http://arxiv.org/abs/2202.05687)

Ashish Hooda, Neal Mangaokar, Ryan Feng, Kassem Fawaz, Somesh Jha, Atul Prakash


[Open-set Adversarial Defense with Clean-Adversarial Mutual Learning. (98%)](http://arxiv.org/abs/2202.05953)

Rui Shao, Pramuditha Perera, Pong C. Yuen, Vishal M. Patel


[Using Random Perturbations to Mitigate Adversarial Attacks on Sentiment Analysis Models. (92%)](http://arxiv.org/abs/2202.05758)

Abigail Swenor, Jugal Kalita


[Fast Adversarial Training with Noise Augmentation: A Unified Perspective on RandStart and GradAlign. (74%)](http://arxiv.org/abs/2202.05488)

Axi Niu, Kang Zhang, Chaoning Zhang, Chenshuang Zhang, In So Kweon, Chang D. Yoo, Yanning Zhang


[Predicting Out-of-Distribution Error with the Projection Norm. (62%)](http://arxiv.org/abs/2202.05834)

Yaodong Yu, Zitong Yang, Alexander Wei, Yi Ma, Jacob Steinhardt


[Jigsaw Puzzle: Selective Backdoor Attack to Subvert Malware Classifiers. (62%)](http://arxiv.org/abs/2202.05470)

Limin Yang, Zhi Chen, Jacopo Cortellazzi, Feargus Pendlebury, Kevin Tu, Fabio Pierazzi, Lorenzo Cavallaro, Gang Wang


[White-Box Attacks on Hate-speech BERT Classifiers in German with Explicit and Implicit Character Level Defense. (12%)](http://arxiv.org/abs/2202.05778)

Shahrukh Khan, Mahnoor Shahid, Navdeeppal Singh


[On the Detection of Adaptive Adversarial Attacks in Speaker Verification Systems. (10%)](http://arxiv.org/abs/2202.05725)

Zesheng Chen


[Improving Generalization via Uncertainty Driven Perturbations. (2%)](http://arxiv.org/abs/2202.05737)

Matteo Pagliardini, Gilberto Manunza, Martin Jaggi, Michael I. Jordan, Tatjana Chavdarova


[CMW-Net: Learning a Class-Aware Sample Weighting Mapping for Robust Deep Learning. (1%)](http://arxiv.org/abs/2202.05613)

Jun Shu, Xiang Yuan, Deyu Meng, Zongben Xu


## 2022-02-10

[FAAG: Fast Adversarial Audio Generation through Interactive Attack Optimisation. (99%)](http://arxiv.org/abs/2202.05416)

Yuantian Miao, Chao Chen, Lei Pan, Jun Zhang, Yang Xiang


[Towards Assessing and Characterizing the Semantic Robustness of Face Recognition. (76%)](http://arxiv.org/abs/2202.04978)

Juan C. Pérez, Motasem Alfarra, Ali Thabet, Pablo Arbeláez, Bernard Ghanem


[Controlling the Complexity and Lipschitz Constant improves polynomial nets. (12%)](http://arxiv.org/abs/2202.05068)

Zhenyu Zhu, Fabian Latorre, Grigorios G Chrysos, Volkan Cevher


[FedAttack: Effective and Covert Poisoning Attack on Federated Recommendation via Hard Sampling. (8%)](http://arxiv.org/abs/2202.04975)

Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang, Xing Xie


[A Field of Experts Prior for Adapting Neural Networks at Test Time. (1%)](http://arxiv.org/abs/2202.05271)

Neerav Karani, Georg Brunner, Ertunc Erdil, Simin Fei, Kerem Tezcan, Krishna Chaitanya, Ender Konukoglu


## 2022-02-09

[Adversarial Attack and Defense of YOLO Detectors in Autonomous Driving Scenarios. (99%)](http://arxiv.org/abs/2202.04781)

Jung Im Choi, Qing Tian


[Gradient Methods Provably Converge to Non-Robust Networks. (82%)](http://arxiv.org/abs/2202.04347)

Gal Vardi, Gilad Yehudai, Ohad Shamir


[False Memory Formation in Continual Learners Through Imperceptible Backdoor Trigger. (22%)](http://arxiv.org/abs/2202.04479)

Muhammad Umer, Robi Polikar


[ARIBA: Towards Accurate and Robust Identification of Backdoor Attacks in Federated Learning. (10%)](http://arxiv.org/abs/2202.04311)

Yuxi Mi, Jihong Guan, Shuigeng Zhou


[Learning to Bootstrap for Combating Label Noise. (2%)](http://arxiv.org/abs/2202.04291)

Yuyin Zhou, Xianhang Li, Fengze Liu, Xuxi Chen, Lequan Yu, Cihang Xie, Matthew P. Lungren, Lei Xing


[Model Architecture Adaption for Bayesian Neural Networks. (1%)](http://arxiv.org/abs/2202.04392)

Duo Wang, Yiren Zhao, Ilia Shumailov, Robert Mullins


## 2022-02-08

[Towards Compositional Adversarial Robustness: Generalizing Adversarial Training to Composite Semantic Perturbations. (99%)](http://arxiv.org/abs/2202.04235)

Lei Hsiung, Yun-Yun Tsai, Pin-Yu Chen, Tsung-Yi Ho


[Verification-Aided Deep Ensemble Selection. (96%)](http://arxiv.org/abs/2202.03898)

Guy Amir, Guy Katz, Michael Schapira


[Adversarial Detection without Model Information. (87%)](http://arxiv.org/abs/2202.04271)

Abhishek Moitra, Youngeun Kim, Priyadarshini Panda


[Towards Making a Trojan-horse Attack on Text-to-Image Retrieval. (68%)](http://arxiv.org/abs/2202.03861)

Fan Hu, Aozhu Chen, Xirong Li


[Robust, Deep, and Reinforcement Learning for Management of Communication and Power Networks. (1%)](http://arxiv.org/abs/2202.05395)

Alireza Sadeghi


## 2022-02-07

[Blind leads Blind: A Zero-Knowledge Attack on Federated Learning. (99%)](http://arxiv.org/abs/2202.05877)

Jiyue Huang, Zilong Zhao, Lydia Y. Chen, Stefanie Roos


[On The Empirical Effectiveness of Unrealistic Adversarial Hardening Against Realistic Adversarial Attacks. (99%)](http://arxiv.org/abs/2202.03277)

Salijona Dyrmishi, Salah Ghamizi, Thibault Simonetto, Yves Le Traon, Maxime Cordy


[Adversarial Attacks and Defense for Non-Parametric Two-Sample Tests. (98%)](http://arxiv.org/abs/2202.03077)

Xilie Xu, Jingfeng Zhang, Feng Liu, Masashi Sugiyama, Mohan Kankanhalli


[Evaluating Robustness of Cooperative MARL: A Model-based Approach. (97%)](http://arxiv.org/abs/2202.03558)

Nhan H. Pham, Lam M. Nguyen, Jie Chen, Hoang Thanh Lam, Subhro Das, Tsui-Wei Weng


[More is Better (Mostly): On the Backdoor Attacks in Federated Graph Neural Networks. (68%)](http://arxiv.org/abs/2202.03195)

Jing Xu, Rui Wang, Kaitai Liang, Stjepan Picek


[Membership Inference Attacks and Defenses in Neural Network Pruning. (50%)](http://arxiv.org/abs/2202.03335)

Xiaoyong Yuan, Lan Zhang


[SimGRACE: A Simple Framework for Graph Contrastive Learning without Data Augmentation. (4%)](http://arxiv.org/abs/2202.03104)

Jun Xia, Lirong Wu, Jintao Chen, Bozhen Hu, Stan Z. Li


[Deletion Inference, Reconstruction, and Compliance in Machine (Un)Learning. (3%)](http://arxiv.org/abs/2202.03460)

Ji Gao, Sanjam Garg, Mohammad Mahmoody, Prashant Nalini Vasudevan


## 2022-02-06

[Tubes Among Us: Analog Attack on Automatic Speaker Identification. (99%)](http://arxiv.org/abs/2202.02751)

Shimaa Ahmed, Yash Wani, Ali Shahin Shamsabadi, Mohammad Yaghini, Ilia Shumailov, Nicolas Papernot, Kassem Fawaz


[Redactor: A Data-centric and Individualized Defense Against Inference Attacks. (8%)](http://arxiv.org/abs/2202.02902)

Geon Heo, Steven Euijong Whang


## 2022-02-05

[Layer-wise Regularized Adversarial Training using Layers Sustainability Analysis (LSA) framework. (99%)](http://arxiv.org/abs/2202.02626)

Mohammad Khalooei, Mohammad Mehdi Homayounpour, Maryam Amirmazlaghani


[Adversarial Detector with Robust Classifier. (93%)](http://arxiv.org/abs/2202.02503)

Takayuki Osakabe, Maungmaung Aprilpyone, Sayaka Shiota, Hitoshi Kiya


[Memory Defense: More Robust Classification via a Memory-Masking Autoencoder. (76%)](http://arxiv.org/abs/2202.02595)

Eashan Lehigh University Adhikarla, Dan Lehigh University Luo, Brian D. Lehigh University Davison


[Improved Certified Defenses against Data Poisoning with (Deterministic) Finite Aggregation. (75%)](http://arxiv.org/abs/2202.02628)

Wenxiao Wang, Alexander Levine, Soheil Feizi


## 2022-02-04

[Pixle: a fast and effective black-box attack based on rearranging pixels. (98%)](http://arxiv.org/abs/2202.02236)

Jary Pomponi, Simone Scardapane, Aurelio Uncini


[Backdoor Defense via Decoupling the Training Process. (80%)](http://arxiv.org/abs/2202.03423)

Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, Kui Ren


[LTU Attacker for Membership Inference. (67%)](http://arxiv.org/abs/2202.02278)

Joseph Pedersen, Rafael Muñoz-Gómez, Jiangnan Huang, Haozhe Sun, Wei-Wei Tu, Isabelle Guyon


[A Survey on Safety-Critical Driving Scenario Generation -- A Methodological Perspective. (1%)](http://arxiv.org/abs/2202.02215)

Wenhao Ding, Chejian Xu, Mansur Arief, Haohong Lin, Bo Li, Ding Zhao


## 2022-02-03

[ObjectSeeker: Certifiably Robust Object Detection against Patch Hiding Attacks via Patch-agnostic Masking. (93%)](http://arxiv.org/abs/2202.01811)

Chong Xiang, Alexander Valtchanov, Saeed Mahloujifar, Prateek Mittal


[Adversarially Robust Models may not Transfer Better: Sufficient Conditions for Domain Transferability from the View of Regularization. (75%)](http://arxiv.org/abs/2202.01832)

Xiaojun Xu, Jacky Yibo Zhang, Evelyn Ma, Danny Son, Oluwasanmi Koyejo, Bo Li


## 2022-02-02

[An Eye for an Eye: Defending against Gradient-based Attacks with Gradients. (99%)](http://arxiv.org/abs/2202.01117)

Hanbin Hong, Yuan Hong, Yu Kong


[Smoothed Embeddings for Certified Few-Shot Learning. (76%)](http://arxiv.org/abs/2202.01186)

Mikhail Pautov, Olesya Kuznetsova, Nurislam Tursynbek, Aleksandr Petiushko, Ivan Oseledets


[Probabilistically Robust Learning: Balancing Average- and Worst-case Performance. (75%)](http://arxiv.org/abs/2202.01136)

Alexander Robey, Luiz F. O. Chamon, George J. Pappas, Hamed Hassani


[Make Some Noise: Reliable and Efficient Single-Step Adversarial Training. (70%)](http://arxiv.org/abs/2202.01181)

Jorge Pau de, Adel Bibi, Riccardo Volpi, Amartya Sanyal, Philip H. S. Torr, Grégory Rogez, Puneet K. Dokania


[Robust Binary Models by Pruning Randomly-initialized Networks. (10%)](http://arxiv.org/abs/2202.01341)

Chen Liu, Ziqi Zhao, Sabine Süsstrunk, Mathieu Salzmann


[NoisyMix: Boosting Robustness by Combining Data Augmentations, Stability Training, and Noise Injections. (10%)](http://arxiv.org/abs/2202.01263)

N. Benjamin Erichson, Soon Hoe Lim, Francisco Utrera, Winnie Xu, Ziang Cao, Michael W. Mahoney


## 2022-02-01

[Language Dependencies in Adversarial Attacks on Speech Recognition Systems. (98%)](http://arxiv.org/abs/2202.00399)

Karla Markert, Donika Mirdita, Konstantin Böttinger


[Finding Biological Plausibility for Adversarially Robust Features via Metameric Tasks. (80%)](http://arxiv.org/abs/2202.00838)

Anne Harrington, Arturo Deza


[Visualizing Automatic Speech Recognition -- Means for a Better Understanding? (64%)](http://arxiv.org/abs/2202.00673)

Karla Markert, Romain Parracone, Mykhailo Kulakov, Philip Sperl, Ching-Yu Kao, Konstantin Böttinger


[Datamodels: Predicting Predictions from Training Data. (2%)](http://arxiv.org/abs/2202.00622)

Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, Aleksander Madry


## 2022-01-31

[Adversarial Robustness in Deep Learning: Attacks on Fragile Neurons. (99%)](http://arxiv.org/abs/2201.12347)

Chandresh Pravin, Ivan Martino, Giuseppe Nicosia, Varun Ojha


[Boundary Defense Against Black-box Adversarial Attacks. (99%)](http://arxiv.org/abs/2201.13444)

Manjushree B. Aithal, Xiaohua Li


[Query Efficient Decision Based Sparse Attacks Against Black-Box Deep Learning Models. (99%)](http://arxiv.org/abs/2202.00091)

Viet Quoc Vo, Ehsan Abbasnejad, Damith C. Ranasinghe


[Can Adversarial Training Be Manipulated By Non-Robust Features? (98%)](http://arxiv.org/abs/2201.13329)

Lue Tao, Lei Feng, Hongxin Wei, Jinfeng Yi, Sheng-Jun Huang, Songcan Chen


[GADoT: GAN-based Adversarial Training for Robust DDoS Attack Detection. (96%)](http://arxiv.org/abs/2201.13102)

Maged Abdelaty, Sandra Scott-Hayward, Roberto Doriguzzi-Corin, Domenico Siracusa


[Rate Coding or Direct Coding: Which One is Better for Accurate, Robust, and Energy-efficient Spiking Neural Networks? (93%)](http://arxiv.org/abs/2202.03133)

Youngeun Kim, Hyoungseob Park, Abhishek Moitra, Abhiroop Bhattacharjee, Yeshwanth Venkatesha, Priyadarshini Panda


[AntidoteRT: Run-time Detection and Correction of Poison Attacks on Neural Networks. (89%)](http://arxiv.org/abs/2202.01179)

Muhammad Usman, Youcheng Sun, Divya Gopinath, Corina S. Pasareanu


[Imperceptible and Multi-channel Backdoor Attack against Deep Neural Networks. (81%)](http://arxiv.org/abs/2201.13164)

Mingfu Xue, Shifeng Ni, Yinghao Wu, Yushu Zhang, Jian Wang, Weiqiang Liu


[On the Robustness of Quality Measures for GANs. (80%)](http://arxiv.org/abs/2201.13019)

Motasem Alfarra, Juan C. Pérez, Anna Frühstück, Philip H. S. Torr, Peter Wonka, Bernard Ghanem


[MEGA: Model Stealing via Collaborative Generator-Substitute Networks. (76%)](http://arxiv.org/abs/2202.00008)

Chi Hong, Jiyue Huang, Lydia Y. Chen


[Learning Robust Representation through Graph Adversarial Contrastive Learning. (26%)](http://arxiv.org/abs/2201.13025)

Jiayan Guo, Shangyang Li, Yue Zhao, Yan Zhang


[UQGAN: A Unified Model for Uncertainty Quantification of Deep Classifiers trained via Conditional GANs. (16%)](http://arxiv.org/abs/2201.13279)

Philipp Oberdiek, Gernot A. Fink, Matthias Rottmann


[Few-Shot Backdoor Attacks on Visual Object Tracking. (10%)](http://arxiv.org/abs/2201.13178)

Yiming Li, Haoxiang Zhong, Xingjun Ma, Yong Jiang, Shu-Tao Xia


[Studying the Robustness of Anti-adversarial Federated Learning Models Detecting Cyberattacks in IoT Spectrum Sensors. (5%)](http://arxiv.org/abs/2202.00137)

Pedro Miguel Sánchez Sánchez, Alberto Huertas Celdrán, Timo Schenk, Adrian Lars Benjamin Iten, Gérôme Bovet, Gregorio Martínez Pérez, Burkhard Stiller


[Securing Federated Sensitive Topic Classification against Poisoning Attacks. (1%)](http://arxiv.org/abs/2201.13086)

Tianyue Chu, Alvaro Garcia-Recuero, Costas Iordanou, Georgios Smaragdakis, Nikolaos Laoutaris


## 2022-01-30

[Improving Corruption and Adversarial Robustness by Enhancing Weak Subnets. (92%)](http://arxiv.org/abs/2201.12765)

Yong Guo, David Stutz, Bernt Schiele


[GARNET: Reduced-Rank Topology Learning for Robust and Scalable Graph Neural Networks. (84%)](http://arxiv.org/abs/2201.12741)

Chenhui Deng, Xiuyu Li, Zhuo Feng, Zhiru Zhang


[TPC: Transformation-Specific Smoothing for Point Cloud Models. (75%)](http://arxiv.org/abs/2201.12733)

Wenda Chu, Linyi Li, Bo Li


## 2022-01-29

[Scale-Invariant Adversarial Attack for Evaluating and Enhancing Adversarial Defenses. (99%)](http://arxiv.org/abs/2201.12527)

Mengting Xu, Tao Zhang, Zhongnian Li, Daoqiang Zhang


[Robustness of Deep Recommendation Systems to Untargeted Interaction Perturbations. (82%)](http://arxiv.org/abs/2201.12686)

Sejoon Oh, Srijan Kumar


[Coordinated Attacks against Contextual Bandits: Fundamental Limits and Defense Mechanisms. (1%)](http://arxiv.org/abs/2201.12700)

Jeongyeol Kwon, Yonathan Efroni, Constantine Caramanis, Shie Mannor


## 2022-01-28

[Adversarial Examples for Good: Adversarial Examples Guided Imbalanced Learning. (87%)](http://arxiv.org/abs/2201.12356)

Jie Zhang, Lei Zhang, Gang Li, Chao Wu


[Feature Visualization within an Automated Design Assessment leveraging Explainable Artificial Intelligence Methods. (81%)](http://arxiv.org/abs/2201.12107)

Raoul Schönhof, Artem Werner, Jannes Elstner, Boldizsar Zopcsak, Ramez Awad, Marco Huber


[Certifying Model Accuracy under Distribution Shifts. (74%)](http://arxiv.org/abs/2201.12440)

Aounon Kumar, Alexander Levine, Tom Goldstein, Soheil Feizi


[Benchmarking Robustness of 3D Point Cloud Recognition Against Common Corruptions. (13%)](http://arxiv.org/abs/2201.12296)

Jiachen Sun, Qingzhao Zhang, Bhavya Kailkhura, Zhiding Yu, Chaowei Xiao, Z. Morley Mao


[Plug & Play Attacks: Towards Robust and Flexible Model Inversion Attacks. (8%)](http://arxiv.org/abs/2201.12179)

Lukas Struppek, Dominik Hintersdorf, Antonio De Almeida Correia, Antonia Adler, Kristian Kersting


[Backdoors Stuck At The Frontdoor: Multi-Agent Backdoor Attacks That Backfire. (3%)](http://arxiv.org/abs/2201.12211)

Siddhartha Datta, Nigel Shadbolt


[Toward Training at ImageNet Scale with Differential Privacy. (1%)](http://arxiv.org/abs/2201.12328)

Alexey Kurakin, Shuang Song, Steve Chien, Roxana Geambasu, Andreas Terzis, Abhradeep Thakurta


## 2022-01-27

[Beyond ImageNet Attack: Towards Crafting Adversarial Examples for Black-box Domains. (99%)](http://arxiv.org/abs/2201.11528)

Qilong Zhang, Xiaodan Li, Yuefeng Chen, Jingkuan Song, Lianli Gao, Yuan He, Hui Xue


[Vision Checklist: Towards Testable Error Analysis of Image Models to Help System Designers Interrogate Model Capabilities. (10%)](http://arxiv.org/abs/2201.11674)

Xin Du, Benedicte Legastelois, Bhargavi Ganesh, Ajitha Rajan, Hana Chockler, Vaishak Belle, Stuart Anderson, Subramanian Ramamoorthy


[SSLGuard: A Watermarking Scheme for Self-supervised Learning Pre-trained Encoders. (2%)](http://arxiv.org/abs/2201.11692)

Tianshuo Cong, Xinlei He, Yang Zhang


[CacheFX: A Framework for Evaluating Cache Security. (1%)](http://arxiv.org/abs/2201.11377)

Daniel Genkin, William Kosasih, Fangfei Liu, Anna Trikalinou, Thomas Unterluggauer, Yuval Yarom


## 2022-01-26

[Boosting 3D Adversarial Attacks with Attacking On Frequency. (98%)](http://arxiv.org/abs/2201.10937)

Binbin Liu, Jinlai Zhang, Lyujie Chen, Jihong Zhu


[How Robust are Discriminatively Trained Zero-Shot Learning Models? (98%)](http://arxiv.org/abs/2201.10972)

Mehmet Kerim Yucel, Ramazan Gokberk Cinbis, Pinar Duygulu


[Autonomous Cyber Defense Introduces Risk: Can We Manage the Risk? (2%)](http://arxiv.org/abs/2201.11148)

Alexandre K. Ligo, Alexander Kott, Igor Linkov


[Automatic detection of access control vulnerabilities via API specification processing. (1%)](http://arxiv.org/abs/2201.10833)

Alexander Barabanov, Denis Dergunov, Denis Makrushin, Aleksey Teplov


## 2022-01-25

[Virtual Adversarial Training for Semi-supervised Breast Mass Classification. (3%)](http://arxiv.org/abs/2201.10675)

Xuxin Chen, Ximin Wang, Ke Zhang, Kar-Ming Fung, Theresa C. Thai, Kathleen Moore, Robert S. Mannel, Hong Liu, Bin Zheng, Yuchen Qiu


[Class-Aware Adversarial Transformers for Medical Image Segmentation. (1%)](http://arxiv.org/abs/2201.10737)

Chenyu You, Ruihan Zhao, Fenglin Liu, Siyuan Dong, Sandeep Chinchali, Ufuk Topcu, Lawrence Staib, James S. Duncan


[SPIRAL: Self-supervised Perturbation-Invariant Representation Learning for Speech Pre-Training. (1%)](http://arxiv.org/abs/2201.10207)

Wenyong Huang, Zhenhe Zhang, Yu Ting Yeung, Xin Jiang, Qun Liu


## 2022-01-24

[What You See is Not What the Network Infers: Detecting Adversarial Examples Based on Semantic Contradiction. (99%)](http://arxiv.org/abs/2201.09650)

Yijun Yang, Ruiyuan Gao, Yu Li, Qiuxia Lai, Qiang Xu


[Identifying a Training-Set Attack's Target Using Renormalized Influence Estimation. (95%)](http://arxiv.org/abs/2201.10055)

Zayd Hammoudeh, Daniel Lowd


[Attacks and Defenses for Free-Riders in Multi-Discriminator GAN. (76%)](http://arxiv.org/abs/2201.09967)

Zilong Zhao, Jiyue Huang, Stefanie Roos, Lydia Y. Chen


[Backdoor Defense with Machine Unlearning. (33%)](http://arxiv.org/abs/2201.09538)

Yang Liu, Mingyuan Fan, Cen Chen, Ximeng Liu, Zhuo Ma, Li Wang, Jianfeng Ma


[On the Complexity of Attacking Elliptic Curve Based Authentication Chips. (1%)](http://arxiv.org/abs/2201.09631)

Ievgen Kabin, Zoya Dyka, Dan Klann, Jan Schaeffner, Peter Langendoerfer


## 2022-01-23

[Efficient and Robust Classification for Sparse Attacks. (83%)](http://arxiv.org/abs/2201.09369)

Mark Beliaev, Payam Delgosha, Hamed Hassani, Ramtin Pedarsani


[Gradient-guided Unsupervised Text Style Transfer via Contrastive Learning. (78%)](http://arxiv.org/abs/2202.00469)

Chenghao Fan, Ziao Li, Wei wei


[Are Your Sensitive Attributes Private? Novel Model Inversion Attribute Inference Attacks on Classification Models. (56%)](http://arxiv.org/abs/2201.09370)

Shagufta Mehnaz, Sayanton V. Dibbo, Ehsanul Kabir, Ninghui Li, Elisa Bertino


[Increasing the Cost of Model Extraction with Calibrated Proof of Work. (22%)](http://arxiv.org/abs/2201.09243)

Adam Dziedzic, Muhammad Ahmad Kaleem, Yu Shen Lu, Nicolas Papernot


## 2022-01-22

[Parallel Rectangle Flip Attack: A Query-based Black-box Attack against Object Detection. (99%)](http://arxiv.org/abs/2201.08970)

Siyuan Liang, Baoyuan Wu, Yanbo Fan, Xingxing Wei, Xiaochun Cao


[Robust Unpaired Single Image Super-Resolution of Faces. (98%)](http://arxiv.org/abs/2201.09109)

Saurabh Goswami, Rajagopalan A. N


[On the Robustness of Counterfactual Explanations to Adverse Perturbations. (10%)](http://arxiv.org/abs/2201.09051)

Marco Virgolin, Saverio Fracaros


## 2022-01-21

[Natural Attack for Pre-trained Models of Code. (99%)](http://arxiv.org/abs/2201.08698)

Zhou Yang, Jieke Shi, Junda He, David Lo


[Toward Enhanced Robustness in Unsupervised Graph Representation Learning: A Graph Information Bottleneck Perspective. (99%)](http://arxiv.org/abs/2201.08557)

Jihong Wang, Minnan Luo, Jundong Li, Ziqi Liu, Jun Zhou, Qinghua Zheng


[The Security of Deep Learning Defences for Medical Imaging. (80%)](http://arxiv.org/abs/2201.08661)

Moshe Levy, Guy Amit, Yuval Elovici, Yisroel Mirsky


[Dangerous Cloaking: Natural Trigger based Backdoor Attacks on Object Detectors in the Physical World. (75%)](http://arxiv.org/abs/2201.08619)

Hua Ma, Yinshan Li, Yansong Gao, Alsharif Abuadbba, Zhi Zhang, Anmin Fu, Hyoungshick Kim, Said F. Al-Sarawi, Nepal Surya, Derek Abbott


[Identifying Adversarial Attacks on Text Classifiers. (73%)](http://arxiv.org/abs/2201.08555)

Zhouhang Xie, Jonathan Brophy, Adam Noack, Wencong You, Kalyani Asthana, Carter Perkins, Sabrina Reis, Sameer Singh, Daniel Lowd


[The Many Faces of Adversarial Risk. (47%)](http://arxiv.org/abs/2201.08956)

Muni Sreenivas Pydi, Varun Jog


## 2022-01-20

[TextHacker: Learning based Hybrid Local Search Algorithm for Text Hard-label Adversarial Attack. (99%)](http://arxiv.org/abs/2201.08193)

Zhen Yu, Xiaosen Wang, Wanxiang Che, Kun He


[Cheating Automatic Short Answer Grading: On the Adversarial Usage of Adjectives and Adverbs. (95%)](http://arxiv.org/abs/2201.08318)

Anna Filighera, Sebastian Ochs, Tim Steuer, Thomas Tregel


[Survey on Federated Learning Threats: concepts, taxonomy on attacks and defences, experimental study and challenges. (93%)](http://arxiv.org/abs/2201.08135)

Nuria Rodríguez-Barroso, Daniel Jiménez López, M. Victoria Luzón, Francisco Herrera, Eugenio Martínez-Cámara


[Low-Interception Waveform: To Prevent the Recognition of Spectrum Waveform Modulation via Adversarial Examples. (83%)](http://arxiv.org/abs/2201.08731)

Haidong Xie, Jia Tan, Xiaoying Zhang, Nan Ji, Haihua Liao, Zuguo Yu, Xueshuang Xiang, Naijin Liu


[Post-Training Detection of Backdoor Attacks for Two-Class and Multi-Attack Scenarios. (70%)](http://arxiv.org/abs/2201.08474)

Zhen Xiang, David J. Miller, George Kesidis


[Adversarial Jamming for a More Effective Constellation Attack. (56%)](http://arxiv.org/abs/2201.08052)

Haidong Xie, Yizhou Xu, Yuanqing Chen, Nan Ji, Shuai Yuan, Naijin Liu, Xueshuang Xiang


[Steerable Pyramid Transform Enables Robust Left Ventricle Quantification. (13%)](http://arxiv.org/abs/2201.08388)

Xiangyang Zhu, Kede Ma, Wufeng Xue


[Black-box Prompt Learning for Pre-trained Language Models. (13%)](http://arxiv.org/abs/2201.08531)

Shizhe Diao, Zhichao Huang, Ruijia Xu, Xuechun Li, Yong Lin, Xiao Zhou, Tong Zhang


[DeepGalaxy: Testing Neural Network Verifiers via Two-Dimensional Input Space Exploration. (1%)](http://arxiv.org/abs/2201.08087)

Xuan Xie, Fuyuan Zhang


## 2022-01-19

[Unsupervised Graph Poisoning Attack via Contrastive Loss Back-propagation. (96%)](http://arxiv.org/abs/2201.07986)

Sixiao Zhang, Hongxu Chen, Xiangguo Sun, Yicong Li, Guandong Xu


[Can't Steal? Cont-Steal! Contrastive Stealing Attacks Against Image Encoders. (8%)](http://arxiv.org/abs/2201.07513)

Zeyang Sha, Xinlei He, Ning Yu, Michael Backes, Yang Zhang


## 2022-01-18

[MetaV: A Meta-Verifier Approach to Task-Agnostic Model Fingerprinting. (99%)](http://arxiv.org/abs/2201.07391)

Xudong Pan, Yifan Yan, Mi Zhang, Min Yang


[Adversarial vulnerability of powerful near out-of-distribution detection. (78%)](http://arxiv.org/abs/2201.07012)

Stanislav Fort


[Model Transferring Attacks to Backdoor HyperNetwork in Personalized Federated Learning. (13%)](http://arxiv.org/abs/2201.07063)

Phung Lai, NhatHai Phan, Abdallah Khreishah, Issa Khalil, Xintao Wu


[Secure IoT Routing: Selective Forwarding Attacks and Trust-based Defenses in RPL Network. (2%)](http://arxiv.org/abs/2201.06937)

Jun Jiang, Yuhong Liu


[Lung Swapping Autoencoder: Learning a Disentangled Structure-texture Representation of Chest Radiographs. (1%)](http://arxiv.org/abs/2201.07344)

Lei Zhou, Joseph Bae, Huidong Liu, Gagandeep Singh, Jeremy Green, Amit Gupta, Dimitris Samaras, Prateek Prasanna


## 2022-01-17

[Masked Faces with Faced Masks. (81%)](http://arxiv.org/abs/2201.06427)

Jiayi Zhu, Qing Guo, Felix Juefei-Xu, Yihao Huang, Yang Liu, Geguang Pu


[Cyberbullying Classifiers are Sensitive to Model-Agnostic Perturbations. (56%)](http://arxiv.org/abs/2201.06384)

Chris Emmery, Ákos Kádár, Grzegorz Chrupała, Walter Daelemans


[AugLy: Data Augmentations for Robustness. (3%)](http://arxiv.org/abs/2201.06494)

Zoe Papakipos, Joanna Bitton


## 2022-01-16

[Fooling the Eyes of Autonomous Vehicles: Robust Physical Adversarial Examples Against Traffic Sign Recognition Systems. (99%)](http://arxiv.org/abs/2201.06192)

Wei Jia, Zhaojun Lu, Haichun Zhang, Zhenglin Liu, Jie Wang, Gang Qu


[ALA: Adversarial Lightness Attack via Naturalness-aware Regularizations. (99%)](http://arxiv.org/abs/2201.06070)

Liangru Sun, Felix Juefei-Xu, Yihao Huang, Qing Guo, Jiayi Zhu, Jincao Feng, Yang Liu, Geguang Pu


[Adversarial Machine Learning Threat Analysis in Open Radio Access Networks. (64%)](http://arxiv.org/abs/2201.06093)

Ron Bitton, Dan Avraham, Eitan Klevansky, Dudu Mimran, Oleg Brodt, Heiko Lehmann, Yuval Elovici, Asaf Shabtai


[Neighboring Backdoor Attacks on Graph Convolutional Network. (22%)](http://arxiv.org/abs/2201.06202)

Liang Chen, Qibiao Peng, Jintang Li, Yang Liu, Jiawei Chen, Yong Li, Zibin Zheng


## 2022-01-15

[Interpretable and Effective Reinforcement Learning for Attacking against Graph-based Rumor Detection. (26%)](http://arxiv.org/abs/2201.05819)

Yuefei Lyu, Xiaoyu Yang, Jiaxin Liu, Philip S. Yu, Sihong Xie, Xi Zhang


[StolenEncoder: Stealing Pre-trained Encoders. (13%)](http://arxiv.org/abs/2201.05889)

Yupei Liu, Jinyuan Jia, Hongbin Liu, Neil Zhenqiang Gong


## 2022-01-14

[CommonsenseQA 2.0: Exposing the Limits of AI through Gamification. (56%)](http://arxiv.org/abs/2201.05320)

Alon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bhagavatula, Yoav Goldberg, Yejin Choi, Jonathan Berant


[Security Orchestration, Automation, and Response Engine for Deployment of Behavioural Honeypots. (1%)](http://arxiv.org/abs/2201.05326)

Upendra Bartwal, Subhasis Mukhopadhyay, Rohit Negi, Sandeep Shukla


## 2022-01-13

[Evaluation of Four Black-box Adversarial Attacks and Some Query-efficient Improvement Analysis. (96%)](http://arxiv.org/abs/2201.05001)

Rui Wang


[The curse of overparametrization in adversarial training: Precise analysis of robust generalization for random features regression. (93%)](http://arxiv.org/abs/2201.05149)

Hamed Hassani, Adel Javanmard


[On Adversarial Robustness of Trajectory Prediction for Autonomous Vehicles. (83%)](http://arxiv.org/abs/2201.05057)

Qingzhao Zhang, Shengtuo Hu, Jiachen Sun, Qi Alfred Chen, Z. Morley Mao


[Reconstructing Training Data with Informed Adversaries. (54%)](http://arxiv.org/abs/2201.04845)

Borja Balle, Giovanni Cherubin, Jamie Hayes


[Jamming Attacks on Federated Learning in Wireless Networks. (2%)](http://arxiv.org/abs/2201.05172)

Yi Shi, Yalin E. Sagduyu


## 2022-01-12

[Adversarially Robust Classification by Conditional Generative Model Inversion. (99%)](http://arxiv.org/abs/2201.04733)

Mitra Alirezaei, Tolga Tasdizen


[Towards Adversarially Robust Deep Image Denoising. (99%)](http://arxiv.org/abs/2201.04397)

Hanshu Yan, Jingfeng Zhang, Jiashi Feng, Masashi Sugiyama, Vincent Y. F. Tan


[Get your Foes Fooled: Proximal Gradient Split Learning for Defense against Model Inversion Attacks on IoMT data. (70%)](http://arxiv.org/abs/2201.04569)

Sunder Ali Khowaja, Ik Hyun Lee, Kapal Dev, Muhammad Aslam Jarwar, Nawab Muhammad Faseeh Qureshi


## 2022-01-11

[Quantifying Robustness to Adversarial Word Substitutions. (99%)](http://arxiv.org/abs/2201.03829)

Yuting Yang, Pei Huang, FeiFei Ma, Juan Cao, Meishan Zhang, Jian Zhang, Jintao Li


[Similarity-based Gray-box Adversarial Attack Against Deep Face Recognition. (99%)](http://arxiv.org/abs/2201.04011)

Hanrui Wang, Shuo Wang, Zhe Jin, Yandan Wang, Cunjian Chen, Massimo Tistarell


## 2022-01-10

[Evaluation of Neural Networks Defenses and Attacks using NDCG and Reciprocal Rank Metrics. (98%)](http://arxiv.org/abs/2201.05071)

Haya Brama, Lihi Dery, Tal Grinshpoun


[IoTGAN: GAN Powered Camouflage Against Machine Learning Based IoT Device Identification. (89%)](http://arxiv.org/abs/2201.03281)

Tao Hou, Tao Wang, Zhuo Lu, Yao Liu, Yalin Sagduyu


[Reciprocal Adversarial Learning for Brain Tumor Segmentation: A Solution to BraTS Challenge 2021 Segmentation Task. (73%)](http://arxiv.org/abs/2201.03777)

Himashi Peiris, Zhaolin Chen, Gary Egan, Mehrtash Harandi


[GMFIM: A Generative Mask-guided Facial Image Manipulation Model for Privacy Preservation. (3%)](http://arxiv.org/abs/2201.03353)

Mohammad Hossein Khojaste, Nastaran Moradzadeh Farid, Ahmad Nickabadi


[Towards Group Robustness in the presence of Partial Group Labels. (1%)](http://arxiv.org/abs/2201.03668)

Vishnu Suresh Lokhande, Kihyuk Sohn, Jinsung Yoon, Madeleine Udell, Chen-Yu Lee, Tomas Pfister


## 2022-01-09

[Rethink Stealthy Backdoor Attacks in Natural Language Processing. (89%)](http://arxiv.org/abs/2201.02993)

Lingfeng Shen, Haiyun Jiang, Lemao Liu, Shuming Shi


[A Retrospective and Futurespective of Rowhammer Attacks and Defenses on DRAM. (76%)](http://arxiv.org/abs/2201.02986)

Zhi Zhang, Jiahao Qi, Yueqiang Cheng, Shijie Jiang, Yiyang Lin, Yansong Gao, Surya Nepal, Yi Zou


[Privacy-aware Early Detection of COVID-19 through Adversarial Training. (10%)](http://arxiv.org/abs/2201.03004)

Omid Rohanian, Samaneh Kouchaki, Andrew Soltan, Jenny Yang, Morteza Rohanian, Yang Yang, David Clifton


## 2022-01-08

[LoMar: A Local Defense Against Poisoning Attack on Federated Learning. (9%)](http://arxiv.org/abs/2201.02873)

Xingyu Li, Zhe Qu, Shangqing Zhao, Bo Tang, Zhuo Lu, Yao Liu


[PocketNN: Integer-only Training and Inference of Neural Networks via Direct Feedback Alignment and Pocket Activations in Pure C++. (1%)](http://arxiv.org/abs/2201.02863)

Jaewoo Song, Fangzhen Lin


## 2022-01-07

[iDECODe: In-distribution Equivariance for Conformal Out-of-distribution Detection. (93%)](http://arxiv.org/abs/2201.02331)

Ramneet Kaur, Susmit Jha, Anirban Roy, Sangdon Park, Edgar Dobriban, Oleg Sokolsky, Insup Lee


[Asymptotic Security using Bayesian Defense Mechanisms with Application to Cyber Deception. (11%)](http://arxiv.org/abs/2201.02351)

Hampei Sasahara, Henrik Sandberg


[Negative Evidence Matters in Interpretable Histology Image Classification. (1%)](http://arxiv.org/abs/2201.02445)

Soufiane Belharbi, Marco Pedersoli, Ismail Ben Ayed, Luke McCaffrey, Eric Granger


## 2022-01-06

[PAEG: Phrase-level Adversarial Example Generation for Neural Machine Translation. (98%)](http://arxiv.org/abs/2201.02009)

Juncheng Wan, Jian Yang, Shuming Ma, Dongdong Zhang, Weinan Zhang, Yong Yu, Zhoujun Li


[Learning to be adversarially robust and differentially private. (31%)](http://arxiv.org/abs/2201.02265)

Jamie Hayes, Borja Balle, M. Pawan Kumar


[Efficient Global Optimization of Two-layer ReLU Networks: Quadratic-time Algorithms and Adversarial Training. (2%)](http://arxiv.org/abs/2201.01965)

Yatong Bai, Tanmay Gautam, Somayeh Sojoudi


## 2022-01-05

[On the Real-World Adversarial Robustness of Real-Time Semantic Segmentation Models for Autonomous Driving. (99%)](http://arxiv.org/abs/2201.01850)

Giulio Rossolini, Federico Nesti, Gianluca D'Amico, Saasha Nair, Alessandro Biondi, Giorgio Buttazzo


[ROOM: Adversarial Machine Learning Attacks Under Real-Time Constraints. (99%)](http://arxiv.org/abs/2201.01621)

Amira Guesmi, Khaled N. Khasawneh, Nael Abu-Ghazaleh, Ihsen Alouani


[Adversarial Robustness in Cognitive Radio Networks. (1%)](http://arxiv.org/abs/2201.01842)

Makan Zamanipour


## 2022-01-04

[Towards Transferable Unrestricted Adversarial Examples with Minimum Changes. (99%)](http://arxiv.org/abs/2201.01102)

Fangcheng Liu, Chao Zhang, Hongyang Zhang


[Towards Understanding and Harnessing the Effect of Image Transformation in Adversarial Detection. (99%)](http://arxiv.org/abs/2201.01080)

Hui Liu, Bo Zhao, Yuefeng Peng, Weidong Li, Peng Liu


[On the Minimal Adversarial Perturbation for Deep Neural Networks with Provable Estimation Error. (86%)](http://arxiv.org/abs/2201.01235)

Fabio Brau, Giulio Rossolini, Alessandro Biondi, Giorgio Buttazzo


[Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness. (31%)](http://arxiv.org/abs/2201.01409)

Amin Eslami Abyane, Derui Zhu, Roberto Souza, Lei Ma, Hadi Hemmati


[Corrupting Data to Remove Deceptive Perturbation: Using Preprocessing Method to Improve System Robustness. (10%)](http://arxiv.org/abs/2201.01399)

Hieu Le, Hans Walker, Dung Tran, Peter Chin


## 2022-01-03

[Compression-Resistant Backdoor Attack against Deep Neural Networks. (75%)](http://arxiv.org/abs/2201.00672)

Mingfu Xue, Xin Wang, Shichang Sun, Yushu Zhang, Jian Wang, Weiqiang Liu


[DeepSight: Mitigating Backdoor Attacks in Federated Learning Through Deep Model Inspection. (68%)](http://arxiv.org/abs/2201.00763)

Phillip Rieger, Thien Duc Nguyen, Markus Miettinen, Ahmad-Reza Sadeghi


[Revisiting PGD Attacks for Stability Analysis of Large-Scale Nonlinear Systems and Perception-Based Control. (11%)](http://arxiv.org/abs/2201.00801)

Aaron Havens, Darioush Keivan, Peter Seiler, Geir Dullerud, Bin Hu


## 2022-01-02

[Actor-Critic Network for Q&A in an Adversarial Environment. (33%)](http://arxiv.org/abs/2201.00455)

Bejan Sadeghian


[On Sensitivity of Deep Learning Based Text Classification Algorithms to Practical Input Perturbations. (12%)](http://arxiv.org/abs/2201.00318)

Aamir Miyajiwala, Arnav Ladkat, Samiksha Jagadale, Raviraj Joshi


## 2022-01-01

[Rethinking Feature Uncertainty in Stochastic Neural Networks for Adversarial Robustness. (87%)](http://arxiv.org/abs/2201.00148)

Hao Yang, Min Wang, Zhengfei Yu, Yun Zhou


[Revisiting Neuron Coverage Metrics and Quality of Deep Neural Networks. (41%)](http://arxiv.org/abs/2201.00191)

Zhou Yang, Jieke Shi, Muhammad Hilmi Asyrofi, David Lo


[Generating Adversarial Samples For Training Wake-up Word Detection Systems Against Confusing Words. (1%)](http://arxiv.org/abs/2201.00167)

Haoxu Wang, Yan Jia, Zeqing Zhao, Xuyang Wang, Junjie Wang, Ming Li


## 2021-12-31

[Adversarial Attack via Dual-Stage Network Erosion. (99%)](http://arxiv.org/abs/2201.00097)

Yexin Duan, Junhua Zou, Xingyu Zhou, Wu Zhang, Jin Zhang, Zhisong Pan


[On Distinctive Properties of Universal Perturbations. (83%)](http://arxiv.org/abs/2112.15329)

Sung Min Park, Kuo-An Wei, Kai Xiao, Jerry Li, Aleksander Madry


## 2021-12-30

[Benign Overfitting in Adversarially Robust Linear Classification. (99%)](http://arxiv.org/abs/2112.15250)

Jinghui Chen, Yuan Cao, Quanquan Gu


[Causal Attention for Interpretable and Generalizable Graph Classification. (1%)](http://arxiv.org/abs/2112.15089)

Yongduo Sui, Xiang Wang, Jiancan Wu, Min Lin, Xiangnan He, Tat-Seng Chua


## 2021-12-29

[Invertible Image Dataset Protection. (92%)](http://arxiv.org/abs/2112.14420)

Kejiang Chen, Xianhan Zeng, Qichao Ying, Sheng Li, Zhenxing Qian, Xinpeng Zhang


[Challenges and Approaches for Mitigating Byzantine Attacks in Federated Learning. (4%)](http://arxiv.org/abs/2112.14468)

Junyu Shi, Wei Wan, Shengshan Hu, Jianrong Lu, Leo Yu Zhang


## 2021-12-28

[Constrained Gradient Descent: A Powerful and Principled Evasion Attack Against Neural Networks. (99%)](http://arxiv.org/abs/2112.14232)

Weiran Lin, Keane Lucas, Lujo Bauer, Michael K. Reiter, Mahmood Sharif


[Closer Look at the Transferability of Adversarial Examples: How They Fool Different Models Differently. (99%)](http://arxiv.org/abs/2112.14337)

Futa Waseda, Sosuke Nishikawa, Trung-Nghia Le, Huy H. Nguyen, Isao Echizen


[Repairing Adversarial Texts through Perturbation. (99%)](http://arxiv.org/abs/2201.02504)

Guoliang Dong, Jingyi Wang, Jun Sun, Sudipta Chattopadhyay, Xinyu Wang, Ting Dai, Jie Shi, Jin Song Dong


[DeepAdversaries: Examining the Robustness of Deep Learning Models for Galaxy Morphology Classification. (91%)](http://arxiv.org/abs/2112.14299)

Aleksandra Ćiprijanović, Diana Kafkes, Gregory Snyder, F. Javier Sánchez, Gabriel Nathan Perdue, Kevin Pedro, Brian Nord, Sandeep Madireddy, Stefan M. Wild


[Super-Efficient Super Resolution for Fast Adversarial Defense at the Edge. (88%)](http://arxiv.org/abs/2112.14340)

Kartikeya Bhardwaj, Dibakar Gope, James Ward, Paul Whatmough, Danny Loh


[A General Framework for Evaluating Robustness of Combinatorial Optimization Solvers on Graphs. (86%)](http://arxiv.org/abs/2201.00402)

Han Lu, Zenan Li, Runzhong Wang, Qibing Ren, Junchi Yan, Xiaokang Yang


[Gas Gauge: A Security Analysis Tool for Smart Contract Out-of-Gas Vulnerabilities. (1%)](http://arxiv.org/abs/2112.14771)

Behkish Nassirzadeh, Huaiying Sun, Sebastian Banescu, Vijay Ganesh


## 2021-12-27

[Adversarial Attack for Asynchronous Event-based Data. (99%)](http://arxiv.org/abs/2112.13534)

Wooju Lee, Hyun Myung


[PRIME: A Few Primitives Can Boost Robustness to Common Corruptions. (81%)](http://arxiv.org/abs/2112.13547)

Apostolos Modas, Rahul Rade, Guillermo Ortiz-Jiménez, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard


[Associative Adversarial Learning Based on Selective Attack. (26%)](http://arxiv.org/abs/2112.13989)

Runqi Wang, Xiaoyue Duan, Baochang Zhang, Song Xue, Wentao Zhu, David Doermann, Guodong Guo


[Learning Robust and Lightweight Model through Separable Structured Transformations. (8%)](http://arxiv.org/abs/2112.13551)

Yanhui Huang, Yangyu Xu, Xian Wei


## 2021-12-26

[Perlin Noise Improve Adversarial Robustness. (99%)](http://arxiv.org/abs/2112.13408)

Chengjun Tang, Kun Zhang, Chunfang Xing, Yong Ding, Zengmin Xu


## 2021-12-25

[Task and Model Agnostic Adversarial Attack on Graph Neural Networks. (99%)](http://arxiv.org/abs/2112.13267)

Kartik Sharma, Samidha Verma, Sourav Medya, Sayan Ranu, Arnab Bhattacharya


[NeuronFair: Interpretable White-Box Fairness Testing through Biased Neuron Identification. (50%)](http://arxiv.org/abs/2112.13214)

Haibin Zheng, Zhiqing Chen, Tianyu Du, Xuhong Zhang, Yao Cheng, Shouling Ji, Jingyi Wang, Yue Yu, Jinyin Chen


## 2021-12-24

[Stealthy Attack on Algorithmic-Protected DNNs via Smart Bit Flipping. (99%)](http://arxiv.org/abs/2112.13162)

Behnam Ghavami, Seyd Movi, Zhenman Fang, Lesley Shannon


[NIP: Neuron-level Inverse Perturbation Against Adversarial Attacks. (98%)](http://arxiv.org/abs/2112.13060)

Ruoxi Chen, Haibo Jin, Jinyin Chen, Haibin Zheng, Yue Yu, Shouling Ji


[CatchBackdoor: Backdoor Testing by Critical Trojan Neural Path Identification via Differential Fuzzing. (82%)](http://arxiv.org/abs/2112.13064)

Haibo Jin, Ruoxi Chen, Jinyin Chen, Yao Cheng, Chong Fu, Ting Wang, Yue Yu, Zhaoyan Ming


[SoK: A Study of the Security on Voice Processing Systems. (9%)](http://arxiv.org/abs/2112.13144)

Robert Chang, Logan Kuo, Arthur Liu, Nader Sehatbakhsh


[DP-UTIL: Comprehensive Utility Analysis of Differential Privacy in Machine Learning. (1%)](http://arxiv.org/abs/2112.12998)

Ismat Jarin, Birhanu Eshete


[Gradient Leakage Attack Resilient Deep Learning. (1%)](http://arxiv.org/abs/2112.13178)

Wenqi Wei, Ling Liu


## 2021-12-23

[Adaptive Modeling Against Adversarial Attacks. (99%)](http://arxiv.org/abs/2112.12431)

Zhiwen Yan, Teck Khim Ng


[Revisiting and Advancing Fast Adversarial Training Through The Lens of Bi-Level Optimization. (99%)](http://arxiv.org/abs/2112.12376)

Yihua Zhang, Guanhua Zhang, Prashant Khanduri, Mingyi Hong, Shiyu Chang, Sijia Liu


[Robust Secretary and Prophet Algorithms for Packing Integer Programs. (2%)](http://arxiv.org/abs/2112.12920)

C. J. Argue, Anupam Gupta, Marco Molinaro, Sahil Singla


[Counterfactual Memorization in Neural Language Models. (2%)](http://arxiv.org/abs/2112.12938)

Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tramèr, Nicholas Carlini


## 2021-12-22

[Adversarial Attacks against Windows PE Malware Detection: A Survey of the State-of-the-Art. (99%)](http://arxiv.org/abs/2112.12310)

Xiang Ling, Lingfei Wu, Jiangyu Zhang, Zhenqing Qu, Wei Deng, Xiang Chen, Yaguan Qian, Chunming Wu, Shouling Ji, Tianyue Luo, Jingzheng Wu, Yanjun Wu


[How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness? (98%)](http://arxiv.org/abs/2112.11668)

Xinhsuai Dong, Luu Anh Tuan, Min Lin, Shuicheng Yan, Hanwang Zhang


[Detect & Reject for Transferability of Black-box Adversarial Attacks Against Network Intrusion Detection Systems. (98%)](http://arxiv.org/abs/2112.12095)

Islam Debicha, Thibault Debatty, Jean-Michel Dricot, Wim Mees, Tayeb Kenaza


[Adversarial Deep Reinforcement Learning for Improving the Robustness of Multi-agent Autonomous Driving Policies. (96%)](http://arxiv.org/abs/2112.11937)

Aizaz Sharif, Dusica Marijan


[Understanding and Measuring Robustness of Multimodal Learning. (69%)](http://arxiv.org/abs/2112.12792)

Nishant Vishwamitra, Hongxin Hu, Ziming Zhao, Long Cheng, Feng Luo


[Evaluating the Robustness of Deep Reinforcement Learning for Autonomous and Adversarial Policies in a Multi-agent Urban Driving Environment. (41%)](http://arxiv.org/abs/2112.11947)

Aizaz Sharif, Dusica Marijan


## 2021-12-21

[A Theoretical View of Linear Backpropagation and Its Convergence. (99%)](http://arxiv.org/abs/2112.11018)

Ziang Li, Yiwen Guo, Haodi Liu, Changshui Zhang


[An Attention Score Based Attacker for Black-box NLP Classifier. (91%)](http://arxiv.org/abs/2112.11660)

Yueyang Liu, Hunmin Lee, Zhipeng Cai


[Covert Communications via Adversarial Machine Learning and Reconfigurable Intelligent Surfaces. (81%)](http://arxiv.org/abs/2112.11414)

Brian Kim, Tugba Erpek, Yalin E. Sagduyu, Sennur Ulukus


[Mind the Gap! A Study on the Transferability of Virtual vs Physical-world Testing of Autonomous Driving Systems. (76%)](http://arxiv.org/abs/2112.11255)

Andrea Stocco, Brian Pulfer, Paolo Tonella


[Input-Specific Robustness Certification for Randomized Smoothing. (68%)](http://arxiv.org/abs/2112.12084)

Ruoxin Chen, Jie Li, Junchi Yan, Ping Li, Bin Sheng


[Improving Robustness with Image Filtering. (68%)](http://arxiv.org/abs/2112.11235)

Matteo Terzi, Mattia Carletti, Gian Antonio Susto


[On the Adversarial Robustness of Causal Algorithmic Recourse. (10%)](http://arxiv.org/abs/2112.11313)

Ricardo Dominguez-Olmedo, Amir-Hossein Karimi, Bernhard Schölkopf


[MIA-Former: Efficient and Robust Vision Transformers via Multi-grained Input-Adaptation. (4%)](http://arxiv.org/abs/2112.11542)

Zhongzhi Yu, Yonggan Fu, Sicheng Li, Chaojian Li, Yingyan Lin


[Exploring Credibility Scoring Metrics of Perception Systems for Autonomous Driving. (2%)](http://arxiv.org/abs/2112.11643)

Viren Khandal, Arth Vidyarthi


[Adversarial Gradient Driven Exploration for Deep Click-Through Rate Prediction. (2%)](http://arxiv.org/abs/2112.11136)

Kailun Wu, Zhangming Chan, Weijie Bian, Lejian Ren, Shiming Xiang, Shuguang Han, Hongbo Deng, Bo Zheng


[Longitudinal Study of the Prevalence of Malware Evasive Techniques. (1%)](http://arxiv.org/abs/2112.11289)

Lorenzo Maffia, Dario Nisi, Platon Kotzias, Giovanni Lagorio, Simone Aonzo, Davide Balzarotti


## 2021-12-20

[Certified Federated Adversarial Training. (98%)](http://arxiv.org/abs/2112.10525)

Giulio Zizzo, Ambrish Rawat, Mathieu Sinn, Sergio Maffeis, Chris Hankin


[Energy-bounded Learning for Robust Models of Code. (83%)](http://arxiv.org/abs/2112.11226)

Nghi D. Q. Bui, Yijun Yu


[Black-Box Testing of Deep Neural Networks through Test Case Diversity. (82%)](http://arxiv.org/abs/2112.12591)

Zohreh Aghababaeyan, Manel Abdellatif, Lionel Briand, Ramesh S, Mojtaba Bagherzadeh


[Unifying Model Explainability and Robustness for Joint Text Classification and Rationale Extraction. (80%)](http://arxiv.org/abs/2112.10424)

Dongfang Li, Baotian Hu, Qingcai Chen, Tujie Xu, Jingcong Tao, Yunan Zhang


[Adversarially Robust Stability Certificates can be Sample-Efficient. (2%)](http://arxiv.org/abs/2112.10690)

Thomas T. C. K. Zhang, Stephen Tu, Nicholas M. Boffi, Jean-Jacques E. Slotine, Nikolai Matni


## 2021-12-19

[Initiative Defense against Facial Manipulation. (67%)](http://arxiv.org/abs/2112.10098)

Qidong Huang, Jie Zhang, Wenbo Zhou, WeimingZhang, Nenghai Yu


## 2021-12-18

[Being Friends Instead of Adversaries: Deep Networks Learn from Data Simplified by Other Networks. (12%)](http://arxiv.org/abs/2112.09968)

Simone Marullo, Matteo Tiezzi, Marco Gori, Stefano Melacci


[Android-COCO: Android Malware Detection with Graph Neural Network for Byte- and Native-Code. (1%)](http://arxiv.org/abs/2112.10038)

Peng Xu


## 2021-12-17

[Reasoning Chain Based Adversarial Attack for Multi-hop Question Answering. (92%)](http://arxiv.org/abs/2112.09658)

Jiayu Fudan University Ding, Siyuan Fudan University Wang, Qin East China Normal University Chen, Zhongyu Fudan University Wei


[Deep Bayesian Learning for Car Hacking Detection. (81%)](http://arxiv.org/abs/2112.09333)

Laha Ale, Scott A. King, Ning Zhang


[Explain, Edit, and Understand: Rethinking User Study Design for Evaluating Model Explanations. (81%)](http://arxiv.org/abs/2112.09669)

Siddhant Arora, Danish Pruthi, Norman Sadeh, William W. Cohen, Zachary C. Lipton, Graham Neubig


[Dynamics-aware Adversarial Attack of 3D Sparse Convolution Network. (80%)](http://arxiv.org/abs/2112.09428)

An Tao, Yueqi Duan, He Wang, Ziyi Wu, Pengliang Ji, Haowen Sun, Jie Zhou, Jiwen Lu


[Provable Adversarial Robustness in the Quantum Model. (62%)](http://arxiv.org/abs/2112.09625)

Khashayar Barooti, Grzegorz Głuch, Ruediger Urbanke


[Domain Adaptation on Point Clouds via Geometry-Aware Implicits. (1%)](http://arxiv.org/abs/2112.09343)

Yuefan Shen, Yanchao Yang, Mi Yan, He Wang, Youyi Zheng, Leonidas Guibas


## 2021-12-16

[Addressing Adversarial Machine Learning Attacks in Smart Healthcare Perspectives. (99%)](http://arxiv.org/abs/2112.08862)

Arawinkumaar Selvakkumar, Shantanu Pal, Zahra Jadidi


[Towards Robust Neural Image Compression: Adversarial Attack and Model Finetuning. (99%)](http://arxiv.org/abs/2112.08691)

Tong Chen, Zhan Ma


[All You Need is RAW: Defending Against Adversarial Attacks with Camera Image Pipelines. (99%)](http://arxiv.org/abs/2112.09219)

Yuxuan Zhang, Bo Dong, Felix Heide


[Robust Upper Bounds for Adversarial Training. (75%)](http://arxiv.org/abs/2112.09279)

Dimitris Bertsimas, Xavier Boix, Kimberly Villalobos Carballo, Dick den Hertog


[TAFIM: Targeted Adversarial Attacks against Facial Image Manipulations. (64%)](http://arxiv.org/abs/2112.09151)

Shivangi Aneja, Lev Markhasin, Matthias Niessner


[Sharpness-Aware Minimization with Dynamic Reweighting. (31%)](http://arxiv.org/abs/2112.08772)

Wenxuan Zhou, Fangyu Liu, Huan Zhang, Muhao Chen


[APTSHIELD: A Stable, Efficient and Real-time APT Detection System for Linux Hosts. (16%)](http://arxiv.org/abs/2112.09008)

Tiantian Zhu, Jinkai Yu, Tieming Chen, Jiayu Wang, Jie Ying, Ye Tian, Mingqi Lv, Yan Chen, Yuan Fan, Ting Wang


[Correlation inference attacks against machine learning models. (13%)](http://arxiv.org/abs/2112.08806)

Ana-Maria Creţu, Florent Guépin, Montjoye Yves-Alexandre de


[Models in the Loop: Aiding Crowdworkers with Generative Annotation Assistants. (2%)](http://arxiv.org/abs/2112.09062)

Max Bartolo, Tristan Thrush, Sebastian Riedel, Pontus Stenetorp, Robin Jia, Douwe Kiela


[Pure Noise to the Rescue of Insufficient Data: Improving Imbalanced Classification by Training on Random Noise Images. (2%)](http://arxiv.org/abs/2112.08810)

Shiran Zada, Itay Benou, Michal Irani


## 2021-12-15

[On the Convergence and Robustness of Adversarial Training. (99%)](http://arxiv.org/abs/2112.08304)

Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, Quanquan Gu


[Temporal Shuffling for Defending Deep Action Recognition Models against Adversarial Attacks. (97%)](http://arxiv.org/abs/2112.07921)

Jaehui Hwang, Huan Zhang, Jun-Ho Choi, Cho-Jui Hsieh, Jong-Seok Lee


[DuQM: A Chinese Dataset of Linguistically Perturbed Natural Questions for Evaluating the Robustness of Question Matching Models. (75%)](http://arxiv.org/abs/2112.08609)

Hongyu Zhu, Yan Chen, Jing Yan, Jing Liu, Yu Hong, Ying Chen, Hua Wu, Haifeng Wang


[Robust Neural Network Classification via Double Regularization. (1%)](http://arxiv.org/abs/2112.08102)

Olof Zetterqvist, Rebecka Jörnsten, Johan Jonasson


## 2021-12-14

[Robustifying automatic speech recognition by extracting slowly varying features. (99%)](http://arxiv.org/abs/2112.07400)

Matias Pizarro, Dorothea Kolossa, Asja Fischer


[Adversarial Examples for Extreme Multilabel Text Classification. (99%)](http://arxiv.org/abs/2112.07512)

Mohammadreza Qaraei, Rohit Babbar


[Dual-Key Multimodal Backdoors for Visual Question Answering. (81%)](http://arxiv.org/abs/2112.07668)

Matthew Walmer, Karan Sikka, Indranil Sur, Abhinav Shrivastava, Susmit Jha


[On the Impact of Hard Adversarial Instances on Overfitting in Adversarial Training. (76%)](http://arxiv.org/abs/2112.07324)

Chen Liu, Zhichao Huang, Mathieu Salzmann, Tong Zhang, Sabine Süsstrunk


[MuxLink: Circumventing Learning-Resilient MUX-Locking Using Graph Neural Network-based Link Prediction. (4%)](http://arxiv.org/abs/2112.07178)

Lilas Alrahis, Satwik Patnaik, Muhammad Shafique, Ozgur Sinanoglu


## 2021-12-13

[Detecting Audio Adversarial Examples with Logit Noising. (99%)](http://arxiv.org/abs/2112.06443)

Namgyu Park, Sangwoo Ji, Jong Kim


[Triangle Attack: A Query-efficient Decision-based Adversarial Attack. (99%)](http://arxiv.org/abs/2112.06569)

Xiaosen Wang, Zeliang Zhang, Kangheng Tong, Dihong Gong, Kun He, Zhifeng Li, Wei Liu


## 2021-12-12

[Interpolated Joint Space Adversarial Training for Robust and Generalizable Defenses. (98%)](http://arxiv.org/abs/2112.06323)

Chun Pong Lau, Jiang Liu, Hossein Souri, Wei-An Lin, Soheil Feizi, Rama Chellappa


[Quantifying and Understanding Adversarial Examples in Discrete Input Spaces. (91%)](http://arxiv.org/abs/2112.06276)

Volodymyr Kuleshov, Evgenii Nikishin, Shantanu Thakoor, Tingfung Lau, Stefano Ermon


[SparseFed: Mitigating Model Poisoning Attacks in Federated Learning with Sparsification. (91%)](http://arxiv.org/abs/2112.06274)

Ashwinee Panda, Saeed Mahloujifar, Arjun N. Bhagoji, Supriyo Chakraborty, Prateek Mittal


[WOOD: Wasserstein-based Out-of-Distribution Detection. (12%)](http://arxiv.org/abs/2112.06384)

Yinan Wang, Wenbo Sun, Jionghua "Judy" Jin, Zhenyu "James" Kong, Xiaowei Yue


## 2021-12-11

[MedAttacker: Exploring Black-Box Adversarial Attacks on Risk Prediction Models in Healthcare. (99%)](http://arxiv.org/abs/2112.06063)

Muchao Ye, Junyu Luo, Guanjie Zheng, Cao Xiao, Ting Wang, Fenglong Ma


[Improving the Transferability of Adversarial Examples with Resized-Diverse-Inputs, Diversity-Ensemble and Region Fitting. (98%)](http://arxiv.org/abs/2112.06011)

Junhua Zou, Zhisong Pan, Junyang Qiu, Xin Liu, Ting Rui, Wei Li


[Stereoscopic Universal Perturbations across Different Architectures and Datasets. (98%)](http://arxiv.org/abs/2112.06116)

Zachary Berger, Parth Agrawal, Tian Yu Liu, Stefano Soatto, Alex Wong


## 2021-12-10

[Learning to Learn Transferable Attack. (99%)](http://arxiv.org/abs/2112.06658)

Shuman Fang, Jie Li, Xianming Lin, Rongrong Ji


[Cross-Modal Transferable Adversarial Attacks from Images to Videos. (99%)](http://arxiv.org/abs/2112.05379)

Zhipeng Wei, Jingjing Chen, Zuxuan Wu, Yu-Gang Jiang


[Attacking Point Cloud Segmentation with Color-only Perturbation. (99%)](http://arxiv.org/abs/2112.05871)

Jiacen Xu, Zhe Zhou, Boyuan Feng, Yufei Ding, Zhou Li


[Preemptive Image Robustification for Protecting Users against Man-in-the-Middle Adversarial Attacks. (92%)](http://arxiv.org/abs/2112.05634)

Seungyong Moon, Gaon An, Hyun Oh Song


[Batch Label Inference and Replacement Attacks in Black-Boxed Vertical Federated Learning. (75%)](http://arxiv.org/abs/2112.05409)

Yang Liu, Tianyuan Zou, Yan Kang, Wenhan Liu, Yuanqin He, Zhihao Yi, Qiang Yang


[Copy, Right? A Testing Framework for Copyright Protection of Deep Learning Models. (68%)](http://arxiv.org/abs/2112.05588)

Jialuo Chen, Jingyi Wang, Tinglan Peng, Youcheng Sun, Peng Cheng, Shouling Ji, Xingjun Ma, Bo Li, Dawn Song


[Efficient Action Poisoning Attacks on Linear Contextual Bandits. (67%)](http://arxiv.org/abs/2112.05367)

Guanlin Liu, Lifeng Lai


[How Private Is Your RL Policy? An Inverse RL Based Analysis Framework. (41%)](http://arxiv.org/abs/2112.05495)

Kritika Prakash, Fiza Husain, Praveen Paruchuri, Sujit P. Gujar


[SoK: On the Security & Privacy in Federated Learning. (5%)](http://arxiv.org/abs/2112.05423)

Gorka Abad, Stjepan Picek, Aitor Urbieta


## 2021-12-09

[Amicable Aid: Turning Adversarial Attack to Benefit Classification. (99%)](http://arxiv.org/abs/2112.04720)

Juyeop Kim, Jun-Ho Choi, Soobeom Jang, Jong-Seok Lee


[Mutual Adversarial Training: Learning together is better than going alone. (99%)](http://arxiv.org/abs/2112.05005)

Jiang Liu, Chun Pong Lau, Hossein Souri, Soheil Feizi, Rama Chellappa


[PARL: Enhancing Diversity of Ensemble Networks to Resist Adversarial Attacks via Pairwise Adversarially Robust Loss Function. (99%)](http://arxiv.org/abs/2112.04948)

Manaar Alam, Shubhajit Datta, Debdeep Mukhopadhyay, Arijit Mondal, Partha Pratim Chakrabarti


[RamBoAttack: A Robust Query Efficient Deep Neural Network Decision Exploit. (99%)](http://arxiv.org/abs/2112.05282)

Viet Quoc Vo, Ehsan Abbasnejad, Damith C. Ranasinghe


[Spinning Language Models: Risks of Propaganda-As-A-Service and Countermeasures. (69%)](http://arxiv.org/abs/2112.05224)

Eugene Bagdasaryan, Vitaly Shmatikov


[Robustness Certificates for Implicit Neural Networks: A Mixed Monotone Contractive Approach. (38%)](http://arxiv.org/abs/2112.05310)

Saber Jafarpour, Matthew Abate, Alexander Davydov, Francesco Bullo, Samuel Coogan


[PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures. (10%)](http://arxiv.org/abs/2112.05135)

Dan Hendrycks, Andy Zou, Mantas Mazeika, Leonard Tang, Dawn Song, Jacob Steinhardt


[Are We There Yet? Timing and Floating-Point Attacks on Differential Privacy Systems. (2%)](http://arxiv.org/abs/2112.05307)

Jiankai Jin, Eleanor McMurtry, Benjamin I. P. Rubinstein, Olga Ohrimenko


[3D-VField: Learning to Adversarially Deform Point Clouds for Robust 3D Object Detection. (1%)](http://arxiv.org/abs/2112.04764)

Alexander Lehner, Stefano Gasperini, Alvaro Marcos-Ramiro, Michael Schmidt, Mohammad-Ali Nikouei Mahani, Nassir Navab, Benjamin Busam, Federico Tombari


## 2021-12-08

[Segment and Complete: Defending Object Detectors against Adversarial Patch Attacks with Robust Patch Detection. (99%)](http://arxiv.org/abs/2112.04532)

Jiang Liu, Alexander Levine, Chun Pong Lau, Rama Chellappa, Soheil Feizi


[On visual self-supervision and its effect on model robustness. (99%)](http://arxiv.org/abs/2112.04367)

Michal Kucer, Diane Oyen, Garrett Kenyon


[SNEAK: Synonymous Sentences-Aware Adversarial Attack on Natural Language Video Localization. (93%)](http://arxiv.org/abs/2112.04154)

Wenbo Gou, Wen Shi, Jian Lou, Lijie Huang, Pan Zhou, Ruixuan Li


[Revisiting Contrastive Learning through the Lens of Neighborhood Component Analysis: an Integrated Framework. (8%)](http://arxiv.org/abs/2112.04468)

Ching-Yun Ko, Jeet Mohapatra, Sijia Liu, Pin-Yu Chen, Luca Daniel, Lily Weng


## 2021-12-07

[Saliency Diversified Deep Ensemble for Robustness to Adversaries. (99%)](http://arxiv.org/abs/2112.03615)

Alex Bogun, Dimche Kostadinov, Damian Borth


[Vehicle trajectory prediction works, but not everywhere. (50%)](http://arxiv.org/abs/2112.03909)

Mohammadhossein Bahari, Saeed Saadatnejad, Ahmad Rahimi, Mohammad Shaverdikondori, Mohammad Shahidzadeh, Seyed-Mohsen Moosavi-Dezfooli, Alexandre Alahi


[Lightning: Striking the Secure Isolation on GPU Clouds with Transient Hardware Faults. (11%)](http://arxiv.org/abs/2112.03662)

Rihui Sun, Pefei Qiu, Yongqiang Lyu, Donsheng Wang, Jiang Dong, Gang Qu


[Membership Inference Attacks From First Principles. (2%)](http://arxiv.org/abs/2112.03570)

Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, Florian Tramer


[Training Deep Models to be Explained with Fewer Examples. (1%)](http://arxiv.org/abs/2112.03508)

Tomoharu Iwata, Yuya Yoshikawa


[Presentation Attack Detection Methods based on Gaze Tracking and Pupil Dynamic: A Comprehensive Survey. (1%)](http://arxiv.org/abs/2112.04038)

Jalil Nourmohammadi Khiarak


## 2021-12-06

[Adversarial Machine Learning In Network Intrusion Detection Domain: A Systematic Review. (99%)](http://arxiv.org/abs/2112.03315)

Huda Ali Alatwi, Charles Morisset


[Decision-based Black-box Attack Against Vision Transformers via Patch-wise Adversarial Removal. (84%)](http://arxiv.org/abs/2112.03492)

Yucheng Shi, Yahong Han, Yu-an Tan, Xiaohui Kuang


[ML Attack Models: Adversarial Attacks and Data Poisoning Attacks. (82%)](http://arxiv.org/abs/2112.02797)

Jing Lin, Long Dang, Mohamed Rahouti, Kaiqi Xiong


[Test-Time Detection of Backdoor Triggers for Poisoned Deep Neural Networks. (82%)](http://arxiv.org/abs/2112.03350)

Xi Li, Zhen Xiang, David J. Miller, George Kesidis


[When the Curious Abandon Honesty: Federated Learning Is Not Private. (68%)](http://arxiv.org/abs/2112.02918)

Franziska Boenisch, Adam Dziedzic, Roei Schuster, Ali Shahin Shamsabadi, Ilia Shumailov, Nicolas Papernot


[Defending against Model Stealing via Verifying Embedded External Features. (33%)](http://arxiv.org/abs/2112.03476)

Yiming Li, Linghui Zhu, Xiaojun Jia, Yong Jiang, Shu-Tao Xia, Xiaochun Cao


[Context-Aware Transfer Attacks for Object Detection. (1%)](http://arxiv.org/abs/2112.03223)

Zikui Cai, Xinxin Xie, Shasha Li, Mingjun Yin, Chengyu Song, Srikanth V. Krishnamurthy, Amit K. Roy-Chowdhury, M. Salman Asif


## 2021-12-05

[Robust Active Learning: Sample-Efficient Training of Robust Deep Learning Models. (96%)](http://arxiv.org/abs/2112.02542)

Yuejun Guo, Qiang Hu, Maxime Cordy, Mike Papadakis, Yves Le Traon


[Stochastic Local Winner-Takes-All Networks Enable Profound Adversarial Robustness. (88%)](http://arxiv.org/abs/2112.02671)

Konstantinos P. Panousis, Sotirios Chatzis, Sergios Theodoridis


[Beyond Robustness: Resilience Verification of Tree-Based Classifiers. (2%)](http://arxiv.org/abs/2112.02705)

Stefano Calzavara, Lorenzo Cazzaro, Claudio Lucchese, Federico Marcuzzi, Salvatore Orlando


[On Impact of Semantically Similar Apps in Android Malware Datasets. (1%)](http://arxiv.org/abs/2112.02606)

Roopak Surendran


## 2021-12-04

[RADA: Robust Adversarial Data Augmentation for Camera Localization in Challenging Weather. (10%)](http://arxiv.org/abs/2112.02469)

Jialu Wang, Muhamad Risqi U. Saputra, Chris Xiaoxuan Lu, Niki Trigon, Andrew Markham


## 2021-12-03

[Single-Shot Black-Box Adversarial Attacks Against Malware Detectors: A Causal Language Model Approach. (99%)](http://arxiv.org/abs/2112.01724)

James Lee Hu, Mohammadreza Ebrahimi, Hsinchun Chen


[Generalized Likelihood Ratio Test for Adversarially Robust Hypothesis Testing. (99%)](http://arxiv.org/abs/2112.02209)

Bhagyashree Puranik, Upamanyu Madhow, Ramtin Pedarsani


[Blackbox Untargeted Adversarial Testing of Automatic Speech Recognition Systems. (98%)](http://arxiv.org/abs/2112.01821)

Xiaoliang Wu, Ajitha Rajan


[Attack-Centric Approach for Evaluating Transferability of Adversarial Samples in Machine Learning Models. (54%)](http://arxiv.org/abs/2112.01777)

Tochukwu Idika, Ismail Akturk


[Adversarial Attacks against a Satellite-borne Multispectral Cloud Detector. (13%)](http://arxiv.org/abs/2112.01723)

Andrew Du, Yee Wei Law, Michele Sasdelli, Bo Chen, Ken Clarke, Michael Brown, Tat-Jun Chin


[A Game-Theoretic Approach for AI-based Botnet Attack Defence. (9%)](http://arxiv.org/abs/2112.02223)

Hooman Alavizadeh, Julian Jang-Jaccard, Tansu Alpcan, Seyit A. Camtepe


## 2021-12-02

[A Unified Framework for Adversarial Attack and Defense in Constrained Feature Space. (99%)](http://arxiv.org/abs/2112.01156)

Thibault Simonetto, Salijona Dyrmishi, Salah Ghamizi, Maxime Cordy, Yves Le Traon


[Is Approximation Universally Defensive Against Adversarial Attacks in Deep Neural Networks? (93%)](http://arxiv.org/abs/2112.01555)

Ayesha Siddique, Khaza Anuarul Hoque


[Is RobustBench/AutoAttack a suitable Benchmark for Adversarial Robustness? (75%)](http://arxiv.org/abs/2112.01601)

Peter Lorenz, Dominik Strassel, Margret Keuper, Janis Keuper


[Training Efficiency and Robustness in Deep Learning. (41%)](http://arxiv.org/abs/2112.01423)

Fartash Faghri


[FedRAD: Federated Robust Adaptive Distillation. (10%)](http://arxiv.org/abs/2112.01405)

Stefán Páll Sturluson, Samuel Trew, Luis Muñoz-González, Matei Grama, Jonathan Passerat-Palmbach, Daniel Rueckert, Amir Alansary


[FIBA: Frequency-Injection based Backdoor Attack in Medical Image Analysis. (3%)](http://arxiv.org/abs/2112.01148)

Yu Feng, Benteng Ma, Jing Zhang, Shanshan Zhao, Yong Xia, Dacheng Tao


[On the Existence of the Adversarial Bayes Classifier (Extended Version). (2%)](http://arxiv.org/abs/2112.01694)

Pranjal Awasthi, Natalie S. Frank, Mehryar Mohri


[Editing a classifier by rewriting its prediction rules. (1%)](http://arxiv.org/abs/2112.01008)

Shibani Santurkar, Dimitris Tsipras, Mahalaxmi Elango, David Bau, Antonio Torralba, Aleksander Madry


## 2021-12-01

[Adversarial Robustness of Deep Reinforcement Learning based Dynamic Recommender Systems. (99%)](http://arxiv.org/abs/2112.00973)

Siyu Wang, Yuanjiang Cao, Xiaocong Chen, Lina Yao, Xianzhi Wang, Quan Z. Sheng


[Push Stricter to Decide Better: A Class-Conditional Feature Adaptive Framework for Improving Adversarial Robustness. (99%)](http://arxiv.org/abs/2112.00323)

Jia-Li Yin, Lehui Xie, Wanqing Zhu, Ximeng Liu, Bo-Hao Chen


[$\ell_\infty$-Robustness and Beyond: Unleashing Efficient Adversarial Training. (99%)](http://arxiv.org/abs/2112.00378)

Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie


[Certified Adversarial Defenses Meet Out-of-Distribution Corruptions: Benchmarking Robustness and Simple Baselines. (96%)](http://arxiv.org/abs/2112.00659)

Jiachen Sun, Akshay Mehra, Bhavya Kailkhura, Pin-Yu Chen, Dan Hendrycks, Jihun Hamm, Z. Morley Mao


[Adv-4-Adv: Thwarting Changing Adversarial Perturbations via Adversarial Domain Adaptation. (95%)](http://arxiv.org/abs/2112.00428)

Tianyue Zheng, Zhe Chen, Shuya Ding, Chao Cai, Jun Luo


[Robustness in Deep Learning for Computer Vision: Mind the gap? (31%)](http://arxiv.org/abs/2112.00639)

Nathan Drenkow, Numair Sani, Ilya Shpitser, Mathias Unberath


[CYBORG: Blending Human Saliency Into the Loss Improves Deep Learning. (1%)](http://arxiv.org/abs/2112.00686)

Aidan Boyd, Patrick Tinsley, Kevin Bowyer, Adam Czajka


## 2021-11-30

[Using a GAN to Generate Adversarial Examples to Facial Image Recognition. (99%)](http://arxiv.org/abs/2111.15213)

Andrew Merrigan, Alan F. Smeaton


[Mitigating Adversarial Attacks by Distributing Different Copies to Different Users. (96%)](http://arxiv.org/abs/2111.15160)

Jiyi Zhang, Wesley Joon-Wie Tann, Ee-Chien Chang


[Human Imperceptible Attacks and Applications to Improve Fairness. (83%)](http://arxiv.org/abs/2111.15603)

Xinru Hua, Huanzhong Xu, Jose Blanchet, Viet Nguyen


[Evaluating Gradient Inversion Attacks and Defenses in Federated Learning. (81%)](http://arxiv.org/abs/2112.00059)

Yangsibo Huang, Samyak Gupta, Zhao Song, Kai Li, Sanjeev Arora


[FROB: Few-shot ROBust Model for Classification and Out-of-Distribution Detection. (78%)](http://arxiv.org/abs/2111.15487)

Nikolaos Dionelis


[COREATTACK: Breaking Up the Core Structure of Graphs. (78%)](http://arxiv.org/abs/2111.15276)

Bo Zhou, Yuqian Lv, Jinhuan Wang, Jian Zhang, Qi Xuan


[Adversarial Attacks Against Deep Generative Models on Data: A Survey. (12%)](http://arxiv.org/abs/2112.00247)

Hui Sun, Tianqing Zhu, Zhiqiu Zhang, Dawei Jin. Ping Xiong, Wanlei Zhou


[A Face Recognition System's Worst Morph Nightmare, Theoretically. (1%)](http://arxiv.org/abs/2111.15416)

Una M. Kelly, Raymond Veldhuis, Luuk Spreeuwers


[New Datasets for Dynamic Malware Classification. (1%)](http://arxiv.org/abs/2111.15205)

Berkant Düzgün, Aykut Çayır, Ferhat Demirkıran, Ceyda Nur Kayha, Buket Gençaydın, Hasan Dağ


[Reliability Assessment and Safety Arguments for Machine Learning Components in Assuring Learning-Enabled Autonomous Systems. (1%)](http://arxiv.org/abs/2112.00646)

Xingyu Zhao, Wei Huang, Vibhav Bharti, Yi Dong, Victoria Cox, Alec Banks, Sen Wang, Sven Schewe, Xiaowei Huang


## 2021-11-29

[MedRDF: A Robust and Retrain-Less Diagnostic Framework for Medical Pretrained Models Against Adversarial Attack. (99%)](http://arxiv.org/abs/2111.14564)

Mengting Xu, Tao Zhang, Daoqiang Zhang


[Adversarial Attacks in Cooperative AI. (82%)](http://arxiv.org/abs/2111.14833)

Ted Fujimoto, Arthur Paul Pedersen


[Living-Off-The-Land Command Detection Using Active Learning. (10%)](http://arxiv.org/abs/2111.15039)

Talha Ongun, Jack W. Stokes, Jonathan Bar Or, Ke Tian, Farid Tajaddodianfar, Joshua Neil, Christian Seifert, Alina Oprea, John C. Platt


[Do Invariances in Deep Neural Networks Align with Human Perception? (9%)](http://arxiv.org/abs/2111.14726)

Vedant Nanda, Ayan Majumdar, Camila Kolling, John P. Dickerson, Krishna P. Gummadi, Bradley C. Love, Adrian Weller


[A Simple Long-Tailed Recognition Baseline via Vision-Language Model. (1%)](http://arxiv.org/abs/2111.14745)

Teli Ma, Shijie Geng, Mengmeng Wang, Jing Shao, Jiasen Lu, Hongsheng Li, Peng Gao, Yu Qiao


[ROBIN : A Benchmark for Robustness to Individual Nuisances in Real-World Out-of-Distribution Shifts. (1%)](http://arxiv.org/abs/2111.14341)

Bingchen Zhao, Shaozuo Yu, Wufei Ma, Mingxin Yu, Shenxiao Mei, Angtian Wang, Ju He, Alan Yuille, Adam Kortylewski


[Pyramid Adversarial Training Improves ViT Performance. (1%)](http://arxiv.org/abs/2111.15121)

Charles Herrmann, Kyle Sargent, Lu Jiang, Ramin Zabih, Huiwen Chang, Ce Liu, Dilip Krishnan, Deqing Sun


## 2021-11-28

[Detecting Adversaries, yet Faltering to Noise? Leveraging Conditional Variational AutoEncoders for Adversary Detection in the Presence of Noisy Images. (96%)](http://arxiv.org/abs/2111.15518)

Dvij Kalaria, Aritra Hazra, Partha Pratim Chakrabarti


[MALIGN: Adversarially Robust Malware Family Detection using Sequence Alignment. (68%)](http://arxiv.org/abs/2111.14185)

Shoumik Saha, Sadia Afroz, Atif Rahman


[Automated Runtime-Aware Scheduling for Multi-Tenant DNN Inference on GPU. (1%)](http://arxiv.org/abs/2111.14255)

Fuxun Yu, Shawn Bray, Di Wang, Longfei Shangguan, Xulong Tang, Chenchen Liu, Xiang Chen


[ExCon: Explanation-driven Supervised Contrastive Learning for Image Classification. (1%)](http://arxiv.org/abs/2111.14271)

Zhibo Zhang, Jongseong Jang, Chiheb Trabelsi, Ruiwen Li, Scott Sanner, Yeonjeong Jeong, Dongsub Shim


## 2021-11-27

[Adaptive Image Transformations for Transfer-based Adversarial Attack. (99%)](http://arxiv.org/abs/2111.13844)

Zheng Yuan, Jie Zhang, Shiguang Shan


[Adaptive Perturbation for Adversarial Attack. (99%)](http://arxiv.org/abs/2111.13841)

Zheng Yuan, Jie Zhang, Shiguang Shan


[Statically Detecting Adversarial Malware through Randomised Chaining. (98%)](http://arxiv.org/abs/2111.14037)

Matthew Crawford, Wei Wang, Ruoxi Sun, Minhui Xue


[Dissecting Malware in the Wild. (1%)](http://arxiv.org/abs/2111.14035)

Hamish Spencer, Wei Wang, Ruoxi Sun, Minhui Xue


## 2021-11-26

[ArchRepair: Block-Level Architecture-Oriented Repairing for Deep Neural Networks. (50%)](http://arxiv.org/abs/2111.13330)

Hua Qi, Zhijie Wang, Qing Guo, Jianlang Chen, Felix Juefei-Xu, Lei Ma, Jianjun Zhao


## 2021-11-25

[Natural & Adversarial Bokeh Rendering via Circle-of-Confusion Predictive Network. (99%)](http://arxiv.org/abs/2111.12971)

Yihao Huang, Felix Juefei-Xu, Qing Guo, Geguang Pu, Yang Liu


[Clustering Effect of (Linearized) Adversarial Robust Models. (97%)](http://arxiv.org/abs/2111.12922)

Yang Bai, Xin Yan, Yong Jiang, Shu-Tao Xia, Yisen Wang


[Simple Contrastive Representation Adversarial Learning for NLP Tasks. (93%)](http://arxiv.org/abs/2111.13301)

Deshui Miao, Jiaqi Zhang, Wenbo Xie, Jian Song, Xin Li, Lijuan Jia, Ning Guo


[Going Grayscale: The Road to Understanding and Improving Unlearnable Examples. (92%)](http://arxiv.org/abs/2111.13244)

Zhuoran Liu, Zhengyu Zhao, Alex Kolmus, Tijn Berns, Laarhoven Twan van, Tom Heskes, Martha Larson


[Towards Practical Deployment-Stage Backdoor Attack on Deep Neural Networks. (92%)](http://arxiv.org/abs/2111.12965)

Xiangyu Qi, Tinghao Xie, Ruizhe Pan, Jifeng Zhu, Yong Yang, Kai Bu


[Gradient Inversion Attack: Leaking Private Labels in Two-Party Split Learning. (3%)](http://arxiv.org/abs/2112.01299)

Sanjay Kariyappa, Moinuddin K Qureshi


[Joint inference and input optimization in equilibrium networks. (1%)](http://arxiv.org/abs/2111.13236)

Swaminathan Gurumurthy, Shaojie Bai, Zachary Manchester, J. Zico Kolter


## 2021-11-24

[Thundernna: a white box adversarial attack. (99%)](http://arxiv.org/abs/2111.12305)

Linfeng Ye


[Unity is strength: Improving the Detection of Adversarial Examples with Ensemble Approaches. (99%)](http://arxiv.org/abs/2111.12631)

Francesco Craighero, Fabrizio Angaroni, Fabio Stella, Chiara Damiani, Marco Antoniotti, Alex Graudenzi


[Robustness against Adversarial Attacks in Neural Networks using Incremental Dissipativity. (92%)](http://arxiv.org/abs/2111.12906)

Bernardo Aquino, Arash Rahnama, Peter Seiler, Lizhen Lin, Vijay Gupta


[WFDefProxy: Modularly Implementing and Empirically Evaluating Website Fingerprinting Defenses. (15%)](http://arxiv.org/abs/2111.12629)

Jiajun Gong, Wuqi Zhang, Charles Zhang, Tao Wang


[Sharpness-aware Quantization for Deep Neural Networks. (10%)](http://arxiv.org/abs/2111.12273)

Jing Liu, Jianfei Cai, Bohan Zhuang


[SLA$^2$P: Self-supervised Anomaly Detection with Adversarial Perturbation. (5%)](http://arxiv.org/abs/2111.12896)

Yizhou Wang, Can Qin, Rongzhe Wei, Yi Xu, Yue Bai, Yun Fu


[An Attack on Facial Soft-biometric Privacy Enhancement. (2%)](http://arxiv.org/abs/2111.12405)

Dailé Osorio-Roig, Christian Rathgeb, Pawel Drozdowski, Philipp Terhörst, Vitomir Štruc, Christoph Busch


[Accelerating Deep Learning with Dynamic Data Pruning. (1%)](http://arxiv.org/abs/2111.12621)

Ravi S Raju, Kyle Daruwalla, Mikko Lipasti


## 2021-11-23

[Adversarial machine learning for protecting against online manipulation. (92%)](http://arxiv.org/abs/2111.12034)

Stefano Cresci, Marinella Petrocchi, Angelo Spognardi, Stefano Tognazzi


[Fixed Points in Cyber Space: Rethinking Optimal Evasion Attacks in the Age of AI-NIDS. (84%)](http://arxiv.org/abs/2111.12197)

Witt Christian Schroeder de, Yongchao Huang, Philip H. S. Torr, Martin Strohmeier


[Subspace Adversarial Training. (69%)](http://arxiv.org/abs/2111.12229)

Tao Li, Yingwen Wu, Sizhe Chen, Kun Fang, Xiaolin Huang


[HERO: Hessian-Enhanced Robust Optimization for Unifying and Improving Generalization and Quantization Performance. (1%)](http://arxiv.org/abs/2111.11986)

Huanrui Yang, Xiaoxuan Yang, Neil Zhenqiang Gong, Yiran Chen


## 2021-11-22

[Adversarial Examples on Segmentation Models Can be Easy to Transfer. (99%)](http://arxiv.org/abs/2111.11368)

Jindong Gu, Hengshuang Zhao, Volker Tresp, Philip Torr


[Evaluating Adversarial Attacks on ImageNet: A Reality Check on Misclassification Classes. (99%)](http://arxiv.org/abs/2111.11056)

Utku Ozbulak, Maura Pintor, Messem Arnout Van, Neve Wesley De


[Imperceptible Transfer Attack and Defense on 3D Point Cloud Classification. (99%)](http://arxiv.org/abs/2111.10990)

Daizong Liu, Wei Hu


[Backdoor Attack through Frequency Domain. (92%)](http://arxiv.org/abs/2111.10991)

Tong Wang, Yuan Yao, Feng Xu, Shengwei An, Hanghang Tong, Ting Wang


[NTD: Non-Transferability Enabled Backdoor Detection. (69%)](http://arxiv.org/abs/2111.11157)

Yinshan Li, Hua Ma, Zhi Zhang, Yansong Gao, Alsharif Abuadbba, Anmin Fu, Yifeng Zheng, Said F. Al-Sarawi, Derek Abbott


[A Comparison of State-of-the-Art Techniques for Generating Adversarial Malware Binaries. (33%)](http://arxiv.org/abs/2111.11487)

Prithviraj Dasgupta, Zachariah Osman


[Poisoning Attacks to Local Differential Privacy Protocols for Key-Value Data. (13%)](http://arxiv.org/abs/2111.11534)

Yongji Wu, Xiaoyu Cao, Jinyuan Jia, Neil Zhenqiang Gong


[Automatic Mapping of the Best-Suited DNN Pruning Schemes for Real-Time Mobile Acceleration. (1%)](http://arxiv.org/abs/2111.11581)

Yifan Gong, Geng Yuan, Zheng Zhan, Wei Niu, Zhengang Li, Pu Zhao, Yuxuan Cai, Sijia Liu, Bin Ren, Xue Lin, Xulong Tang, Yanzhi Wang


[Electric Vehicle Attack Impact on Power Grid Operation. (1%)](http://arxiv.org/abs/2111.11317)

Mohammad Ali Sayed, Ribal Atallah, Chadi Assi, Mourad Debbabi


## 2021-11-21

[Stochastic Variance Reduced Ensemble Adversarial Attack for Boosting the Adversarial Transferability. (99%)](http://arxiv.org/abs/2111.10752)

Yifeng Xiong, Jiadong Lin, Min Zhang, John E. Hopcroft, Kun He


[Adversarial Mask: Real-World Universal Adversarial Attack on Face Recognition Model. (99%)](http://arxiv.org/abs/2111.10759)

Alon Zolfi, Shai Avidan, Yuval Elovici, Asaf Shabtai


[Medical Aegis: Robust adversarial protectors for medical images. (99%)](http://arxiv.org/abs/2111.10969)

Qingsong Yao, Zecheng He, S. Kevin Zhou


[Local Linearity and Double Descent in Catastrophic Overfitting. (73%)](http://arxiv.org/abs/2111.10754)

Varun Sivashankar, Nikil Selvam


[Denoised Internal Models: a Brain-Inspired Autoencoder against Adversarial Attacks. (62%)](http://arxiv.org/abs/2111.10844)

Kaiyuan Liu, Xingyu Li, Yi Zhou, Jisong Guan, Yurui Lai, Ge Zhang, Hang Su, Jiachen Wang, Chunxu Guo


## 2021-11-20

[Are Vision Transformers Robust to Patch Perturbations? (98%)](http://arxiv.org/abs/2111.10659)

Jindong Gu, Volker Tresp, Yao Qin


## 2021-11-19

[Towards Efficiently Evaluating the Robustness of Deep Neural Networks in IoT Systems: A GAN-based Method. (99%)](http://arxiv.org/abs/2111.10055)

Tao Bai, Jun Zhao, Jinlin Zhu, Shoudong Han, Jiefeng Chen, Bo Li, Alex Kot


[Meta Adversarial Perturbations. (99%)](http://arxiv.org/abs/2111.10291)

Chia-Hung Yuan, Pin-Yu Chen, Chia-Mu Yu


[Resilience from Diversity: Population-based approach to harden models against adversarial attacks. (99%)](http://arxiv.org/abs/2111.10272)

Jasser Jasser, Ivan Garibay


[Enhanced countering adversarial attacks via input denoising and feature restoring. (99%)](http://arxiv.org/abs/2111.10075)

Yanni Li, Wenhui Zhang, Jiawei Liu, Xiaoli Kou, Hui Li, Jiangtao Cui


[PatchCensor: Patch Robustness Certification for Transformers via Exhaustive Testing. (99%)](http://arxiv.org/abs/2111.10481)

Yuheng Huang, Lei Ma, Yuanchun Li


[Fooling Adversarial Training with Inducing Noise. (98%)](http://arxiv.org/abs/2111.10130)

Zhirui Wang, Yifei Wang, Yisen Wang


[Exposing Weaknesses of Malware Detectors with Explainability-Guided Evasion Attacks. (86%)](http://arxiv.org/abs/2111.10085)

Wei Wang, Ruoxi Sun, Tian Dong, Shaofeng Li, Minhui Xue, Gareth Tyson, Haojin Zhu


## 2021-11-18

[TnT Attacks! Universal Naturalistic Adversarial Patches Against Deep Neural Network Systems. (99%)](http://arxiv.org/abs/2111.09999)

Bao Gia Doan, Minhui Xue, Shiqing Ma, Ehsan Abbasnejad, Damith C. Ranasinghe


[A Review of Adversarial Attack and Defense for Classification Methods. (99%)](http://arxiv.org/abs/2111.09961)

Yao Li, Minhao Cheng, Cho-Jui Hsieh, Thomas C. M. Lee


[Robust Person Re-identification with Multi-Modal Joint Defence. (98%)](http://arxiv.org/abs/2111.09571)

Yunpeng Gong, Lifei Chen


[Enhancing the Insertion of NOP Instructions to Obfuscate Malware via Deep Reinforcement Learning. (96%)](http://arxiv.org/abs/2111.09626)

Daniel Gibert, Matt Fredrikson, Carles Mateu, Jordi Planes, Quan Le


[How to Build Robust FAQ Chatbot with Controllable Question Generator? (80%)](http://arxiv.org/abs/2112.03007)

Yan Pan, Mingyang Ma, Bernhard Pflugfelder, Georg Groh


[Adversarial attacks on voter model dynamics in complex networks. (76%)](http://arxiv.org/abs/2111.09561)

Katsumi Chiyomaru, Kazuhiro Takemoto


[Enhanced Membership Inference Attacks against Machine Learning Models. (12%)](http://arxiv.org/abs/2111.09679)

Jiayuan Ye, Aadyaa Maddi, Sasi Kumar Murakonda, Reza Shokri


[Wiggling Weights to Improve the Robustness of Classifiers. (2%)](http://arxiv.org/abs/2111.09779)

Sadaf Gulshad, Ivan Sosnovik, Arnold Smeulders


[Improving Transferability of Representations via Augmentation-Aware Self-Supervision. (1%)](http://arxiv.org/abs/2111.09613)

Hankook Lee, Kibok Lee, Kimin Lee, Honglak Lee, Jinwoo Shin


## 2021-11-17

[TraSw: Tracklet-Switch Adversarial Attacks against Multi-Object Tracking. (99%)](http://arxiv.org/abs/2111.08954)

Delv Lin, Qi Chen, Chengyu Zhou, Kun He


[Generating Unrestricted 3D Adversarial Point Clouds. (99%)](http://arxiv.org/abs/2111.08973)

Xuelong Dai, Yanjie Li, Hua Dai, Bin Xiao


[SmoothMix: Training Confidence-calibrated Smoothed Classifiers for Certified Robustness. (93%)](http://arxiv.org/abs/2111.09277)

Jongheon Jeong, Sejun Park, Minkyu Kim, Heung-Chang Lee, Doguk Kim, Jinwoo Shin


[Attacking Deep Learning AI Hardware with Universal Adversarial Perturbation. (92%)](http://arxiv.org/abs/2111.09488)

Mehdi Sadi, B. M. S. Bahar Talukder, Kaniz Mishty, Md Tauhidur Rahman


[Do Not Trust Prediction Scores for Membership Inference Attacks. (33%)](http://arxiv.org/abs/2111.09076)

Dominik Hintersdorf, Lukas Struppek, Kristian Kersting


## 2021-11-16

[Robustness of Bayesian Neural Networks to White-Box Adversarial Attacks. (99%)](http://arxiv.org/abs/2111.08591)

Adaku Uchendu, Daniel Campoy, Christopher Menart, Alexandra Hildenbrandt


[Improving the robustness and accuracy of biomedical language models through adversarial training. (99%)](http://arxiv.org/abs/2111.08529)

Milad Moradi, Matthias Samwald


[Detecting AutoAttack Perturbations in the Frequency Domain. (99%)](http://arxiv.org/abs/2111.08785)

Peter Lorenz, Paula Harder, Dominik Strassel, Margret Keuper, Janis Keuper


[Adversarial Tradeoffs in Linear Inverse Problems and Robust StateEstimation. (92%)](http://arxiv.org/abs/2111.08864)

Bruce D. Lee, Thomas T. C. K. Zhang, Hamed Hassani, Nikolai Matni


[Consistent Semantic Attacks on Optical Flow. (81%)](http://arxiv.org/abs/2111.08485)

Tom Koren, Lior Talker, Michael Dinerstein, Roy J Jevnisek


[An Overview of Backdoor Attacks Against Deep Neural Networks and Possible Defences. (54%)](http://arxiv.org/abs/2111.08429)

Wei Guo, Benedetta Tondi, Mauro Barni


[Enabling equivariance for arbitrary Lie groups. (1%)](http://arxiv.org/abs/2111.08251)

Lachlan Ewen MacDonald, Sameera Ramasinghe, Simon Lucey


## 2021-11-15

[A Survey on Adversarial Attacks for Malware Analysis. (98%)](http://arxiv.org/abs/2111.08223)

Kshitiz Aryal, Maanak Gupta, Mahmoud Abdelsalam


[Triggerless Backdoor Attack for NLP Tasks with Clean Labels. (68%)](http://arxiv.org/abs/2111.07970)

Leilei Gan, Jiwei Li, Tianwei Zhang, Xiaoya Li, Yuxian Meng, Fei Wu, Shangwei Guo, Chun Fan


[Property Inference Attacks Against GANs. (67%)](http://arxiv.org/abs/2111.07608)

Junhao Zhou, Yufei Chen, Chao Shen, Yang Zhang


## 2021-11-14

[Generating Band-Limited Adversarial Surfaces Using Neural Networks. (99%)](http://arxiv.org/abs/2111.07424)

Roee Ben-Shlomo, Yevgeniy Men, Ido Imanuel


[Finding Optimal Tangent Points for Reducing Distortions of Hard-label Attacks. (76%)](http://arxiv.org/abs/2111.07492)

Chen Ma, Xiangyu Guo, Li Chen, Jun-Hai Yong, Yisen Wang


[Towards Interpretability of Speech Pause in Dementia Detection using Adversarial Learning. (75%)](http://arxiv.org/abs/2111.07454)

Youxiang Zhu, Bang Tran, Xiaohui Liang, John A. Batsis, Robert M. Roth


[Improving Compound Activity Classification via Deep Transfer and Representation Learning. (1%)](http://arxiv.org/abs/2111.07439)

Vishal Dey, Raghu Machiraju, Xia Ning


## 2021-11-13

[Robust and Accurate Object Detection via Self-Knowledge Distillation. (62%)](http://arxiv.org/abs/2111.07239)

Weipeng Xu, Pengzhi Chu, Renhao Xie, Xiongziyan Xiao, Hongcheng Huang


[UNTANGLE: Unlocking Routing and Logic Obfuscation Using Graph Neural Networks-based Link Prediction. (2%)](http://arxiv.org/abs/2111.07062)

Lilas Alrahis, Satwik Patnaik, Muhammad Abdullah Hanif, Muhammad Shafique, Ozgur Sinanoglu


## 2021-11-12

[Neural Population Geometry Reveals the Role of Stochasticity in Robust Perception. (99%)](http://arxiv.org/abs/2111.06979)

Joel Dapello, Jenelle Feather, Hang Le, Tiago Marques, David D. Cox, Josh H. McDermott, James J. DiCarlo, SueYeon Chung


[Measuring the Contribution of Multiple Model Representations in Detecting Adversarial Instances. (98%)](http://arxiv.org/abs/2111.07035)

Daniel Steinberg, Paul Munro


[Adversarially Robust Learning for Security-Constrained Optimal Power Flow. (10%)](http://arxiv.org/abs/2111.06961)

Priya L. Donti, Aayushya Agarwal, Neeraj Vijay Bedmutha, Larry Pileggi, J. Zico Kolter


[On Transferability of Prompt Tuning for Natural Language Understanding. (8%)](http://arxiv.org/abs/2111.06719)

Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai Lin, Zhiyuan Liu, Peng Li, Juanzi Li, Lei Hou, Maosong Sun, Jie Zhou


[A Bayesian Nash equilibrium-based moving target defense against stealthy sensor attacks. (1%)](http://arxiv.org/abs/2111.06682)

David Umsonst, Serkan Sarıtaş, György Dán, Henrik Sandberg


[Resilient Consensus-based Multi-agent Reinforcement Learning. (1%)](http://arxiv.org/abs/2111.06776)

Martin Figura, Yixuan Lin, Ji Liu, Vijay Gupta


## 2021-11-11

[On the Equivalence between Neural Network and Support Vector Machine. (1%)](http://arxiv.org/abs/2111.06063)

Yilan Chen, Wei Huang, Lam M. Nguyen, Tsui-Wei Weng


## 2021-11-10

[Trustworthy Medical Segmentation with Uncertainty Estimation. (93%)](http://arxiv.org/abs/2111.05978)

Giuseppina Carannante, Dimah Dera, Nidhal C. Bouaynaya, Ghulam Rasool, Hassan M. Fathallah-Shaykh


[Robust Learning via Ensemble Density Propagation in Deep Neural Networks. (2%)](http://arxiv.org/abs/2111.05953)

Giuseppina Carannante, Dimah Dera, Ghulam Rasool, Nidhal C. Bouaynaya, Lyudmila Mihaylova


## 2021-11-09

[Tightening the Approximation Error of Adversarial Risk with Auto Loss Function Search. (99%)](http://arxiv.org/abs/2111.05063)

Pengfei Xia, Ziqiang Li, Bin Li


[MixACM: Mixup-Based Robustness Transfer via Distillation of Activated Channel Maps. (99%)](http://arxiv.org/abs/2111.05073)

Muhammad Awais, Fengwei Zhou, Chuanlong Xie, Jiawei Li, Sung-Ho Bae, Zhenguo Li


[Sparse Adversarial Video Attacks with Spatial Transformations. (98%)](http://arxiv.org/abs/2111.05468)

Ronghui Mu, Wenjie Ruan, Leandro Soriano Marcolino, Qiang Ni


[A Statistical Difference Reduction Method for Escaping Backdoor Detection. (97%)](http://arxiv.org/abs/2111.05077)

Pengfei Xia, Hongjing Niu, Ziqiang Li, Bin Li


[Data Augmentation Can Improve Robustness. (73%)](http://arxiv.org/abs/2111.05328)

Sylvestre-Alvise Rebuffi, Sven Gowal, Dan A. Calian, Florian Stimberg, Olivia Wiles, Timothy Mann


[Are Transformers More Robust Than CNNs? (67%)](http://arxiv.org/abs/2111.05464)

Yutong Bai, Jieru Mei, Alan Yuille, Cihang Xie


## 2021-11-08

[Geometrically Adaptive Dictionary Attack on Face Recognition. (99%)](http://arxiv.org/abs/2111.04371)

Junyoung Byun, Hyojun Go, Changick Kim


[Defense Against Explanation Manipulation. (98%)](http://arxiv.org/abs/2111.04303)

Ruixiang Tang, Ninghao Liu, Fan Yang, Na Zou, Xia Hu


[DeepSteal: Advanced Model Extractions Leveraging Efficient Weight Stealing in Memories. (98%)](http://arxiv.org/abs/2111.04625)

Adnan Siraj Rakin, Md Hafizul Islam Chowdhuryy, Fan Yao, Deliang Fan


[On Assessing The Safety of Reinforcement Learning algorithms Using Formal Methods. (75%)](http://arxiv.org/abs/2111.04865)

Paulina Stevia Nouwou Mindom, Amin Nikanjam, Foutse Khomh, John Mullins


[Get a Model! Model Hijacking Attack Against Machine Learning Models. (69%)](http://arxiv.org/abs/2111.04394)

Ahmed Salem, Michael Backes, Yang Zhang


[Robust and Information-theoretically Safe Bias Classifier against Adversarial Attacks. (69%)](http://arxiv.org/abs/2111.04404)

Lijia Yu, Xiao-Shan Gao


[Characterizing the adversarial vulnerability of speech self-supervised learning. (68%)](http://arxiv.org/abs/2111.04330)

Haibin Wu, Bo Zheng, Xu Li, Xixin Wu, Hung-yi Lee, Helen Meng


[HAPSSA: Holistic Approach to PDF Malware Detection Using Signal and Statistical Analysis. (67%)](http://arxiv.org/abs/2111.04703)

Tajuddin Manhar Mohammed, Lakshmanan Nataraj, Satish Chikkagoudar, Shivkumar Chandrasekaran, B. S. Manjunath


[Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning. (67%)](http://arxiv.org/abs/2111.04314)

Qinkai Zheng, Xu Zou, Yuxiao Dong, Yukuo Cen, Da Yin, Jiarong Xu, Yang Yang, Jie Tang


[BARFED: Byzantine Attack-Resistant Federated Averaging Based on Outlier Elimination. (45%)](http://arxiv.org/abs/2111.04550)

Ece Isik-Polat, Gorkem Polat, Altan Kocyigit


## 2021-11-07

[Generative Dynamic Patch Attack. (99%)](http://arxiv.org/abs/2111.04266)

Xiang Li, Shihao Ji


[Natural Adversarial Objects. (81%)](http://arxiv.org/abs/2111.04204)

Felix Lau, Nishant Subramani, Sasha Harrison, Aerin Kim, Elliot Branson, Rosanne Liu


## 2021-11-06

["How Does It Detect A Malicious App?" Explaining the Predictions of AI-based Android Malware Detector. (11%)](http://arxiv.org/abs/2111.05108)

Zhi Lu, Vrizlynn L. L. Thing


## 2021-11-05

[A Unified Game-Theoretic Interpretation of Adversarial Robustness. (98%)](http://arxiv.org/abs/2111.03536)

Jie Ren, Die Zhang, Yisen Wang, Lu Chen, Zhanpeng Zhou, Yiting Chen, Xu Cheng, Xin Wang, Meng Zhou, Jie Shi, Quanshi Zhang


[Sequential Randomized Smoothing for Adversarially Robust Speech Recognition. (96%)](http://arxiv.org/abs/2112.03000)

Raphael Olivier, Bhiksha Raj


[Federated Learning Attacks Revisited: A Critical Discussion of Gaps, Assumptions, and Evaluation Setups. (2%)](http://arxiv.org/abs/2111.03363)

Aidmar Wainakh, Ephraim Zimmer, Sandeep Subedi, Jens Keim, Tim Grube, Shankar Karuppayah, Alejandro Sanchez Guinea, Max Mühlhäuser


## 2021-11-04

[Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models. (99%)](http://arxiv.org/abs/2111.02840)

Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, Bo Li


[Adversarial Attacks on Graph Classification via Bayesian Optimisation. (87%)](http://arxiv.org/abs/2111.02842)

Xingchen Wan, Henry Kenlay, Binxin Ru, Arno Blaas, Michael A. Osborne, Xiaowen Dong


[Adversarial Attacks on Knowledge Graph Embeddings via Instance Attribution Methods. (47%)](http://arxiv.org/abs/2111.03120)

Peru Bhardwaj, John Kelleher, Luca Costabello, Declan O'Sullivan


[Attacking Deep Reinforcement Learning-Based Traffic Signal Control Systems with Colluding Vehicles. (3%)](http://arxiv.org/abs/2111.02845)

Ao Qu, Yihong Tang, Wei Ma


## 2021-11-03

[LTD: Low Temperature Distillation for Robust Adversarial Training. (88%)](http://arxiv.org/abs/2111.02331)

Erh-Chung Chen, Che-Rung Lee


[Multi-Glimpse Network: A Robust and Efficient Classification Architecture based on Recurrent Downsampled Attention. (41%)](http://arxiv.org/abs/2111.02018)

Sia Huat Tan, Runpei Dong, Kaisheng Ma


## 2021-11-02

[Effective and Imperceptible Adversarial Textual Attack via Multi-objectivization. (99%)](http://arxiv.org/abs/2111.01528)

Shengcai Liu, Ning Lu, Wenjing Hong, Chao Qian, Ke Tang


[Meta-Learning the Search Distribution of Black-Box Random Search Based Adversarial Attacks. (96%)](http://arxiv.org/abs/2111.01714)

Maksym Yatsura, Jan Hendrik Metzen, Matthias Hein


[Training Certifiably Robust Neural Networks with Efficient Local Lipschitz Bounds. (70%)](http://arxiv.org/abs/2111.01395)

Yujia Huang, Huan Zhang, Yuanyuan Shi, J Zico Kolter, Anima Anandkumar


[Pareto Adversarial Robustness: Balancing Spatial Robustness and Sensitivity-based Robustness. (68%)](http://arxiv.org/abs/2111.01996)

Ke Sun, Mingjie Li, Zhouchen Lin


[Knowledge Cross-Distillation for Membership Privacy. (38%)](http://arxiv.org/abs/2111.01363)

Rishav Chourasia, Batnyam Enkhtaivan, Kunihiro Ito, Junki Mori, Isamu Teranishi, Hikaru Tsuchida


[Adversarially Perturbed Wavelet-based Morphed Face Generation. (9%)](http://arxiv.org/abs/2111.01965)

Kelsey O'Haire, Sobhan Soleymani, Baaria Chaudhary, Poorya Aghdaie, Jeremy Dawson, Nasser M. Nasrabadi


## 2021-11-01

[Graph Structural Attack by Spectral Distance. (93%)](http://arxiv.org/abs/2111.00684)

Lu Lin, Ethan Blaser, Hongning Wang


[Availability Attacks Create Shortcuts. (89%)](http://arxiv.org/abs/2111.00898)

Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, Tie-Yan Liu


[Robustness of deep learning algorithms in astronomy -- galaxy morphology studies. (83%)](http://arxiv.org/abs/2111.00961)

A. Ćiprijanović, D. Kafkes, G. N. Perdue, K. Pedro, G. Snyder, F. J. Sánchez, S. Madireddy, S. Wild, B. Nord


[When Does Contrastive Learning Preserve Adversarial Robustness from Pretraining to Finetuning? (69%)](http://arxiv.org/abs/2111.01124)

Lijie Fan, Sijia Liu, Pin-Yu Chen, Gaoyuan Zhang, Chuang Gan


[ZeBRA: Precisely Destroying Neural Networks with Zero-Data Based Repeated Bit Flip Attack. (9%)](http://arxiv.org/abs/2111.01080)

Dahoon Park, Kon-Woo Kwon, Sunghoon Im, Jaeha Kung


## 2021-10-31

[An Actor-Critic Method for Simulation-Based Optimization. (56%)](http://arxiv.org/abs/2111.00435)

Kuo Li, Qing-Shan Jia, Jiaqi Yan


## 2021-10-30

[Get Fooled for the Right Reason: Improving Adversarial Robustness through a Teacher-guided Curriculum Learning Approach. (97%)](http://arxiv.org/abs/2111.00295)

Anindya Sarkar, Anirban Sarkar, Sowrya Gali, Vineeth N Balasubramanian


[AdvCodeMix: Adversarial Attack on Code-Mixed Data. (93%)](http://arxiv.org/abs/2111.00350)

Sourya Dipta Das, Ayan Basak, Soumil Mandal, Dipankar Das


[Backdoor Pre-trained Models Can Transfer to All. (3%)](http://arxiv.org/abs/2111.00197)

Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen, Jie Shi, Chengfang Fang, Jianwei Yin, Ting Wang


[Trojan Source: Invisible Vulnerabilities. (1%)](http://arxiv.org/abs/2111.00169)

Nicholas Boucher, Ross Anderson


## 2021-10-29

[Attacking Video Recognition Models with Bullet-Screen Comments. (99%)](http://arxiv.org/abs/2110.15629)

Kai Chen, Zhipeng Wei, Jingjing Chen, Zuxuan Wu, Yu-Gang Jiang


[Adversarial Robustness with Semi-Infinite Constrained Learning. (92%)](http://arxiv.org/abs/2110.15767)

Alexander Robey, Luiz F. O. Chamon, George J. Pappas, Hamed Hassani, Alejandro Ribeiro


[{\epsilon}-weakened Robustness of Deep Neural Networks. (62%)](http://arxiv.org/abs/2110.15764)

Pei Huang, Yuting Yang, Minghao Liu, Fuqi Jia, Feifei Ma, Jian Zhang


[You are caught stealing my winning lottery ticket! Making a lottery ticket claim its ownership. (11%)](http://arxiv.org/abs/2111.00162)

Xuxi Chen, Tianlong Chen, Zhenyu Zhang, Zhangyang Wang


## 2021-10-28

[Bridge the Gap Between CV and NLP! A Gradient-based Textual Adversarial Attack Framework. (99%)](http://arxiv.org/abs/2110.15317)

Lifan Yuan, Yichi Zhang, Yangyi Chen, Wei Wei


[AEVA: Black-box Backdoor Detection Using Adversarial Extreme Value Analysis. (92%)](http://arxiv.org/abs/2110.14880)

Junfeng Guo, Ang Li, Cong Liu


[The magnitude vector of images. (1%)](http://arxiv.org/abs/2110.15188)

Michael F. Adamer, Leslie O'Bray, Brouwer Edward De, Bastian Rieck, Karsten Borgwardt


## 2021-10-27

[Towards Evaluating the Robustness of Neural Networks Learned by Transduction. (98%)](http://arxiv.org/abs/2110.14735)

Jiefeng Chen, Xi Wu, Yang Guo, Yingyu Liang, Somesh Jha


[CAP: Co-Adversarial Perturbation on Weights and Features for Improving Generalization of Graph Neural Networks. (98%)](http://arxiv.org/abs/2110.14855)

Haotian Xue, Kaixiong Zhou, Tianlong Chen, Kai Guo, Xia Hu, Yi Chang, Xin Wang


[Towards Robust Reasoning over Knowledge Graphs. (83%)](http://arxiv.org/abs/2110.14693)

Zhaohan Xi, Ren Pang, Changjiang Li, Shouling Ji, Xiapu Luo, Xusheng Xiao, Ting Wang


[Binarized ResNet: Enabling Robust Automatic Modulation Classification at the resource-constrained Edge. (80%)](http://arxiv.org/abs/2110.14357)

Deepsayan Sadhukhan, Nitin Priyadarshini Shankar, Nancy Nayak, Thulasi Tholeti, Sheetal Kalyani


[Generalized Depthwise-Separable Convolutions for Adversarially Robust and Efficient Neural Networks. (74%)](http://arxiv.org/abs/2110.14871)

Hassan Dbouk, Naresh R. Shanbhag


[Adversarial Neuron Pruning Purifies Backdoored Deep Models. (15%)](http://arxiv.org/abs/2110.14430)

Dongxian Wu, Yisen Wang


[From Intrinsic to Counterfactual: On the Explainability of Contextualized Recommender Systems. (5%)](http://arxiv.org/abs/2110.14844)

Yao Zhou, Haonan Wang, Jingrui He, Haixun Wang


[Robust Contrastive Learning Using Negative Samples with Diminished Semantics. (1%)](http://arxiv.org/abs/2110.14189)

Songwei Ge, Shlok Mishra, Haohan Wang, Chun-Liang Li, David Jacobs


[RoMA: Robust Model Adaptation for Offline Model-based Optimization. (1%)](http://arxiv.org/abs/2110.14188)

Sihyun Yu, Sungsoo Ahn, Le Song, Jinwoo Shin


## 2021-10-26

[Can't Fool Me: Adversarially Robust Transformer for Video Understanding. (99%)](http://arxiv.org/abs/2110.13950)

Divya Choudhary, Palash Goyal, Saurabh Sahu


[Frequency Centric Defense Mechanisms against Adversarial Examples. (99%)](http://arxiv.org/abs/2110.13935)

Sanket B. Shah, Param Raval, Harin Khakhi, Mehul S. Raval


[ScaleCert: Scalable Certified Defense against Adversarial Patches with Sparse Superficial Layers. (99%)](http://arxiv.org/abs/2110.14120)

Husheng Han, Kaidi Xu, Xing Hu, Xiaobing Chen, Ling Liang, Zidong Du, Qi Guo, Yanzhi Wang, Yunji Chen


[Drawing Robust Scratch Tickets: Subnetworks with Inborn Robustness Are Found within Randomly Initialized Networks. (99%)](http://arxiv.org/abs/2110.14068)

Yonggan Fu, Qixuan Yu, Yang Zhang, Shang Wu, Xu Ouyang, David Cox, Yingyan Lin


[FL-WBC: Enhancing Robustness against Model Poisoning Attacks in Federated Learning from a Client Perspective. (98%)](http://arxiv.org/abs/2110.13864)

Jingwei Sun, Ang Li, Louis DiValentin, Amin Hassanzadeh, Yiran Chen, Hai Li


[A Frequency Perspective of Adversarial Robustness. (98%)](http://arxiv.org/abs/2111.00861)

Shishira R Maiya, Max Ehrlich, Vatsal Agarwal, Ser-Nam Lim, Tom Goldstein, Abhinav Shrivastava


[Disrupting Deep Uncertainty Estimation Without Harming Accuracy. (86%)](http://arxiv.org/abs/2110.13741)

Ido Galil, Ran El-Yaniv


[Improving Local Effectiveness for Global robust training. (83%)](http://arxiv.org/abs/2110.14030)

Jingyue Lu, M. Pawan Kumar


[Robustness of Graph Neural Networks at Scale. (76%)](http://arxiv.org/abs/2110.14038)

Simon Geisler, Tobias Schmidt, Hakan Şirin, Daniel Zügner, Aleksandar Bojchevski, Stephan Günnemann


[Adversarial Attacks and Defenses for Social Network Text Processing Applications: Techniques, Challenges and Future Research Directions. (75%)](http://arxiv.org/abs/2110.13980)

Izzat Alsmadi, Kashif Ahmad, Mahmoud Nazzal, Firoj Alam, Ala Al-Fuqaha, Abdallah Khreishah, Abdulelah Algosaibi


[Adversarial Robustness in Multi-Task Learning: Promises and Illusions. (64%)](http://arxiv.org/abs/2110.15053)

Salah Ghamizi, Maxime Cordy, Mike Papadakis, Yves Le Traon


[AugMax: Adversarial Composition of Random Augmentations for Robust Training. (56%)](http://arxiv.org/abs/2110.13771)

Haotao Wang, Chaowei Xiao, Jean Kossaifi, Zhiding Yu, Anima Anandkumar, Zhangyang Wang


[Qu-ANTI-zation: Exploiting Quantization Artifacts for Achieving Adversarial Outcomes. (50%)](http://arxiv.org/abs/2110.13541)

Sanghyun Hong, Michael-Andrei Panaitescu-Liess, Yiğitcan Kaya, Tudor Dumitraş


[Semantic Host-free Trojan Attack. (10%)](http://arxiv.org/abs/2110.13414)

Haripriya Harikumar, Kien Do, Santu Rana, Sunil Gupta, Svetha Venkatesh


[CAFE: Catastrophic Data Leakage in Vertical Federated Learning. (3%)](http://arxiv.org/abs/2110.15122)

Xiao Jin, Pin-Yu Chen, Chia-Yi Hsu, Chia-Mu Yu, Tianyi Chen


[MEST: Accurate and Fast Memory-Economic Sparse Training Framework on the Edge. (1%)](http://arxiv.org/abs/2110.14032)

Geng Yuan, Xiaolong Ma, Wei Niu, Zhengang Li, Zhenglun Kong, Ning Liu, Yifan Gong, Zheng Zhan, Chaoyang He, Qing Jin, Siyue Wang, Minghai Qin, Bin Ren, Yanzhi Wang, Sijia Liu, Xue Lin


[Reliable and Trustworthy Machine Learning for Health Using Dataset Shift Detection. (1%)](http://arxiv.org/abs/2110.14019)

Chunjong Park, Anas Awadalla, Tadayoshi Kohno, Shwetak Patel


[Defensive Tensorization. (1%)](http://arxiv.org/abs/2110.13859)

Adrian Bulat, Jean Kossaifi, Sourav Bhattacharya, Yannis Panagakis, Timothy Hospedales, Georgios Tzimiropoulos, Nicholas D Lane, Maja Pantic


[Task-Aware Meta Learning-based Siamese Neural Network for Classifying Obfuscated Malware. (1%)](http://arxiv.org/abs/2110.13409)

Jinting Zhu, Julian Jang-Jaccard, Amardeep Singh, Paul A. Watters, Seyit Camtepe


## 2021-10-25

[Stable Neural ODE with Lyapunov-Stable Equilibrium Points for Defending Against Adversarial Attacks. (99%)](http://arxiv.org/abs/2110.12976)

Qiyu Kang, Yang Song, Qinxu Ding, Wee Peng Tay


[Generating Watermarked Adversarial Texts. (99%)](http://arxiv.org/abs/2110.12948)

Mingjie Li, Hanzhou Wu, Xinpeng Zhang


[Beyond $L_p$ clipping: Equalization-based Psychoacoustic Attacks against ASRs. (92%)](http://arxiv.org/abs/2110.13250)

Hadi Abdullah, Muhammad Sajidur Rahman, Christian Peeters, Cassidy Gibson, Washington Garcia, Vincent Bindschaedler, Thomas Shrimpton, Patrick Traynor


[Fast Gradient Non-sign Methods. (92%)](http://arxiv.org/abs/2110.12734)

Yaya Cheng, Jingkuan Song, Xiaosu Zhu, Qilong Zhang, Lianli Gao, Heng Tao Shen


[Ensemble Federated Adversarial Training with Non-IID data. (87%)](http://arxiv.org/abs/2110.14814)

Shuang Luo, Didi Zhu, Zexi Li, Chao Wu


[GANash -- A GAN approach to steganography. (81%)](http://arxiv.org/abs/2110.13650)

Venkatesh Subramaniyan, Vignesh Sivakumar, A. K. Vagheesan, S. Sakthivelan, K. J. Jegadish Kumar, K. K. Nagarajan


[A Dynamical System Perspective for Lipschitz Neural Networks. (81%)](http://arxiv.org/abs/2110.12690)

Laurent Meunier, Blaise Delattre, Alexandre Araujo, Alexandre Allauzen


[An Adaptive Structural Learning of Deep Belief Network for Image-based Crack Detection in Concrete Structures Using SDNET2018. (13%)](http://arxiv.org/abs/2110.12700)

Shin Kamada, Takumi Ichimura, Takashi Iwasaki


## 2021-10-24

[Towards A Conceptually Simple Defensive Approach for Few-shot classifiers Against Adversarial Support Samples. (80%)](http://arxiv.org/abs/2110.12357)

Yi Xiang Marcus Tan, Penny Chong, Jiamei Sun, Ngai-man Cheung, Yuval Elovici, Alexander Binder


## 2021-10-23

[ADC: Adversarial attacks against object Detection that evade Context consistency checks. (99%)](http://arxiv.org/abs/2110.12321)

Mingjun Yin, Shasha Li, Chengyu Song, M. Salman Asif, Amit K. Roy-Chowdhury, Srikanth V. Krishnamurthy


[A Layer-wise Adversarial-aware Quantization Optimization for Improving Robustness. (81%)](http://arxiv.org/abs/2110.12308)

Chang Song, Riya Ranjan, Hai Li


## 2021-10-22

[Improving Robustness of Malware Classifiers using Adversarial Strings Generated from Perturbed Latent Representations. (99%)](http://arxiv.org/abs/2110.11987)

Marek Galovic, Branislav Bosansky, Viliam Lisy


[How and When Adversarial Robustness Transfers in Knowledge Distillation? (91%)](http://arxiv.org/abs/2110.12072)

Rulin Shao, Jinfeng Yi, Pin-Yu Chen, Cho-Jui Hsieh


[Fairness Degrading Adversarial Attacks Against Clustering Algorithms. (86%)](http://arxiv.org/abs/2110.12020)

Anshuman Chhabra, Adish Singla, Prasant Mohapatra


[Adversarial robustness for latent models: Revisiting the robust-standard accuracies tradeoff. (80%)](http://arxiv.org/abs/2110.11950)

Adel Javanmard, Mohammad Mehrabi


[PRECAD: Privacy-Preserving and Robust Federated Learning via Crypto-Aided Differential Privacy. (15%)](http://arxiv.org/abs/2110.11578)

Xiaolan Gu, Ming Li, Li Xiong


[ProtoShotXAI: Using Prototypical Few-Shot Architecture for Explainable AI. (15%)](http://arxiv.org/abs/2110.11597)

Samuel Hess, Gregory Ditzler


[Spoofing Detection on Hand Images Using Quality Assessment. (1%)](http://arxiv.org/abs/2110.12923)

Asish Bera, Ratnadeep Dey, Debotosh Bhattacharjee, Mita Nasipuri, Hubert P. H. Shum


[Text Counterfactuals via Latent Optimization and Shapley-Guided Search. (1%)](http://arxiv.org/abs/2110.11589)

Quintin Pope, Xiaoli Z. Fern


[On the Necessity of Auditable Algorithmic Definitions for Machine Unlearning. (1%)](http://arxiv.org/abs/2110.11891)

Anvith Thudi, Hengrui Jia, Ilia Shumailov, Nicolas Papernot


[MANDERA: Malicious Node Detection in Federated Learning via Ranking. (1%)](http://arxiv.org/abs/2110.11736)

Wanchuang Zhu, Benjamin Zi Hao Zhao, Simon Luo, Tongliang Liu, Ke Deng


## 2021-10-21

[CAPTIVE: Constrained Adversarial Perturbations to Thwart IC Reverse Engineering. (98%)](http://arxiv.org/abs/2110.11459)

Amir Hosein Afandizadeh Zargari, Marzieh AshrafiAmiri, Minjun Seo, Sai Manoj Pudukotai Dinakarrao, Mohammed E. Fouda, Fadi Kurdahi


[PROVES: Establishing Image Provenance using Semantic Signatures. (93%)](http://arxiv.org/abs/2110.11411)

Mingyang Xie, Manav Kulshrestha, Shaojie Wang, Jinghan Yang, Ayan Chakrabarti, Ning Zhang, Yevgeniy Vorobeychik


[RoMA: a Method for Neural Network Robustness Measurement and Assessment. (92%)](http://arxiv.org/abs/2110.11088)

Natan Levy, Guy Katz


[Anti-Backdoor Learning: Training Clean Models on Poisoned Data. (83%)](http://arxiv.org/abs/2110.11571)

Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, Xingjun Ma


[PipAttack: Poisoning Federated Recommender Systems forManipulating Item Promotion. (68%)](http://arxiv.org/abs/2110.10926)

Shijie Zhang, Hongzhi Yin, Tong Chen, Zi Huang, Quoc Viet Hung Nguyen, Lizhen Cui


[Robustness through Data Augmentation Loss Consistency. (61%)](http://arxiv.org/abs/2110.11205)

Tianjian Huang, Shaunak Halbe, Chinnadhurai Sankar, Pooyan Amini, Satwik Kottur, Alborz Geramifard, Meisam Razaviyayn, Ahmad Beirami


[Generalization of Neural Combinatorial Solvers Through the Lens of Adversarial Robustness. (61%)](http://arxiv.org/abs/2110.10942)

Simon Geisler, Johanna Sommer, Jan Schuchardt, Aleksandar Bojchevski, Stephan Günnemann


[Watermarking Graph Neural Networks based on Backdoor Attacks. (31%)](http://arxiv.org/abs/2110.11024)

Jing Xu, Stjepan Picek


[Physical Side-Channel Attacks on Embedded Neural Networks: A Survey. (8%)](http://arxiv.org/abs/2110.11290)

Maria Méndez Real, Rubén Salvador


## 2021-10-20

[Adversarial Socialbot Learning via Multi-Agent Deep Hierarchical Reinforcement Learning. (83%)](http://arxiv.org/abs/2110.10655)

Thai Le, Long Tran-Thanh, Dongwon Lee


[Surrogate Representation Learning with Isometric Mapping for Gray-box Graph Adversarial Attacks. (62%)](http://arxiv.org/abs/2110.10482)

Zihan Liul, Yun Luo, Zelin Zang, Stan Z. Li


[Moir\'e Attack (MA): A New Potential Risk of Screen Photos. (56%)](http://arxiv.org/abs/2110.10444)

Dantong Niu, Ruohao Guo, Yisen Wang


[Adversarial attacks against Bayesian forecasting dynamic models. (13%)](http://arxiv.org/abs/2110.10783)

Roi Naveiro


[No One Representation to Rule Them All: Overlapping Features of Training Methods. (1%)](http://arxiv.org/abs/2110.12899)

Raphael Gontijo-Lopes, Yann Dauphin, Ekin D. Cubuk


## 2021-10-19

[Multi-concept adversarial attacks. (99%)](http://arxiv.org/abs/2110.10287)

Vibha Belavadi, Yan Zhou, Murat Kantarcioglu, Bhavani M. Thuraisingham


[A Regularization Method to Improve Adversarial Robustness of Neural Networks for ECG Signal Classification. (96%)](http://arxiv.org/abs/2110.09759)

Linhai Ma, Liang Liang


[TESSERACT: Gradient Flip Score to Secure Federated Learning Against Model Poisoning Attacks. (69%)](http://arxiv.org/abs/2110.10108)

Atul Sharma, Wei Chen, Joshua Zhao, Qiang Qiu, Somali Chaterji, Saurabh Bagchi


[Understanding Convolutional Neural Networks from Theoretical Perspective via Volterra Convolution. (61%)](http://arxiv.org/abs/2110.09902)

Tenghui Li, Guoxu Zhou, Yuning Qiu, Qibin Zhao


[Detecting Backdoor Attacks Against Point Cloud Classifiers. (26%)](http://arxiv.org/abs/2110.10354)

Zhen Xiang, David J. Miller, Siheng Chen, Xi Li, George Kesidis


[Speech Pattern based Black-box Model Watermarking for Automatic Speech Recognition. (13%)](http://arxiv.org/abs/2110.09814)

Haozhe Chen, Weiming Zhang, Kunlin Liu, Kejiang Chen, Han Fang, Nenghai Yu


[A Deeper Look into RowHammer`s Sensitivities: Experimental Analysis of Real DRAM Chips and Implications on Future Attacks and Defenses. (5%)](http://arxiv.org/abs/2110.10291)

Lois Orosa, Abdullah Giray Yağlıkçı, Haocong Luo, Ataberk Olgun, Jisung Park, Hasan Hassan, Minesh Patel, Jeremie S. Kim, Onur Mutlu


## 2021-10-18

[Boosting the Transferability of Video Adversarial Examples via Temporal Translation. (99%)](http://arxiv.org/abs/2110.09075)

Zhipeng Wei, Jingjing Chen, Zuxuan Wu, Yu-Gang Jiang


[Black-box Adversarial Attacks on Commercial Speech Platforms with Minimal Information. (99%)](http://arxiv.org/abs/2110.09714)

Baolin Zheng, Peipei Jiang, Qian Wang, Qi Li, Chao Shen, Cong Wang, Yunjie Ge, Qingyang Teng, Shenyi Zhang


[Improving Robustness using Generated Data. (97%)](http://arxiv.org/abs/2110.09468)

Sven Gowal, Sylvestre-Alvise Rebuffi, Olivia Wiles, Florian Stimberg, Dan Andrei Calian, Timothy Mann


[MEMO: Test Time Robustness via Adaptation and Augmentation. (13%)](http://arxiv.org/abs/2110.09506)

Marvin Zhang, Sergey Levine, Chelsea Finn


[Minimal Multi-Layer Modifications of Deep Neural Networks. (4%)](http://arxiv.org/abs/2110.09929)

Idan Refaeli, Guy Katz


## 2021-10-17

[Unrestricted Adversarial Attacks on ImageNet Competition. (99%)](http://arxiv.org/abs/2110.09903)

Yuefeng Chen, Xiaofeng Mao, Yuan He, Hui Xue, Chao Li, Yinpeng Dong, Qi-An Fu, Xiao Yang, Wenzhao Xiang, Tianyu Pang, Hang Su, Jun Zhu, Fangcheng Liu, Chao Zhang, Hongyang Zhang, Yichi Zhang, Shilong Liu, Chang Liu, Wenzhao Xiang, Yajie Wang, Huipeng Zhou, Haoran Lyu, Yidan Xu, Zixuan Xu, Taoyu Zhu, Wenjun Li, Xianfeng Gao, Guoqiu Wang, Huanqian Yan, Ying Guo, Chaoning Zhang, Zheng Fang, Yang Wang, Bingyang Fu, Yunfei Zheng, Yekui Wang, Haorong Luo, Zhen Yang


[Improving Robustness of Reinforcement Learning for Power System Control with Adversarial Training. (99%)](http://arxiv.org/abs/2110.08956)

Alexander Daniel Pan, Daniel Yongkyun, Lee, Huan Zhang, Yize Chen, Yuanyuan Shi


[ECG-ATK-GAN: Robustness against Adversarial Attacks on ECGs using Conditional Generative Adversarial Networks. (99%)](http://arxiv.org/abs/2110.09983)

Khondker Fariha Hossain, Sharif Amit Kamran, Alireza Tavakkoli, Xingjun Ma


[Adapting Membership Inference Attacks to GNN for Graph Classification: Approaches and Implications. (22%)](http://arxiv.org/abs/2110.08760)

Bang Wu, Xiangwen Yang, Shirui Pan, Xingliang Yuan


[Poisoning Attacks on Fair Machine Learning. (12%)](http://arxiv.org/abs/2110.08932)

Minh-Hao Van, Wei Du, Xintao Wu, Aidong Lu


## 2021-10-16

[Black-box Adversarial Attacks on Network-wide Multi-step Traffic State Prediction Models. (99%)](http://arxiv.org/abs/2110.08712)

Bibek Poudel, Weizi Li


[Analyzing Dynamic Adversarial Training Data in the Limit. (82%)](http://arxiv.org/abs/2110.08514)

Eric Wallace, Adina Williams, Robin Jia, Douwe Kiela


[Characterizing Improper Input Validation Vulnerabilities of Mobile Crowdsourcing Services. (5%)](http://arxiv.org/abs/2110.08517)

Sojhal Ismail Khan, Dominika Woszczyk, Chengzeng You, Soteris Demetriou, Muhammad Naveed


[Tackling the Imbalance for GNNs. (4%)](http://arxiv.org/abs/2110.08690)

Rui Wang, Weixuan Xiong, Qinghu Hou, Ou Wu


## 2021-10-15

[Adversarial Attacks on Gaussian Process Bandits. (99%)](http://arxiv.org/abs/2110.08449)

Eric Han, Jonathan Scarlett


[Generating Natural Language Adversarial Examples through An Improved Beam Search Algorithm. (99%)](http://arxiv.org/abs/2110.08036)

Tengfei Zhao, Zhaocheng Ge, Hanping Hu, Dingmeng Shi


[Adversarial Attacks on ML Defense Models Competition. (99%)](http://arxiv.org/abs/2110.08042)

Yinpeng Dong, Qi-An Fu, Xiao Yang, Wenzhao Xiang, Tianyu Pang, Hang Su, Jun Zhu, Jiayu Tang, Yuefeng Chen, XiaoFeng Mao, Yuan He, Hui Xue, Chao Li, Ye Liu, Qilong Zhang, Lianli Gao, Yunrui Yu, Xitong Gao, Zhe Zhao, Daquan Lin, Jiadong Lin, Chuanbiao Song, Zihao Wang, Zhennan Wu, Yang Guo, Jiequan Cui, Xiaogang Xu, Pengguang Chen


[Mitigating Membership Inference Attacks by Self-Distillation Through a Novel Ensemble Architecture. (76%)](http://arxiv.org/abs/2110.08324)

Xinyu Tang, Saeed Mahloujifar, Liwei Song, Virat Shejwalkar, Milad Nasr, Amir Houmansadr, Prateek Mittal


[Robustness of different loss functions and their impact on networks learning capability. (76%)](http://arxiv.org/abs/2110.08322)

Vishal Rajput


[Chunked-Cache: On-Demand and Scalable Cache Isolation for Security Architectures. (22%)](http://arxiv.org/abs/2110.08139)

Ghada Dessouky, Alexander Gruler, Pouya Mahmoody, Ahmad-Reza Sadeghi, Emmanuel Stapf


[Textual Backdoor Attacks Can Be More Harmful via Two Simple Tricks. (10%)](http://arxiv.org/abs/2110.08247)

Yangyi Chen, Fanchao Qi, Zhiyuan Liu, Maosong Sun


[Understanding and Improving Robustness of Vision Transformers through Patch-based Negative Augmentation. (8%)](http://arxiv.org/abs/2110.07858)

Yao Qin, Chiyuan Zhang, Ting Chen, Balaji Lakshminarayanan, Alex Beutel, Xuezhi Wang


[Hand Me Your PIN! Inferring ATM PINs of Users Typing with a Covered Hand. (1%)](http://arxiv.org/abs/2110.08113)

Matteo Cardaioli, Stefano Cecconello, Mauro Conti, Simone Milani, Stjepan Picek, Eugen Saraci


## 2021-10-14

[Adversarial examples by perturbing high-level features in intermediate decoder layers. (99%)](http://arxiv.org/abs/2110.07182)

Vojtěch Čermák, Lukáš Adam


[DI-AA: An Interpretable White-box Attack for Fooling Deep Neural Networks. (99%)](http://arxiv.org/abs/2110.07305)

Yixiang Wang, Jiqiang Liu, Xiaolin Chang, Jianhua Wang, Ricardo J. Rodríguez


[Adversarial Purification through Representation Disentanglement. (99%)](http://arxiv.org/abs/2110.07801)

Tao Bai, Jun Zhao, Lanqing Guo, Bihan Wen


[RAP: Robustness-Aware Perturbations for Defending against Backdoor Attacks on NLP Models. (93%)](http://arxiv.org/abs/2110.07831)

Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, Xu Sun


[An Optimization Perspective on Realizing Backdoor Injection Attacks on Deep Neural Networks in Hardware. (87%)](http://arxiv.org/abs/2110.07683)

M. Caner Tol, Saad Islam, Berk Sunar, Ziming Zhang


[Interactive Analysis of CNN Robustness. (80%)](http://arxiv.org/abs/2110.07667)

Stefan Sietzen, Mathias Lechner, Judy Borowski, Ramin Hasani, Manuela Waldner


[On Adversarial Vulnerability of PHM algorithms: An Initial Study. (69%)](http://arxiv.org/abs/2110.07462)

Weizhong Yan, Zhaoyuan Yang, Jianwei Qiu


[Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models. (61%)](http://arxiv.org/abs/2110.07736)

Tianlu Wang, Diyi Yang, Xuezhi Wang


[Toward Degradation-Robust Voice Conversion. (9%)](http://arxiv.org/abs/2110.07537)

Chien-yu Huang, Kai-Wei Chang, Hung-yi Lee


[Interpreting the Robustness of Neural NLP Models to Textual Perturbations. (9%)](http://arxiv.org/abs/2110.07159)

Yunxiang Zhang, Liangming Pan, Samson Tan, Min-Yen Kan


[Retrieval-guided Counterfactual Generation for QA. (2%)](http://arxiv.org/abs/2110.07596)

Bhargavi Paranjape, Matthew Lamm, Ian Tenney


[Effective Certification of Monotone Deep Equilibrium Models. (1%)](http://arxiv.org/abs/2110.08260)

Mark Niklas Müller, Robin Staab, Marc Fischer, Martin Vechev


## 2021-10-13

[A Framework for Verification of Wasserstein Adversarial Robustness. (99%)](http://arxiv.org/abs/2110.06816)

Tobias Wegel, Felix Assion, David Mickisch, Florens Greßner


[Identification of Attack-Specific Signatures in Adversarial Examples. (99%)](http://arxiv.org/abs/2110.06802)

Hossein Souri, Pirazh Khorramshahi, Chun Pong Lau, Micah Goldblum, Rama Chellappa


[Model-Agnostic Meta-Attack: Towards Reliable Evaluation of Adversarial Robustness. (99%)](http://arxiv.org/abs/2110.08256)

Xiao Yang, Yinpeng Dong, Wenzhao Xiang, Tianyu Pang, Hang Su, Jun Zhu


[Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer. (98%)](http://arxiv.org/abs/2110.07139)

Fanchao Qi, Yangyi Chen, Xurui Zhang, Mukai Li, Zhiyuan Liu, Maosong Sun


[Brittle interpretations: The Vulnerability of TCAV and Other Concept-based Explainability Tools to Adversarial Attack. (93%)](http://arxiv.org/abs/2110.07120)

Davis Brown, Henry Kvinge


[Poison Forensics: Traceback of Data Poisoning Attacks in Neural Networks. (92%)](http://arxiv.org/abs/2110.06904)

Shawn Shan, Arjun Nitin Bhagoji, Haitao Zheng, Ben Y. Zhao


[Boosting the Certified Robustness of L-infinity Distance Nets. (1%)](http://arxiv.org/abs/2110.06850)

Bohang Zhang, Du Jiang, Di He, Liwei Wang


[Benchmarking the Robustness of Spatial-Temporal Models Against Corruptions. (1%)](http://arxiv.org/abs/2110.06513)

Chenyu Yi, Siyuan Yang, Haoliang Li, Yap-peng Tan, Alex Kot


## 2021-10-12

[Adversarial Attack across Datasets. (99%)](http://arxiv.org/abs/2110.07718)

Yunxiao Qin, Yuanhao Xiong, Jinfeng Yi, Cho-Jui Hsieh


[Graph-Fraudster: Adversarial Attacks on Graph Neural Network Based Vertical Federated Learning. (99%)](http://arxiv.org/abs/2110.06468)

Jinyin Chen, Guohan Huang, Haibin Zheng, Shanqing Yu, Wenrong Jiang, Chen Cui


[SEPP: Similarity Estimation of Predicted Probabilities for Defending and Detecting Adversarial Text. (92%)](http://arxiv.org/abs/2110.05748)

Hoang-Quoc Nguyen-Son, Seira Hidano, Kazuhide Fukushima, Shinsaku Kiyomoto


[On the Security Risks of AutoML. (45%)](http://arxiv.org/abs/2110.06018)

Ren Pang, Zhaohan Xi, Shouling Ji, Xiapu Luo, Ting Wang


[Zero-bias Deep Neural Network for Quickest RF Signal Surveillance. (1%)](http://arxiv.org/abs/2110.05797)

Yongxin Liu, Yingjie Chen, Jian Wang, Shuteng Niu, Dahai Liu, Houbing Song


## 2021-10-11

[Boosting Fast Adversarial Training with Learnable Adversarial Initialization. (99%)](http://arxiv.org/abs/2110.05007)

Xiaojun Jia, Yong Zhang, Baoyuan Wu, Jue Wang, Xiaochun Cao


[Parameterizing Activation Functions for Adversarial Robustness. (98%)](http://arxiv.org/abs/2110.05626)

Sihui Dai, Saeed Mahloujifar, Prateek Mittal


[Amicable examples for informed source separation. (86%)](http://arxiv.org/abs/2110.05059)

Naoya Takahashi, Yuki Mitsufuji


[Doubly-Trained Adversarial Data Augmentation for Neural Machine Translation. (12%)](http://arxiv.org/abs/2110.05691)

Weiting Tan, Shuoyang Ding, Huda Khayrallah, Philipp Koehn


[Intriguing Properties of Input-dependent Randomized Smoothing. (1%)](http://arxiv.org/abs/2110.05365)

Peter Súkeník, Aleksei Kuvshinov, Stephan Günnemann


[Hiding Images into Images with Real-world Robustness. (1%)](http://arxiv.org/abs/2110.05689)

Qichao Ying, Hang Zhou, Xianhan Zeng, Haisheng Xu, Zhenxing Qian, Xinpeng Zhang


[Source Mixing and Separation Robust Audio Steganography. (1%)](http://arxiv.org/abs/2110.05054)

Naoya Takahashi, Mayank Kumar Singh, Yuki Mitsufuji


[Homogeneous Learning: Self-Attention Decentralized Deep Learning. (1%)](http://arxiv.org/abs/2110.05290)

Yuwei Sun, Hideya Ochiai


[Large Language Models Can Be Strong Differentially Private Learners. (1%)](http://arxiv.org/abs/2110.05679)

Xuechen Li, Florian Tramèr, Percy Liang, Tatsunori Hashimoto


[A Closer Look at Prototype Classifier for Few-shot Image Classification. (1%)](http://arxiv.org/abs/2110.05076)

Mingcheng Hou, Issei Sato


[Certified Patch Robustness via Smoothed Vision Transformers. (1%)](http://arxiv.org/abs/2110.07719)

Hadi Salman, Saachi Jain, Eric Wong, Aleksander Mądry


## 2021-10-10

[Adversarial Attacks in a Multi-view Setting: An Empirical Study of the Adversarial Patches Inter-view Transferability. (98%)](http://arxiv.org/abs/2110.04887)

Bilel Tarchoun, Ihsen Alouani, Anouar Ben Khalifa, Mohamed Ali Mahjoub


[Universal Adversarial Attacks on Neural Networks for Power Allocation in a Massive MIMO System. (92%)](http://arxiv.org/abs/2110.04731)

Pablo Millán Santos, B. R. Manoj, Meysam Sadeghi, Erik G. Larsson


## 2021-10-09

[Demystifying the Transferability of Adversarial Attacks in Computer Networks. (99%)](http://arxiv.org/abs/2110.04488)

Ehsan Nowroozi, Yassine Mekdad, Mohammad Hajian Berenjestanaki, Mauro Conti, Abdeslam EL Fergougui


[Provably Efficient Black-Box Action Poisoning Attacks Against Reinforcement Learning. (93%)](http://arxiv.org/abs/2110.04471)

Guanlin Liu, Lifeng Lai


[Widen The Backdoor To Let More Attackers In. (13%)](http://arxiv.org/abs/2110.04571)

Siddhartha Datta, Giulio Lovisotto, Ivan Martinovic, Nigel Shadbolt


## 2021-10-08

[Explainability-Aware One Point Attack for Point Cloud Neural Networks. (99%)](http://arxiv.org/abs/2110.04158)

Hanxiao Tan, Helena Kotthaus


[Game Theory for Adversarial Attacks and Defenses. (98%)](http://arxiv.org/abs/2110.06166)

Shorya Sharma


[Graphs as Tools to Improve Deep Learning Methods. (10%)](http://arxiv.org/abs/2110.03999)

Carlos Lassance, Myriam Bontonou, Mounia Hamidouche, Bastien Pasdeloup, Lucas Drumetz, Vincent Gripon


[IHOP: Improved Statistical Query Recovery against Searchable Symmetric Encryption through Quadratic Optimization. (3%)](http://arxiv.org/abs/2110.04180)

Simon Oya, Florian Kerschbaum


[A Wireless Intrusion Detection System for 802.11 WPA3 Networks. (1%)](http://arxiv.org/abs/2110.04259)

Neil Dalal, Nadeem Akhtar, Anubhav Gupta, Nikhil Karamchandani, Gaurav S. Kasbekar, Jatin Parekh


[Salient ImageNet: How to discover spurious features in Deep Learning? (1%)](http://arxiv.org/abs/2110.04301)

Sahil Singla, Soheil Feizi


## 2021-10-07

[Robust Feature-Level Adversaries are Interpretability Tools. (99%)](http://arxiv.org/abs/2110.03605)

Stephen Casper, Max Nadeau, Dylan Hadfield-Menell, Gabriel Kreiman


[EvadeDroid: A Practical Evasion Attack on Machine Learning for Black-box Android Malware Detection. (99%)](http://arxiv.org/abs/2110.03301)

Hamid Bostani, Veelasha Moonsamy


[Adversarial Attack by Limited Point Cloud Surface Modifications. (98%)](http://arxiv.org/abs/2110.03745)

Atrin Arya, Hanieh Naderi, Shohreh Kasaei


[Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks. (98%)](http://arxiv.org/abs/2110.03825)

Hanxun Huang, Yisen Wang, Sarah Monazam Erfani, Quanquan Gu, James Bailey, Xingjun Ma


[Dyn-Backdoor: Backdoor Attack on Dynamic Link Prediction. (80%)](http://arxiv.org/abs/2110.03875)

Jinyin Chen, Haiyang Xiong, Haibin Zheng, Jian Zhang, Guodong Jiang, Yi Liu


[Fingerprinting Multi-exit Deep Neural Network Models via Inference Time. (62%)](http://arxiv.org/abs/2110.03175)

Tian Dong, Han Qiu, Tianwei Zhang, Jiwei Li, Hewu Li, Jialiang Lu


[Adversarial Unlearning of Backdoors via Implicit Hypergradient. (56%)](http://arxiv.org/abs/2110.03735)

Yi Zeng, Si Chen, Won Park, Z. Morley Mao, Ming Jin, Ruoxi Jia


[MPSN: Motion-aware Pseudo Siamese Network for Indoor Video Head Detection in Buildings. (1%)](http://arxiv.org/abs/2110.03302)

Kailai Sun, Xiaoteng Ma, Peng Liu, Qianchuan Zhao


## 2021-10-06

[HIRE-SNN: Harnessing the Inherent Robustness of Energy-Efficient Deep Spiking Neural Networks by Training with Crafted Input Noise. (99%)](http://arxiv.org/abs/2110.11417)

Souvik Kundu, Massoud Pedram, Peter A. Beerel


[Reversible adversarial examples against local visual perturbation. (99%)](http://arxiv.org/abs/2110.02700)

Zhaoxia Yin, Li Chen, Shaowei Zhu


[Attack as the Best Defense: Nullifying Image-to-image Translation GANs via Limit-aware Adversarial Attack. (99%)](http://arxiv.org/abs/2110.02516)

Chin-Yuan Yeh, Hsi-Wen Chen, Hong-Han Shuai, De-Nian Yang, Ming-Syan Chen


[Adversarial Robustness Comparison of Vision Transformer and MLP-Mixer to CNNs. (99%)](http://arxiv.org/abs/2110.02797)

Philipp Benz, Soomin Ham, Chaoning Zhang, Adil Karjauv, In So Kweon


[Adversarial Attacks on Machinery Fault Diagnosis. (99%)](http://arxiv.org/abs/2110.02498)

Jiahao Chen, Diqun Yan


[Adversarial Attacks on Spiking Convolutional Networks for Event-based Vision. (98%)](http://arxiv.org/abs/2110.02929)

Julian Büchel, Gregor Lenz, Yalun Hu, Sadique Sheik, Martino Sorbaro


[A Uniform Framework for Anomaly Detection in Deep Neural Networks. (97%)](http://arxiv.org/abs/2110.03092)

Fangzhen Zhao, Chenyi Zhang, Naipeng Dong, Zefeng You, Zhenxin Wu


[Double Descent in Adversarial Training: An Implicit Label Noise Perspective. (88%)](http://arxiv.org/abs/2110.03135)

Chengyu Dong, Liyuan Liu, Jingbo Shang


[Improving Adversarial Robustness for Free with Snapshot Ensemble. (83%)](http://arxiv.org/abs/2110.03124)

Yihao Wang


[DoubleStar: Long-Range Attack Towards Depth Estimation based Obstacle Avoidance in Autonomous Systems. (45%)](http://arxiv.org/abs/2110.03154)

Ce Michigan State University Zhou, Qiben Michigan State University Yan, Yan Michigan State University Shi, Lichao Lehigh University Sun


[Inference Attacks Against Graph Neural Networks. (2%)](http://arxiv.org/abs/2110.02631)

Zhikun Zhang, Min Chen, Michael Backes, Yun Shen, Yang Zhang


[Data-driven behavioural biometrics for continuous and adaptive user verification using Smartphone and Smartwatch. (1%)](http://arxiv.org/abs/2110.03149)

Akriti Verma, Valeh Moghaddam, Adnan Anwar


[On The Vulnerability of Recurrent Neural Networks to Membership Inference Attacks. (1%)](http://arxiv.org/abs/2110.03054)

Yunhao Yang, Parham Gohari, Ufuk Topcu


[Efficient Sharpness-aware Minimization for Improved Training of Neural Networks. (1%)](http://arxiv.org/abs/2110.03141)

Jiawei Du, Hanshu Yan, Jiashi Feng, Joey Tianyi Zhou, Liangli Zhen, Rick Siow Mong Goh, Vincent Y. F. Tan


[Stegomalware: A Systematic Survey of MalwareHiding and Detection in Images, Machine LearningModels and Research Challenges. (1%)](http://arxiv.org/abs/2110.02504)

Rajasekhar Chaganti, Vinayakumar Ravi, Mamoun Alazab, Tuan D. Pham


[Exploring the Common Principal Subspace of Deep Features in Neural Networks. (1%)](http://arxiv.org/abs/2110.02863)

Haoran Liu, Haoyi Xiong, Yaqing Wang, Haozhe An, Dongrui Wu, Dejing Dou


[Generalizing Neural Networks by Reflecting Deviating Data in Production. (1%)](http://arxiv.org/abs/2110.02718)

Yan Xiao, Yun Lin, Ivan Beschastnikh, Changsheng Sun, David S. Rosenblum, Jin Song Dong


## 2021-10-05

[Adversarial Robustness Verification and Attack Synthesis in Stochastic Systems. (99%)](http://arxiv.org/abs/2110.02125)

Lisa Oakley, Alina Oprea, Stavros Tripakis


[Adversarial Attacks on Black Box Video Classifiers: Leveraging the Power of Geometric Transformations. (99%)](http://arxiv.org/abs/2110.01823)

Shasha Li, Abhishek Aich, Shitong Zhu, M. Salman Asif, Chengyu Song, Amit K. Roy-Chowdhury, Srikanth Krishnamurthy


[Adversarial defenses via a mixture of generators. (99%)](http://arxiv.org/abs/2110.02364)

Maciej Żelaszczyk, Jacek Mańdziuk


[Neural Network Adversarial Attack Method Based on Improved Genetic Algorithm. (92%)](http://arxiv.org/abs/2110.01818)

Dingming Yang, Yanrong Cui, Hongqiang Yuan


[BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models. (33%)](http://arxiv.org/abs/2110.02467)

Kangjie Chen, Yuxian Meng, Xiaofei Sun, Shangwei Guo, Tianwei Zhang, Jiwei Li, Chun Fan


[Spectral Bias in Practice: The Role of Function Frequency in Generalization. (1%)](http://arxiv.org/abs/2110.02424)

Sara Fridovich-Keil, Raphael Gontijo-Lopes, Rebecca Roelofs


[CADA: Multi-scale Collaborative Adversarial Domain Adaptation for Unsupervised Optic Disc and Cup Segmentation. (1%)](http://arxiv.org/abs/2110.02417)

Peng Liu, Charlie T. Tran, Bin Kong, Ruogu Fang


[Noisy Feature Mixup. (1%)](http://arxiv.org/abs/2110.02180)

Soon Hoe Lim, N. Benjamin Erichson, Francisco Utrera, Winnie Xu, Michael W. Mahoney


## 2021-10-04

[Benchmarking Safety Monitors for Image Classifiers with Machine Learning. (1%)](http://arxiv.org/abs/2110.01232)

Raul Sena LAAS Ferreira, Jean LAAS Arlat, Jeremie LAAS Guiochet, Hélène LAAS Waeselynck


## 2021-10-03

[Adversarial Examples Generation for Reducing Implicit Gender Bias in Pre-trained Models. (82%)](http://arxiv.org/abs/2110.01094)

Wenqian Ye, Fei Xu, Yaojia Huang, Cassie Huang, Ji A


## 2021-10-02

[Evaluating Deep Learning Models and Adversarial Attacks on Accelerometer-Based Gesture Authentication. (98%)](http://arxiv.org/abs/2110.14597)

Elliu Huang, Troia Fabio Di, Mark Stamp


[Anti-aliasing Deep Image Classifiers using Novel Depth Adaptive Blurring and Activation Function. (13%)](http://arxiv.org/abs/2110.00899)

Md Tahmid Hossain, Shyh Wei Teng, Ferdous Sohel, Guojun Lu


## 2021-10-01

[Calibrated Adversarial Training. (98%)](http://arxiv.org/abs/2110.00623)

Tianjin Huang, Vlado Menkovski, Yulong Pei, Mykola Pechenizkiy


[Universal Adversarial Spoofing Attacks against Face Recognition. (87%)](http://arxiv.org/abs/2110.00708)

Takuma Amada, Seng Pei Liew, Kazuya Kakizaki, Toshinori Araki


[Score-Based Generative Classifiers. (84%)](http://arxiv.org/abs/2110.00473)

Roland S. Zimmermann, Lukas Schott, Yang Song, Benjamin A. Dunn, David A. Klindt


[One Timestep is All You Need: Training Spiking Neural Networks with Ultra Low Latency. (1%)](http://arxiv.org/abs/2110.05929)

Sayeed Shafayet Chowdhury, Nitin Rathi, Kaushik Roy


## 2021-09-30

[Mitigating Black-Box Adversarial Attacks via Output Noise Perturbation. (98%)](http://arxiv.org/abs/2109.15160)

Manjushree B. Aithal, Xiaohua Li


[You Cannot Easily Catch Me: A Low-Detectable Adversarial Patch for Object Detectors. (95%)](http://arxiv.org/abs/2109.15177)

Zijian Zhu, Hang Su, Chang Liu, Wenzhao Xiang, Shibao Zheng


[Adversarial Semantic Contour for Object Detection. (92%)](http://arxiv.org/abs/2109.15009)

Yichi Zhang, Zijian Zhu, Xiao Yang, Jun Zhu


[From Zero-Shot Machine Learning to Zero-Day Attack Detection. (10%)](http://arxiv.org/abs/2109.14868)

Mohanad Sarhan, Siamak Layeghy, Marcus Gallagher, Marius Portmann


## 2021-09-29

[On Brightness Agnostic Adversarial Examples Against Face Recognition Systems. (99%)](http://arxiv.org/abs/2109.14205)

Inderjeet Singh, Satoru Momiyama, Kazuya Kakizaki, Toshinori Araki


[Back in Black: A Comparative Evaluation of Recent State-Of-The-Art Black-Box Attacks. (70%)](http://arxiv.org/abs/2109.15031)

Kaleel Mahmood, Rigel Mahmood, Ethan Rathbun, Dijk Marten van


[BulletTrain: Accelerating Robust Neural Network Training via Boundary Example Mining. (41%)](http://arxiv.org/abs/2109.14707)

Weizhe Hua, Yichi Zhang, Chuan Guo, Zhiru Zhang, G. Edward Suh


[Mitigation of Adversarial Policy Imitation via Constrained Randomization of Policy (CRoP). (10%)](http://arxiv.org/abs/2109.14678)

Nancirose Piazza, Vahid Behzadan


## 2021-09-28

[slimTrain -- A Stochastic Approximation Method for Training Separable Deep Neural Networks. (1%)](http://arxiv.org/abs/2109.14002)

Elizabeth Newman, Julianne Chung, Matthias Chung, Lars Ruthotto


## 2021-09-27

[MUTEN: Boosting Gradient-Based Adversarial Attacks via Mutant-Based Ensembles. (99%)](http://arxiv.org/abs/2109.12838)

Yuejun Guo, Qiang Hu, Maxime Cordy, Michail Papadakis, Yves Le Traon


[Cluster Attack: Query-based Adversarial Attacks on Graphs with Graph-Dependent Priors. (99%)](http://arxiv.org/abs/2109.13069)

Zhengyi Wang, Zhongkai Hao, Ziqiao Wang, Hang Su, Jun Zhu


[Classification and Adversarial examples in an Overparameterized Linear Model: A Signal Processing Perspective. (98%)](http://arxiv.org/abs/2109.13215)

Adhyyan Narang, Vidya Muthukumar, Anant Sahai


[GANG-MAM: GAN based enGine for Modifying Android Malware. (64%)](http://arxiv.org/abs/2109.13297)

Renjith G, Sonia Laudanna, Aji S, Corrado Aaron Visaggio, Vinod P


[Distributionally Robust Multi-Output Regression Ranking. (3%)](http://arxiv.org/abs/2109.12803)

Shahabeddin Sotudian, Ruidi Chen, Ioannis Paschalidis


[Improving Uncertainty of Deep Learning-based Object Classification on Radar Spectra using Label Smoothing. (1%)](http://arxiv.org/abs/2109.12851)

Kanil Patel, William Beluch, Kilian Rambach, Michael Pfeiffer, Bin Yang


[Federated Deep Learning with Bayesian Privacy. (1%)](http://arxiv.org/abs/2109.13012)

Hanlin Gu, Lixin Fan, Bowen Li, Yan Kang, Yuan Yao, Qiang Yang


## 2021-09-26

[Distributionally Robust Multiclass Classification and Applications in Deep CNN Image Classifiers. (11%)](http://arxiv.org/abs/2109.12772)

Ruidi Chen, Boran Hao, Ioannis Paschalidis


## 2021-09-25

[Two Souls in an Adversarial Image: Towards Universal Adversarial Example Detection using Multi-view Inconsistency. (99%)](http://arxiv.org/abs/2109.12459)

Sohaib Kiani, Sana Awan, Chao Lan, Fengjun Li, Bo Luo


[Contributions to Large Scale Bayesian Inference and Adversarial Machine Learning. (98%)](http://arxiv.org/abs/2109.13232)

Víctor Gallego


[MINIMAL: Mining Models for Data Free Universal Adversarial Triggers. (93%)](http://arxiv.org/abs/2109.12406)

Swapnil Parekh, Yaman Singla Kumar, Somesh Singh, Changyou Chen, Balaji Krishnamurthy, Rajiv Ratn Shah


## 2021-09-24

[Local Intrinsic Dimensionality Signals Adversarial Perturbations. (98%)](http://arxiv.org/abs/2109.11803)

Sandamal Weerasinghe, Tansu Alpcan, Sarah M. Erfani, Christopher Leckie, Benjamin I. P. Rubinstein


## 2021-09-23

[Breaking BERT: Understanding its Vulnerabilities for Biomedical Named Entity Recognition through Adversarial Attack. (98%)](http://arxiv.org/abs/2109.11308)

Anne Dirkson, Suzan Verberne, Wessel Kraaij


[FooBaR: Fault Fooling Backdoor Attack on Neural Network Training. (88%)](http://arxiv.org/abs/2109.11249)

Jakub Breier, Xiaolu Hou, Martín Ochoa, Jesus Solano


[AES Systems Are Both Overstable And Oversensitive: Explaining Why And Proposing Defenses. (68%)](http://arxiv.org/abs/2109.11728)

Yaman Kumar Singla, Swapnil Parekh, Somesh Singh, Junyi Jessy Li, Rajiv Ratn Shah, Changyou Chen


[DeepAID: Interpreting and Improving Deep Learning-based Anomaly Detection in Security Applications. (1%)](http://arxiv.org/abs/2109.11495)

Dongqi Han, Zhiliang Wang, Wenqi Chen, Ying Zhong, Su Wang, Han Zhang, Jiahai Yang, Xingang Shi, Xia Yin


## 2021-09-22

[Exploring Adversarial Examples for Efficient Active Learning in Machine Learning Classifiers. (99%)](http://arxiv.org/abs/2109.10770)

Honggang Yu, Shihfeng Zeng, Teng Zhang, Ing-Chao Lin, Yier Jin


[CC-Cert: A Probabilistic Approach to Certify General Robustness of Neural Networks. (81%)](http://arxiv.org/abs/2109.10696)

Mikhail Pautov, Nurislam Tursynbek, Marina Munkhoeva, Nikita Muravev, Aleksandr Petiushko, Ivan Oseledets


[Security Analysis of Capsule Network Inference using Horizontal Collaboration. (69%)](http://arxiv.org/abs/2109.11041)

Adewale Adeyemo, Faiq Khalid, Tolulope A. Odetola, Syed Rafay Hasan


[Adversarial Transfer Attacks With Unknown Data and Class Overlap. (62%)](http://arxiv.org/abs/2109.11125)

Luke E. Richards, André Nguyen, Ryan Capps, Steven Forsythe, Cynthia Matuszek, Edward Raff


[Pushing the Right Buttons: Adversarial Evaluation of Quality Estimation. (1%)](http://arxiv.org/abs/2109.10859)

Diptesh Kanojia, Marina Fomicheva, Tharindu Ranasinghe, Frédéric Blain, Constantin Orăsan, Lucia Specia


[Backdoor Attacks on Federated Learning with Lottery Ticket Hypothesis. (1%)](http://arxiv.org/abs/2109.10512)

Zeyuan Yin, Ye Yuan, Panfeng Guo, Pan Zhou


## 2021-09-21

[Attacks on Visualization-Based Malware Detection: Balancing Effectiveness and Executability. (99%)](http://arxiv.org/abs/2109.10417)

Hadjer Benkraouda, Jingyu Qian, Hung Quoc Tran, Berkay Kaplan


[3D Point Cloud Completion with Geometric-Aware Adversarial Augmentation. (93%)](http://arxiv.org/abs/2109.10161)

Mengxi Wu, Hao Huang, Yi Fang


[DeSMP: Differential Privacy-exploited Stealthy Model Poisoning Attacks in Federated Learning. (76%)](http://arxiv.org/abs/2109.09955)

Md Tamjid Hossain, Shafkat Islam, Shahriar Badsha, Haoting Shen


[Privacy, Security, and Utility Analysis of Differentially Private CPES Data. (13%)](http://arxiv.org/abs/2109.09963)

Md Tamjid Hossain, Shahriar Badsha, Haoting Shen


## 2021-09-20

[Robust Physical-World Attacks on Face Recognition. (99%)](http://arxiv.org/abs/2109.09320)

Xin Zheng, Yanbo Fan, Baoyuan Wu, Yong Zhang, Jue Wang, Shirui Pan


[Modeling Adversarial Noise for Adversarial Defense. (99%)](http://arxiv.org/abs/2109.09901)

Dawei Zhou, Nannan Wang, Bo Han, Tongliang Liu


[Can We Leverage Predictive Uncertainty to Detect Dataset Shift and Adversarial Examples in Android Malware Detection? (99%)](http://arxiv.org/abs/2109.09654)

Deqiang Li, Tian Qiu, Shuo Chen, Qianmu Li, Shouhuai Xu


[Robustness Analysis of Deep Learning Frameworks on Mobile Platforms. (10%)](http://arxiv.org/abs/2109.09869)

Amin Eslami Abyane, Hadi Hemmati


["Hello, It's Me": Deep Learning-based Speech Synthesis Attacks in the Real World. (2%)](http://arxiv.org/abs/2109.09598)

Emily Wenger, Max Bronckers, Christian Cianfarani, Jenna Cryan, Angela Sha, Haitao Zheng, Ben Y. Zhao


[Towards Energy-Efficient and Secure Edge AI: A Cross-Layer Framework. (1%)](http://arxiv.org/abs/2109.09829)

Muhammad Shafique, Alberto Marchisio, Rachmad Vidya Wicaksana Putra, Muhammad Abdullah Hanif


## 2021-09-19

[On the Noise Stability and Robustness of Adversarially Trained Networks on NVM Crossbars. (99%)](http://arxiv.org/abs/2109.09060)

Deboleena Roy, Chun Tao, Indranil Chakraborty, Kaushik Roy


[Adversarial Training with Contrastive Learning in NLP. (16%)](http://arxiv.org/abs/2109.09075)

Daniela N. Rim, DongNyeong Heo, Heeyoul Choi


## 2021-09-18

[Clean-label Backdoor Attack against Deep Hashing based Retrieval. (98%)](http://arxiv.org/abs/2109.08868)

Kuofeng Gao, Jiawang Bai, Bin Chen, Dongxian Wu, Shu-Tao Xia


## 2021-09-17

[Messing Up 3D Virtual Environments: Transferable Adversarial 3D Objects. (98%)](http://arxiv.org/abs/2109.08465)

Enrico Meloni, Matteo Tiezzi, Luca Pasqualini, Marco Gori, Stefano Melacci


[Exploring the Training Robustness of Distributional Reinforcement Learning against Noisy State Observations. (8%)](http://arxiv.org/abs/2109.08776)

Ke Sun, Yingnan Zhao, Shangling Jui, Linglong Kong


## 2021-09-16

[Harnessing Perceptual Adversarial Patches for Crowd Counting. (99%)](http://arxiv.org/abs/2109.07986)

Shunchang Liu, Jiakai Wang, Aishan Liu, Yingwei Li, Yijie Gao, Xianglong Liu, Dacheng Tao


[KATANA: Simple Post-Training Robustness Using Test Time Augmentations. (98%)](http://arxiv.org/abs/2109.08191)

Gilad Cohen, Raja Giryes


[Targeted Attack on Deep RL-based Autonomous Driving with Learned Visual Patterns. (96%)](http://arxiv.org/abs/2109.07723)

Prasanth Buddareddygari, Travis Zhang, Yezhou Yang, Yi Ren


[Adversarial Attacks against Deep Learning Based Power Control in Wireless Communications. (95%)](http://arxiv.org/abs/2109.08139)

Brian Kim, Yi Shi, Yalin E. Sagduyu, Tugba Erpek, Sennur Ulukus


[Don't Search for a Search Method -- Simple Heuristics Suffice for Adversarial Text Attacks. (68%)](http://arxiv.org/abs/2109.07926)

Nathaniel Berger, Stefan Riezler, Artem Sokolov, Sebastian Ebert


[Membership Inference Attacks Against Recommender Systems. (3%)](http://arxiv.org/abs/2109.08045)

Minxing Zhang, Zhaochun Ren, Zihan Wang, Pengjie Ren, Zhumin Chen, Pengfei Hu, Yang Zhang


## 2021-09-15

[Universal Adversarial Attack on Deep Learning Based Prognostics. (99%)](http://arxiv.org/abs/2109.07142)

Arghya Basak, Pradeep Rathore, Sri Harsha Nistala, Sagar Srinivas, Venkataramana Runkana


[Balancing detectability and performance of attacks on the control channel of Markov Decision Processes. (98%)](http://arxiv.org/abs/2109.07171)

Alessio Russo, Alexandre Proutiere


[FCA: Learning a 3D Full-coverage Vehicle Camouflage for Multi-view Physical Adversarial Attack. (95%)](http://arxiv.org/abs/2109.07193)

DonghuaWang, Tingsong Jiang, Jialiang Sun, Weien Zhou, Xiaoya Zhang, Zhiqiang Gong, Wen Yao, Xiaoqian Chen


[BERT is Robust! A Case Against Synonym-Based Adversarial Examples in Text Classification. (92%)](http://arxiv.org/abs/2109.07403)

Jens Hauser, Zhao Meng, Damián Pascual, Roger Wattenhofer


[Adversarial Mixing Policy for Relaxing Locally Linear Constraints in Mixup. (13%)](http://arxiv.org/abs/2109.07177)

Guang Liu, Yuzhao Mao, Hailong Huang, Weiguo Gao, Xuan Li


[Can one hear the shape of a neural network?: Snooping the GPU via Magnetic Side Channel. (10%)](http://arxiv.org/abs/2109.07395)

Henrique Teles Maia, Chang Xiao, Dingzeyu Li, Eitan Grinspun, Changxi Zheng


## 2021-09-14

[A Novel Data Encryption Method Inspired by Adversarial Attacks. (99%)](http://arxiv.org/abs/2109.06634)

Praveen Fernando, Jin Wei-Kocsis


[Improving Gradient-based Adversarial Training for Text Classification by Contrastive Learning and Auto-Encoder. (99%)](http://arxiv.org/abs/2109.06536)

Yao Qiu, Jinchao Zhang, Jie Zhou


[PETGEN: Personalized Text Generation Attack on Deep Sequence Embedding-based Classification Models. (99%)](http://arxiv.org/abs/2109.06777)

Bing He, Mustaque Ahamad, Srijan Kumar


[EVAGAN: Evasion Generative Adversarial Network for Low Data Regimes. (76%)](http://arxiv.org/abs/2109.08026)

Rizwan Hamid Randhawa, Nauman Aslam, Muhammad Alauthman, Husnain Rafiq, Muhammad Khalid


[Dodging Attack Using Carefully Crafted Natural Makeup. (47%)](http://arxiv.org/abs/2109.06467)

Nitzan Guetta, Asaf Shabtai, Inderjeet Singh, Satoru Momiyama, Yuval Elovici


[Avengers Ensemble! Improving Transferability of Authorship Obfuscation. (12%)](http://arxiv.org/abs/2109.07028)

Muhammad Haroon, Muhammad Fareed Zaffar, Padmini Srinivasan, Zubair Shafiq


[ARCH: Efficient Adversarial Regularized Training with Caching. (8%)](http://arxiv.org/abs/2109.07048)

Simiao Zuo, Chen Liang, Haoming Jiang, Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, Tuo Zhao


## 2021-09-13

[Adversarial Bone Length Attack on Action Recognition. (99%)](http://arxiv.org/abs/2109.05830)

Nariki Tanaka, Hiroshi Kera, Kazuhiko Kawamoto


[Randomized Substitution and Vote for Textual Adversarial Example Detection. (99%)](http://arxiv.org/abs/2109.05698)

Xiaosen Wang, Yifeng Xiong, Kun He


[Improving the Robustness of Adversarial Attacks Using an Affine-Invariant Gradient Estimator. (99%)](http://arxiv.org/abs/2109.05820)

Wenzhao Xiang, Hang Su, Chang Liu, Yandong Guo, Shibao Zheng


[Evolving Architectures with Gradient Misalignment toward Low Adversarial Transferability. (98%)](http://arxiv.org/abs/2109.05919)

Kevin Richard G. Operiano, Wanchalerm Pora, Hitoshi Iba, Hiroshi Kera


[A Practical Adversarial Attack on Contingency Detection of Smart Energy Systems. (98%)](http://arxiv.org/abs/2109.06358)

Moein Sabounchi, Jin Wei-Kocsis


[Adversarial Examples for Evaluating Math Word Problem Solvers. (96%)](http://arxiv.org/abs/2109.05925)

Vivek Kumar, Rishabh Maheshwary, Vikram Pudi


[PAT: Pseudo-Adversarial Training For Detecting Adversarial Videos. (86%)](http://arxiv.org/abs/2109.05695)

Nupur Thakur, Baoxin Li


[Byzantine-robust Federated Learning through Collaborative Malicious Gradient Filtering. (81%)](http://arxiv.org/abs/2109.05872)

Jian Xu, Shao-Lun Huang, Linqi Song, Tian Lan


[Formalizing and Estimating Distribution Inference Risks. (62%)](http://arxiv.org/abs/2109.06024)

Anshuman Suri, David Evans


[Virtual Data Augmentation: A Robust and General Framework for Fine-tuning Pre-trained Models. (50%)](http://arxiv.org/abs/2109.05793)

Kun Zhou, Wayne Xin Zhao, Sirui Wang, Fuzheng Zhang, Wei Wu, Ji-Rong Wen


[Sensor Adversarial Traits: Analyzing Robustness of 3D Object Detection Sensor Fusion Models. (16%)](http://arxiv.org/abs/2109.06363)

Won Park, Nan Li, Qi Alfred Chen, Z. Morley Mao


[Adversarially Trained Object Detector for Unsupervised Domain Adaptation. (3%)](http://arxiv.org/abs/2109.05751)

Kazuma Fujii, Hiroshi Kera, Kazuhiko Kawamoto


[Perturbation CheckLists for Evaluating NLG Evaluation Metrics. (1%)](http://arxiv.org/abs/2109.05771)

Ananya B. Sai, Tanay Dixit, Dev Yashpal Sheth, Sreyas Mohan, Mitesh M. Khapra


[How to Select One Among All? An Extensive Empirical Study Towards the Robustness of Knowledge Distillation in Natural Language Understanding. (1%)](http://arxiv.org/abs/2109.05696)

Tianda Li, Ahmad Rashid, Aref Jafari, Pranav Sharma, Ali Ghodsi, Mehdi Rezagholizadeh


[Detecting Safety Problems of Multi-Sensor Fusion in Autonomous Driving. (1%)](http://arxiv.org/abs/2109.06404)

Ziyuan Zhong, Zhisheng Hu, Shengjian Guo, Xinyang Zhang, Zhenyu Zhong, Baishakhi Ray


## 2021-09-12

[TREATED:Towards Universal Defense against Textual Adversarial Attacks. (99%)](http://arxiv.org/abs/2109.06176)

Bin Zhu, Zhaoquan Gu, Le Wang, Zhihong Tian


[CoG: a Two-View Co-training Framework for Defending Adversarial Attacks on Graph. (98%)](http://arxiv.org/abs/2109.05558)

Xugang Wu, Huijun Wu, Xu Zhou, Kai Lu


[Check Your Other Door! Creating Backdoor Attacks in the Frequency Domain. (93%)](http://arxiv.org/abs/2109.05507)

Hasan Abed Al Kader Hammoud, Bernard Ghanem


[RockNER: A Simple Method to Create Adversarial Examples for Evaluating the Robustness of Named Entity Recognition Models. (84%)](http://arxiv.org/abs/2109.05620)

Bill Yuchen Lin, Wenyang Gao, Jun Yan, Ryan Moreno, Xiang Ren


[Shape-Biased Domain Generalization via Shock Graph Embeddings. (2%)](http://arxiv.org/abs/2109.05671)

Maruthi Narayanan, Vickram Rajendran, Benjamin Kimia


[Source Inference Attacks in Federated Learning. (1%)](http://arxiv.org/abs/2109.05659)

Hongsheng Hu, Zoran Salcic, Lichao Sun, Gillian Dobbie, Xuyun Zhang


## 2021-09-11

[RobustART: Benchmarking Robustness on Architecture Design and Training Techniques. (98%)](http://arxiv.org/abs/2109.05211)

Shiyu Tang, Ruihao Gong, Yan Wang, Aishan Liu, Jiakai Wang, Xinyun Chen, Fengwei Yu, Xianglong Liu, Dawn Song, Alan Yuille, Philip H. S. Torr, Dacheng Tao


[2-in-1 Accelerator: Enabling Random Precision Switch for Winning Both Adversarial Robustness and Efficiency. (81%)](http://arxiv.org/abs/2109.05223)

Yonggan Fu, Yang Zhao, Qixuan Yu, Chaojian Li, Yingyan Lin


## 2021-09-10

[A Strong Baseline for Query Efficient Attacks in a Black Box Setting. (99%)](http://arxiv.org/abs/2109.04775)

Rishabh Maheshwary, Saket Maheshwary, Vikram Pudi


## 2021-09-09

[Contrasting Human- and Machine-Generated Word-Level Adversarial Examples for Text Classification. (99%)](http://arxiv.org/abs/2109.04385)

Maximilian Mozes, Max Bartolo, Pontus Stenetorp, Bennett Kleinberg, Lewis D. Griffin


[Energy Attack: On Transferring Adversarial Examples. (99%)](http://arxiv.org/abs/2109.04300)

Ruoxi Shi, Borui Yang, Yangzhou Jiang, Chenglong Zhao, Bingbing Ni


[Protein Folding Neural Networks Are Not Robust. (99%)](http://arxiv.org/abs/2109.04460)

Sumit Kumar Jha, Arvind Ramanathan, Rickard Ewetz, Alvaro Velasquez, Susmit Jha


[Towards Transferable Adversarial Attacks on Vision Transformers. (99%)](http://arxiv.org/abs/2109.04176)

Zhipeng Wei, Jingjing Chen, Micah Goldblum, Zuxuan Wu, Tom Goldstein, Yu-Gang Jiang


[Multi-granularity Textual Adversarial Attack with Behavior Cloning. (98%)](http://arxiv.org/abs/2109.04367)

Yangyi Chen, Jin Su, Wei Wei


[Spatially Focused Attack against Spatiotemporal Graph Neural Networks. (81%)](http://arxiv.org/abs/2109.04608)

Fuqiang Liu, Luis Miranda-Moreno, Lijun Sun


[Differential Privacy in Personalized Pricing with Nonparametric Demand Models. (26%)](http://arxiv.org/abs/2109.04615)

Xi Chen, Sentao Miao, Yining Wang


[EvilModel 2.0: Bringing Neural Network Models into Malware Attacks. (5%)](http://arxiv.org/abs/2109.04344)

Zhi Wang, Chaoge Liu, Xiang Cui, Jie Yin, Xutong Wang


## 2021-09-08

[Membership Inference Attacks Against Temporally Correlated Data in Deep Reinforcement Learning. (89%)](http://arxiv.org/abs/2109.03975)

Maziar Gomrokchi, Susan Amin, Hossein Aboutalebi, Alexander Wong, Doina Precup


[Robust Optimal Classification Trees Against Adversarial Examples. (80%)](http://arxiv.org/abs/2109.03857)

Daniël Vos, Sicco Verwer


## 2021-09-07

[Adversarial Parameter Defense by Multi-Step Risk Minimization. (98%)](http://arxiv.org/abs/2109.02889)

Zhiyuan Zhang, Ruixuan Luo, Xuancheng Ren, Qi Su, Liangyou Li, Xu Sun


[POW-HOW: An enduring timing side-channel to evade online malware sandboxes. (12%)](http://arxiv.org/abs/2109.02979)

Antonio Nappa, Panagiotis Papadopoulos, Matteo Varvello, Daniel Aceituno Gomez, Juan Tapiador, Andrea Lanzi


[Unpaired Adversarial Learning for Single Image Deraining with Rain-Space Contrastive Constraints. (1%)](http://arxiv.org/abs/2109.02973)

Xiang Chen, Jinshan Pan, Kui Jiang, Yufeng Huang, Caihua Kong, Longgang Dai, Yufeng Li


## 2021-09-06

[Robustness and Generalization via Generative Adversarial Training. (82%)](http://arxiv.org/abs/2109.02765)

Omid Poursaeed, Tianxing Jiang, Harry Yang, Serge Belongie, SerNam Lim


[Trojan Signatures in DNN Weights. (33%)](http://arxiv.org/abs/2109.02836)

Greg Fields, Mohammad Samragh, Mojan Javaheripi, Farinaz Koushanfar, Tara Javidi


[Automated Robustness with Adversarial Training as a Post-Processing Step. (4%)](http://arxiv.org/abs/2109.02532)

Ambrish Rawat, Mathieu Sinn, Beat Buesser


[Exposing Length Divergence Bias of Textual Matching Models. (2%)](http://arxiv.org/abs/2109.02431)

Lan Jiang, Tianshu Lyu, Chong Meng, Xiaoyong Lyu, Dawei Yin


## 2021-09-05

[Efficient Combinatorial Optimization for Word-level Adversarial Textual Attack. (98%)](http://arxiv.org/abs/2109.02229)

Shengcai Liu, Ning Lu, Cheng Chen, Ke Tang


[Tolerating Adversarial Attacks and Byzantine Faults in Distributed Machine Learning. (2%)](http://arxiv.org/abs/2109.02018)

Yusen Wu, Hao Chen, Xin Wang, Chao Liu, Phuong Nguyen, Yelena Yesha


[DexRay: A Simple, yet Effective Deep Learning Approach to Android Malware Detection based on Image Representation of Bytecode. (1%)](http://arxiv.org/abs/2109.03326)

Nadia Daoudi, Jordan Samhi, Abdoul Kader Kabore, Kevin Allix, Tegawendé F. Bissyandé, Jacques Klein


## 2021-09-04

[Real-World Adversarial Examples involving Makeup Application. (99%)](http://arxiv.org/abs/2109.03329)

Chang-Sheng Lin, Chia-Yi Hsu, Pin-Yu Chen, Chia-Mu Yu


[Utilizing Adversarial Targeted Attacks to Boost Adversarial Robustness. (99%)](http://arxiv.org/abs/2109.01945)

Uriya Pesso, Koby Bibas, Meir Feder


[Training Meta-Surrogate Model for Transferable Adversarial Attack. (99%)](http://arxiv.org/abs/2109.01983)

Yunxiao Qin, Yuanhao Xiong, Jinfeng Yi, Cho-Jui Hsieh


## 2021-09-03

[SEC4SR: A Security Analysis Platform for Speaker Recognition. (99%)](http://arxiv.org/abs/2109.01766)

Guangke Chen, Zhe Zhao, Fu Song, Sen Chen, Lingling Fan, Yang Liu


[Risk Assessment for Connected Vehicles under Stealthy Attacks on Vehicle-to-Vehicle Networks. (1%)](http://arxiv.org/abs/2109.01553)

Tianci Yang, Carlos Murguia, Chen Lv


## 2021-09-02

[A Synergetic Attack against Neural Network Classifiers combining Backdoor and Adversarial Examples. (99%)](http://arxiv.org/abs/2109.01275)

Guanxiong Liu, Issa Khalil, Abdallah Khreishah, NhatHai Phan


[Impact of Attention on Adversarial Robustness of Image Classification Models. (99%)](http://arxiv.org/abs/2109.00936)

Prachi Agrawal, Narinder Singh Punn, Sanjay Kumar Sonbhadra, Sonali Agarwal


[Adversarial Robustness for Unsupervised Domain Adaptation. (98%)](http://arxiv.org/abs/2109.00946)

Muhammad Awais, Fengwei Zhou, Hang Xu, Lanqing Hong, Ping Luo, Sung-Ho Bae, Zhenguo Li


[Real World Robustness from Systematic Noise. (91%)](http://arxiv.org/abs/2109.00864)

Yan Wang, Yuhang Li, Ruihao Gong


[Building Compact and Robust Deep Neural Networks with Toeplitz Matrices. (61%)](http://arxiv.org/abs/2109.00959)

Alexandre Araujo


## 2021-09-01

[Towards Improving Adversarial Training of NLP Models. (98%)](http://arxiv.org/abs/2109.00544)

Jin Yong Yoo, Yanjun Qi


[Excess Capacity and Backdoor Poisoning. (97%)](http://arxiv.org/abs/2109.00685)

Naren Sarayu Manoj, Avrim Blum


[Regional Adversarial Training for Better Robust Generalization. (96%)](http://arxiv.org/abs/2109.00678)

Chuanbiao Song, Yanbo Fan, Yicheng Yang, Baoyuan Wu, Yiming Li, Zhifeng Li, Kun He


[R-SNN: An Analysis and Design Methodology for Robustifying Spiking Neural Networks against Adversarial Attacks through Noise Filters for Dynamic Vision Sensors. (86%)](http://arxiv.org/abs/2109.00533)

Alberto Marchisio, Giacomo Pira, Maurizio Martina, Guido Masera, Muhammad Shafique


[Proof Transfer for Neural Network Verification. (9%)](http://arxiv.org/abs/2109.00542)

Christian Sprecher, Marc Fischer, Dimitar I. Dimitrov, Gagandeep Singh, Martin Vechev


[Guarding Machine Learning Hardware Against Physical Side-Channel Attacks. (2%)](http://arxiv.org/abs/2109.00187)

Anuj Dubey, Rosario Cammarota, Vikram Suresh, Aydin Aysu


## 2021-08-31

[EG-Booster: Explanation-Guided Booster of ML Evasion Attacks. (99%)](http://arxiv.org/abs/2108.13930)

Abderrahmen Amich, Birhanu Eshete


[Morphence: Moving Target Defense Against Adversarial Examples. (99%)](http://arxiv.org/abs/2108.13952)

Abderrahmen Amich, Birhanu Eshete


[DPA: Learning Robust Physical Adversarial Camouflages for Object Detectors. (93%)](http://arxiv.org/abs/2109.00124)

Yexin Duan, Jialin Chen, Xingyu Zhou, Junhua Zou, Zhengyun He, Wu Zhang, Jin Zhang, Zhisong Pan


[Black-Box Attacks on Sequential Recommenders via Data-Free Model Extraction. (83%)](http://arxiv.org/abs/2109.01165)

Zhenrui Yue, Zhankui He, Huimin Zeng, Julian McAuley


[Segmentation Fault: A Cheap Defense Against Adversarial Machine Learning. (75%)](http://arxiv.org/abs/2108.13617)

Doha Al Bared, Mohamed Nassar


[Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning. (4%)](http://arxiv.org/abs/2108.13888)

Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, Xipeng Qiu


## 2021-08-30

[Sample Efficient Detection and Classification of Adversarial Attacks via Self-Supervised Embeddings. (99%)](http://arxiv.org/abs/2108.13797)

Mazda Moayeri, Soheil Feizi


[Investigating Vulnerabilities of Deep Neural Policies. (99%)](http://arxiv.org/abs/2108.13093)

Ezgi Korkmaz


[Adversarial Example Devastation and Detection on Speech Recognition System by Adding Random Noise. (99%)](http://arxiv.org/abs/2108.13562)

Mingyu Dong, Diqun Yan, Yongkang Gong, Rangding Wang


[Single Node Injection Attack against Graph Neural Networks. (68%)](http://arxiv.org/abs/2108.13049)

Shuchang Tao, Qi Cao, Huawei Shen, Junjie Huang, Yunfan Wu, Xueqi Cheng


[Benchmarking the Accuracy and Robustness of Feedback Alignment Algorithms. (41%)](http://arxiv.org/abs/2108.13446)

Albert Jiménez Sanfiz, Mohamed Akrout


[Adaptive perturbation adversarial training: based on reinforcement learning. (41%)](http://arxiv.org/abs/2108.13239)

Zhishen Nie, Ying Lin, Sp Ren, Lan Zhang


[How Does Adversarial Fine-Tuning Benefit BERT? (33%)](http://arxiv.org/abs/2108.13602)

Javid Ebrahimi, Hao Yang, Wei Zhang


[ML-based IoT Malware Detection Under Adversarial Settings: A Systematic Evaluation. (26%)](http://arxiv.org/abs/2108.13373)

Ahmed Abusnaina, Afsah Anwar, Sultan Alshamrani, Abdulrahman Alabduljabbar, RhongHo Jang, Daehun Nyang, David Mohaisen


[DuTrust: A Sentiment Analysis Dataset for Trustworthiness Evaluation. (1%)](http://arxiv.org/abs/2108.13140)

Lijie Wang, Hao Liu, Shuyuan Peng, Hongxuan Tang, Xinyan Xiao, Ying Chen, Hua Wu, Haifeng Wang


## 2021-08-29

[Searching for an Effective Defender: Benchmarking Defense against Adversarial Word Substitution. (99%)](http://arxiv.org/abs/2108.12777)

Zongyi Li, Jianhan Xu, Jiehang Zeng, Linyang Li, Xiaoqing Zheng, Qi Zhang, Kai-Wei Chang, Cho-Jui Hsieh


[Reinforcement Learning Based Sparse Black-box Adversarial Attack on Video Recognition Models. (98%)](http://arxiv.org/abs/2108.13872)

Zeyuan Wang, Chaofeng Sha, Su Yang


[DropAttack: A Masked Weight Adversarial Training Method to Improve Generalization of Neural Networks. (82%)](http://arxiv.org/abs/2108.12805)

Shiwen Ni, Jiawen Li, Hung-Yu Kao


[HAT4RD: Hierarchical Adversarial Training for Rumor Detection on Social Media. (81%)](http://arxiv.org/abs/2110.00425)

Shiwen Ni, Jiawen Li, Hung-Yu Kao


## 2021-08-27

[Mal2GCN: A Robust Malware Detection Approach Using Deep Graph Convolutional Networks With Non-Negative Weights. (99%)](http://arxiv.org/abs/2108.12473)

Omid Kargarnovin, Amir Mahdi Sadeghzadeh, Rasool Jalili


[Disrupting Adversarial Transferability in Deep Neural Networks. (98%)](http://arxiv.org/abs/2108.12492)

Christopher Wiedeman, Ge Wang


[Evaluating the Robustness of Neural Language Models to Input Perturbations. (16%)](http://arxiv.org/abs/2108.12237)

Milad Moradi, Matthias Samwald


[Deep learning models are not robust against noise in clinical text. (1%)](http://arxiv.org/abs/2108.12242)

Milad Moradi, Kathrin Blagec, Matthias Samwald


## 2021-08-26

[Understanding the Logit Distributions of Adversarially-Trained Deep Neural Networks. (99%)](http://arxiv.org/abs/2108.12001)

Landan Seguin, Anthony Ndirango, Neeli Mishra, SueYeon Chung, Tyler Lee


[A Hierarchical Assessment of Adversarial Severity. (98%)](http://arxiv.org/abs/2108.11785)

Guillaume Jeanneret, Juan C Perez, Pablo Arbelaez


[Physical Adversarial Attacks on an Aerial Imagery Object Detector. (96%)](http://arxiv.org/abs/2108.11765)

Andrew Du, Bo Chen, Tat-Jun Chin, Yee Wei Law, Michele Sasdelli, Ramesh Rajasegaran, Dillon Campbell


[Why Adversarial Reprogramming Works, When It Fails, and How to Tell the Difference. (80%)](http://arxiv.org/abs/2108.11673)

Yang Zheng, Xiaoyi Feng, Zhaoqiang Xia, Xiaoyue Jiang, Ambra Demontis, Maura Pintor, Battista Biggio, Fabio Roli


[Detection and Continual Learning of Novel Face Presentation Attacks. (2%)](http://arxiv.org/abs/2108.12081)

Mohammad Rostami, Leonidas Spinoulas, Mohamed Hussein, Joe Mathai, Wael Abd-Almageed


## 2021-08-25

[Uncertify: Attacks Against Neural Network Certification. (99%)](http://arxiv.org/abs/2108.11299)

Tobias Lorenz, Marta Kwiatkowska, Mario Fritz


[Adversarially Robust One-class Novelty Detection. (99%)](http://arxiv.org/abs/2108.11168)

Shao-Yuan Lo, Poojan Oza, Vishal M. Patel


[Bridged Adversarial Training. (93%)](http://arxiv.org/abs/2108.11135)

Hoki Kim, Woojin Lee, Sungyoon Lee, Jaewook Lee


[Generalized Real-World Super-Resolution through Adversarial Robustness. (93%)](http://arxiv.org/abs/2108.11505)

Angela Castillo, María Escobar, Juan C. Pérez, Andrés Romero, Radu Timofte, Gool Luc Van, Pablo Arbeláez


## 2021-08-24

[Improving Visual Quality of Unrestricted Adversarial Examples with Wavelet-VAE. (99%)](http://arxiv.org/abs/2108.11032)

Wenzhao Xiang, Chang Liu, Shibao Zheng


[Are socially-aware trajectory prediction models really socially-aware? (92%)](http://arxiv.org/abs/2108.10879)

Saeed Saadatnejad, Mohammadhossein Bahari, Pedram Khorsandi, Mohammad Saneian, Seyed-Mohsen Moosavi-Dezfooli, Alexandre Alahi


[OOWL500: Overcoming Dataset Collection Bias in the Wild. (76%)](http://arxiv.org/abs/2108.10992)

Brandon Leung, Chih-Hui Ho, Amir Persekian, David Orozco, Yen Chang, Erik Sandstrom, Bo Liu, Nuno Vasconcelos


[StyleAugment: Learning Texture De-biased Representations by Style Augmentation without Pre-defined Textures. (1%)](http://arxiv.org/abs/2108.10549)

Sanghyuk Chun, Song Park


## 2021-08-23

[Adversarial Robustness of Deep Learning: Theory, Algorithms, and Applications. (99%)](http://arxiv.org/abs/2108.10451)

Wenjie Ruan, Xinping Yi, Xiaowei Huang


[Semantic-Preserving Adversarial Text Attacks. (99%)](http://arxiv.org/abs/2108.10015)

Xinghao Yang, Weifeng Liu, James Bailey, Tianqing Zhu, Dacheng Tao, Wei Liu


[Deep Bayesian Image Set Classification: A Defence Approach against Adversarial Attacks. (99%)](http://arxiv.org/abs/2108.10217)

Nima Mirnateghi, Syed Afaq Ali Shah, Mohammed Bennamoun


[Kryptonite: An Adversarial Attack Using Regional Focus. (99%)](http://arxiv.org/abs/2108.10251)

Yogesh Kulkarni, Krisha Bhambani


[Back to the Drawing Board: A Critical Evaluation of Poisoning Attacks on Federated Learning. (73%)](http://arxiv.org/abs/2108.10241)

Virat Shejwalkar, Amir Houmansadr, Peter Kairouz, Daniel Ramage


[SegMix: Co-occurrence Driven Mixup for Semantic Segmentation and Adversarial Robustness. (4%)](http://arxiv.org/abs/2108.09929)

Md Amirul Islam, Matthew Kowal, Konstantinos G. Derpanis, Neil D. B. Bruce


## 2021-08-22

[Robustness-via-Synthesis: Robust Training with Generative Adversarial Perturbations. (99%)](http://arxiv.org/abs/2108.09713)

Inci M. Baytas, Debayan Deb


[Multi-Expert Adversarial Attack Detection in Person Re-identification Using Context Inconsistency. (98%)](http://arxiv.org/abs/2108.09891)

Xueping Wang, Shasha Li, Min Liu, Yaonan Wang, Amit K. Roy-Chowdhury


[Relating CNNs with brain: Challenges and findings. (10%)](http://arxiv.org/abs/2108.09768)

Reem Abdel-Salam


## 2021-08-21

[A Hard Label Black-box Adversarial Attack Against Graph Neural Networks. (99%)](http://arxiv.org/abs/2108.09513)

Jiaming Mu, Binghui Wang, Qi Li, Kun Sun, Mingwei Xu, Zhuotao Liu


["Adversarial Examples" for Proof-of-Learning. (98%)](http://arxiv.org/abs/2108.09454)

Rui Zhang, Jian Liu, Yuan Ding, Qingbiao Wu, Kui Ren


[Regularizing Instabilities in Image Reconstruction Arising from Learned Denoisers. (2%)](http://arxiv.org/abs/2108.13551)

Abinash Nayak


## 2021-08-20

[AdvDrop: Adversarial Attack to DNNs by Dropping Information. (99%)](http://arxiv.org/abs/2108.09034)

Ranjie Duan, Yuefeng Chen, Dantong Niu, Yun Yang, A. K. Qin, Yuan He


[PatchCleanser: Certifiably Robust Defense against Adversarial Patches for Any Image Classifier. (99%)](http://arxiv.org/abs/2108.09135)

Chong Xiang, Saeed Mahloujifar, Prateek Mittal


[Integer-arithmetic-only Certified Robustness for Quantized Neural Networks. (98%)](http://arxiv.org/abs/2108.09413)

Haowen Lin, Jian Lou, Li Xiong, Cyrus Shahabi


[Towards Understanding the Generative Capability of Adversarially Robust Classifiers. (98%)](http://arxiv.org/abs/2108.09093)

Yao Zhu, Jiacheng Ma, Jiacheng Sun, Zewei Chen, Rongxin Jiang, Zhenguo Li


[Detecting and Segmenting Adversarial Graphics Patterns from Images. (93%)](http://arxiv.org/abs/2108.09383)

Xiangyu Purdue University Qu, Stanley H. Purdue University Chan


[UnSplit: Data-Oblivious Model Inversion, Model Stealing, and Label Inference Attacks Against Split Learning. (1%)](http://arxiv.org/abs/2108.09033)

Ege Erdogan, Alptekin Kupcu, A. Ercument Cicek


[Early-exit deep neural networks for distorted images: providing an efficient edge offloading. (1%)](http://arxiv.org/abs/2108.09343)

Roberto G. Pacheco, Fernanda D. V. R. Oliveira, Rodrigo S. Couto


## 2021-08-19

[Application of Adversarial Examples to Physical ECG Signals. (99%)](http://arxiv.org/abs/2108.08972)

Taiga Waseda University Ono, Takeshi The University of Electro-Communications Sugawara, Jun University of Tsukuba Sakuma, Tatsuya Waseda University RIKEN AIP Mori


[Pruning in the Face of Adversaries. (99%)](http://arxiv.org/abs/2108.08560)

Florian Merkle, Maximilian Samsinger, Pascal Schöttle


[ASAT: Adaptively Scaled Adversarial Training in Time Series. (98%)](http://arxiv.org/abs/2108.08976)

Zhiyuan Zhang, Wei Li, Ruihan Bao, Keiko Harimoto, Yunfang Wu, Xu Sun


[Amplitude-Phase Recombination: Rethinking Robustness of Convolutional Neural Networks in Frequency Domain. (80%)](http://arxiv.org/abs/2108.08487)

Guangyao Chen, Peixi Peng, Li Ma, Jia Li, Lin Du, Yonghong Tian


## 2021-08-18

[Revisiting Adversarial Robustness Distillation: Robust Soft Labels Make Student Better. (99%)](http://arxiv.org/abs/2108.07969)

Bojia Zi, Shihao Zhao, Xingjun Ma, Yu-Gang Jiang


[Exploiting Multi-Object Relationships for Detecting Adversarial Attacks in Complex Scenes. (98%)](http://arxiv.org/abs/2108.08421)

Mingjun Yin, Shasha Li, Zikui Cai, Chengyu Song, M. Salman Asif, Amit K. Roy-Chowdhury, Srikanth V. Krishnamurthy


[MBRS : Enhancing Robustness of DNN-based Watermarking by Mini-Batch of Real and Simulated JPEG Compression. (45%)](http://arxiv.org/abs/2108.08211)

Zhaoyang Jia, Han Fang, Weiming Zhang


[Proceedings of the 1st International Workshop on Adaptive Cyber Defense. (1%)](http://arxiv.org/abs/2108.08476)

Damian Marriott, Kimberly Ferguson-Walter, Sunny Fugate, Marco Carvalho


## 2021-08-17

[When Should You Defend Your Classifier -- A Game-theoretical Analysis of Countermeasures against Adversarial Examples. (98%)](http://arxiv.org/abs/2108.07602)

Maximilian Samsinger, Florian Merkle, Pascal Schöttle, Tomas Pevny


[Adversarial Relighting Against Face Recognition. (98%)](http://arxiv.org/abs/2108.07920)

Qian Zhang, Qing Guo, Ruijun Gao, Felix Juefei-Xu, Hongkai Yu, Wei Feng


[Semantic Perturbations with Normalizing Flows for Improved Generalization. (13%)](http://arxiv.org/abs/2108.07958)

Oguz Kaan Yuksel, Sebastian U. Stich, Martin Jaggi, Tatjana Chavdarova


[Coalesced Multi-Output Tsetlin Machines with Clause Sharing. (1%)](http://arxiv.org/abs/2108.07594)

Sondre Glimsdal, Ole-Christoffer Granmo


[Appearance Based Deep Domain Adaptation for the Classification of Aerial Images. (1%)](http://arxiv.org/abs/2108.07779)

Dennis Wittich, Franz Rottensteiner


## 2021-08-16

[Exploring Transferable and Robust Adversarial Perturbation Generation from the Perspective of Network Hierarchy. (99%)](http://arxiv.org/abs/2108.07033)

Ruikui Wang, Yuanfang Guo, Ruijie Yang, Yunhong Wang


[Interpreting Attributions and Interactions of Adversarial Attacks. (83%)](http://arxiv.org/abs/2108.06895)

Xin Wang, Shuyun Lin, Hao Zhang, Yufei Zhu, Quanshi Zhang


[Patch Attack Invariance: How Sensitive are Patch Attacks to 3D Pose? (62%)](http://arxiv.org/abs/2108.07229)

Max Lennon, Nathan Drenkow, Philippe Burlina


[NeuraCrypt is not private. (10%)](http://arxiv.org/abs/2108.07256)

Nicholas Carlini, Sanjam Garg, Somesh Jha, Saeed Mahloujifar, Mohammad Mahmoody, Florian Tramer


[Identifying and Exploiting Structures for Reliable Deep Learning. (2%)](http://arxiv.org/abs/2108.07083)

Amartya Sanyal


[On the Opportunities and Risks of Foundation Models. (2%)](http://arxiv.org/abs/2108.07258)

Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Arx Sydney von, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, Percy Liang


## 2021-08-15

[Neural Architecture Dilation for Adversarial Robustness. (81%)](http://arxiv.org/abs/2108.06885)

Yanxi Li, Zhaohui Yang, Yunhe Wang, Chang Xu


[Deep Adversarially-Enhanced k-Nearest Neighbors. (74%)](http://arxiv.org/abs/2108.06797)

Ren Wang, Tianqi Chen


[IADA: Iterative Adversarial Data Augmentation Using Formal Verification and Expert Guidance. (1%)](http://arxiv.org/abs/2108.06871)

Ruixuan Liu, Changliu Liu


## 2021-08-14

[LinkTeller: Recovering Private Edges from Graph Neural Networks via Influence Analysis. (1%)](http://arxiv.org/abs/2108.06504)

Fan Wu, Yunhui Long, Ce Zhang, Bo Li


## 2021-08-13

[Evaluating the Robustness of Semantic Segmentation for Autonomous Driving against Real-World Adversarial Patch Attacks. (99%)](http://arxiv.org/abs/2108.06179)

Federico Nesti, Giulio Rossolini, Saasha Nair, Alessandro Biondi, Giorgio Buttazzo


[Optical Adversarial Attack. (98%)](http://arxiv.org/abs/2108.06247)

Abhiram Gnanasambandam, Alex M. Sherman, Stanley H. Chan


[Understanding Structural Vulnerability in Graph Convolutional Networks. (96%)](http://arxiv.org/abs/2108.06280)

Liang Chen, Jintang Li, Qibiao Peng, Yang Liu, Zibin Zheng, Carl Yang


[The Forgotten Threat of Voltage Glitching: A Case Study on Nvidia Tegra X2 SoCs. (1%)](http://arxiv.org/abs/2108.06131)

Otto Bittner, Thilo Krachenfels, Andreas Galauner, Jean-Pierre Seifert


## 2021-08-12

[AGKD-BML: Defense Against Adversarial Attack by Attention Guided Knowledge Distillation and Bi-directional Metric Learning. (99%)](http://arxiv.org/abs/2108.06017)

Hong Wang, Yuefan Deng, Shinjae Yoo, Haibin Ling, Yuewei Lin


[Deep adversarial attack on target detection systems. (99%)](http://arxiv.org/abs/2108.05948)

Uche M. Osahor, Nasser M. Nasrabadi


[Hatemoji: A Test Suite and Adversarially-Generated Dataset for Benchmarking and Detecting Emoji-based Hate. (69%)](http://arxiv.org/abs/2108.05921)

Hannah Rose Kirk, Bertram Vidgen, Paul Röttger, Tristan Thrush, Scott A. Hale


## 2021-08-11

[Turning Your Strength against You: Detecting and Mitigating Robust and Universal Adversarial Patch Attacks. (99%)](http://arxiv.org/abs/2108.05075)

Zitao Chen, Pritam Dash, Karthik Pattabiraman


[Attacks against Ranking Algorithms with Text Embeddings: a Case Study on Recruitment Algorithms. (78%)](http://arxiv.org/abs/2108.05490)

Anahita Samadi, Debapriya Banerjee, Shirin Nilizadeh


[Are Neural Ranking Models Robust? (4%)](http://arxiv.org/abs/2108.05018)

Chen Wu, Ruqing Zhang, Jiafeng Guo, Yixing Fan, Xueqi Cheng


[Logic Explained Networks. (1%)](http://arxiv.org/abs/2108.05149)

Gabriele Ciravegna, Pietro Barbiero, Francesco Giannini, Marco Gori, Pietro Lió, Marco Maggini, Stefano Melacci


## 2021-08-10

[Simple black-box universal adversarial attacks on medical image classification based on deep neural networks. (99%)](http://arxiv.org/abs/2108.04979)

Kazuki Koga, Kazuhiro Takemoto


[On the Effect of Pruning on Adversarial Robustness. (81%)](http://arxiv.org/abs/2108.04890)

Artur Jordao, Helio Pedrini


[SoK: How Robust is Image Classification Deep Neural Network Watermarking? (Extended Version). (68%)](http://arxiv.org/abs/2108.04974)

Nils Lukas, Edward Jiang, Xinda Li, Florian Kerschbaum


[Perturbing Inputs for Fragile Interpretations in Deep Natural Language Processing. (64%)](http://arxiv.org/abs/2108.04990)

Sanchit Sinha, Hanjie Chen, Arshdeep Sekhon, Yangfeng Ji, Yanjun Qi


[UniNet: A Unified Scene Understanding Network and Exploring Multi-Task Relationships through the Lens of Adversarial Attacks. (2%)](http://arxiv.org/abs/2108.04584)

NareshKumar Gurulingan, Elahe Arani, Bahram Zonooz


[Instance-wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation. (1%)](http://arxiv.org/abs/2108.04547)

Weilun Wang, Wengang Zhou, Jianmin Bao, Dong Chen, Houqiang Li


## 2021-08-09

[Meta Gradient Adversarial Attack. (99%)](http://arxiv.org/abs/2108.04204)

Zheng Yuan, Jie Zhang, Yunpei Jia, Chuanqi Tan, Tao Xue, Shiguang Shan


[On Procedural Adversarial Noise Attack And Defense. (99%)](http://arxiv.org/abs/2108.04409)

Jun Yan, Xiaoyang Deng, Huilin Yin, Wancheng Ge


[Enhancing Knowledge Tracing via Adversarial Training. (98%)](http://arxiv.org/abs/2108.04430)

Xiaopeng Guo, Zhijie Huang, Jie Gao, Mingyu Shang, Maojing Shu, Jun Sun


[Neural Network Repair with Reachability Analysis. (96%)](http://arxiv.org/abs/2108.04214)

Xiaodong Yang, Tom Yamaguchi, Hoang-Dung Tran, Bardh Hoxha, Taylor T Johnson, Danil Prokhorov


[Classification Auto-Encoder based Detector against Diverse Data Poisoning Attacks. (92%)](http://arxiv.org/abs/2108.04206)

Fereshteh Razmi, Li Xiong


[Mis-spoke or mis-lead: Achieving Robustness in Multi-Agent Communicative Reinforcement Learning. (82%)](http://arxiv.org/abs/2108.03803)

Wanqi Xue, Wei Qiu, Bo An, Zinovi Rabinovich, Svetlana Obraztsova, Chai Kiat Yeo


[Privacy-Preserving Machine Learning: Methods, Challenges and Directions. (16%)](http://arxiv.org/abs/2108.04417)

Runhua Xu, Nathalie Baracaldo, James Joshi


[Explainable AI and susceptibility to adversarial attacks: a case study in classification of breast ultrasound images. (15%)](http://arxiv.org/abs/2108.04345)

Hamza Rasaee, Hassan Rivaz


## 2021-08-07

[Jointly Attacking Graph Neural Network and its Explanations. (96%)](http://arxiv.org/abs/2108.03388)

Wenqi Fan, Wei Jin, Xiaorui Liu, Han Xu, Xianfeng Tang, Suhang Wang, Qing Li, Jiliang Tang, Jianping Wang, Charu Aggarwal


[Membership Inference Attacks on Lottery Ticket Networks. (33%)](http://arxiv.org/abs/2108.03506)

Aadesh Bagmar, Shishira R Maiya, Shruti Bidwalka, Amol Deshpande


[Information Bottleneck Approach to Spatial Attention Learning. (1%)](http://arxiv.org/abs/2108.03418)

Qiuxia Lai, Yu Li, Ailing Zeng, Minhao Liu, Hanqiu Sun, Qiang Xu


## 2021-08-06

[Evaluating Adversarial Attacks on Driving Safety in Vision-Based Autonomous Vehicles. (80%)](http://arxiv.org/abs/2108.02940)

Jindi Zhang, Yang Lou, Jianping Wang, Kui Wu, Kejie Lu, Xiaohua Jia


[Ensemble Augmentation for Deep Neural Networks Using 1-D Time Series Vibration Data. (2%)](http://arxiv.org/abs/2108.03288)

Atik Faysal, Ngui Wai Keng, M. H. Lim


## 2021-08-05

[BOSS: Bidirectional One-Shot Synthesis of Adversarial Examples. (99%)](http://arxiv.org/abs/2108.02756)

Ismail Alkhouri, Alvaro Velasquez, George Atia


[Poison Ink: Robust and Invisible Backdoor Attack. (99%)](http://arxiv.org/abs/2108.02488)

Jie Zhang, Dongdong Chen, Jing Liao, Qidong Huang, Gang Hua, Weiming Zhang, Nenghai Yu


[Imperceptible Adversarial Examples by Spatial Chroma-Shift. (99%)](http://arxiv.org/abs/2108.02502)

Ayberk Aydin, Deniz Sen, Berat Tuna Karli, Oguz Hanoglu, Alptekin Temizel


[Householder Activations for Provable Robustness against Adversarial Attacks. (83%)](http://arxiv.org/abs/2108.04062)

Sahil Singla, Surbhi Singla, Soheil Feizi


[Fairness Properties of Face Recognition and Obfuscation Systems. (68%)](http://arxiv.org/abs/2108.02707)

Harrison Rosenberg, Brian Tang, Kassem Fawaz, Somesh Jha


[Exploring Structure Consistency for Deep Model Watermarking. (10%)](http://arxiv.org/abs/2108.02360)

Jie Zhang, Dongdong Chen, Jing Liao, Han Fang, Zehua Ma, Weiming Zhang, Gang Hua, Nenghai Yu


[Locally Interpretable One-Class Anomaly Detection for Credit Card Fraud Detection. (1%)](http://arxiv.org/abs/2108.02501)

Tungyu Wu, Youting Wang


## 2021-08-04

[Robust Transfer Learning with Pretrained Language Models through Adapters. (82%)](http://arxiv.org/abs/2108.02340)

Wenjuan Han, Bo Pang, Yingnian Wu


[Semi-supervised Conditional GAN for Simultaneous Generation and Detection of Phishing URLs: A Game theoretic Perspective. (31%)](http://arxiv.org/abs/2108.01852)

Sharif Amit Kamran, Shamik Sengupta, Alireza Tavakkoli


## 2021-08-03

[On the Robustness of Domain Adaption to Adversarial Attacks. (99%)](http://arxiv.org/abs/2108.01807)

Liyuan Zhang, Yuhang Zhou, Lei Zhang


[On the Exploitability of Audio Machine Learning Pipelines to Surreptitious Adversarial Examples. (99%)](http://arxiv.org/abs/2108.02010)

Adelin Travers, Lorna Licollari, Guanghan Wang, Varun Chandrasekaran, Adam Dziedzic, David Lie, Nicolas Papernot


[AdvRush: Searching for Adversarially Robust Neural Architectures. (99%)](http://arxiv.org/abs/2108.01289)

Jisoo Mok, Byunggook Na, Hyeokjun Choe, Sungroh Yoon


[The Devil is in the GAN: Backdoor Attacks and Defenses in Deep Generative Models. (88%)](http://arxiv.org/abs/2108.01644)

Ambrish Rawat, Killian Levacher, Mathieu Sinn


[DeepFreeze: Cold Boot Attacks and High Fidelity Model Recovery on Commercial EdgeML Device. (69%)](http://arxiv.org/abs/2108.01281)

Yoo-Seung Won, Soham Chatterjee, Dirmanto Jap, Arindam Basu, Shivam Bhasin


[Tutorials on Testing Neural Networks. (1%)](http://arxiv.org/abs/2108.01734)

Nicolas Berthier, Youcheng Sun, Wei Huang, Yanghao Zhang, Wenjie Ruan, Xiaowei Huang


## 2021-08-02

[Hybrid Classical-Quantum Deep Learning Models for Autonomous Vehicle Traffic Image Classification Under Adversarial Attack. (98%)](http://arxiv.org/abs/2108.01125)

Reek Majumder, Sakib Mahmud Khan, Fahim Ahmed, Zadid Khan, Frank Ngeni, Gurcan Comert, Judith Mwakalonge, Dimitra Michalaka, Mashrur Chowdhury


[Adversarial Attacks Against Deep Reinforcement Learning Framework in Internet of Vehicles. (10%)](http://arxiv.org/abs/2108.00833)

Anum Talpur, Mohan Gurusamy


[Information Stealing in Federated Learning Systems Based on Generative Adversarial Networks. (9%)](http://arxiv.org/abs/2108.00701)

Yuwei Sun, Ng Chong, Hideya Ochiai


[Efficacy of Statistical and Artificial Intelligence-based False Information Cyberattack Detection Models for Connected Vehicles. (1%)](http://arxiv.org/abs/2108.01124)

Sakib Mahmud Khan, Gurcan Comert, Mashrur Chowdhury


## 2021-08-01

[Advances in adversarial attacks and defenses in computer vision: A survey. (92%)](http://arxiv.org/abs/2108.00401)

Naveed Akhtar, Ajmal Mian, Navid Kardan, Mubarak Shah


[Certified Defense via Latent Space Randomized Smoothing with Orthogonal Encoders. (80%)](http://arxiv.org/abs/2108.00491)

Huimin Zeng, Jiahao Su, Furong Huang


[An Effective and Robust Detector for Logo Detection. (70%)](http://arxiv.org/abs/2108.00422)

Xiaojun Jia, Huanqian Yan, Yonglin Wu, Xingxing Wei, Xiaochun Cao, Yong Zhang


[Style Curriculum Learning for Robust Medical Image Segmentation. (2%)](http://arxiv.org/abs/2108.00402)

Zhendong Liu, Van Manh, Xin Yang, Xiaoqiong Huang, Karim Lekadir, Víctor Campello, Nishant Ravikumar, Alejandro F Frangi, Dong Ni


## 2021-07-31

[Delving into Deep Image Prior for Adversarial Defense: A Novel Reconstruction-based Defense Framework. (99%)](http://arxiv.org/abs/2108.00180)

Li Ding, Yongwei Wang, Xin Ding, Kaiwen Yuan, Ping Wang, Hua Huang, Z. Jane Wang


[Adversarial Robustness of Deep Code Comment Generation. (99%)](http://arxiv.org/abs/2108.00213)

Yu Zhou, Xiaoqing Zhang, Juanjuan Shen, Tingting Han, Taolue Chen, Harald Gall


[Towards Adversarially Robust and Domain Generalizable Stereo Matching by Rethinking DNN Feature Backbones. (93%)](http://arxiv.org/abs/2108.00335)

Kelvin Cheng, Christopher Healey, Tianfu Wu


[T$_k$ML-AP: Adversarial Attacks to Top-$k$ Multi-Label Learning. (81%)](http://arxiv.org/abs/2108.00146)

Shu Hu, Lipeng Ke, Xin Wang, Siwei Lyu


[BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning. (67%)](http://arxiv.org/abs/2108.00352)

Jinyuan Jia, Yupei Liu, Neil Zhenqiang Gong


[Fair Representation Learning using Interpolation Enabled Disentanglement. (1%)](http://arxiv.org/abs/2108.00295)

Akshita Jha, Bhanukiran Vinzamuri, Chandan K. Reddy


## 2021-07-30

[Who's Afraid of Thomas Bayes? (92%)](http://arxiv.org/abs/2107.14601)

Erick Galinkin


[Practical Attacks on Voice Spoofing Countermeasures. (86%)](http://arxiv.org/abs/2107.14642)

Andre Kassis, Urs Hengartner


[Can You Hear It? Backdoor Attacks via Ultrasonic Triggers. (50%)](http://arxiv.org/abs/2107.14569)

Stefanos Koffas, Jing Xu, Mauro Conti, Stjepan Picek


[Unveiling the potential of Graph Neural Networks for robust Intrusion Detection. (13%)](http://arxiv.org/abs/2107.14756)

David Pujol-Perich, José Suárez-Varela, Albert Cabellos-Aparicio, Pere Barlet-Ros


## 2021-07-29

[Feature Importance-aware Transferable Adversarial Attacks. (99%)](http://arxiv.org/abs/2107.14185)

Zhibo Wang, Hengchang Guo, Zhifei Zhang, Wenxin Liu, Zhan Qin, Kui Ren


[Enhancing Adversarial Robustness via Test-time Transformation Ensembling. (98%)](http://arxiv.org/abs/2107.14110)

Juan C. Pérez, Motasem Alfarra, Guillaume Jeanneret, Laura Rueda, Ali Thabet, Bernard Ghanem, Pablo Arbeláez


[The Robustness of Graph k-shell Structure under Adversarial Attacks. (93%)](http://arxiv.org/abs/2107.13962)

B. Zhou, Y. Q. Lv, Y. C. Mao, J. H. Wang, S. Q. Yu, Q. Xuan


[Understanding the Effects of Adversarial Personalized Ranking Optimization Method on Recommendation Quality. (31%)](http://arxiv.org/abs/2107.13876)

Vito Walter Anelli, Yashar Deldjoo, Noia Tommaso Di, Felice Antonio Merra


[Towards robust vision by multi-task learning on monkey visual cortex. (3%)](http://arxiv.org/abs/2107.14344)

Shahd Safarani, Arne Nix, Konstantin Willeke, Santiago A. Cadena, Kelli Restivo, George Denfield, Andreas S. Tolias, Fabian H. Sinz


## 2021-07-28

[Imbalanced Adversarial Training with Reweighting. (86%)](http://arxiv.org/abs/2107.13639)

Wentao Wang, Han Xu, Xiaorui Liu, Yaxin Li, Bhavani Thuraisingham, Jiliang Tang


[Towards Robustness Against Natural Language Word Substitutions. (73%)](http://arxiv.org/abs/2107.13541)

Xinshuai Dong, Anh Tuan Luu, Rongrong Ji, Hong Liu


[Models of Computational Profiles to Study the Likelihood of DNN Metamorphic Test Cases. (67%)](http://arxiv.org/abs/2107.13491)

Ettore Merlo, Mira Marhaba, Foutse Khomh, Houssem Ben Braiek, Giuliano Antoniol


[WaveCNet: Wavelet Integrated CNNs to Suppress Aliasing Effect for Noise-Robust Image Classification. (15%)](http://arxiv.org/abs/2107.13335)

Qiufu Li, Linlin Shen, Sheng Guo, Zhihui Lai


[TableGAN-MCA: Evaluating Membership Collisions of GAN-Synthesized Tabular Data Releasing. (2%)](http://arxiv.org/abs/2107.13190)

Aoting Hu, Renjie Xie, Zhigang Lu, Aiqun Hu, Minhui Xue


## 2021-07-27

[Towards Black-box Attacks on Deep Learning Apps. (89%)](http://arxiv.org/abs/2107.12732)

Hongchen Cao, Shuai Li, Yuming Zhou, Ming Fan, Xuejiao Zhao, Yutian Tang


[Poisoning Online Learning Filters: DDoS Attacks and Countermeasures. (50%)](http://arxiv.org/abs/2107.12612)

Wesley Joon-Wie Tann, Ee-Chien Chang


[PDF-Malware: An Overview on Threats, Detection and Evasion Attacks. (8%)](http://arxiv.org/abs/2107.12873)

Nicolas Fleury, Theo Dubrunquez, Ihsen Alouani


## 2021-07-26

[Benign Adversarial Attack: Tricking Models for Goodness. (99%)](http://arxiv.org/abs/2107.11986)

Jitao Sang, Xian Zhao, Jiaming Zhang, Zhiyu Lin


[Learning to Adversarially Blur Visual Object Tracking. (98%)](http://arxiv.org/abs/2107.12085)

Qing Guo, Ziyi Cheng, Felix Juefei-Xu, Lei Ma, Xiaofei Xie, Yang Liu, Jianjun Zhao


[Adversarial Attacks with Time-Scale Representations. (96%)](http://arxiv.org/abs/2107.12473)

Alberto Santamaria-Pang, Jianwei Qiu, Aritra Chowdhury, James Kubricht, Peter Tu, Iyer Naresh, Nurali Virani


## 2021-07-24

[Adversarial training may be a double-edged sword. (99%)](http://arxiv.org/abs/2107.11671)

Ali Rahmati, Seyed-Mohsen Moosavi-Dezfooli, Huaiyu Dai


[Detecting Adversarial Examples Is (Nearly) As Hard As Classifying Them. (98%)](http://arxiv.org/abs/2107.11630)

Florian Tramèr


[Stress Test Evaluation of Biomedical Word Embeddings. (73%)](http://arxiv.org/abs/2107.11652)

Vladimir Araujo, Andrés Carvallo, Carlos Aspillaga, Camilo Thorne, Denis Parra


[X-GGM: Graph Generative Modeling for Out-of-Distribution Generalization in Visual Question Answering. (1%)](http://arxiv.org/abs/2107.11576)

Jingjing Jiang, Ziyi Liu, Yifan Liu, Zhixiong Nan, Nanning Zheng


## 2021-07-23

[A Differentiable Language Model Adversarial Attack on Text Classifiers. (99%)](http://arxiv.org/abs/2107.11275)

Ivan Fursov, Alexey Zaytsev, Pavel Burnyshev, Ekaterina Dmitrieva, Nikita Klyuchnikov, Andrey Kravchenko, Ekaterina Artemova, Evgeny Burnaev


[Structack: Structure-based Adversarial Attacks on Graph Neural Networks. (86%)](http://arxiv.org/abs/2107.11327)

Hussain Hussain, Tomislav Duricic, Elisabeth Lex, Denis Helic, Markus Strohmaier, Roman Kern


[Adversarial Reinforced Instruction Attacker for Robust Vision-Language Navigation. (45%)](http://arxiv.org/abs/2107.11252)

Bingqian Lin, Yi Zhu, Yanxin Long, Xiaodan Liang, Qixiang Ye, Liang Lin


[Clipped Hyperbolic Classifiers Are Super-Hyperbolic Classifiers. (8%)](http://arxiv.org/abs/2107.11472)

Yunhui Guo, Xudong Wang, Yubei Chen, Stella X. Yu


## 2021-07-22

[On the Certified Robustness for Ensemble Models and Beyond. (99%)](http://arxiv.org/abs/2107.10873)

Zhuolin Yang, Linyi Li, Xiaojun Xu, Bhavya Kailkhura, Tao Xie, Bo Li


[Unsupervised Detection of Adversarial Examples with Model Explanations. (99%)](http://arxiv.org/abs/2107.10480)

Gihyuk Ko, Gyumin Lim


[Membership Inference Attack and Defense for Wireless Signal Classifiers with Deep Learning. (83%)](http://arxiv.org/abs/2107.12173)

Yi Shi, Yalin E. Sagduyu


[Towards Explaining Adversarial Examples Phenomenon in Artificial Neural Networks. (75%)](http://arxiv.org/abs/2107.10599)

Ramin Barati, Reza Safabakhsh, Mohammad Rahmati


[Estimating Predictive Uncertainty Under Program Data Distribution Shift. (1%)](http://arxiv.org/abs/2107.10989)

Yufei Li, Simin Chen, Wei Yang


[Ready for Emerging Threats to Recommender Systems? A Graph Convolution-based Generative Shilling Attack. (1%)](http://arxiv.org/abs/2107.10457)

Fan Wu, Min Gao, Junliang Yu, Zongwei Wang, Kecheng Liu, Xu Wange


## 2021-07-21

[Fast and Scalable Adversarial Training of Kernel SVM via Doubly Stochastic Gradients. (98%)](http://arxiv.org/abs/2107.09937)

Huimin Wu, Zhengmian Hu, Bin Gu


[Improved Text Classification via Contrastive Adversarial Training. (84%)](http://arxiv.org/abs/2107.10137)

Lin Pan, Chung-Wei Hang, Avirup Sil, Saloni Potdar


[Black-box Probe for Unsupervised Domain Adaptation without Model Transferring. (81%)](http://arxiv.org/abs/2107.10174)

Kunhong Wu, Yucheng Shi, Yahong Han, Yunfeng Shao, Bingshuai Li


[Defending against Reconstruction Attack in Vertical Federated Learning. (10%)](http://arxiv.org/abs/2107.09898)

Jiankai Sun, Yuanshun Yao, Weihao Gao, Junyuan Xie, Chong Wang


[Generative Models for Security: Attacks, Defenses, and Opportunities. (10%)](http://arxiv.org/abs/2107.10139)

Luke A. Bauer, Vincent Bindschaedler


[A Tandem Framework Balancing Privacy and Security for Voice User Interfaces. (5%)](http://arxiv.org/abs/2107.10045)

Ranya Aloufi, Hamed Haddadi, David Boyle


[Spinning Sequence-to-Sequence Models with Meta-Backdoors. (4%)](http://arxiv.org/abs/2107.10443)

Eugene Bagdasaryan, Vitaly Shmatikov


[On the Convergence of Prior-Guided Zeroth-Order Optimization Algorithms. (2%)](http://arxiv.org/abs/2107.10110)

Shuyu Cheng, Guoqiang Wu, Jun Zhu


## 2021-07-20

[Using Undervolting as an On-Device Defense Against Adversarial Machine Learning Attacks. (99%)](http://arxiv.org/abs/2107.09804)

Saikat Majumdar, Mohammad Hossein Samavatian, Kristin Barber, Radu Teodorescu


[A Markov Game Model for AI-based Cyber Security Attack Mitigation. (10%)](http://arxiv.org/abs/2107.09258)

Hooman Alavizadeh, Julian Jang-Jaccard, Tansu Alpcan, Seyit A. Camtepe


[Leaking Secrets through Modern Branch Predictor in the Speculative World. (1%)](http://arxiv.org/abs/2107.09833)

Md Hafizul Islam Chowdhuryy, Fan Yao


## 2021-07-19

[Discriminator-Free Generative Adversarial Attack. (99%)](http://arxiv.org/abs/2107.09225)

Shaohao Lu, Yuqiao Xian, Ke Yan, Yi Hu, Xing Sun, Xiaowei Guo, Feiyue Huang, Wei-Shi Zheng


[Feature-Filter: Detecting Adversarial Examples through Filtering off Recessive Features. (99%)](http://arxiv.org/abs/2107.09502)

Hui Liu, Bo Zhao, Yuefeng Peng, Jiabao Guo, Peng Liu


[Examining the Human Perceptibility of Black-Box Adversarial Attacks on Face Recognition. (98%)](http://arxiv.org/abs/2107.09126)

Benjamin Spetter-Goldstein, Nataniel Ruiz, Sarah Adel Bargal


[On the Veracity of Local, Model-agnostic Explanations in Audio Classification: Targeted Investigations with Adversarial Examples. (80%)](http://arxiv.org/abs/2107.09045)

Verena Praher, Katharina Prinz, Arthur Flexer, Gerhard Widmer


[MEGEX: Data-Free Model Extraction Attack against Gradient-Based Explainable AI. (33%)](http://arxiv.org/abs/2107.08909)

Takayuki Miura, Satoshi Hasegawa, Toshiki Shibahara


[Structural Watermarking to Deep Neural Networks via Network Channel Pruning. (11%)](http://arxiv.org/abs/2107.08688)

Xiangyu Zhao, Yinzhe Yao, Hanzhou Wu, Xinpeng Zhang


[Generative Adversarial Neural Cellular Automata. (1%)](http://arxiv.org/abs/2108.04328)

Maximilian Otte, Quentin Delfosse, Johannes Czech, Kristian Kersting


[Improving Interpretability of Deep Neural Networks in Medical Diagnosis by Investigating the Individual Units. (1%)](http://arxiv.org/abs/2107.08767)

Woo-Jeoung Nam, Seong-Whan Lee


[Just Train Twice: Improving Group Robustness without Training Group Information. (1%)](http://arxiv.org/abs/2107.09044)

Evan Zheran Liu, Behzad Haghgoo, Annie S. Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, Chelsea Finn


## 2021-07-18

[RobustFed: A Truth Inference Approach for Robust Federated Learning. (1%)](http://arxiv.org/abs/2107.08402)

Farnaz Tahmasebian, Jian Lou, Li Xiong


## 2021-07-17

[BEDS-Bench: Behavior of EHR-models under Distributional Shift--A Benchmark. (9%)](http://arxiv.org/abs/2107.08189)

Anand Avati, Martin Seneviratne, Emily Xue, Zhen Xu, Balaji Lakshminarayanan, Andrew M. Dai


## 2021-07-16

[EGC2: Enhanced Graph Classification with Easy Graph Compression. (89%)](http://arxiv.org/abs/2107.07737)

Jinyin Chen, Haiyang Xiong, Haibin Zhenga, Dunjie Zhang, Jian Zhang, Mingwei Jia, Yi Liu


[Proceedings of ICML 2021 Workshop on Theoretic Foundation, Criticism, and Application Trend of Explainable AI. (1%)](http://arxiv.org/abs/2107.08821)

Quanshi Zhang, Tian Han, Lixin Fan, Zhanxing Zhu, Hang Su, Ying Nian Wu, Jie Ren, Hao Zhang


## 2021-07-15

[Self-Supervised Contrastive Learning with Adversarial Perturbations for Defending Word Substitution-based Attacks. (99%)](http://arxiv.org/abs/2107.07610)

Zhao Meng, Yihan Dong, Mrinmaya Sachan, Roger Wattenhofer


[Adversarial Attacks on Multi-task Visual Perception for Autonomous Driving. (98%)](http://arxiv.org/abs/2107.07449)

Ibrahim Sobh, Ahmed Hamed, Varun Ravi Kumar, Senthil Yogamani


[ECG-Adv-GAN: Detecting ECG Adversarial Examples with Conditional Generative Adversarial Networks. (92%)](http://arxiv.org/abs/2107.07677)

Khondker Fariha Hossain, Sharif Amit Kamran, Alireza Tavakkoli, Lei Pan, Xingjun Ma, Sutharshan Rajasegarar, Chandan Karmaker


[Adversarial Attack for Uncertainty Estimation: Identifying Critical Regions in Neural Networks. (80%)](http://arxiv.org/abs/2107.07618)

Ismail Alarab, Simant Prakoonwit


[Subnet Replacement: Deployment-stage backdoor attack against deep neural networks in gray-box setting. (16%)](http://arxiv.org/abs/2107.07240)

Xiangyu Qi, Jifeng Zhu, Chulin Xie, Yong Yang


[Tailor: Generating and Perturbing Text with Semantic Controls. (3%)](http://arxiv.org/abs/2107.07150)

Alexis Ross, Tongshuang Wu, Hao Peng, Matthew E. Peters, Matt Gardner


[Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks. (1%)](http://arxiv.org/abs/2107.07455)

Andrey Malinin, Neil Band, Ganshin, Alexander, German Chesnokov, Yarin Gal, Mark J. F. Gales, Alexey Noskov, Andrey Ploskonosov, Liudmila Prokhorenkova, Ivan Provilkov, Vatsal Raina, Vyas Raina, Roginskiy, Denis, Mariya Shmatova, Panos Tigas, Boris Yangel


## 2021-07-14

[AdvFilter: Predictive Perturbation-aware Filtering against Adversarial Attack via Multi-domain Learning. (99%)](http://arxiv.org/abs/2107.06501)

Yihao Huang, Qing Guo, Felix Juefei-Xu, Lei Ma, Weikai Miao, Yang Liu, Geguang Pu


[Conservative Objective Models for Effective Offline Model-Based Optimization. (67%)](http://arxiv.org/abs/2107.06882)

Brandon Trabucco, Aviral Kumar, Xinyang Geng, Sergey Levine


## 2021-07-13

[AID-Purifier: A Light Auxiliary Network for Boosting Adversarial Defense. (88%)](http://arxiv.org/abs/2107.06456)

Duhun Hwang, Eunjung Lee, Wonjong Rhee


[Using BERT Encoding to Tackle the Mad-lib Attack in SMS Spam Detection. (69%)](http://arxiv.org/abs/2107.06400)

Sergio Rojas-Galeano


[Correlation Analysis between the Robustness of Sparse Neural Networks and their Random Hidden Structural Priors. (41%)](http://arxiv.org/abs/2107.06158)

M. Ben Amor, J. Stier, M. Granitzer


[What classifiers know what they don't? (1%)](http://arxiv.org/abs/2107.06217)

Mohamed Ishmael Belghazi, David Lopez-Paz


## 2021-07-12

[EvoBA: An Evolution Strategy as a Strong Baseline forBlack-Box Adversarial Attacks. (99%)](http://arxiv.org/abs/2107.05754)

Andrei Ilie, Marius Popescu, Alin Stefanescu


[Detect and Defense Against Adversarial Examples in Deep Learning using Natural Scene Statistics and Adaptive Denoising. (99%)](http://arxiv.org/abs/2107.05780)

Anouar Kherchouche, Sid Ahmed Fezza, Wassim Hamidouche


[Perceptual-based deep-learning denoiser as a defense against adversarial attacks on ASR systems. (96%)](http://arxiv.org/abs/2107.05222)

Anirudh Sreeram, Nicholas Mehlman, Raghuveer Peri, Dillon Knox, Shrikanth Narayanan


[Putting words into the system's mouth: A targeted attack on neural machine translation using monolingual data poisoning. (81%)](http://arxiv.org/abs/2107.05243)

Jun Wang, Chang Xu, Francisco Guzman, Ahmed El-Kishky, Yuqing Tang, Benjamin I. P. Rubinstein, Trevor Cohn


[A Closer Look at the Adversarial Robustness of Information Bottleneck Models. (70%)](http://arxiv.org/abs/2107.05712)

Iryna Korshunova, David Stutz, Alexander A. Alemi, Olivia Wiles, Sven Gowal


[SoftHebb: Bayesian inference in unsupervised Hebbian soft winner-take-all networks. (56%)](http://arxiv.org/abs/2107.05747)

Timoleon Moraitis, Dmitry Toichkin, Yansong Chua, Qinghai Guo


## 2021-07-11

[Adversarial for Good? How the Adversarial ML Community's Values Impede Socially Beneficial Uses of Attacks. (76%)](http://arxiv.org/abs/2107.10302)

Kendra Albert, Maggie Delano, Bogdan Kulynych, Ram Shankar Siva Kumar


[Stateful Detection of Model Extraction Attacks. (2%)](http://arxiv.org/abs/2107.05166)

Soham Pal, Yash Gupta, Aditya Kanade, Shirish Shevade


[Attack Rules: An Adversarial Approach to Generate Attacks for Industrial Control Systems using Machine Learning. (1%)](http://arxiv.org/abs/2107.05127)

Muhammad Azmi Umer, Chuadhry Mujeeb Ahmed, Muhammad Taha Jilani, Aditya P. Mathur


## 2021-07-10

[Hack The Box: Fooling Deep Learning Abstraction-Based Monitors. (91%)](http://arxiv.org/abs/2107.04764)

Sara Hajj Ibrahim, Mohamed Nassar


[HOMRS: High Order Metamorphic Relations Selector for Deep Neural Networks. (88%)](http://arxiv.org/abs/2107.04863)

Florian Tambon, Giulio Antoniol, Foutse Khomh


[Identifying Layers Susceptible to Adversarial Attacks. (83%)](http://arxiv.org/abs/2107.04827)

Shoaib Ahmed Siddiqui, Thomas Breuel


[Out of Distribution Detection and Adversarial Attacks on Deep Neural Networks for Robust Medical Image Analysis. (22%)](http://arxiv.org/abs/2107.04882)

Anisie Uwimana1, Ransalu Senanayake


[Cyber-Security Challenges in Aviation Industry: A Review of Current and Future Trends. (1%)](http://arxiv.org/abs/2107.04910)

Elochukwu Ukwandu, Mohamed Amine Ben Farah, Hanan Hindy, Miroslav Bures, Robert Atkinson, Christos Tachtatzis, Xavier Bellekens


## 2021-07-09

[Learning to Detect Adversarial Examples Based on Class Scores. (99%)](http://arxiv.org/abs/2107.04435)

Tobias Uelwer, Felix Michels, Candido Oliver De


[Resilience of Autonomous Vehicle Object Category Detection to Universal Adversarial Perturbations. (99%)](http://arxiv.org/abs/2107.04749)

Mohammad Nayeem Teli, Seungwon Oh


[Universal 3-Dimensional Perturbations for Black-Box Attacks on Video Recognition Systems. (99%)](http://arxiv.org/abs/2107.04284)

Shangyu Xie, Han Wang, Yu Kong, Yuan Hong


[GGT: Graph-Guided Testing for Adversarial Sample Detection of Deep Neural Network. (98%)](http://arxiv.org/abs/2107.07043)

Zuohui Chen, Renxuan Wang, Jingyang Xiang, Yue Yu, Xin Xia, Shouling Ji, Qi Xuan, Xiaoniu Yang


[Towards Robust General Medical Image Segmentation. (83%)](http://arxiv.org/abs/2107.04263)

Laura Daza, Juan C. Pérez, Pablo Arbeláez


[ARC: Adversarially Robust Control Policies for Autonomous Vehicles. (38%)](http://arxiv.org/abs/2107.04487)

Sampo Kuutti, Saber Fallah, Richard Bowden


## 2021-07-08

[Output Randomization: A Novel Defense for both White-box and Black-box Adversarial Models. (99%)](http://arxiv.org/abs/2107.03806)

Daniel Park, Haidar Khan, Azer Khan, Alex Gittens, Bülent Yener


[Improving Model Robustness with Latent Distribution Locally and Globally. (99%)](http://arxiv.org/abs/2107.04401)

Zhuang Qian, Shufei Zhang, Kaizhu Huang, Qiufeng Wang, Rui Zhang, Xinping Yi


[Analytically Tractable Hidden-States Inference in Bayesian Neural Networks. (50%)](http://arxiv.org/abs/2107.03759)

Luong-Ha Nguyen, James-A. Goulet


[Understanding the Limits of Unsupervised Domain Adaptation via Data Poisoning. (33%)](http://arxiv.org/abs/2107.03919)

Akshay Mehra, Bhavya Kailkhura, Pin-Yu Chen, Jihun Hamm


## 2021-07-07

[Controlled Caption Generation for Images Through Adversarial Attacks. (99%)](http://arxiv.org/abs/2107.03050)

Nayyer Aafaq, Naveed Akhtar, Wei Liu, Mubarak Shah, Ajmal Mian


[Incorporating Label Uncertainty in Understanding Adversarial Robustness. (38%)](http://arxiv.org/abs/2107.03250)

Xiao Zhang, David Evans


[RoFL: Attestable Robustness for Secure Federated Learning. (2%)](http://arxiv.org/abs/2107.03311)

Lukas Burkhalter, Hidde Lycklama, Alexander Viand, Nicolas Küchler, Anwar Hithnawi


## 2021-07-06

[GradDiv: Adversarial Robustness of Randomized Neural Networks via Gradient Diversity Regularization. (99%)](http://arxiv.org/abs/2107.02425)

Sungyoon Lee, Hoki Kim, Jaewook Lee


[Self-Adversarial Training incorporating Forgery Attention for Image Forgery Localization. (95%)](http://arxiv.org/abs/2107.02434)

Long Zhuo, Shunquan Tan, Bin Li, Jiwu Huang


[ROPUST: Improving Robustness through Fine-tuning with Photonic Processors and Synthetic Gradients. (76%)](http://arxiv.org/abs/2108.04217)

Alessandro Cappelli, Julien Launay, Laurent Meunier, Ruben Ohana, Iacopo Poli


[On Generalization of Graph Autoencoders with Adversarial Training. (12%)](http://arxiv.org/abs/2107.02658)

Tianjin huang, Yulong Pei, Vlado Menkovski, Mykola Pechenizkiy


[On Robustness of Lane Detection Models to Physical-World Adversarial Attacks in Autonomous Driving. (1%)](http://arxiv.org/abs/2107.02488)

Takami Sato, Qi Alfred Chen


## 2021-07-05

[When and How to Fool Explainable Models (and Humans) with Adversarial Examples. (99%)](http://arxiv.org/abs/2107.01943)

Jon Vadillo, Roberto Santana, Jose A. Lozano


[Boosting Transferability of Targeted Adversarial Examples via Hierarchical Generative Networks. (99%)](http://arxiv.org/abs/2107.01809)

Xiao Yang, Yinpeng Dong, Tianyu Pang, Hang Su, Jun Zhu


[Adversarial Robustness of Probabilistic Network Embedding for Link Prediction. (87%)](http://arxiv.org/abs/2107.01936)

Xi Chen, Bo Kang, Jefrey Lijffijt, Bie Tijl De


[Dealing with Adversarial Player Strategies in the Neural Network Game iNNk through Ensemble Learning. (69%)](http://arxiv.org/abs/2107.02052)

Mathias Löwe, Jennifer Villareale, Evan Freed, Aleksanteri Sladek, Jichen Zhu, Sebastian Risi


[Understanding the Security of Deepfake Detection. (33%)](http://arxiv.org/abs/2107.02045)

Xiaoyu Cao, Neil Zhenqiang Gong


[Evaluating the Cybersecurity Risk of Real World, Machine Learning Production Systems. (15%)](http://arxiv.org/abs/2107.01806)

Ron Bitton, Nadav Maman, Inderjeet Singh, Satoru Momiyama, Yuval Elovici, Asaf Shabtai


[Poisoning Attack against Estimating from Pairwise Comparisons. (15%)](http://arxiv.org/abs/2107.01854)

Ke Ma, Qianqian Xu, Jinshan Zeng, Xiaochun Cao, Qingming Huang


[Confidence Conditioned Knowledge Distillation. (10%)](http://arxiv.org/abs/2107.06993)

Sourav Mishra, Suresh Sundaram


## 2021-07-04

[Certifiably Robust Interpretation via Renyi Differential Privacy. (67%)](http://arxiv.org/abs/2107.01561)

Ao Liu, Xiaoyu Chen, Sijia Liu, Lirong Xia, Chuang Gan


[Mirror Mirror on the Wall: Next-Generation Wireless Jamming Attacks Based on Software-Controlled Surfaces. (1%)](http://arxiv.org/abs/2107.01709)

Paul Staat, Harald Elders-Boll, Christian Zenger, Christof Paar


## 2021-07-03

[Demiguise Attack: Crafting Invisible Semantic Adversarial Perturbations with Perceptual Similarity. (99%)](http://arxiv.org/abs/2107.01396)

Yajie Wang, Shangbo Wu, Wenyi Jiang, Shengang Hao, Yu-an Tan, Quanxin Zhang


## 2021-07-01

[Using Anomaly Feature Vectors for Detecting, Classifying and Warning of Outlier Adversarial Examples. (99%)](http://arxiv.org/abs/2107.00561)

Nelson Manohar-Alers, Ryan Feng, Sahib Singh, Jiguo Song, Atul Prakash


[DVS-Attacks: Adversarial Attacks on Dynamic Vision Sensors for Spiking Neural Networks. (99%)](http://arxiv.org/abs/2107.00415)

Alberto Marchisio, Giacomo Pira, Maurizio Martina, Guido Masera, Muhammad Shafique


[CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding. (68%)](http://arxiv.org/abs/2107.00440)

Dong Wang, Ning Ding, Piji Li, Hai-Tao Zheng


[Adversarial Sample Detection for Speaker Verification by Neural Vocoders. (41%)](http://arxiv.org/abs/2107.00309)

Haibin Wu, Po-chun Hsu, Ji Gao, Shanshan Zhang, Shen Huang, Jian Kang, Zhiyong Wu, Helen Meng, Hung-yi Lee


[The Interplay between Distribution Parameters and the Accuracy-Robustness Tradeoff in Classification. (16%)](http://arxiv.org/abs/2107.00247)

Alireza Mousavi Hosseini, Amir Mohammad Abouei, Mohammad Hossein Rohban


[Reinforcement Learning for Feedback-Enabled Cyber Resilience. (10%)](http://arxiv.org/abs/2107.00783)

Yunhan Huang, Linan Huang, Quanyan Zhu


## 2021-06-30

[Single-Step Adversarial Training for Semantic Segmentation. (96%)](http://arxiv.org/abs/2106.15998)

Daniel Wiens, Barbara Hammer


[Adversarial examples within the training distribution: A widespread challenge. (93%)](http://arxiv.org/abs/2106.16198)

Spandan Madan, Tomotake Sasaki, Hanspeter Pfister, Tzu-Mao Li, Xavier Boix


[Understanding Adversarial Attacks on Observations in Deep Reinforcement Learning. (84%)](http://arxiv.org/abs/2106.15860)

You Qiaoben, Chengyang Ying, Xinning Zhou, Hang Su, Jun Zhu, Bo Zhang


[Explanation-Guided Diagnosis of Machine Learning Evasion Attacks. (82%)](http://arxiv.org/abs/2106.15820)

Abderrahmen Amich, Birhanu Eshete


[Bi-Level Poisoning Attack Model and Countermeasure for Appliance Consumption Data of Smart Homes. (8%)](http://arxiv.org/abs/2107.02897)

Mustain Billah, Adnan Anwar, Ziaur Rahman, Syed Md. Galib


[Exploring Robustness of Neural Networks through Graph Measures. (8%)](http://arxiv.org/abs/2106.15850)

Asim Rowan University Waqas, Ghulam Rowan University Rasool, Hamza University of Minnesota Farooq, Nidhal C. Rowan University Bouaynaya


[A Context-Aware Information-Based Clone Node Attack Detection Scheme in Internet of Things. (1%)](http://arxiv.org/abs/2106.15890)

Khizar Hameed, Saurabh Garg, Muhammad Bilal Amin, Byeong Kang, Abid Khan


[Understanding and Improving Early Stopping for Learning with Noisy Labels. (1%)](http://arxiv.org/abs/2106.15853)

Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong Li, Yinian Mao, Gang Niu, Tongliang Liu


## 2021-06-29

[Adversarial Machine Learning for Cybersecurity and Computer Vision: Current Developments and Challenges. (99%)](http://arxiv.org/abs/2107.02894)

Bowei Xi


[Understanding Adversarial Examples Through Deep Neural Network's Response Surface and Uncertainty Regions. (99%)](http://arxiv.org/abs/2107.00003)

Juan Shu, Bowei Xi, Charles Kamhoua


[Attack Transferability Characterization for Adversarially Robust Multi-label Classification. (99%)](http://arxiv.org/abs/2106.15360)

Zhuo Yang, Yufei Han, Xiangliang Zhang


[Inconspicuous Adversarial Patches for Fooling Image Recognition Systems on Mobile Devices. (99%)](http://arxiv.org/abs/2106.15202)

Tao Bai, Jinqi Luo, Jun Zhao


[Bio-Inspired Adversarial Attack Against Deep Neural Networks. (98%)](http://arxiv.org/abs/2107.02895)

Bowei Xi, Yujie Chen, Fan Fei, Zhan Tu, Xinyan Deng


[Do Not Deceive Your Employer with a Virtual Background: A Video Conferencing Manipulation-Detection System. (62%)](http://arxiv.org/abs/2106.15130)

Mauro Conti, Simone Milani, Ehsan Nowroozi, Gabriele Orazi


[The Threat of Offensive AI to Organizations. (54%)](http://arxiv.org/abs/2106.15764)

Yisroel Mirsky, Ambra Demontis, Jaidip Kotak, Ram Shankar, Deng Gelei, Liu Yang, Xiangyu Zhang, Wenke Lee, Yuval Elovici, Battista Biggio


[Local Reweighting for Adversarial Training. (22%)](http://arxiv.org/abs/2106.15776)

Ruize Gao, Feng Liu, Kaiwen Zhou, Gang Niu, Bo Han, James Cheng


[On the Interaction of Belief Bias and Explanations. (15%)](http://arxiv.org/abs/2106.15355)

Ana Valeria Gonzalez, Anna Rogers, Anders Søgaard


## 2021-06-28

[Feature Importance Guided Attack: A Model Agnostic Adversarial Attack. (99%)](http://arxiv.org/abs/2106.14815)

Gilad Gressel, Niranjan Hegde, Archana Sreekumar, Michael Darling


[Evading Adversarial Example Detection Defenses with Orthogonal Projected Gradient Descent. (99%)](http://arxiv.org/abs/2106.15023)

Oliver Bryniarski, Nabeel Hingun, Pedro Pachuca, Vincent Wang, Nicholas Carlini


[Improving Transferability of Adversarial Patches on Face Recognition with Generative Models. (99%)](http://arxiv.org/abs/2106.15058)

Zihao Xiao, Xianfeng Gao, Chilin Fu, Yinpeng Dong, Wei Gao, Xiaolu Zhang, Jun Zhou, Jun Zhu


[Data Poisoning Won't Save You From Facial Recognition. (97%)](http://arxiv.org/abs/2106.14851)

Evani Radiya-Dixit, Florian Tramèr


[Adversarial Robustness of Streaming Algorithms through Importance Sampling. (61%)](http://arxiv.org/abs/2106.14952)

Vladimir Braverman, Avinatan Hassidim, Yossi Matias, Mariano Schain, Sandeep Silwal, Samson Zhou


[Test-Time Adaptation to Distribution Shift by Confidence Maximization and Input Transformation. (2%)](http://arxiv.org/abs/2106.14999)

Chaithanya Kumar Mummadi, Robin Hutmacher, Kilian Rambach, Evgeny Levinkov, Thomas Brox, Jan Hendrik Metzen


[Certified Robustness via Randomized Smoothing over Multiplicative Parameters. (1%)](http://arxiv.org/abs/2106.14432)

Nikita Muravev, Aleksandr Petiushko


[Realtime Robust Malicious Traffic Detection via Frequency Domain Analysis. (1%)](http://arxiv.org/abs/2106.14707)

Chuanpu Fu, Qi Li, Meng Shen, Ke Xu


## 2021-06-27

[RAILS: A Robust Adversarial Immune-inspired Learning System. (98%)](http://arxiv.org/abs/2107.02840)

Ren Wang, Tianqi Chen, Stephen Lindsly, Cooper Stansbury, Alnawaz Rehemtulla, Indika Rajapakse, Alfred Hero


[Who is Responsible for Adversarial Defense? (93%)](http://arxiv.org/abs/2106.14152)

Kishor Datta Gupta, Dipankar Dasgupta


[ASK: Adversarial Soft k-Nearest Neighbor Attack and Defense. (82%)](http://arxiv.org/abs/2106.14300)

Ren Wang, Tianqi Chen, Philip Yao, Sijia Liu, Indika Rajapakse, Alfred Hero


[Immuno-mimetic Deep Neural Networks (Immuno-Net). (64%)](http://arxiv.org/abs/2107.02842)

Ren Wang, Tianqi Chen, Stephen Lindsly, Cooper Stansbury, Indika Rajapakse, Alfred Hero


[Stabilizing Equilibrium Models by Jacobian Regularization. (1%)](http://arxiv.org/abs/2106.14342)

Shaojie Bai, Vladlen Koltun, J. Zico Kolter


## 2021-06-26

[Multi-stage Optimization based Adversarial Training. (99%)](http://arxiv.org/abs/2106.15357)

Xiaosen Wang, Chuanbiao Song, Liwei Wang, Kun He


[The Feasibility and Inevitability of Stealth Attacks. (69%)](http://arxiv.org/abs/2106.13997)

Ivan Y. Tyukin, Desmond J. Higham, Eliyas Woldegeorgis, Alexander N. Gorban


## 2021-06-24

[On the (Un-)Avoidability of Adversarial Examples. (99%)](http://arxiv.org/abs/2106.13326)

Sadia Chowdhury, Ruth Urner


[Countering Adversarial Examples: Combining Input Transformation and Noisy Training. (99%)](http://arxiv.org/abs/2106.13394)

Cheng Zhang, Pan Gao


[Break it, Fix it: Attack and Defense for "Add-on'' Access Control Solutions in Distributed Data Analytics Platforms. (8%)](http://arxiv.org/abs/2106.13123)

Fahad Data Security Technologies Shaon, Sazzadur University of Arizona Rahaman, Murat Data Security Technologies Kantarcioglu


## 2021-06-23

[Adversarial Examples in Multi-Layer Random ReLU Networks. (81%)](http://arxiv.org/abs/2106.12611)

Peter L. Bartlett, Sébastien Bubeck, Yeshwanth Cherapanamjeri


[Teacher Model Fingerprinting Attacks Against Transfer Learning. (2%)](http://arxiv.org/abs/2106.12478)

Yufei Chen, Chao Shen, Cong Wang, Yang Zhang


[Meaningfully Explaining Model Mistakes Using Conceptual Counterfactuals. (1%)](http://arxiv.org/abs/2106.12723)

Abubakar Abid, Mert Yuksekgonul, James Zou


[Feature Attributions and Counterfactual Explanations Can Be Manipulated. (1%)](http://arxiv.org/abs/2106.12563)

Dylan Slack, Sophie Hilgard, Sameer Singh, Himabindu Lakkaraju


## 2021-06-22

[DetectX -- Adversarial Input Detection using Current Signatures in Memristive XBar Arrays. (99%)](http://arxiv.org/abs/2106.12021)

Abhishek Moitra, Priyadarshini Panda


[Self-Supervised Iterative Contextual Smoothing for Efficient Adversarial Defense against Gray- and Black-Box Attack. (99%)](http://arxiv.org/abs/2106.11644)

Sungmin Cha, Naeun Ko, Youngjoon Yoo, Taesup Moon


[Long-term Cross Adversarial Training: A Robust Meta-learning Method for Few-shot Classification Tasks. (83%)](http://arxiv.org/abs/2106.12900)

Fan Liu, Shuyu Zhao, Xuelong Dai, Bin Xiao


[On Adversarial Robustness of Synthetic Code Generation. (81%)](http://arxiv.org/abs/2106.11629)

Mrinal Anand, Pratik Kayal, Mayank Singh


[NetFense: Adversarial Defenses against Privacy Attacks on Neural Networks for Graph Data. (67%)](http://arxiv.org/abs/2106.11865)

I-Chung Hsieh, Cheng-Te Li


[FLEA: Provably Robust Fair Multisource Learning from Unreliable Training Data. (1%)](http://arxiv.org/abs/2106.11732)

Eugenia Iofinova, Nikola Konstantinov, Christoph H. Lampert


## 2021-06-21

[Policy Smoothing for Provably Robust Reinforcement Learning. (99%)](http://arxiv.org/abs/2106.11420)

Aounon Kumar, Alexander Levine, Soheil Feizi


[Delving into the pixels of adversarial samples. (98%)](http://arxiv.org/abs/2106.10996)

Blerta Lindqvist


[HODA: Hardness-Oriented Detection of Model Extraction Attacks. (98%)](http://arxiv.org/abs/2106.11424)

Amir Mahdi Sadeghzadeh, Amir Mohammad Sobhanian, Faezeh Dehghan, Rasool Jalili


[Friendly Training: Neural Networks Can Adapt Data To Make Learning Easier. (91%)](http://arxiv.org/abs/2106.10974)

Simone Marullo, Matteo Tiezzi, Marco Gori, Stefano Melacci


[Membership Inference on Word Embedding and Beyond. (38%)](http://arxiv.org/abs/2106.11384)

Saeed Mahloujifar, Huseyin A. Inan, Melissa Chase, Esha Ghosh, Marcello Hasegawa


[An Alternative Auxiliary Task for Enhancing Image Classification. (11%)](http://arxiv.org/abs/2106.11478)

Chen Liu


[Zero-shot learning approach to adaptive Cybersecurity using Explainable AI. (1%)](http://arxiv.org/abs/2106.14647)

Dattaraj Rao, Shraddha Mane


## 2021-06-20

[Adversarial Examples Make Strong Poisons. (98%)](http://arxiv.org/abs/2106.10807)

Liam Fowl, Micah Goldblum, Ping-yeh Chiang, Jonas Geiping, Wojtek Czaja, Tom Goldstein


[Adversarial Attack on Graph Neural Networks as An Influence Maximization Problem. (95%)](http://arxiv.org/abs/2106.10785)

Jiaqi Ma, Junwei Deng, Qiaozhu Mei


[Generative Model Adversarial Training for Deep Compressed Sensing. (8%)](http://arxiv.org/abs/2106.10696)

Ashkan Esmaeili


## 2021-06-19

[Attack to Fool and Explain Deep Networks. (99%)](http://arxiv.org/abs/2106.10606)

Naveed Akhtar, Muhammad A. A. K. Jalwana, Mohammed Bennamoun, Ajmal Mian


[A Stealthy and Robust Fingerprinting Scheme for Generative Models. (47%)](http://arxiv.org/abs/2106.11760)

Li Guanlin, Guo Shangwei, Wang Run, Xu Guowen, Zhang Tianwei


## 2021-06-18

[Residual Error: a New Performance Measure for Adversarial Robustness. (99%)](http://arxiv.org/abs/2106.10212)

Hossein Aboutalebi, Mohammad Javad Shafiee, Michelle Karg, Christian Scharfenberger, Alexander Wong


[Indicators of Attack Failure: Debugging and Improving Optimization of Adversarial Examples. (99%)](http://arxiv.org/abs/2106.09947)

Maura Pintor, Luca Demetrio, Angelo Sotgiu, Ambra Demontis, Nicholas Carlini, Battista Biggio, Fabio Roli


[The Dimpled Manifold Model of Adversarial Examples in Machine Learning. (99%)](http://arxiv.org/abs/2106.10151)

Adi Shamir, Odelia Melamed, Oriel BenShmuel


[Exploring Counterfactual Explanations Through the Lens of Adversarial Examples: A Theoretical and Empirical Analysis. (99%)](http://arxiv.org/abs/2106.09992)

Martin Pawelczyk, Chirag Agarwal, Shalmali Joshi, Sohini Upadhyay, Himabindu Lakkaraju


[Light Lies: Optical Adversarial Attack. (92%)](http://arxiv.org/abs/2106.09908)

Kyulim Kim, JeongSoo Kim, Seungri Song, Jun-Ho Choi, Chulmin Joo, Jong-Seok Lee


[BinarizedAttack: Structural Poisoning Attacks to Graph-based Anomaly Detection. (82%)](http://arxiv.org/abs/2106.09989)

Yulin Zhu, Yuni Lai, Kaifa Zhao, Xiapu Luo, Mingquan Yuan, Jian Ren, Kai Zhou


[Less is More: Feature Selection for Adversarial Robustness with Compressive Counter-Adversarial Attacks. (80%)](http://arxiv.org/abs/2106.10252)

Emre Ozfatura, Muhammad Zaid Hameed, Kerem Ozfatura, Deniz Gunduz


[Group-Structured Adversarial Training. (68%)](http://arxiv.org/abs/2106.10324)

Farzan Farnia, Amirali Aghazadeh, James Zou, David Tse


[Accumulative Poisoning Attacks on Real-time Data. (45%)](http://arxiv.org/abs/2106.09993)

Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, Jun Zhu


[Evaluating the Robustness of Trigger Set-Based Watermarks Embedded in Deep Neural Networks. (45%)](http://arxiv.org/abs/2106.10147)

Suyoung Lee, Wonho Song, Suman Jana, Meeyoung Cha, Sooel Son


[Federated Robustness Propagation: Sharing Adversarial Robustness in Federated Learning. (5%)](http://arxiv.org/abs/2106.10196)

Junyuan Hong, Haotao Wang, Zhangyang Wang, Jiayu Zhou


## 2021-06-17

[Analyzing Adversarial Robustness of Deep Neural Networks in Pixel Space: a Semantic Perspective. (99%)](http://arxiv.org/abs/2106.09872)

Lina Wang, Xingshu Chen, Yulong Wang, Yawei Yue, Yi Zhu, Xuemei Zeng, Wei Wang


[Bad Characters: Imperceptible NLP Attacks. (99%)](http://arxiv.org/abs/2106.09898)

Nicholas Boucher, Ilia Shumailov, Ross Anderson, Nicolas Papernot


[DeepInsight: Interpretability Assisting Detection of Adversarial Samples on Graphs. (99%)](http://arxiv.org/abs/2106.09501)

Junhao Zhu, Yalu Shan, Jinhuan Wang, Shanqing Yu, Guanrong Chen, Qi Xuan


[Adversarial Visual Robustness by Causal Intervention. (99%)](http://arxiv.org/abs/2106.09534)

Kaihua Tang, Mingyuan Tao, Hanwang Zhang


[Adversarial Detection Avoidance Attacks: Evaluating the robustness of perceptual hashing-based client-side scanning. (92%)](http://arxiv.org/abs/2106.09820)

Shubham Jain, Ana-Maria Cretu, Montjoye Yves-Alexandre de


[Invisible for both Camera and LiDAR: Security of Multi-Sensor Fusion based Perception in Autonomous Driving Under Physical-World Attacks. (91%)](http://arxiv.org/abs/2106.09249)

Yulong *co-first authors Cao*, Ningfei *co-first authors Wang*, Chaowei *co-first authors Xiao*, Dawei *co-first authors Yang*, Jin *co-first authors Fang, Ruigang *co-first authors Yang, Qi Alfred *co-first authors Chen, Mingyan *co-first authors Liu, Bo *co-first authors Li


[Modeling Realistic Adversarial Attacks against Network Intrusion Detection Systems. (82%)](http://arxiv.org/abs/2106.09380)

Giovanni Apruzzese, Mauro Andreolini, Luca Ferretti, Mirco Marchetti, Michele Colajanni


[Poisoning and Backdooring Contrastive Learning. (70%)](http://arxiv.org/abs/2106.09667)

Nicholas Carlini, Andreas Terzis


[CROP: Certifying Robust Policies for Reinforcement Learning through Functional Smoothing. (69%)](http://arxiv.org/abs/2106.09292)

Fan Wu, Linyi Li, Zijian Huang, Yevgeniy Vorobeychik, Ding Zhao, Bo Li


[CoCoFuzzing: Testing Neural Code Models with Coverage-Guided Fuzzing. (64%)](http://arxiv.org/abs/2106.09242)

Moshi Wei, Yuchao Huang, Jinqiu Yang, Junjie Wang, Song Wang


[On Deep Neural Network Calibration by Regularization and its Impact on Refinement. (3%)](http://arxiv.org/abs/2106.09385)

Aditya Singh, Alessandro Bay, Biswa Sengupta, Andrea Mirabile


[Effective Model Sparsification by Scheduled Grow-and-Prune Methods. (1%)](http://arxiv.org/abs/2106.09857)

Xiaolong Ma, Minghai Qin, Fei Sun, Zejiang Hou, Kun Yuan, Yi Xu, Yanzhi Wang, Yen-Kuang Chen, Rong Jin, Yuan Xie


## 2021-06-16

[Real-time Adversarial Perturbations against Deep Reinforcement Learning Policies: Attacks and Defenses. (99%)](http://arxiv.org/abs/2106.08746)

Buse G. A. Tekgul, Shelly Wang, Samuel Marchal, N. Asokan


[Localized Uncertainty Attacks. (99%)](http://arxiv.org/abs/2106.09222)

Ousmane Amadou Dia, Theofanis Karaletsos, Caner Hazirbas, Cristian Canton Ferrer, Ilknur Kaynar Kabul, Erik Meijer


[Evaluating the Robustness of Bayesian Neural Networks Against Different Types of Attacks. (67%)](http://arxiv.org/abs/2106.09223)

Yutian Pang, Sheng Cheng, Jueming Hu, Yongming Liu


[Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch. (38%)](http://arxiv.org/abs/2106.08970)

Hossein Souri, Liam Fowl, Rama Chellappa, Micah Goldblum, Tom Goldstein


[Explainable AI for Natural Adversarial Images. (13%)](http://arxiv.org/abs/2106.09106)

Tomas Folke, ZhaoBin Li, Ravi B. Sojitra, Scott Cheng-Hsin Yang, Patrick Shafto


[A Winning Hand: Compressing Deep Networks Can Improve Out-Of-Distribution Robustness. (2%)](http://arxiv.org/abs/2106.09129)

James Diffenderfer, Brian R. Bartoldson, Shreya Chaganti, Jize Zhang, Bhavya Kailkhura


[Scaling-up Diverse Orthogonal Convolutional Networks with a Paraunitary Framework. (1%)](http://arxiv.org/abs/2106.09121)

Jiahao Su, Wonmin Byeon, Furong Huang


[Loki: Hardening Code Obfuscation Against Automated Attacks. (1%)](http://arxiv.org/abs/2106.08913)

Moritz Schloegel, Tim Blazytko, Moritz Contag, Cornelius Aschermann, Julius Basler, Thorsten Holz, Ali Abbasi


## 2021-06-15

[Adversarial Attacks on Deep Models for Financial Transaction Records. (99%)](http://arxiv.org/abs/2106.08361)

Ivan Fursov, Matvey Morozov, Nina Kaploukhaya, Elizaveta Kovtun, Rodrigo Rivera-Castro, Gleb Gusev, Dmitry Babaev, Ivan Kireev, Alexey Zaytsev, Evgeny Burnaev


[Model Extraction and Adversarial Attacks on Neural Networks using Switching Power Information. (99%)](http://arxiv.org/abs/2106.08299)

Tommy Li, Cory Merkel


[Towards Adversarial Robustness via Transductive Learning. (80%)](http://arxiv.org/abs/2106.08387)

Jiefeng Chen, Yang Guo, Xi Wu, Tianqi Li, Qicheng Lao, Yingyu Liang, Somesh Jha


[Voting for the right answer: Adversarial defense for speaker verification. (78%)](http://arxiv.org/abs/2106.07868)

Haibin Wu, Yang Zhang, Zhiyong Wu, Dong Wang, Hung-yi Lee


[Detect and remove watermark in deep neural networks via generative adversarial networks. (68%)](http://arxiv.org/abs/2106.08104)

Haoqi Wang, Mingfu Xue, Shichang Sun, Yushu Zhang, Jian Wang, Weiqiang Liu


[CRFL: Certifiably Robust Federated Learning against Backdoor Attacks. (13%)](http://arxiv.org/abs/2106.08283)

Chulin Xie, Minghao Chen, Pin-Yu Chen, Bo Li


[Securing Face Liveness Detection Using Unforgeable Lip Motion Patterns. (12%)](http://arxiv.org/abs/2106.08013)

Man Senior Member, IEEE Zhou, Qian Senior Member, IEEE Wang, Qi Senior Member, IEEE Li, Peipei Senior Member, IEEE Jiang, Jingxiao Senior Member, IEEE Yang, Chao Senior Member, IEEE Shen, Cong Fellow, IEEE Wang, Shouhong Ding


[Probabilistic Margins for Instance Reweighting in Adversarial Training. (8%)](http://arxiv.org/abs/2106.07904)

Qizhou Wang, Feng Liu, Bo Han, Tongliang Liu, Chen Gong, Gang Niu, Mingyuan Zhou, Masashi Sugiyama


[CAN-LOC: Spoofing Detection and Physical Intrusion Localization on an In-Vehicle CAN Bus Based on Deep Features of Voltage Signals. (1%)](http://arxiv.org/abs/2106.07895)

Efrat Levy, Asaf Shabtai, Bogdan Groza, Pal-Stefan Murvay, Yuval Elovici


## 2021-06-14

[PopSkipJump: Decision-Based Attack for Probabilistic Classifiers. (99%)](http://arxiv.org/abs/2106.07445)

Carl-Johann Simon-Gabriel, Noman Ahmed Sheikh, Andreas Krause


[Now You See It, Now You Dont: Adversarial Vulnerabilities in Computational Pathology. (99%)](http://arxiv.org/abs/2106.08153)

Alex Foote, Amina Asif, Ayesha Azam, Tim Marshall-Cox, Nasir Rajpoot, Fayyaz Minhas


[Audio Attacks and Defenses against AED Systems -- A Practical Study. (99%)](http://arxiv.org/abs/2106.07428)

Rodrigo dos Santos, Shirin Nilizadeh


[Backdoor Learning Curves: Explaining Backdoor Poisoning Beyond Influence Functions. (92%)](http://arxiv.org/abs/2106.07214)

Antonio Emanuele Cinà, Kathrin Grosse, Sebastiano Vascon, Ambra Demontis, Battista Biggio, Fabio Roli, Marcello Pelillo


[Evading Malware Classifiers via Monte Carlo Mutant Feature Discovery. (81%)](http://arxiv.org/abs/2106.07860)

John Boutsikas, Maksim E. Eren, Charles Varga, Edward Raff, Cynthia Matuszek, Charles Nicholas


[On the Relationship between Heterophily and Robustness of Graph Neural Networks. (81%)](http://arxiv.org/abs/2106.07767)

Jiong Zhu, Junchen Jin, Donald Loveland, Michael T. Schaub, Danai Koutra


[Partial success in closing the gap between human and machine vision. (15%)](http://arxiv.org/abs/2106.07411)

Robert Geirhos, Kantharaju Narayanappa, Benjamin Mitzkus, Tizian Thieringer, Matthias Bethge, Felix A. Wichmann, Wieland Brendel


[Text Generation with Efficient (Soft) Q-Learning. (2%)](http://arxiv.org/abs/2106.07704)

Han Guo, Bowen Tan, Zhengzhong Liu, Eric P. Xing, Zhiting Hu


[Resilient Control of Platooning Networked Robitic Systems via Dynamic Watermarking. (1%)](http://arxiv.org/abs/2106.07541)

Matthew Porter, Arnav Joshi, Sidhartha Dey, Qirui Wu, Pedro Hespanhol, Anil Aswani, Matthew Johnson-Roberson, Ram Vasudevan


[Self-training Guided Adversarial Domain Adaptation For Thermal Imagery. (1%)](http://arxiv.org/abs/2106.07165)

Ibrahim Batuhan Akkaya, Fazil Altinel, Ugur Halici


[Code Integrity Attestation for PLCs using Black Box Neural Network Predictions. (1%)](http://arxiv.org/abs/2106.07851)

Yuqi Chen, Christopher M. Poskitt, Jun Sun


## 2021-06-13

[Target Model Agnostic Adversarial Attacks with Query Budgets on Language Understanding Models. (99%)](http://arxiv.org/abs/2106.07047)

Jatin Chauhan, Karan Bhukar, Manohar Kaul


[Selection of Source Images Heavily Influences the Effectiveness of Adversarial Attacks. (99%)](http://arxiv.org/abs/2106.07141)

Utku Ozbulak, Esla Timothy Anzaku, Neve Wesley De, Messem Arnout Van


[ATRAS: Adversarially Trained Robust Architecture Search. (96%)](http://arxiv.org/abs/2106.06917)

Yigit Alparslan, Edward Kim


[Security Analysis of Camera-LiDAR Semantic-Level Fusion Against Black-Box Attacks on Autonomous Vehicles. (64%)](http://arxiv.org/abs/2106.07098)

R. Spencer Hallyburton, Yupei Liu, Miroslav Pajic


[Weakly-supervised High-resolution Segmentation of Mammography Images for Breast Cancer Diagnosis. (1%)](http://arxiv.org/abs/2106.07049)

Kangning Liu, Yiqiu Shen, Nan Wu, Jakub Chłędowski, Carlos Fernandez-Granda, Krzysztof J. Geras


[HistoTransfer: Understanding Transfer Learning for Histopathology. (1%)](http://arxiv.org/abs/2106.07068)

Yash Sharma, Lubaina Ehsan, Sana Syed, Donald E. Brown


## 2021-06-12

[Adversarial Robustness via Fisher-Rao Regularization. (67%)](http://arxiv.org/abs/2106.06685)

Marine Picot, Francisco Messina, Malik Boudiaf, Fabrice Labeau, Ismail Ben Ayed, Pablo Piantanida


[What can linearized neural networks actually say about generalization? (31%)](http://arxiv.org/abs/2106.06770)

Guillermo Ortiz-Jiménez, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard


[FeSHI: Feature Map Based Stealthy Hardware Intrinsic Attack. (2%)](http://arxiv.org/abs/2106.06895)

Tolulope Odetola, Faiq Khalid, Travis Sandefur, Hawzhin Mohammed, Syed Rafay Hasan


## 2021-06-11

[CausalAdv: Adversarial Robustness through the Lens of Causality. (99%)](http://arxiv.org/abs/2106.06196)

Yonggang Zhang, Mingming Gong, Tongliang Liu, Gang Niu, Xinmei Tian, Bo Han, Bernhard Schölkopf, Kun Zhang


[Knowledge Enhanced Machine Learning Pipeline against Diverse Adversarial Attacks. (99%)](http://arxiv.org/abs/2106.06235)

Nezihe Merve Gürel, Xiangyu Qi, Luka Rimanic, Ce Zhang, Bo Li


[Adversarial purification with Score-based generative models. (89%)](http://arxiv.org/abs/2106.06041)

Jongmin Yoon, Sung Ju Hwang, Juho Lee


[Relaxing Local Robustness. (80%)](http://arxiv.org/abs/2106.06624)

Klas Leino, Matt Fredrikson


[TDGIA:Effective Injection Attacks on Graph Neural Networks. (76%)](http://arxiv.org/abs/2106.06663)

Xu Zou, Qinkai Zheng, Yuxiao Dong, Xinyu Guan, Evgeny Kharlamov, Jialiang Lu, Jie Tang


[Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution. (56%)](http://arxiv.org/abs/2106.06361)

Fanchao Qi, Yuan Yao, Sophia Xu, Zhiyuan Liu, Maosong Sun


[CARTL: Cooperative Adversarially-Robust Transfer Learning. (8%)](http://arxiv.org/abs/2106.06667)

Dian Chen, Hongxin Hu, Qian Wang, Yinli Li, Cong Wang, Chao Shen, Qi Li


[A Shuffling Framework for Local Differential Privacy. (1%)](http://arxiv.org/abs/2106.06603)

Casey Meehan, Amrita Roy Chowdhury, Kamalika Chaudhuri, Somesh Jha


## 2021-06-10

[Sparse and Imperceptible Adversarial Attack via a Homotopy Algorithm. (99%)](http://arxiv.org/abs/2106.06027)

Mingkang Zhu, Tianlong Chen, Zhangyang Wang


[Deep neural network loses attention to adversarial images. (99%)](http://arxiv.org/abs/2106.05657)

Shashank Kotyan, Danilo Vasconcellos Vargas


[Verifying Quantized Neural Networks using SMT-Based Model Checking. (92%)](http://arxiv.org/abs/2106.05997)

Luiz Sena, Xidan Song, Erickson Alves, Iury Bessa, Edoardo Manino, Lucas Cordeiro, Eddie de Lima Filho


[Progressive-Scale Boundary Blackbox Attack via Projective Gradient Estimation. (80%)](http://arxiv.org/abs/2106.06056)

Jiawei Zhang, Linyi Li, Huichen Li, Xiaolu Zhang, Shuang Yang, Bo Li


[An Ensemble Approach Towards Adversarial Robustness. (41%)](http://arxiv.org/abs/2106.05996)

Haifeng Qian


[Towards an Automated Pipeline for Detecting and Classifying Malware through Machine Learning. (1%)](http://arxiv.org/abs/2106.05625)

Nicola Loi, Claudio Borile, Daniele Ucci


[Fair Classification with Adversarial Perturbations. (1%)](http://arxiv.org/abs/2106.05964)

L. Elisa Celis, Anay Mehrotra, Nisheeth K. Vishnoi


## 2021-06-09

[HASI: Hardware-Accelerated Stochastic Inference, A Defense Against Adversarial Machine Learning Attacks. (99%)](http://arxiv.org/abs/2106.05825)

Mohammad Hossein Samavatian, Saikat Majumdar, Kristin Barber, Radu Teodorescu


[Towards Defending against Adversarial Examples via Attack-Invariant Features. (99%)](http://arxiv.org/abs/2106.05036)

Dawei Zhou, Tongliang Liu, Bo Han, Nannan Wang, Chunlei Peng, Xinbo Gao


[Improving White-box Robustness of Pre-processing Defenses via Joint Adversarial Training. (99%)](http://arxiv.org/abs/2106.05453)

Dawei Zhou, Nannan Wang, Xinbo Gao, Bo Han, Jun Yu, Xiaoyu Wang, Tongliang Liu


[Attacking Adversarial Attacks as A Defense. (99%)](http://arxiv.org/abs/2106.04938)

Boxi Wu, Heng Pan, Li Shen, Jindong Gu, Shuai Zhao, Zhifeng Li, Deng Cai, Xiaofei He, Wei Liu


[We Can Always Catch You: Detecting Adversarial Patched Objects WITH or WITHOUT Signature. (98%)](http://arxiv.org/abs/2106.05261)

Bin Liang, Jiachun Li, Jianjun Huang


[Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep RL. (97%)](http://arxiv.org/abs/2106.05087)

Yanchao Sun, Ruijie Zheng, Yongyuan Liang, Furong Huang


[URLTran: Improving Phishing URL Detection Using Transformers. (10%)](http://arxiv.org/abs/2106.05256)

Pranav Maneriker, Jack W. Stokes, Edir Garcia Lazo, Diana Carutasu, Farid Tajaddodianfar, Arun Gururajan


[ZoPE: A Fast Optimizer for ReLU Networks with Low-Dimensional Inputs. (5%)](http://arxiv.org/abs/2106.05325)

Christopher A. Strong, Sydney M. Katz, Anthony L. Corso, Mykel J. Kochenderfer


[Practical Machine Learning Safety: A Survey and Primer. (4%)](http://arxiv.org/abs/2106.04823)

Sina Mohseni, Haotao Wang, Zhiding Yu, Chaowei Xiao, Zhangyang Wang, Jay Yadawa


[Network insensitivity to parameter noise via adversarial regularization. (2%)](http://arxiv.org/abs/2106.05009)

Julian Büchel, Fynn Faber, Dylan R. Muir


## 2021-06-08

[On Improving Adversarial Transferability of Vision Transformers. (99%)](http://arxiv.org/abs/2106.04169)

Muzammal Naseer, Kanchana Ranasinghe, Salman Khan, Fahad Shahbaz Khan, Fatih Porikli


[Simulated Adversarial Testing of Face Recognition Models. (99%)](http://arxiv.org/abs/2106.04569)

Nataniel Ruiz, Adam Kortylewski, Weichao Qiu, Cihang Xie, Sarah Adel Bargal, Alan Yuille, Stan Sclaroff


[Towards the Memorization Effect of Neural Networks in Adversarial Training. (93%)](http://arxiv.org/abs/2106.04794)

Han Xu, Xiaorui Liu, Wentao Wang, Wenbiao Ding, Zhongqin Wu, Zitao Liu, Anil Jain, Jiliang Tang


[Handcrafted Backdoors in Deep Neural Networks. (92%)](http://arxiv.org/abs/2106.04690)

Sanghyun Hong, Nicholas Carlini, Alexey Kurakin


[Enhancing Robustness of Neural Networks through Fourier Stabilization. (73%)](http://arxiv.org/abs/2106.04435)

Netanel Raviv, Aidan Kelley, Michael Guo, Yevgeny Vorobeychik


[Provably Robust Detection of Out-of-distribution Data (almost) for free. (26%)](http://arxiv.org/abs/2106.04260)

Alexander Meinke, Julian Bitterwolf, Matthias Hein


## 2021-06-07

[Adversarial Attack and Defense in Deep Ranking. (99%)](http://arxiv.org/abs/2106.03614)

Mo Zhou, Le Wang, Zhenxing Niu, Qilin Zhang, Nanning Zheng, Gang Hua


[Reveal of Vision Transformers Robustness against Adversarial Attacks. (99%)](http://arxiv.org/abs/2106.03734)

Ahmed Aldahdooh, Wassim Hamidouche, Olivier Deforges


[Position Bias Mitigation: A Knowledge-Aware Graph Model for EmotionCause Extraction. (89%)](http://arxiv.org/abs/2106.03518)

Hanqi Yan, Lin Gui, Gabriele Pergola, Yulan He


[3DB: A Framework for Debugging Computer Vision Models. (45%)](http://arxiv.org/abs/2106.03805)

Guillaume Leclerc, Hadi Salman, Andrew Ilyas, Sai Vemprala, Logan Engstrom, Vibhav Vineet, Kai Xiao, Pengchuan Zhang, Shibani Santurkar, Greg Yang, Ashish Kapoor, Aleksander Madry


[RoSearch: Search for Robust Student Architectures When Distilling Pre-trained Language Models. (11%)](http://arxiv.org/abs/2106.03613)

Xin Guo, Jianlei Yang, Haoyi Zhou, Xucheng Ye, Jianxin Li


[Semantically Adversarial Scenario Generation with Explicit Knowledge Guidance. (1%)](http://arxiv.org/abs/2106.04066)

Wenhao Ding, Haohong Lin, Bo Li, Ding Zhao


## 2021-06-06

[A Primer on Multi-Neuron Relaxation-based Adversarial Robustness Certification. (98%)](http://arxiv.org/abs/2106.03099)

Kevin Roth


[Zero-Shot Knowledge Distillation from a Decision-Based Black-Box Model. (4%)](http://arxiv.org/abs/2106.03310)

Zi Wang


## 2021-06-05

[Ensemble Defense with Data Diversity: Weak Correlation Implies Strong Robustness. (92%)](http://arxiv.org/abs/2106.02867)

Renjue Li, Hanwei Zhang, Pengfei Yang, Cheng-Chao Huang, Aimin Zhou, Bai Xue, Lijun Zhang


[Robust Stochastic Linear Contextual Bandits Under Adversarial Attacks. (69%)](http://arxiv.org/abs/2106.02978)

Qin Ding, Cho-Jui Hsieh, James Sharpnack


[RDA: Robust Domain Adaptation via Fourier Adversarial Attacking. (2%)](http://arxiv.org/abs/2106.02874)

Jiaxing Huang, Dayan Guan, Aoran Xiao, Shijian Lu


## 2021-06-04

[Revisiting Hilbert-Schmidt Information Bottleneck for Adversarial Robustness. (99%)](http://arxiv.org/abs/2106.02734)

Zifeng Wang, Tong Jian, Aria Masoomi, Stratis Ioannidis, Jennifer Dy


[BO-DBA: Query-Efficient Decision-Based Adversarial Attacks via Bayesian Optimization. (99%)](http://arxiv.org/abs/2106.02732)

Zhuosheng Zhang, Shucheng Yu


[Human-Adversarial Visual Question Answering. (31%)](http://arxiv.org/abs/2106.02280)

Sasha Sheng, Amanpreet Singh, Vedanuj Goswami, Jose Alberto Lopez Magana, Wojciech Galuba, Devi Parikh, Douwe Kiela


[Predify: Augmenting deep neural networks with brain-inspired predictive coding dynamics. (15%)](http://arxiv.org/abs/2106.02749)

Bhavin Choksi, Milad Mozafari, Callum Biggs O'May, Benjamin Ador, Andrea Alamia, Rufin VanRullen


[DOCTOR: A Simple Method for Detecting Misclassification Errors. (1%)](http://arxiv.org/abs/2106.02395)

Federica Granese, Marco Romanelli, Daniele Gorla, Catuscia Palamidessi, Pablo Piantanida


[Teaching keyword spotters to spot new keywords with limited examples. (1%)](http://arxiv.org/abs/2106.02443)

Abhijeet Awasthi, Kevin Kilgour, Hassan Rom


## 2021-06-03

[Improving the Transferability of Adversarial Examples with New Iteration Framework and Input Dropout. (99%)](http://arxiv.org/abs/2106.01617)

Pengfei Xie, Linyuan Wang, Ruoxi Qin, Kai Qiao, Shuhao Shi, Guoen Hu, Bin Yan


[Imperceptible Adversarial Examples for Fake Image Detection. (99%)](http://arxiv.org/abs/2106.01615)

Quanyu Liao, Yuezun Li, Xin Wang, Bin Kong, Bin Zhu, Siwei Lyu, Youbing Yin, Qi Song, Xi Wu


[A Little Robustness Goes a Long Way: Leveraging Universal Features for Targeted Transfer Attacks. (99%)](http://arxiv.org/abs/2106.02105)

Jacob M. Springer, Melanie Mitchell, Garrett T. Kenyon


[Transferable Adversarial Examples for Anchor Free Object Detection. (99%)](http://arxiv.org/abs/2106.01618)

Quanyu Liao, Xin Wang, Bin Kong, Siwei Lyu, Bin Zhu, Youbing Yin, Qi Song, Xi Wu


[Exploring Memorization in Adversarial Training. (98%)](http://arxiv.org/abs/2106.01606)

Yinpeng Dong, Ke Xu, Xiao Yang, Tianyu Pang, Zhijie Deng, Hang Su, Jun Zhu


[Improving Neural Network Robustness via Persistency of Excitation. (68%)](http://arxiv.org/abs/2106.02078)

Kaustubh Sridhar, Oleg Sokolsky, Insup Lee, James Weimer


[Defending against Backdoor Attacks in Natural Language Generation. (38%)](http://arxiv.org/abs/2106.01810)

Chun Fan, Xiaoya Li, Yuxian Meng, Xiaofei Sun, Xiang Ao, Fei Wu, Jiwei Li, Tianwei Zhang


[Sneak Attack against Mobile Robotic Networks under Formation Control. (1%)](http://arxiv.org/abs/2106.02240)

Yushan Li, Jianping He, Xuda Ding, Lin Cai, Xinping Guan


## 2021-06-02

[PDPGD: Primal-Dual Proximal Gradient Descent Adversarial Attack. (99%)](http://arxiv.org/abs/2106.01538)

Alexander Matyasko, Lap-Pui Chau


[Towards Robustness of Text-to-SQL Models against Synonym Substitution. (75%)](http://arxiv.org/abs/2106.01065)

Yujian Gan, Xinyun Chen, Qiuping Huang, Matthew Purver, John R. Woodward, Jinxia Xie, Pengsheng Huang


[BERT-Defense: A Probabilistic Model Based on BERT to Combat Cognitively Inspired Orthographic Adversarial Attacks. (62%)](http://arxiv.org/abs/2106.01452)

Yannik Keller, Jan Mackensen, Steffen Eger


## 2021-06-01

[Adversarial Defense for Automatic Speaker Verification by Self-Supervised Learning. (99%)](http://arxiv.org/abs/2106.00273)

Haibin Wu, Xu Li, Andy T. Liu, Zhiyong Wu, Helen Meng, Hung-yi Lee


[Improving Compositionality of Neural Networks by Decoding Representations to Inputs. (68%)](http://arxiv.org/abs/2106.00769)

Mike Wu, Noah Goodman, Stefano Ermon


[Markpainting: Adversarial Machine Learning meets Inpainting. (12%)](http://arxiv.org/abs/2106.00660)

David Khachaturov, Ilia Shumailov, Yiren Zhao, Nicolas Papernot, Ross Anderson


[On the Efficacy of Adversarial Data Collection for Question Answering: Results from a Large-Scale Randomized Study. (9%)](http://arxiv.org/abs/2106.00872)

Divyansh Kaushik, Douwe Kiela, Zachary C. Lipton, Wen-tau Yih


[Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models. (5%)](http://arxiv.org/abs/2106.00245)

Linjie Li, Jie Lei, Zhe Gan, Jingjing Liu


[Memory Wrap: a Data-Efficient and Interpretable Extension to Image Classification Models. (1%)](http://arxiv.org/abs/2106.01440)

Rosa Biagio La, Roberto Capobianco, Daniele Nardi


[Concurrent Adversarial Learning for Large-Batch Training. (1%)](http://arxiv.org/abs/2106.00221)

Yong Liu, Xiangning Chen, Minhao Cheng, Cho-Jui Hsieh, Yang You


## 2021-05-31

[Adaptive Feature Alignment for Adversarial Training. (99%)](http://arxiv.org/abs/2105.15157)

Tao Wang, Ruixin Zhang, Xingyu Chen, Kai Zhao, Xiaolin Huang, Yuge Huang, Shaoxin Li, Jilin Li, Feiyue Huang


[QueryNet: An Efficient Attack Framework with Surrogates Carrying Multiple Identities. (99%)](http://arxiv.org/abs/2105.15010)

Sizhe Chen, Zhehao Huang, Qinghua Tao, Xiaolin Huang


[Transferable Sparse Adversarial Attack. (99%)](http://arxiv.org/abs/2105.14727)

Ziwen He, Wei Wang, Jing Dong, Tieniu Tan


[Adversarial Training with Rectified Rejection. (99%)](http://arxiv.org/abs/2105.14785)

Tianyu Pang, Huishuai Zhang, Di He, Yinpeng Dong, Hang Su, Wei Chen, Jun Zhu, Tie-Yan Liu


[Robustifying $\ell_\infty$ Adversarial Training to the Union of Perturbation Models. (82%)](http://arxiv.org/abs/2105.14710)

Ameya D. Patil, Michael Tuttle, Alexander G. Schwing, Naresh R. Shanbhag


[Dominant Patterns: Critical Features Hidden in Deep Neural Networks. (80%)](http://arxiv.org/abs/2105.15057)

Zhixing Ye, Shaofei Qin, Sizhe Chen, Xiaolin Huang


[Exploration and Exploitation: Two Ways to Improve Chinese Spelling Correction Models. (75%)](http://arxiv.org/abs/2105.14813)

Chong Li, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang


[Gradient-based Data Subversion Attack Against Binary Classifiers. (73%)](http://arxiv.org/abs/2105.14803)

Rosni K Vasu, Sanjay Seetharaman, Shubham Malaviya, Manish Shukla, Sachin Lodha


[DISSECT: Disentangled Simultaneous Explanations via Concept Traversals. (1%)](http://arxiv.org/abs/2105.15164)

Asma Ghandeharioun, Been Kim, Chun-Liang Li, Brendan Jou, Brian Eoff, Rosalind W. Picard


[The effectiveness of feature attribution methods and its correlation with automatic evaluation scores. (1%)](http://arxiv.org/abs/2105.14944)

Giang Nguyen, Daeyoung Kim, Anh Nguyen


## 2021-05-30

[Generating Adversarial Examples with Graph Neural Networks. (99%)](http://arxiv.org/abs/2105.14644)

Florian Jaeckle, M. Pawan Kumar


[Defending Pre-trained Language Models from Adversarial Word Substitutions Without Performance Sacrifice. (98%)](http://arxiv.org/abs/2105.14553)

Rongzhou Bao, Jiayi Wang, Hai Zhao


[NoiLIn: Do Noisy Labels Always Hurt Adversarial Training? (62%)](http://arxiv.org/abs/2105.14676)

Jingfeng Zhang, Xilie Xu, Bo Han, Tongliang Liu, Gang Niu, Lizhen Cui, Masashi Sugiyama


[Evaluating Resilience of Encrypted Traffic Classification Against Adversarial Evasion Attacks. (62%)](http://arxiv.org/abs/2105.14564)

Ramy Maarouf, Danish Sattar, Ashraf Matrawy


[DAAIN: Detection of Anomalous and Adversarial Input using Normalizing Flows. (12%)](http://arxiv.org/abs/2105.14638)

Baußnern Samuel von, Johannes Otterbach, Adrian Loy, Mathieu Salzmann, Thomas Wollmann


[EEG-based Cross-Subject Driver Drowsiness Recognition with an Interpretable Convolutional Neural Network. (1%)](http://arxiv.org/abs/2107.09507)

Jian Cui, Zirui Lan, Olga Sourina, Wolfgang Müller-Wittig


## 2021-05-29

[Detecting Backdoor in Deep Neural Networks via Intentional Adversarial Perturbations. (99%)](http://arxiv.org/abs/2105.14259)

Mingfu Xue, Yinghao Wu, Zhiyu Wu, Jian Wang, Yushu Zhang, Weiqiang Liu


[Analysis and Applications of Class-wise Robustness in Adversarial Training. (99%)](http://arxiv.org/abs/2105.14240)

Qi Tian, Kun Kuang, Kelu Jiang, Fei Wu, Yisen Wang


[A Measurement Study on the (In)security of End-of-Life (EoL) Embedded Devices. (2%)](http://arxiv.org/abs/2105.14298)

Dingding Wang, Muhui Jiang, Rui Chang, Yajin Zhou, Baolei Hou, Xiapu Luo, Lei Wu, Kui Ren


## 2021-05-28

[Demotivate adversarial defense in remote sensing. (99%)](http://arxiv.org/abs/2105.13902)

Adrien Chan-Hon-Tong, Gaston Lenczner, Aurelien Plyer


[AdvParams: An Active DNN Intellectual Property Protection Technique via Adversarial Perturbation Based Parameter Encryption. (92%)](http://arxiv.org/abs/2105.13697)

Mingfu Xue, Zhiyu Wu, Jian Wang, Yushu Zhang, Weiqiang Liu


[Robust Regularization with Adversarial Labelling of Perturbed Samples. (83%)](http://arxiv.org/abs/2105.13745)

Xiaohui Guo, Richong Zhang, Yaowei Zheng, Yongyi Mao


[SafeAMC: Adversarial training for robust modulation recognition models. (83%)](http://arxiv.org/abs/2105.13746)

Javier Maroto, Gérôme Bovet, Pascal Frossard


[Towards optimally abstaining from prediction. (81%)](http://arxiv.org/abs/2105.14119)

Adam Tauman Kalai, Varun Kanade


[Rethinking Noisy Label Models: Labeler-Dependent Noise with Adversarial Awareness. (76%)](http://arxiv.org/abs/2105.14083)

Glenn Dawson, Robi Polikar


[Visualizing Representations of Adversarially Perturbed Inputs. (68%)](http://arxiv.org/abs/2105.14116)

Daniel Steinberg, Paul Munro


[Chromatic and spatial analysis of one-pixel attacks against an image classifier. (15%)](http://arxiv.org/abs/2105.13771)

Janne Alatalo, Joni Korpihalkola, Tuomo Sipola, Tero Kokkonen


[FoveaTer: Foveated Transformer for Image Classification. (10%)](http://arxiv.org/abs/2105.14173)

Aditya Jonnalagadda, William Yang Wang, B. S. Manjunath, Miguel P. Eckstein


[DeepMoM: Robust Deep Learning With Median-of-Means. (1%)](http://arxiv.org/abs/2105.14035)

Shih-Ting Huang, Johannes Lederer


## 2021-05-27

[A BIC-based Mixture Model Defense against Data Poisoning Attacks on Classifiers. (84%)](http://arxiv.org/abs/2105.13530)

Xi Li, David J. Miller, Zhen Xiang, George Kesidis


## 2021-05-26

[Deep Repulsive Prototypes for Adversarial Robustness. (99%)](http://arxiv.org/abs/2105.12427)

Alex Serban, Erik Poll, Joost Visser


[Adversarial Attack Framework on Graph Embedding Models with Limited Knowledge. (98%)](http://arxiv.org/abs/2105.12419)

Heng Chang, Yu Rong, Tingyang Xu, Wenbing Huang, Honglei Zhang, Peng Cui, Xin Wang, Wenwu Zhu, Junzhou Huang


[Adversarial robustness against multiple $l_p$-threat models at the price of one and how to quickly fine-tune robust models to another threat model. (93%)](http://arxiv.org/abs/2105.12508)

Francesco Croce, Matthias Hein


[Can Linear Programs Have Adversarial Examples? A Causal Perspective. (81%)](http://arxiv.org/abs/2105.12697)

Matej Zečević, Devendra Singh Dhami, Kristian Kersting


[Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger. (61%)](http://arxiv.org/abs/2105.12400)

Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang, Maosong Sun


[Fooling Partial Dependence via Data Poisoning. (13%)](http://arxiv.org/abs/2105.12837)

Hubert Baniecki, Wojciech Kretowicz, Przemyslaw Biecek


## 2021-05-25

[Practical Convex Formulation of Robust One-hidden-layer Neural Network Training. (98%)](http://arxiv.org/abs/2105.12237)

Yatong Bai, Tanmay Gautam, Yu Gai, Somayeh Sojoudi


[Adversarial Attack Driven Data Augmentation for Accurate And Robust Medical Image Segmentation. (98%)](http://arxiv.org/abs/2105.12106)

Mst. Tasnim Pervin, Linmi Tao, Aminul Huq, Zuoxiang He, Li Huo


[Honest-but-Curious Nets: Sensitive Attributes of Private Inputs Can Be Secretly Coded into the Classifiers' Outputs. (67%)](http://arxiv.org/abs/2105.12049)

Mohammad Malekzadeh, Anastasia Borovykh, Deniz Gündüz


[Robust Value Iteration for Continuous Control Tasks. (9%)](http://arxiv.org/abs/2105.12189)

Michael Lutter, Shie Mannor, Jan Peters, Dieter Fox, Animesh Garg


## 2021-05-24

[OFEI: A Semi-black-box Android Adversarial Sample Attack Framework Against DLaaS. (99%)](http://arxiv.org/abs/2105.11593)

Guangquan Xu, GuoHua Xin, Litao Jiao, Jian Liu, Shaoying Liu, Meiqi Feng, Xi Zheng


[Learning Security Classifiers with Verified Global Robustness Properties. (92%)](http://arxiv.org/abs/2105.11363)

Yizheng Chen, Shiqi Wang, Yue Qin, Xiaojing Liao, Suman Jana, David Wagner


[Feature Space Targeted Attacks by Statistic Alignment. (82%)](http://arxiv.org/abs/2105.11645)

Lianli Gao, Yaya Cheng, Qilong Zhang, Xing Xu, Jingkuan Song


[Improved OOD Generalization via Adversarial Training and Pre-training. (12%)](http://arxiv.org/abs/2105.11144)

Mingyang Yi, Lu Hou, Jiacheng Sun, Lifeng Shang, Xin Jiang, Qun Liu, Zhi-Ming Ma


[Out-of-Distribution Detection in Dermatology using Input Perturbation and Subset Scanning. (5%)](http://arxiv.org/abs/2105.11160)

Hannah Kim, Girmaw Abebe Tadesse, Celia Cintas, Skyler Speakman, Kush Varshney


[AirNet: Neural Network Transmission over the Air. (1%)](http://arxiv.org/abs/2105.11166)

Mikolaj Jankowski, Deniz Gunduz, Krystian Mikolajczyk


[Every Byte Matters: Traffic Analysis of Bluetooth Wearable Devices. (1%)](http://arxiv.org/abs/2105.11172)

Ludovic Barman, Alexandre Dumur, Apostolos Pyrgelis, Jean-Pierre Hubaux


[Using Adversarial Attacks to Reveal the Statistical Bias in Machine Reading Comprehension Models. (1%)](http://arxiv.org/abs/2105.11136)

Jieyu Lin, Jiajie Zou, Nai Ding


[Dissecting Click Fraud Autonomy in the Wild. (1%)](http://arxiv.org/abs/2105.11103)

Tong Zhu, Yan Meng, Haotian Hu, Xiaokuan Zhang, Minhui Xue, Haojin Zhu


## 2021-05-23

[Killing Two Birds with One Stone: Stealing Model and Inferring Attribute from BERT-based APIs. (99%)](http://arxiv.org/abs/2105.10909)

Lingjuan Lyu, Xuanli He, Fangzhao Wu, Lichao Sun


[CMUA-Watermark: A Cross-Model Universal Adversarial Watermark for Combating Deepfakes. (92%)](http://arxiv.org/abs/2105.10872)

Hao Huang, Yongtao Wang, Zhaoyu Chen, Yuheng Li, Zhi Tang, Wei Chu, Jingdong Chen, Weisi Lin, Kai-Kuang Ma


[Regularization Can Help Mitigate Poisoning Attacks... with the Right Hyperparameters. (12%)](http://arxiv.org/abs/2105.10948)

Javier Carnerero-Cano, Luis Muñoz-González, Phillippa Spencer, Emil C. Lupu


## 2021-05-22

[Adversarial Attacks and Mitigation for Anomaly Detectors of Cyber-Physical Systems. (99%)](http://arxiv.org/abs/2105.10707)

Yifan Jia, Jingyi Wang, Christopher M. Poskitt, Sudipta Chattopadhyay, Jun Sun, Yuqi Chen


[Exploring Robustness of Unsupervised Domain Adaptation in Semantic Segmentation. (98%)](http://arxiv.org/abs/2105.10843)

Jinyu Yang, Chunyuan Li, Weizhi An, Hehuan Ma, Yuzhi Guo, Yu Rong, Peilin Zhao, Junzhou Huang


[Securing Optical Networks using Quantum-secured Blockchain: An Overview. (1%)](http://arxiv.org/abs/2105.10663)

Purva Sharma, Vimal Bhatia, Shashi Prakash


## 2021-05-21

[ReLUSyn: Synthesizing Stealthy Attacks for Deep Neural Network Based Cyber-Physical Systems. (81%)](http://arxiv.org/abs/2105.10393)

Aarti Kashyap, Syed Mubashir Iqbal, Karthik Pattabiraman, Margo Seltzer


[Exploring Misclassifications of Robust Neural Networks to Enhance Adversarial Attacks. (76%)](http://arxiv.org/abs/2105.10304)

Leo Schwinn, René Raab, An Nguyen, Dario Zanca, Bjoern Eskofier


[Backdoor Attacks on Self-Supervised Learning. (68%)](http://arxiv.org/abs/2105.10123)

Aniruddha Saha, Ajinkya Tejankar, Soroush Abbasi Koohpayegani, Hamed Pirsiavash


[Intriguing Properties of Vision Transformers. (8%)](http://arxiv.org/abs/2105.10497)

Muzammal Naseer, Kanchana Ranasinghe, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang


[Explainable Enterprise Credit Rating via Deep Feature Crossing Network. (1%)](http://arxiv.org/abs/2105.13843)

Weiyu Guo, Zhijiang Yang, Shu Wu, Fu Chen


## 2021-05-20

[Simple Transparent Adversarial Examples. (99%)](http://arxiv.org/abs/2105.09685)

Jaydeep Borkar, Pin-Yu Chen


[Anomaly Detection of Adversarial Examples using Class-conditional Generative Adversarial Networks. (99%)](http://arxiv.org/abs/2105.10101)

Hang Wang, David J. Miller, George Kesidis


[Preventing Machine Learning Poisoning Attacks Using Authentication and Provenance. (11%)](http://arxiv.org/abs/2105.10051)

Jack W. Stokes, Paul England, Kevin Kane


[TestRank: Bringing Order into Unlabeled Test Instances for Deep Learning Tasks. (1%)](http://arxiv.org/abs/2105.10113)

Yu Li, Min Li, Qiuxia Lai, Yannan Liu, Qiang Xu


## 2021-05-19

[Attack on practical speaker verification system using universal adversarial perturbations. (99%)](http://arxiv.org/abs/2105.09022)

Weiyi Zhang, Shuning Zhao, Le Liu, Jianmin Li, Xingliang Cheng, Thomas Fang Zheng, Xiaolin Hu


[Local Aggressive Adversarial Attacks on 3D Point Cloud. (99%)](http://arxiv.org/abs/2105.09090)

Yiming Sun, Feng Chen, Zhiyu Chen, Mingjie Wang


[An Orthogonal Classifier for Improving the Adversarial Robustness of Neural Networks. (76%)](http://arxiv.org/abs/2105.09109)

Cong Xu, Xiang Li, Min Yang


[Balancing Robustness and Sensitivity using Feature Contrastive Learning. (15%)](http://arxiv.org/abs/2105.09394)

Seungyeon Kim, Daniel Glasner, Srikumar Ramalingam, Cho-Jui Hsieh, Kishore Papineni, Sanjiv Kumar


[DeepStrike: Remotely-Guided Fault Injection Attacks on DNN Accelerator in Cloud-FPGA. (1%)](http://arxiv.org/abs/2105.09453)

Yukui Luo, Cheng Gongye, Yunsi Fei, Xiaolin Xu


[User Label Leakage from Gradients in Federated Learning. (1%)](http://arxiv.org/abs/2105.09369)

Aidmar Wainakh, Fabrizio Ventola, Till Müßig, Jens Keim, Carlos Garcia Cordero, Ephraim Zimmer, Tim Grube, Kristian Kersting, Max Mühlhäuser


[Hunter in the Dark: Deep Ensemble Networks for Discovering Anomalous Activity from Smart Networks. (1%)](http://arxiv.org/abs/2105.09157)

Shiyi Yang, Nour Moustafa, Hui Guo


## 2021-05-18

[Sparta: Spatially Attentive and Adversarially Robust Activation. (99%)](http://arxiv.org/abs/2105.08269)

Qing Guo, Felix Juefei-Xu, Changqing Zhou, Wei Feng, Yang Liu, Song Wang


[Detecting Adversarial Examples with Bayesian Neural Network. (99%)](http://arxiv.org/abs/2105.08620)

Yao Li, Tongyi Tang, Cho-Jui Hsieh, Thomas C. M. Lee


[Fighting Gradients with Gradients: Dynamic Defenses against Adversarial Attacks. (98%)](http://arxiv.org/abs/2105.08714)

Dequan Wang, An Ju, Evan Shelhamer, David Wagner, Trevor Darrell


[On the Robustness of Domain Constraints. (98%)](http://arxiv.org/abs/2105.08619)

Ryan Sheatsley, Blaine Hoak, Eric Pauley, Yohan Beugin, Michael J. Weisman, Patrick McDaniel


[Learning and Certification under Instance-targeted Poisoning. (82%)](http://arxiv.org/abs/2105.08709)

Ji Gao, Amin Karbasi, Mohammad Mahmoody


## 2021-05-17

[Towards Robust Vision Transformer. (95%)](http://arxiv.org/abs/2105.07926)

Xiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Ranjie Duan, Shaokai Ye, Yuan He, Hui Xue


[Gradient Masking and the Underestimated Robustness Threats of Differential Privacy in Deep Learning. (93%)](http://arxiv.org/abs/2105.07985)

Franziska Boenisch, Philip Sperl, Konstantin Böttinger


[An SDE Framework for Adversarial Training, with Convergence and Robustness Analysis. (69%)](http://arxiv.org/abs/2105.08037)

Haotian Gu, Xin Guo


[A Fusion-Denoising Attack on InstaHide with Data Augmentation. (1%)](http://arxiv.org/abs/2105.07754)

Xinjian Luo, Xiaokui Xiao, Yuncheng Wu, Juncheng Liu, Beng Chin Ooi


## 2021-05-16

[Vision Transformers are Robust Learners. (99%)](http://arxiv.org/abs/2105.07581)

Sayak Paul, Pin-Yu Chen


[Prototype-supervised Adversarial Network for Targeted Attack of Deep Hashing. (99%)](http://arxiv.org/abs/2105.07553)

Xunguang Wang, Zheng Zhang, Baoyuan Wu, Fumin Shen, Guangming Lu


[SoundFence: Securing Ultrasonic Sensors in Vehicles Using Physical-Layer Defense. (2%)](http://arxiv.org/abs/2105.07574)

Jianzhi Lou, Qiben Yan, Qing Hui, Huacheng Zeng


## 2021-05-15

[Real-time Detection of Practical Universal Adversarial Perturbations. (99%)](http://arxiv.org/abs/2105.07334)

Kenneth T. Co, Luis Muñoz-González, Leslie Kanthan, Emil C. Lupu


## 2021-05-14

[Salient Feature Extractor for Adversarial Defense on Deep Neural Networks. (99%)](http://arxiv.org/abs/2105.06807)

Jinyin Chen, Ruoxi Chen, Haibin Zheng, Zhaoyan Ming, Wenrong Jiang, Chen Cui


[High-Robustness, Low-Transferability Fingerprinting of Neural Networks. (9%)](http://arxiv.org/abs/2105.07078)

Siyue Wang, Xiao Wang, Pin-Yu Chen, Pu Zhao, Xue Lin


[Information-theoretic Evolution of Model Agnostic Global Explanations. (1%)](http://arxiv.org/abs/2105.06956)

Sukriti Verma, Nikaash Puri, Piyush Gupta, Balaji Krishnamurthy


[Iterative Algorithms for Assessing Network Resilience Against Structured Perturbations. (1%)](http://arxiv.org/abs/2105.07080)

Shenyu Liu, Sonia Martinez, Jorge Cortes


## 2021-05-13

[Stochastic-Shield: A Probabilistic Approach Towards Training-Free Adversarial Defense in Quantized CNNs. (98%)](http://arxiv.org/abs/2105.06512)

Lorena Qendro, Sangwon Ha, Jong René de, Partha Maji


[When Human Pose Estimation Meets Robustness: Adversarial Algorithms and Benchmarks. (5%)](http://arxiv.org/abs/2105.06152)

Jiahang Wang, Sheng Jin, Wentao Liu, Weizhong Liu, Chen Qian, Ping Luo


[DeepObliviate: A Powerful Charm for Erasing Data Residual Memory in Deep Neural Networks. (1%)](http://arxiv.org/abs/2105.06209)

Yingzhe He, Guozhu Meng, Kai Chen, Jinwen He, Xingbo Hu


[Biometrics: Trust, but Verify. (1%)](http://arxiv.org/abs/2105.06625)

Anil K. Jain, Debayan Deb, Joshua J. Engelsma


## 2021-05-12

[AVA: Adversarial Vignetting Attack against Visual Recognition. (99%)](http://arxiv.org/abs/2105.05558)

Binyu Tian, Felix Juefei-Xu, Qing Guo, Xiaofei Xie, Xiaohong Li, Yang Liu


[OutFlip: Generating Out-of-Domain Samples for Unknown Intent Detection with Natural Language Attack. (70%)](http://arxiv.org/abs/2105.05601)

DongHyun Choi, Myeong Cheol Shin, EungGyun Kim, Dong Ryeol Shin


[Adversarial Reinforcement Learning in Dynamic Channel Access and Power Control. (2%)](http://arxiv.org/abs/2105.05817)

Feng Wang, M. Cenk Gursoy, Senem Velipasalar


[A Statistical Threshold for Adversarial Classification in Laplace Mechanisms. (1%)](http://arxiv.org/abs/2105.05610)

Ayşe Ünsal, Melek Önen


## 2021-05-11

[Poisoning MorphNet for Clean-Label Backdoor Attack to Point Clouds. (99%)](http://arxiv.org/abs/2105.04839)

Guiyu Tian, Wenhao Jiang, Wei Liu, Yadong Mu


[Improving Adversarial Transferability with Gradient Refining. (99%)](http://arxiv.org/abs/2105.04834)

Guoqiu Wang, Huanqian Yan, Ying Guo, Xingxing Wei


[Accuracy-Privacy Trade-off in Deep Ensemble: A Membership Inference Perspective. (16%)](http://arxiv.org/abs/2105.05381)

Shahbaz Rezaei, Zubair Shafiq, Xin Liu


## 2021-05-10

[Adversarial examples attack based on random warm restart mechanism and improved Nesterov momentum. (99%)](http://arxiv.org/abs/2105.05029)

Tiangang Li


[Examining and Mitigating Kernel Saturation in Convolutional Neural Networks using Negative Images. (1%)](http://arxiv.org/abs/2105.04128)

Nidhi Gowdra, Roopak Sinha, Stephen MacDonell


## 2021-05-09

[Automated Decision-based Adversarial Attacks. (99%)](http://arxiv.org/abs/2105.03931)

Qi-An Fu, Yinpeng Dong, Hang Su, Jun Zhu


[Efficiency-driven Hardware Optimization for Adversarially Robust Neural Networks. (88%)](http://arxiv.org/abs/2105.04003)

Abhiroop Bhattacharjee, Abhishek Moitra, Priyadarshini Panda


[Security Concerns on Machine Learning Solutions for 6G Networks in mmWave Beam Prediction. (81%)](http://arxiv.org/abs/2105.03905)

Ferhat Ozgur Catak, Evren Catak, Murat Kuzlu, Umit Cali


[Robust Training Using Natural Transformation. (13%)](http://arxiv.org/abs/2105.04070)

Shuo Wang, Lingjuan Lyu, Surya Nepal, Carsten Rudolph, Marthie Grobler, Kristen Moore


[Learning Image Attacks toward Vision Guided Autonomous Vehicles. (4%)](http://arxiv.org/abs/2105.03834)

Hyung-Jin Yoon, Hamidreza Jafarnejadsani, Petros Voulgaris


[Combining Time-Dependent Force Perturbations in Robot-Assisted Surgery Training. (1%)](http://arxiv.org/abs/2105.03917)

Yarden Sharon, Daniel Naftalovich, Lidor Bahar, Yael Refaely, Ilana Nisky


## 2021-05-08

[Self-Supervised Adversarial Example Detection by Disentangled Representation. (99%)](http://arxiv.org/abs/2105.03689)

Zhaoxi Zhang, Leo Yu Zhang, Xufei Zheng, Shengshan Hu, Jinyu Tian, Jiantao Zhou


[De-Pois: An Attack-Agnostic Defense against Data Poisoning Attacks. (96%)](http://arxiv.org/abs/2105.03592)

Jian Chen, Xuxin Zhang, Rui Zhang, Chen Wang, Ling Liu


[Certified Robustness to Text Adversarial Attacks by Randomized [MASK]. (93%)](http://arxiv.org/abs/2105.03743)

Jiehang Zeng, Xiaoqing Zheng, Jianhan Xu, Linyang Li, Liping Yuan, Xuanjing Huang


[Provable Guarantees against Data Poisoning Using Self-Expansion and Compatibility. (81%)](http://arxiv.org/abs/2105.03692)

Charles Jin, Melinda Sun, Martin Rinard


[Mental Models of Adversarial Machine Learning. (16%)](http://arxiv.org/abs/2105.03726)

Lukas Bieringer, Kathrin Grosse, Michael Backes, Battista Biggio, Katharina Krombholz


## 2021-05-07

[Adv-Makeup: A New Imperceptible and Transferable Attack on Face Recognition. (99%)](http://arxiv.org/abs/2105.03162)

Bangjie Yin, Wenxuan Wang, Taiping Yao, Junfeng Guo, Zelun Kong, Shouhong Ding, Jilin Li, Cong Liu


[Uniform Convergence, Adversarial Spheres and a Simple Remedy. (15%)](http://arxiv.org/abs/2105.03491)

Gregor Bachmann, Seyed-Mohsen Moosavi-Dezfooli, Thomas Hofmann


## 2021-05-06

[Dynamic Defense Approach for Adversarial Robustness in Deep Neural Networks via Stochastic Ensemble Smoothed Model. (99%)](http://arxiv.org/abs/2105.02803)

Ruoxi Qin, Linyuan Wang, Xingyuan Chen, Xuehui Du, Bin Yan


[A Simple and Strong Baseline for Universal Targeted Attacks on Siamese Visual Tracking. (99%)](http://arxiv.org/abs/2105.02480)

Zhenbang Li, Yaya Shi, Jin Gao, Shaoru Wang, Bing Li, Pengpeng Liang, Weiming Hu


[Understanding Catastrophic Overfitting in Adversarial Training. (92%)](http://arxiv.org/abs/2105.02942)

Peilin Kang, Seyed-Mohsen Moosavi-Dezfooli


[Attestation Waves: Platform Trust via Remote Power Analysis. (1%)](http://arxiv.org/abs/2105.02435)

Ignacio M. Delgado-Lozano, Macarena C. Martínez-Rodríguez, Alexandros Bakas, Billy Bob Brumley, Antonis Michalas


## 2021-05-05

[Attack-agnostic Adversarial Detection on Medical Data Using Explainable Machine Learning. (99%)](http://arxiv.org/abs/2105.01959)

Matthew Durham University, Durham, UK Watson, Noura Al Durham University, Durham, UK Moubayed


[Exploiting Vulnerabilities in Deep Neural Networks: Adversarial and Fault-Injection Attacks. (97%)](http://arxiv.org/abs/2105.03251)

Faiq Khalid, Muhammad Abdullah Hanif, Muhammad Shafique


[Contrastive Learning and Self-Training for Unsupervised Domain Adaptation in Semantic Segmentation. (1%)](http://arxiv.org/abs/2105.02001)

Robert A. Marsden, Alexander Bartler, Mario Döbler, Bin Yang


[A Theoretical-Empirical Approach to Estimating Sample Complexity of DNNs. (1%)](http://arxiv.org/abs/2105.01867)

Devansh Bisla, Apoorva Nandini Saridena, Anna Choromanska


## 2021-05-04

[Poisoning the Unlabeled Dataset of Semi-Supervised Learning. (92%)](http://arxiv.org/abs/2105.01622)

Nicholas Carlini


[Broadly Applicable Targeted Data Sample Omission Attacks. (68%)](http://arxiv.org/abs/2105.01560)

Guy Barash, Eitan Farchi, Sarit Kraus, Onn Shehory


[An Overview of Laser Injection against Embedded Neural Network Models. (2%)](http://arxiv.org/abs/2105.01403)

Mathieu Dumont, Pierre-Alain Moellic, Raphael Viera, Jean-Max Dutertre, Rémi Bernhard


## 2021-05-03

[Physical world assistive signals for deep neural network classifiers -- neither defense nor attack. (83%)](http://arxiv.org/abs/2105.00622)

Camilo Pestana, Wei Liu, David Glance, Robyn Owens, Ajmal Mian


[Black-Box Dissector: Towards Erasing-based Hard-Label Model Stealing Attack. (73%)](http://arxiv.org/abs/2105.00623)

Yixu Wang, Jie Li, Hong Liu, Yan Wang, Yongjian Wu, Feiyue Huang, Rongrong Ji


## 2021-05-02

[Intriguing Usage of Applicability Domain: Lessons from Cheminformatics Applied to Adversarial Learning. (99%)](http://arxiv.org/abs/2105.00495)

Luke Chang, Katharina Dost, Kaiqi Zhao, Ambra Demontis, Fabio Roli, Gill Dobbie, Jörg Wicker


[Who's Afraid of Adversarial Transferability? (99%)](http://arxiv.org/abs/2105.00433)

Ziv Katzir, Yuval Elovici


[Multi-Robot Coordination and Planning in Uncertain and Adversarial Environments. (10%)](http://arxiv.org/abs/2105.00389)

Lifeng Zhou, Pratap Tokekar


[GRNN: Generative Regression Neural Network -- A Data Leakage Attack for Federated Learning. (2%)](http://arxiv.org/abs/2105.00529)

Hanchi Ren, Jingjing Deng, Xianghua Xie


[Spinner: Automated Dynamic Command Subsystem Perturbation. (1%)](http://arxiv.org/abs/2105.00391)

Meng Wang, Chijung Jung, Ali Ahad, Yonghwi Kwon


## 2021-05-01

[Adversarial Example Detection for DNN Models: A Review and Experimental Comparison. (99%)](http://arxiv.org/abs/2105.00203)

Ahmed Aldahdooh, Wassim Hamidouche, Sid Ahmed Fezza, Olivier Deforges


[A Perceptual Distortion Reduction Framework: Towards Generating Adversarial Examples with High Perceptual Quality and Attack Success Rate. (98%)](http://arxiv.org/abs/2105.00278)

Ruijie Yang, Yunhong Wang, Ruikui Wang, Yuanfang Guo


[On the Adversarial Robustness of Quantized Neural Networks. (75%)](http://arxiv.org/abs/2105.00227)

Micah Gorsline, James Smith, Cory Merkel


[Hidden Backdoors in Human-Centric Language Models. (73%)](http://arxiv.org/abs/2105.00164)

Shaofeng Li, Hui Liu, Tian Dong, Benjamin Zi Hao Zhao, Minhui Xue, Haojin Zhu, Jialiang Lu


[One Detector to Rule Them All: Towards a General Deepfake Attack Detection Framework. (62%)](http://arxiv.org/abs/2105.00187)

Shahroz Tariq, Sangyup Lee, Simon S. Woo


[A Master Key Backdoor for Universal Impersonation Attack against DNN-based Face Verification. (62%)](http://arxiv.org/abs/2105.00249)

Wei Guo, Benedetta Tondi, Mauro Barni


[Load Oscillating Attacks of Smart Grids: Demand Strategies and Vulnerability Analysis. (2%)](http://arxiv.org/abs/2105.00350)

Falah Alanazi, Jinsub Kim, Eduardo Cotilla-Sanchez


[RATT: Leveraging Unlabeled Data to Guarantee Generalization. (1%)](http://arxiv.org/abs/2105.00303)

Saurabh Garg, Sivaraman Balakrishnan, J. Zico Kolter, Zachary C. Lipton


## 2021-04-30

[Deep Image Destruction: A Comprehensive Study on Vulnerability of Deep Image-to-Image Models against Adversarial Attacks. (99%)](http://arxiv.org/abs/2104.15022)

Jun-Ho Choi, Huan Zhang, Jun-Hyuk Kim, Cho-Jui Hsieh, Jong-Seok Lee


[Black-box Gradient Attack on Graph Neural Networks: Deeper Insights in Graph-based Attack and Defense. (99%)](http://arxiv.org/abs/2104.15061)

Haoxi Zhan, Xiaobing Pei


[Black-box adversarial attacks using Evolution Strategies. (98%)](http://arxiv.org/abs/2104.15064)

Hao Qiu, Leonardo Lucio Custode, Giovanni Iacca


[IPatch: A Remote Adversarial Patch. (97%)](http://arxiv.org/abs/2105.00113)

Yisroel Mirsky


[DeFiRanger: Detecting Price Manipulation Attacks on DeFi Applications. (10%)](http://arxiv.org/abs/2104.15068)

Siwei Wu, Dabao Wang, Jianting He, Yajin Zhou, Lei Wu, Xingliang Yuan, Qinming He, Kui Ren


[FIPAC: Thwarting Fault- and Software-Induced Control-Flow Attacks with ARM Pointer Authentication. (2%)](http://arxiv.org/abs/2104.14993)

Robert Schilling, Pascal Nasahl, Stefan Mangard


## 2021-04-29

[GasHis-Transformer: A Multi-scale Visual Transformer Approach for Gastric Histopathology Image Classification. (67%)](http://arxiv.org/abs/2104.14528)

Haoyuan Chen, Chen Li, Xiaoyan Li, Ge Wang, Weiming Hu, Yixin Li, Wanli Liu, Changhao Sun, Yudong Yao, Yueyang Teng, Marcin Grzegorzek


[A neural anisotropic view of underspecification in deep learning. (26%)](http://arxiv.org/abs/2104.14372)

Guillermo Ortiz-Jimenez, Itamar Franco Salazar-Reque, Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard


[Analytical bounds on the local Lipschitz constants of ReLU networks. (12%)](http://arxiv.org/abs/2104.14672)

Trevor Avant, Kristi A. Morgansen


[Learning Robust Variational Information Bottleneck with Reference. (5%)](http://arxiv.org/abs/2104.14379)

Weizhu Qian, Bowei Chen, Xiaowei Huang


## 2021-04-28

[AdvHaze: Adversarial Haze Attack. (99%)](http://arxiv.org/abs/2104.13673)

Ruijun Gao, Qing Guo, Felix Juefei-Xu, Hongkai Yu, Wei Feng


## 2021-04-27

[Improved and Efficient Text Adversarial Attacks using Target Information. (97%)](http://arxiv.org/abs/2104.13484)

Mahmoud Hossam, Trung Le, He Zhao, Viet Huynh, Dinh Phung


[Metamorphic Detection of Repackaged Malware. (91%)](http://arxiv.org/abs/2104.13295)

Shirish Singh, Gail Kaiser


[Structure-Aware Hierarchical Graph Pooling using Information Bottleneck. (2%)](http://arxiv.org/abs/2104.13012)

Kashob Kumar Roy, Amit Roy, A K M Mahbubur Rahman, M Ashraful Amin, Amin Ahsan Ali


[Property Inference Attacks on Convolutional Neural Networks: Influence and Implications of Target Model's Complexity. (1%)](http://arxiv.org/abs/2104.13061)

Mathias P. M. Parisot, Balazs Pejo, Dayana Spagnuelo


## 2021-04-26

[Launching Adversarial Attacks against Network Intrusion Detection Systems for IoT. (99%)](http://arxiv.org/abs/2104.12426)

Pavlos Papadopoulos, Essen Oliver Thornewill von, Nikolaos Pitropakis, Christos Chrysoulas, Alexios Mylonas, William J. Buchanan


[Delving into Data: Effectively Substitute Training for Black-box Attack. (99%)](http://arxiv.org/abs/2104.12378)

Wenxuan Wang, Bangjie Yin, Taiping Yao, Li Zhang, Yanwei Fu, Shouhong Ding, Jilin Li, Feiyue Huang, Xiangyang Xue


[secml-malware: Pentesting Windows Malware Classifiers with Adversarial EXEmples in Python. (99%)](http://arxiv.org/abs/2104.12848)

Luca Demetrio, Battista Biggio


[Good Artists Copy, Great Artists Steal: Model Extraction Attacks Against Image Translation Generative Adversarial Networks. (98%)](http://arxiv.org/abs/2104.12623)

Sebastian Szyller, Vasisht Duddu, Tommi Gröndahl, N. Asokan


[Impact of Spatial Frequency Based Constraints on Adversarial Robustness. (98%)](http://arxiv.org/abs/2104.12679)

Rémi Bernhard, Pierre-Alain Moellic, Martial Mermillod, Yannick Bourrier, Romain Cohendet, Miguel Solinas, Marina Reyboz


[PatchGuard++: Efficient Provable Attack Detection against Adversarial Patches. (87%)](http://arxiv.org/abs/2104.12609)

Chong Xiang, Prateek Mittal


## 2021-04-25

[3D Adversarial Attacks Beyond Point Cloud. (99%)](http://arxiv.org/abs/2104.12146)

Jinlai Zhang, Lyujie Chen, Binbin Liu, Bo Ouyang, Qizhi Xie, Jihong Zhu, Weiming Li, Yanmei Meng


[Making Generated Images Hard To Spot: A Transferable Attack On Synthetic Image Detectors. (81%)](http://arxiv.org/abs/2104.12069)

Xinwei Zhao, Matthew C. Stamm


## 2021-04-24

[Influence Based Defense Against Data Poisoning Attacks in Online Learning. (99%)](http://arxiv.org/abs/2104.13230)

Sanjay Seetharaman, Shubham Malaviya, Rosni KV, Manish Shukla, Sachin Lodha


## 2021-04-23

[Theoretical Study of Random Noise Defense against Query-Based Black-Box Attacks. (98%)](http://arxiv.org/abs/2104.11470)

Zeyu Qin, Yanbo Fan, Hongyuan Zha, Baoyuan Wu


[Evaluating Deception Detection Model Robustness To Linguistic Variation. (82%)](http://arxiv.org/abs/2104.11729)

Maria Glenski, Ellyn Ayton, Robin Cosbey, Dustin Arendt, Svitlana Volkova


[Lightweight Detection of Out-of-Distribution and Adversarial Samples via Channel Mean Discrepancy. (3%)](http://arxiv.org/abs/2104.11408)

Xin Dong, Junfeng Guo, Wei-Te Ting, H. T. Kung


[Improving Neural Silent Speech Interface Models by Adversarial Training. (1%)](http://arxiv.org/abs/2104.11601)

Amin Honarmandi Shandiz, László Tóth, Gábor Gosztolya, Alexandra Markó, Tamás Gábor Csapó


## 2021-04-22

[Towards Adversarial Patch Analysis and Certified Defense against Crowd Counting. (99%)](http://arxiv.org/abs/2104.10868)

Qiming Wu, Zhikang Zou, Pan Zhou, Xiaoqing Ye, Binghui Wang, Ang Li


[Learning Transferable 3D Adversarial Cloaks for Deep Trained Detectors. (98%)](http://arxiv.org/abs/2104.11101)

Arman Maesumi, Mingkang Zhu, Yi Wang, Tianlong Chen, Zhangyang Wang, Chandrajit Bajaj


[Performance Evaluation of Adversarial Attacks: Discrepancies and Solutions. (86%)](http://arxiv.org/abs/2104.11103)

Jing Wu, Mingyi Zhou, Ce Zhu, Yipeng Liu, Mehrtash Harandi, Li Li


[Operator Shifting for General Noisy Matrix Systems. (56%)](http://arxiv.org/abs/2104.11294)

Philip Etter, Lexing Ying


[SPECTRE: Defending Against Backdoor Attacks Using Robust Statistics. (22%)](http://arxiv.org/abs/2104.11315)

Jonathan Hayase, Weihao Kong, Raghav Somani, Sewoong Oh


## 2021-04-21

[Dual Head Adversarial Training. (99%)](http://arxiv.org/abs/2104.10377)

Yujing Jiang, Xingjun Ma, Sarah Monazam Erfani, James Bailey


[Mixture of Robust Experts (MoRE): A Flexible Defense Against Multiple Perturbations. (99%)](http://arxiv.org/abs/2104.10586)

Kaidi Xu, Chenan Wang, Xue Lin, Bhavya Kailkhura, Ryan Goldhahn


[Robust Certification for Laplace Learning on Geometric Graphs. (96%)](http://arxiv.org/abs/2104.10837)

Matthew Thorpe, Bao Wang


[Jacobian Regularization for Mitigating Universal Adversarial Perturbations. (95%)](http://arxiv.org/abs/2104.10459)

Kenneth T. Co, David Martinez Rego, Emil C. Lupu


[Dataset Inference: Ownership Resolution in Machine Learning. (83%)](http://arxiv.org/abs/2104.10706)

Pratyush Maini, Mohammad Yaghini, Nicolas Papernot


## 2021-04-20

[Adversarial Training for Deep Learning-based Intrusion Detection Systems. (99%)](http://arxiv.org/abs/2104.09852)

Islam Debicha, Thibault Debatty, Jean-Michel Dricot, Wim Mees


[MixDefense: A Defense-in-Depth Framework for Adversarial Example Detection Based on Statistical and Semantic Analysis. (99%)](http://arxiv.org/abs/2104.10076)

Yijun Yang, Ruiyuan Gao, Yu Li, Qiuxia Lai, Qiang Xu


[MagicPai at SemEval-2021 Task 7: Method for Detecting and Rating Humor Based on Multi-Task Adversarial Training. (64%)](http://arxiv.org/abs/2104.10336)

Jian Ma, Shuyi Xie, Haiqin Yang, Lianxin Jiang, Mengyuan Zhou, Xiaoyi Ruan, Yang Mo


[Does enhanced shape bias improve neural network robustness to common corruptions? (26%)](http://arxiv.org/abs/2104.09789)

Chaithanya Kumar Mummadi, Ranjitha Subramaniam, Robin Hutmacher, Julien Vitay, Volker Fischer, Jan Hendrik Metzen


[Robust Sensor Fusion Algorithms Against Voice Command Attacks in Autonomous Vehicles. (9%)](http://arxiv.org/abs/2104.09872)

Jiwei Guan, Xi Zheng, Chen Wang, Yipeng Zhou, Alireza Jolfa


[Network Defense is Not a Game. (1%)](http://arxiv.org/abs/2104.10262)

Andres Molina-Markham, Ransom K. Winder, Ahmad Ridley


## 2021-04-19

[Staircase Sign Method for Boosting Adversarial Attacks. (99%)](http://arxiv.org/abs/2104.09722)

Qilong Zhang, Xiaosu Zhu, Jingkuan Song, Lianli Gao, Heng Tao Shen


[Improving Adversarial Robustness Using Proxy Distributions. (99%)](http://arxiv.org/abs/2104.09425)

Vikash Sehwag, Saeed Mahloujifar, Tinashe Handina, Sihui Dai, Chong Xiang, Mung Chiang, Prateek Mittal


[Adversarial Diffusion Attacks on Graph-based Traffic Prediction Models. (99%)](http://arxiv.org/abs/2104.09369)

Lyuyi Zhu, Kairui Feng, Ziyuan Pu, Wei Ma


[LAFEAT: Piercing Through Adversarial Defenses with Latent Features. (99%)](http://arxiv.org/abs/2104.09284)

Yunrui Yu, Xitong Gao, Cheng-Zhong Xu


[Removing Adversarial Noise in Class Activation Feature Space. (99%)](http://arxiv.org/abs/2104.09197)

Dawei Zhou, Nannan Wang, Chunlei Peng, Xinbo Gao, Xiaoyu Wang, Jun Yu, Tongliang Liu


[Direction-Aggregated Attack for Transferable Adversarial Examples. (99%)](http://arxiv.org/abs/2104.09172)

Tianjin Huang, Vlado Menkovski, Yulong Pei, YuHao Wang, Mykola Pechenizkiy


[Manipulating SGD with Data Ordering Attacks. (95%)](http://arxiv.org/abs/2104.09667)

Ilia Shumailov, Zakhar Shumaylov, Dmitry Kazhdan, Yiren Zhao, Nicolas Papernot, Murat A. Erdogdu, Ross Anderson


[Provable Robustness of Adversarial Training for Learning Halfspaces with Noise. (22%)](http://arxiv.org/abs/2104.09437)

Difan Zou, Spencer Frei, Quanquan Gu


[Protecting the Intellectual Properties of Deep Neural Networks with an Additional Class and Steganographic Images. (11%)](http://arxiv.org/abs/2104.09203)

Shichang Sun, Mingfu Xue, Jian Wang, Weiqiang Liu


[Semi-Supervised Domain Adaptation with Prototypical Alignment and Consistency Learning. (1%)](http://arxiv.org/abs/2104.09136)

Kai Li, Chang Liu, Handong Zhao, Yulun Zhang, Yun Fu


## 2021-04-18

[Best Practices for Noise-Based Augmentation to Improve the Performance of Emotion Recognition "In the Wild". (83%)](http://arxiv.org/abs/2104.08806)

Mimansa Jaiswal, Emily Mower Provost


[Making Attention Mechanisms More Robust and Interpretable with Virtual Adversarial Training. (68%)](http://arxiv.org/abs/2104.08763)

Shunsuke Kitada, Hitoshi Iyatomi


[On the Sensitivity and Stability of Model Interpretations in NLP. (1%)](http://arxiv.org/abs/2104.08782)

Fan Yin, Zhouxing Shi, Cho-Jui Hsieh, Kai-Wei Chang


## 2021-04-17

[Attacking Text Classifiers via Sentence Rewriting Sampler. (99%)](http://arxiv.org/abs/2104.08453)

Lei Xu, Kalyan Veeramachaneni


[Rethinking Image-Scaling Attacks: The Interplay Between Vulnerabilities in Machine Learning Systems. (99%)](http://arxiv.org/abs/2104.08690)

Yue Gao, Ilia Shumailov, Kassem Fawaz


[Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation. (98%)](http://arxiv.org/abs/2104.08678)

Max Bartolo, Tristan Thrush, Robin Jia, Sebastian Riedel, Pontus Stenetorp, Douwe Kiela


[Improving Zero-Shot Cross-Lingual Transfer Learning via Robust Training. (87%)](http://arxiv.org/abs/2104.08645)

Kuan-Hao Huang, Wasi Uddin Ahmad, Nanyun Peng, Kai-Wei Chang


[AM2iCo: Evaluating Word Meaning in Context across Low-ResourceLanguages with Adversarial Examples. (15%)](http://arxiv.org/abs/2104.08639)

Qianchu Liu, Edoardo M. Ponti, Diana McCarthy, Ivan Vulić, Anna Korhonen


## 2021-04-16

[Fashion-Guided Adversarial Attack on Person Segmentation. (99%)](http://arxiv.org/abs/2104.08422)

Marc Treu, Trung-Nghia Le, Huy H. Nguyen, Junichi Yamagishi, Isao Echizen


[Towards Variable-Length Textual Adversarial Attacks. (99%)](http://arxiv.org/abs/2104.08139)

Junliang Guo, Zhirui Zhang, Linlin Zhang, Linli Xu, Boxing Chen, Enhong Chen, Weihua Luo


[An Adversarially-Learned Turing Test for Dialog Generation Models. (96%)](http://arxiv.org/abs/2104.08231)

Xiang Gao, Yizhe Zhang, Michel Galley, Bill Dolan


[Random and Adversarial Bit Error Robustness: Energy-Efficient and Secure DNN Accelerators. (83%)](http://arxiv.org/abs/2104.08323)

David Stutz, Nandhini Chandramoorthy, Matthias Hein, Bernt Schiele


[Lower Bounds on Cross-Entropy Loss in the Presence of Test-time Adversaries. (2%)](http://arxiv.org/abs/2104.08382)

Arjun Nitin Bhagoji, Daniel Cullina, Vikash Sehwag, Prateek Mittal


## 2021-04-15

[Gradient-based Adversarial Attacks against Text Transformers. (99%)](http://arxiv.org/abs/2104.13733)

Chuan Guo, Alexandre Sablayrolles, Hervé Jégou, Douwe Kiela


[Robust Backdoor Attacks against Deep Neural Networks in Real Physical World. (86%)](http://arxiv.org/abs/2104.07395)

Mingfu Xue, Can He, Shichang Sun, Jian Wang, Weiqiang Liu


[Are Multilingual BERT models robust? A Case Study on Adversarial Attacks for Multilingual Question Answering. (12%)](http://arxiv.org/abs/2104.07646)

Sara Rosenthal, Mihaela Bornea, Avirup Sil


[Federated Learning for Malware Detection in IoT Devices. (10%)](http://arxiv.org/abs/2104.09994)

Valerian Rey, Pedro Miguel Sánchez Sánchez, Alberto Huertas Celdrán, Gérôme Bovet, Martin Jaggi


## 2021-04-14

[Meaningful Adversarial Stickers for Face Recognition in Physical World. (98%)](http://arxiv.org/abs/2104.06728)

Ying Guo, Xingxing Wei, Guoqiu Wang, Bo Zhang


[Orthogonalizing Convolutional Layers with the Cayley Transform. (80%)](http://arxiv.org/abs/2104.07167)

Asher Trockman, J. Zico Kolter


[Defending Against Adversarial Denial-of-Service Data Poisoning Attacks. (38%)](http://arxiv.org/abs/2104.06744)

Nicolas M. Müller, Simon Roschmann, Konstantin Böttinger


[Improved Branch and Bound for Neural Network Verification via Lagrangian Decomposition. (1%)](http://arxiv.org/abs/2104.06718)

Palma Alessandro De, Rudy Bunel, Alban Desmaison, Krishnamurthy Dvijotham, Pushmeet Kohli, Philip H. S. Torr, M. Pawan Kumar


## 2021-04-13

[Mitigating Adversarial Attack for Compute-in-Memory Accelerator Utilizing On-chip Finetune. (99%)](http://arxiv.org/abs/2104.06377)

Shanshi Huang, Hongwu Jiang, Shimeng Yu


[Detecting Operational Adversarial Examples for Reliable Deep Learning. (82%)](http://arxiv.org/abs/2104.06015)

Xingyu Zhao, Wei Huang, Sven Schewe, Yi Dong, Xiaowei Huang


[Fall of Giants: How popular text-based MLaaS fall against a simple evasion attack. (75%)](http://arxiv.org/abs/2104.05996)

Luca Pajola, Mauro Conti


## 2021-04-12

[Sparse Coding Frontend for Robust Neural Networks. (99%)](http://arxiv.org/abs/2104.05353)

Can Bakiskan, Metehan Cekic, Ahmet Dundar Sezer, Upamanyu Madhow


[A Backdoor Attack against 3D Point Cloud Classifiers. (96%)](http://arxiv.org/abs/2104.05808)

Zhen Xiang, David J. Miller, Siheng Chen, Xi Li, George Kesidis


[Plot-guided Adversarial Example Construction for Evaluating Open-domain Story Generation. (56%)](http://arxiv.org/abs/2104.05801)

Sarik Ghazarian, Zixi Liu, Akash SM, Ralph Weischedel, Aram Galstyan, Nanyun Peng


[Double Perturbation: On the Robustness of Robustness and Counterfactual Bias Evaluation. (50%)](http://arxiv.org/abs/2104.05232)

Chong Zhang, Jieyu Zhao, Huan Zhang, Kai-Wei Chang, Cho-Jui Hsieh


[Thief, Beware of What Get You There: Towards Understanding Model Extraction Attack. (1%)](http://arxiv.org/abs/2104.05921)

Xinyi Zhang, Chengfang Fang, Jie Shi


## 2021-04-11

[Achieving Model Robustness through Discrete Adversarial Training. (99%)](http://arxiv.org/abs/2104.05062)

Maor Ivgi, Jonathan Berant


[Pay attention to your loss: understanding misconceptions about 1-Lipschitz neural networks. (1%)](http://arxiv.org/abs/2104.05097)

Louis Béthune, Thibaut Boissin, Mathieu Serrurier, Franck Mamalet, Corentin Friedrich, Alberto González-Sanz


## 2021-04-10

[Distributed Estimation over Directed Graphs Resilient to Sensor Spoofing. (69%)](http://arxiv.org/abs/2104.04680)

Shamik Bhattacharyya, Kiran Rokade, Rachel Kalpana Kalaimani


[Fool Me Twice: Entailment from Wikipedia Gamification. (61%)](http://arxiv.org/abs/2104.04725)

Julian Martin Eisenschlos, Bhuwan Dhingra, Jannis Bulian, Benjamin Börschinger, Jordan Boyd-Graber


[Adversarial Regularization as Stackelberg Game: An Unrolled Optimization Approach. (15%)](http://arxiv.org/abs/2104.04886)

Simiao Zuo, Chen Liang, Haoming Jiang, Xiaodong Liu, Pengcheng He, Jianfeng Gao, Weizhu Chen, Tuo Zhao


[Disentangled Contrastive Learning for Learning Robust Textual Representations. (11%)](http://arxiv.org/abs/2104.04907)

Xiang Chen, Xin Xie, Zhen Bi, Hongbin Ye, Shumin Deng, Ningyu Zhang, Huajun Chen


## 2021-04-09

[Relating Adversarially Robust Generalization to Flat Minima. (99%)](http://arxiv.org/abs/2104.04448)

David Stutz, Matthias Hein, Bernt Schiele


[SPoTKD: A Protocol for Symmetric Key Distribution over Public Channels Using Self-Powered Timekeeping Devices. (1%)](http://arxiv.org/abs/2104.04553)

Mustafizur Rahman, Liang Zhou, Shantanu Chakrabartty


[Reversible Watermarking in Deep Convolutional Neural Networks for Integrity Authentication. (1%)](http://arxiv.org/abs/2104.04268)

Xiquan Guan, Huamin Feng, Weiming Zhang, Hang Zhou, Jie Zhang, Nenghai Yu


[Learning Sampling Policy for Faster Derivative Free Optimization. (1%)](http://arxiv.org/abs/2104.04405)

Zhou Zhai, Bin Gu, Heng Huang


## 2021-04-08

[FACESEC: A Fine-grained Robustness Evaluation Framework for Face Recognition Systems. (98%)](http://arxiv.org/abs/2104.04107)

Liang Tong, Zhengzhang Chen, Jingchao Ni, Wei Cheng, Dongjin Song, Haifeng Chen, Yevgeniy Vorobeychik


[Explainability-based Backdoor Attacks Against Graph Neural Networks. (15%)](http://arxiv.org/abs/2104.03674)

Jing Jason Xu, Jason Minhui, Xue, Stjepan Picek


[A single gradient step finds adversarial examples on random two-layers neural networks. (10%)](http://arxiv.org/abs/2104.03863)

Sébastien Bubeck, Yeshwanth Cherapanamjeri, Gauthier Gidel, Rémi Tachet des Combes


[Adversarial Learning Inspired Emerging Side-Channel Attacks and Defenses. (8%)](http://arxiv.org/abs/2104.04054)

Abhijitt Dhavlle


## 2021-04-07

[Universal Adversarial Training with Class-Wise Perturbations. (99%)](http://arxiv.org/abs/2104.03000)

Philipp Benz, Chaoning Zhang, Adil Karjauv, In So Kweon


[The art of defense: letting networks fool the attacker. (98%)](http://arxiv.org/abs/2104.02963)

Jinlai Zhang, Yinpeng Dong, Binbin Liu, Bo Ouyang, Jihong Zhu, Minchi Kuang, Houqing Wang, Yanmei Meng


[Universal Spectral Adversarial Attacks for Deformable Shapes. (81%)](http://arxiv.org/abs/2104.03356)

Arianna Rampini, Franco Pestarini, Luca Cosmo, Simone Melzi, Emanuele Rodolà


[Adversarial Robustness Guarantees for Gaussian Processes. (68%)](http://arxiv.org/abs/2104.03180)

Andrea Patane, Arno Blaas, Luca Laurenti, Luca Cardelli, Stephen Roberts, Marta Kwiatkowska


[Rethinking the Backdoor Attacks' Triggers: A Frequency Perspective. (61%)](http://arxiv.org/abs/2104.03413)

Yi Zeng, Won Park, Z. Morley Mao, Ruoxi Jia


[Improving Robustness of Deep Reinforcement Learning Agents: Environment Attacks based on Critic Networks. (10%)](http://arxiv.org/abs/2104.03154)

Lucas Schott, Manon Césaire, Hatem Hajri, Sylvain Lamprier


[Sparse Oblique Decision Trees: A Tool to Understand and Manipulate Neural Net Features. (3%)](http://arxiv.org/abs/2104.02922)

Suryabhan Singh Hada, Miguel Á. Carreira-Perpiñán, Arman Zharmagambetov


[An Object Detection based Solver for Google's Image reCAPTCHA v2. (1%)](http://arxiv.org/abs/2104.03366)

Md Imran Hossen, Yazhou Tu, Md Fazle Rabby, Md Nazmul Islam, Hui Cao, Xiali Hei


## 2021-04-06

[Exploring Targeted Universal Adversarial Perturbations to End-to-end ASR Models. (93%)](http://arxiv.org/abs/2104.02757)

Zhiyun Lu, Wei Han, Yu Zhang, Liangliang Cao


[Adversarial Robustness under Long-Tailed Distribution. (89%)](http://arxiv.org/abs/2104.02703)

Tong Wu, Ziwei Liu, Qingqiu Huang, Yu Wang, Dahua Lin


[Robust Adversarial Classification via Abstaining. (75%)](http://arxiv.org/abs/2104.02334)

Abed AlRahman Al Makdah, Vaibhav Katewa, Fabio Pasqualetti


[Backdoor Attack in the Physical World. (2%)](http://arxiv.org/abs/2104.02361)

Yiming Li, Tongqing Zhai, Yong Jiang, Zhifeng Li, Shu-Tao Xia


## 2021-04-05

[Robust Classification Under $\ell_0$ Attack for the Gaussian Mixture Model. (99%)](http://arxiv.org/abs/2104.02189)

Payam Delgosha, Hamed Hassani, Ramtin Pedarsani


[Adaptive Clustering of Robust Semantic Representations for Adversarial Image Purification. (98%)](http://arxiv.org/abs/2104.02155)

Samuel Henrique Silva, Arun Das, Ian Scarff, Peyman Najafirad


[BBAEG: Towards BERT-based Biomedical Adversarial Example Generation for Text Classification. (96%)](http://arxiv.org/abs/2104.01782)

Ishani Mondal


[Deep Learning-Based Autonomous Driving Systems: A Survey of Attacks and Defenses. (74%)](http://arxiv.org/abs/2104.01789)

Yao Deng, Tiehua Zhang, Guannan Lou, Xi Zheng, Jiong Jin, Qing-Long Han


[Can audio-visual integration strengthen robustness under multimodal attacks? (68%)](http://arxiv.org/abs/2104.02000)

Yapeng Tian, Chenliang Xu


[Jekyll: Attacking Medical Image Diagnostics using Deep Generative Models. (33%)](http://arxiv.org/abs/2104.02107)

Neal Mangaokar, Jiameng Pu, Parantapa Bhattacharya, Chandan K. Reddy, Bimal Viswanath


[Unified Detection of Digital and Physical Face Attacks. (8%)](http://arxiv.org/abs/2104.02156)

Debayan Deb, Xiaoming Liu, Anil K. Jain


[Beyond Categorical Label Representations for Image Classification. (2%)](http://arxiv.org/abs/2104.02226)

Boyuan Chen, Yu Li, Sunand Raghupathi, Hod Lipson


[Rethinking Perturbations in Encoder-Decoders for Fast Training. (1%)](http://arxiv.org/abs/2104.01853)

Sho Takase, Shun Kiyono


## 2021-04-04

[Semantically Stealthy Adversarial Attacks against Segmentation Models. (99%)](http://arxiv.org/abs/2104.01732)

Zhenhua Chen, Chuhua Wang, David J. Crandall


[Reliably fast adversarial training via latent adversarial perturbation. (93%)](http://arxiv.org/abs/2104.01575)

Geon Yeong Park, Sang Wan Lee


## 2021-04-03

[Mitigating Gradient-based Adversarial Attacks via Denoising and Compression. (99%)](http://arxiv.org/abs/2104.01494)

Rehana Mahfuz, Rajeev Sahay, Aly El Gamal


[Gradient-based Adversarial Deep Modulation Classification with Data-driven Subsampling. (93%)](http://arxiv.org/abs/2104.06375)

Jinho Yi, Aly El Gamal


[Property-driven Training: All You (N)Ever Wanted to Know About. (38%)](http://arxiv.org/abs/2104.01396)

Marco Casadio, Matthew Daggitt, Ekaterina Komendantskaya, Wen Kokke, Daniel Kienitz, Rob Stewart


## 2021-04-02

[Defending Against Image Corruptions Through Adversarial Augmentations. (92%)](http://arxiv.org/abs/2104.01086)

Dan A. Calian, Florian Stimberg, Olivia Wiles, Sylvestre-Alvise Rebuffi, Andras Gyorgy, Timothy Mann, Sven Gowal


[RABA: A Robust Avatar Backdoor Attack on Deep Neural Network. (83%)](http://arxiv.org/abs/2104.01026)

Ying He, Zhili Shen, Chang Xia, Jingyu Hua, Wei Tong, Sheng Zhong


[Diverse Gaussian Noise Consistency Regularization for Robustness and Uncertainty Calibration under Noise Domain Shifts. (2%)](http://arxiv.org/abs/2104.01231)

Athanasios Tsiligkaridis, Theodoros Tsiligkaridis


[Fast-adapting and Privacy-preserving Federated Recommender System. (1%)](http://arxiv.org/abs/2104.00919)

Qinyong Wang, Hongzhi Yin, Tong Chen, Junliang Yu, Alexander Zhou, Xiangliang Zhang


## 2021-04-01

[TRS: Transferability Reduced Ensemble via Encouraging Gradient Diversity and Model Smoothness. (99%)](http://arxiv.org/abs/2104.00671)

Zhuolin Yang, Linyi Li, Xiaojun Xu, Shiliang Zuo, Qian Chen, Benjamin Rubinstein, Pan Zhou, Ce Zhang, Bo Li


[Domain Invariant Adversarial Learning. (98%)](http://arxiv.org/abs/2104.00322)

Matan Levi, Idan Attias, Aryeh Kontorovich


[Normal vs. Adversarial: Salience-based Analysis of Adversarial Samples for Relation Extraction. (93%)](http://arxiv.org/abs/2104.00312)

Luoqiu Li, Xiang Chen, Ningyu Zhang, Shumin Deng, Xin Xie, Chuanqi Tan, Mosha Chen, Fei Huang, Huajun Chen


[Towards Evaluating and Training Verifiably Robust Neural Networks. (45%)](http://arxiv.org/abs/2104.00447)

Zhaoyang Lyu, Minghao Guo, Tong Wu, Guodong Xu, Kehuan Zhang, Dahua Lin


[Augmenting Zero Trust Architecture to Endpoints Using Blockchain: A Systematic Review. (3%)](http://arxiv.org/abs/2104.00460)

Lampis Alevizos, Vinh Thong Ta, Max Hashem Eiza


[Learning from Noisy Labels via Dynamic Loss Thresholding. (1%)](http://arxiv.org/abs/2104.02570)

Hao Yang, Youzhi Jin, Ziyin Li, Deng-Bao Wang, Lei Miao, Xin Geng, Min-Ling Zhang


## 2021-03-31

[Adversarial Heart Attack: Neural Networks Fooled to Segment Heart Symbols in Chest X-Ray Images. (99%)](http://arxiv.org/abs/2104.00139)

Gerda Bortsova, Florian Dubost, Laurens Hogeweg, Ioannis Katramados, Bruijne Marleen de


[Adversarial Attacks and Defenses for Speech Recognition Systems. (99%)](http://arxiv.org/abs/2103.17122)

Piotr Żelasko, Sonal Joshi, Yiwen Shao, Jesus Villalba, Jan Trmal, Najim Dehak, Sanjeev Khudanpur


[Fast Certified Robust Training with Short Warmup. (86%)](http://arxiv.org/abs/2103.17268)

Zhouxing Shi, Yihan Wang, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh


[Fast Jacobian-Vector Product for Deep Networks. (22%)](http://arxiv.org/abs/2104.00219)

Randall Balestriero, Richard Baraniuk


[Too Expensive to Attack: A Joint Defense Framework to Mitigate Distributed Attacks for the Internet of Things Grid. (2%)](http://arxiv.org/abs/2104.00236)

Jianhua Li, Ximeng Liu, Jiong Jin, Shui Yu


[Digital Forensics vs. Anti-Digital Forensics: Techniques, Limitations and Recommendations. (1%)](http://arxiv.org/abs/2103.17028)

Jean-Paul A. Yaacoub, Hassan N. Noura, Ola Salman, Ali Chehab


## 2021-03-30

[On the Robustness of Vision Transformers to Adversarial Examples. (99%)](http://arxiv.org/abs/2104.02610)

Kaleel Mahmood, Rigel Mahmood, Dijk Marten van


[Class-Aware Robust Adversarial Training for Object Detection. (96%)](http://arxiv.org/abs/2103.16148)

Pin-Chun Chen, Bo-Han Kung, Jun-Cheng Chen


[PointBA: Towards Backdoor Attacks in 3D Point Cloud. (92%)](http://arxiv.org/abs/2103.16074)

Xinke Li, Zhiru Chen, Yue Zhao, Zekun Tong, Yabang Zhao, Andrew Lim, Joey Tianyi Zhou


[What Causes Optical Flow Networks to be Vulnerable to Physical Adversarial Attacks. (91%)](http://arxiv.org/abs/2103.16255)

Simon Schrodi, Tonmoy Saikia, Thomas Brox


[Statistical inference for individual fairness. (67%)](http://arxiv.org/abs/2103.16714)

Subha Maity, Songkai Xue, Mikhail Yurochkin, Yuekai Sun


[Learning Lipschitz Feedback Policies from Expert Demonstrations: Closed-Loop Guarantees, Generalization and Robustness. (47%)](http://arxiv.org/abs/2103.16629)

Abed AlRahman Al Makdah, Vishaal Krishnan, Fabio Pasqualetti


[Improving robustness against common corruptions with frequency biased models. (1%)](http://arxiv.org/abs/2103.16241)

Tonmoy Saikia, Cordelia Schmid, Thomas Brox


## 2021-03-29

[Lagrangian Objective Function Leads to Improved Unforeseen Attack Generalization in Adversarial Training. (99%)](http://arxiv.org/abs/2103.15385)

Mohammad Azizmalayeri, Mohammad Hossein Rohban


[Enhancing the Transferability of Adversarial Attacks through Variance Tuning. (99%)](http://arxiv.org/abs/2103.15571)

Xiaosen Wang, Kun He


[On the Adversarial Robustness of Vision Transformers. (99%)](http://arxiv.org/abs/2103.15670)

Rulin Shao, Zhouxing Shi, Jinfeng Yi, Pin-Yu Chen, Cho-Jui Hsieh


[ZeroGrad : Mitigating and Explaining Catastrophic Overfitting in FGSM Adversarial Training. (95%)](http://arxiv.org/abs/2103.15476)

Zeinab Golgooni, Mehrdad Saberi, Masih Eskandar, Mohammad Hossein Rohban


[Certifiably-Robust Federated Adversarial Learning via Randomized Smoothing. (93%)](http://arxiv.org/abs/2103.16031)

Cheng Chen, Bhavya Kailkhura, Ryan Goldhahn, Yi Zhou


[Fooling LiDAR Perception via Adversarial Trajectory Perturbation. (83%)](http://arxiv.org/abs/2103.15326)

Yiming Li, Congcong Wen, Felix Juefei-Xu, Chen Feng


[Robust Reinforcement Learning under model misspecification. (31%)](http://arxiv.org/abs/2103.15370)

Lebin Yu, Jian Wang, Xudong Zhang


[Automating Defense Against Adversarial Attacks: Discovery of Vulnerabilities and Application of Multi-INT Imagery to Protect Deployed Models. (16%)](http://arxiv.org/abs/2103.15897)

Josh Kalin, David Noever, Matthew Ciolino, Dominick Hambrick, Gerry Dozier


[MISA: Online Defense of Trojaned Models using Misattributions. (10%)](http://arxiv.org/abs/2103.15918)

Panagiota Kiourti, Wenchao Li, Anirban Roy, Karan Sikka, Susmit Jha


[Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models. (9%)](http://arxiv.org/abs/2103.15543)

Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun, Bin He


[Selective Output Smoothing Regularization: Regularize Neural Networks by Softening Output Distributions. (1%)](http://arxiv.org/abs/2103.15383)

Xuan Cheng, Tianshu Xie, Xiaomin Wang, Qifeng Weng, Minghui Liu, Jiali Deng, Ming Liu


## 2021-03-28

[Improved Autoregressive Modeling with Distribution Smoothing. (86%)](http://arxiv.org/abs/2103.15089)

Chenlin Meng, Jiaming Song, Yang Song, Shengjia Zhao, Stefano Ermon


## 2021-03-27

[On the benefits of robust models in modulation recognition. (99%)](http://arxiv.org/abs/2103.14977)

Javier Maroto, Gérôme Bovet, Pascal Frossard


[IoU Attack: Towards Temporally Coherent Black-Box Adversarial Attack for Visual Object Tracking. (99%)](http://arxiv.org/abs/2103.14938)

Shuai Jia, Yibing Song, Chao Ma, Xiaokang Yang


[LiBRe: A Practical Bayesian Approach to Adversarial Detection. (99%)](http://arxiv.org/abs/2103.14835)

Zhijie Deng, Xiao Yang, Shizhen Xu, Hang Su, Jun Zhu


## 2021-03-26

[Cyclic Defense GAN Against Speech Adversarial Attacks. (99%)](http://arxiv.org/abs/2103.14717)

Mohammad Esmaeilpour, Patrick Cardinal, Alessandro Lameiras Koerich


[Combating Adversaries with Anti-Adversaries. (93%)](http://arxiv.org/abs/2103.14347)

Motasem Alfarra, Juan C. Pérez, Ali Thabet, Adel Bibi, Philip H. S. Torr, Bernard Ghanem


[On Generating Transferable Targeted Perturbations. (93%)](http://arxiv.org/abs/2103.14641)

Muzammal Naseer, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Fatih Porikli


[Building Reliable Explanations of Unreliable Neural Networks: Locally Smoothing Perspective of Model Interpretation. (86%)](http://arxiv.org/abs/2103.14332)

Dohun Lim, Hyeonseok Lee, Sungchan Kim


[Ensemble-in-One: Learning Ensemble within Random Gated Networks for Enhanced Adversarial Robustness. (83%)](http://arxiv.org/abs/2103.14795)

Yi Cai, Xuefei Ning, Huazhong Yang, Yu Wang


[Visual Explanations from Spiking Neural Networks using Interspike Intervals. (62%)](http://arxiv.org/abs/2103.14441)

Youngeun Kim, Priyadarshini Panda


[Unsupervised Robust Domain Adaptation without Source Data. (13%)](http://arxiv.org/abs/2103.14577)

Peshal Agarwal, Danda Pani Paudel, Jan-Nico Zaech, Gool Luc Van


## 2021-03-25

[Adversarial Attacks are Reversible with Natural Supervision. (99%)](http://arxiv.org/abs/2103.14222)

Chengzhi Mao, Mia Chiquier, Hao Wang, Junfeng Yang, Carl Vondrick


[Adversarial Attacks on Deep Learning Based mmWave Beam Prediction in 5G and Beyond. (98%)](http://arxiv.org/abs/2103.13989)

Brian Kim, Yalin E. Sagduyu, Tugba Erpek, Sennur Ulukus


[MagDR: Mask-guided Detection and Reconstruction for Defending Deepfakes. (81%)](http://arxiv.org/abs/2103.14211)

Zhikai Chen, Lingxi Xie, Shanmin Pang, Yong He, Bo Zhang


[Deep-RBF Networks for Anomaly Detection in Automotive Cyber-Physical Systems. (70%)](http://arxiv.org/abs/2103.14172)

Matthew Burruss, Shreyas Ramakrishna, Abhishek Dubey


[Orthogonal Projection Loss. (45%)](http://arxiv.org/abs/2103.14021)

Kanchana Ranasinghe, Muzammal Naseer, Munawar Hayat, Salman Khan, Fahad Shahbaz Khan


[THAT: Two Head Adversarial Training for Improving Robustness at Scale. (26%)](http://arxiv.org/abs/2103.13612)

Zuxuan Wu, Tom Goldstein, Larry S. Davis, Ser-Nam Lim


[A Survey of Microarchitectural Side-channel Vulnerabilities, Attacks and Defenses in Cryptography. (11%)](http://arxiv.org/abs/2103.14244)

Xiaoxuan Lou, Tianwei Zhang, Jun Jiang, Yinqian Zhang


[HufuNet: Embedding the Left Piece as Watermark and Keeping the Right Piece for Ownership Verification in Deep Neural Networks. (10%)](http://arxiv.org/abs/2103.13628)

Peizhuo Lv, Pan Li, Shengzhi Zhang, Kai Chen, Ruigang Liang, Yue Zhao, Yingjiu Li


[The Geometry of Over-parameterized Regression and Adversarial Perturbations. (2%)](http://arxiv.org/abs/2103.14108)

Jason W. Rocks, Pankaj Mehta


[Synthesize-It-Classifier: Learning a Generative Classifier through RecurrentSelf-analysis. (1%)](http://arxiv.org/abs/2103.14212)

Arghya Pal, Rapha Phan, KokSheik Wong


[Spirit Distillation: Precise Real-time Prediction with Insufficient Data. (1%)](http://arxiv.org/abs/2103.13733)

Zhiyuan Wu, Hong Qi, Yu Jiang, Chupeng Cui, Zongmin Yang, Xinhui Xue


[Recent Advances in Large Margin Learning. (1%)](http://arxiv.org/abs/2103.13598)

Yiwen Guo, Changshui Zhang


## 2021-03-24

[Towards Both Accurate and Robust Neural Networks without Extra Data. (99%)](http://arxiv.org/abs/2103.13124)

Faqiang Liu, Rong Zhao


[Vulnerability of Appearance-based Gaze Estimation. (97%)](http://arxiv.org/abs/2103.13134)

Mingjie Xu, Haofei Wang, Yunfei Liu, Feng Lu


[Black-box Detection of Backdoor Attacks with Limited Information and Data. (96%)](http://arxiv.org/abs/2103.13127)

Yinpeng Dong, Xiao Yang, Zhijie Deng, Tianyu Pang, Zihao Xiao, Hang Su, Jun Zhu


[Deepfake Forensics via An Adversarial Game. (10%)](http://arxiv.org/abs/2103.13567)

Zhi Wang, Yiwen Guo, Wangmeng Zuo


## 2021-03-23

[Robust and Accurate Object Detection via Adversarial Learning. (98%)](http://arxiv.org/abs/2103.13886)

Xiangning Chen, Cihang Xie, Mingxing Tan, Li Zhang, Cho-Jui Hsieh, Boqing Gong


[CLIP: Cheap Lipschitz Training of Neural Networks. (96%)](http://arxiv.org/abs/2103.12531)

Leon Bungert, René Raab, Tim Roith, Leo Schwinn, Daniel Tenbrinck


[The Hammer and the Nut: Is Bilevel Optimization Really Needed to Poison Linear Classifiers? (92%)](http://arxiv.org/abs/2103.12399)

Antonio Emanuele Cinà, Sebastiano Vascon, Ambra Demontis, Battista Biggio, Fabio Roli, Marcello Pelillo


[Characterizing and Improving the Robustness of Self-Supervised Learning through Background Augmentations. (87%)](http://arxiv.org/abs/2103.12719)

Chaitanya K. Ryali, David J. Schwab, Ari S. Morcos


[RPATTACK: Refined Patch Attack on General Object Detectors. (76%)](http://arxiv.org/abs/2103.12469)

Hao Huang, Yongtao Wang, Zhaoyu Chen, Zhi Tang, Wenqiang Zhang, Kai-Kuang Ma


[NNrepair: Constraint-based Repair of Neural Network Classifiers. (50%)](http://arxiv.org/abs/2103.12535)

Muhammad Usman, Divya Gopinath, Youcheng Sun, Yannic Noller, Corina Pasareanu


[Are all outliers alike? On Understanding the Diversity of Outliers for Detecting OODs. (31%)](http://arxiv.org/abs/2103.12628)

Ramneet Kaur, Susmit Jha, Anirban Roy, Oleg Sokolsky, Insup Lee


[Improved Estimation of Concentration Under $\ell_p$-Norm Distance Metrics Using Half Spaces. (22%)](http://arxiv.org/abs/2103.12913)

Jack Prescott, Xiao Zhang, David Evans


[ESCORT: Ethereum Smart COntRacTs Vulnerability Detection using Deep Neural Network and Transfer Learning. (1%)](http://arxiv.org/abs/2103.12607)

Oliver Lutz, Huili Chen, Hossein Fereidooni, Christoph Sendner, Alexandra Dmitrienko, Ahmad Reza Sadeghi, Farinaz Koushanfar


## 2021-03-22

[Grey-box Adversarial Attack And Defence For Sentiment Classification. (99%)](http://arxiv.org/abs/2103.11576)

Ying Xu, Xu Zhong, Antonio Jimeno Yepes, Jey Han Lau


[Fast Approximate Spectral Normalization for Robust Deep Neural Networks. (98%)](http://arxiv.org/abs/2103.13815)

Zhixin Pan, Prabhat Mishra


[Spatio-Temporal Sparsification for General Robust Graph Convolution Networks. (87%)](http://arxiv.org/abs/2103.12256)

Mingming Lu, Ya Zhang


[RA-BNN: Constructing Robust & Accurate Binary Neural Network to Simultaneously Defend Adversarial Bit-Flip Attack and Improve Accuracy. (75%)](http://arxiv.org/abs/2103.13813)

Adnan Siraj Rakin, Li Yang, Jingtao Li, Fan Yao, Chaitali Chakrabarti, Yu Cao, Jae-sun Seo, Deliang Fan


[Adversarial Feature Augmentation and Normalization for Visual Recognition. (13%)](http://arxiv.org/abs/2103.12171)

Tianlong Chen, Yu Cheng, Zhe Gan, Jianfeng Wang, Lijuan Wang, Zhangyang Wang, Jingjing Liu


[Adversarially Optimized Mixup for Robust Classification. (13%)](http://arxiv.org/abs/2103.11589)

Jason Bunk, Srinjoy Chattopadhyay, B. S. Manjunath, Shivkumar Chandrasekaran


## 2021-03-21

[ExAD: An Ensemble Approach for Explanation-based Adversarial Detection. (99%)](http://arxiv.org/abs/2103.11526)

Raj Vardhan, Ninghao Liu, Phakpoom Chinprutthiwong, Weijie Fu, Zhenyu Hu, Xia Ben Hu, Guofei Gu


[TextFlint: Unified Multilingual Robustness Evaluation Toolkit for Natural Language Processing. (75%)](http://arxiv.org/abs/2103.11441)

Tao Gui, Xiao Wang, Qi Zhang, Qin Liu, Yicheng Zou, Xin Zhou, Rui Zheng, Chong Zhang, Qinzhuo Wu, Jiacheng Ye, Zexiong Pang, Yongxin Zhang, Zhengyan Li, Ruotian Ma, Zichu Fei, Ruijian Cai, Jun Zhao, Xinwu Hu, Zhiheng Yan, Yiding Tan, Yuan Hu, Qiyuan Bian, Zhihua Liu, Bolin Zhu, Shan Qin, Xiaoyu Xing, Jinlan Fu, Yue Zhang, Minlong Peng, Xiaoqing Zheng, Yaqian Zhou, Zhongyu Wei, Xipeng Qiu, Xuanjing Huang


[Natural Perturbed Training for General Robustness of Neural Network Classifiers. (38%)](http://arxiv.org/abs/2103.11372)

Sadaf Gulshad, Arnold Smeulders


[Self adversarial attack as an augmentation method for immunohistochemical stainings. (33%)](http://arxiv.org/abs/2103.11362)

Jelica Vasiljević, Friedrich Feuerhake, Cédric Wemmert, Thomas Lampert


## 2021-03-20

[Robust Models Are More Interpretable Because Attributions Look Normal. (15%)](http://arxiv.org/abs/2103.11257)

Zifan Wang, Matt Fredrikson, Anupam Datta


## 2021-03-19

[LSDAT: Low-Rank and Sparse Decomposition for Decision-based Adversarial Attack. (99%)](http://arxiv.org/abs/2103.10787)

Ashkan Esmaeili, Marzieh Edraki, Nazanin Rahnavard, Mubarak Shah, Ajmal Mian


[SoK: A Modularized Approach to Study the Security of Automatic Speech Recognition Systems. (93%)](http://arxiv.org/abs/2103.10651)

Yuxuan Chen, Jiangshan Zhang, Xuejing Yuan, Shengzhi Zhang, Kai Chen, Xiaofeng Wang, Shanqing Guo


[Attribution of Gradient Based Adversarial Attacks for Reverse Engineering of Deceptions. (86%)](http://arxiv.org/abs/2103.11002)

Michael Goebel, Jason Bunk, Srinjoy Chattopadhyay, Lakshmanan Nataraj, Shivkumar Chandrasekaran, B. S. Manjunath


[Interpretable Deep Learning: Interpretation, Interpretability, Trustworthiness, and Beyond. (2%)](http://arxiv.org/abs/2103.10689)

Xuhong Li, Haoyi Xiong, Xingjian Li, Xuanyu Wu, Xiao Zhang, Ji Liu, Jiang Bian, Dejing Dou


## 2021-03-18

[Generating Adversarial Computer Programs using Optimized Obfuscations. (99%)](http://arxiv.org/abs/2103.11882)

Shashank Srikant, Sijia Liu, Tamara Mitrovska, Shiyu Chang, Quanfu Fan, Gaoyuan Zhang, Una-May O'Reilly


[Boosting Adversarial Transferability through Enhanced Momentum. (99%)](http://arxiv.org/abs/2103.10609)

Xiaosen Wang, Jiadong Lin, Han Hu, Jingdong Wang, Kun He


[Explainable Adversarial Attacks in Deep Neural Networks Using Activation Profiles. (98%)](http://arxiv.org/abs/2103.10229)

Gabriel D. Cantareira, Rodrigo F. Mello, Fernando V. Paulovich


[Enhancing Transformer for Video Understanding Using Gated Multi-Level Attention and Temporal Adversarial Training. (76%)](http://arxiv.org/abs/2103.10043)

Saurabh Sahu, Palash Goyal


[Model Extraction and Adversarial Transferability, Your BERT is Vulnerable! (69%)](http://arxiv.org/abs/2103.10013)

Xuanli He, Lingjuan Lyu, Qiongkai Xu, Lichao Sun


[TOP: Backdoor Detection in Neural Networks via Transferability of Perturbation. (61%)](http://arxiv.org/abs/2103.10274)

Todd Huster, Emmanuel Ekwedike


[Noise Modulation: Let Your Model Interpret Itself. (54%)](http://arxiv.org/abs/2103.10603)

Haoyang Li, Xinggang Wang


[KoDF: A Large-scale Korean DeepFake Detection Dataset. (16%)](http://arxiv.org/abs/2103.10094)

Patrick Kwon, Jaeseong You, Gyuhyeon Nam, Sungwoo Park, Gyeongsu Chae


[Reading Isn't Believing: Adversarial Attacks On Multi-Modal Neurons. (9%)](http://arxiv.org/abs/2103.10480)

David A. Noever, Samantha E. Miller Noever


## 2021-03-17

[Can Targeted Adversarial Examples Transfer When the Source and Target Models Have No Label Space Overlap? (99%)](http://arxiv.org/abs/2103.09916)

Nathan Inkawhich, Kevin J Liang, Jingyang Zhang, Huanrui Yang, Hai Li, Yiran Chen


[Adversarial Attacks on Camera-LiDAR Models for 3D Car Detection. (98%)](http://arxiv.org/abs/2103.09448)

Mazen Abdelfattah, Kaiwen Yuan, Z. Jane Wang, Rabab Ward


[Improved, Deterministic Smoothing for L1 Certified Robustness. (82%)](http://arxiv.org/abs/2103.10834)

Alexander Levine, Soheil Feizi


[Understanding Generalization in Adversarial Training via the Bias-Variance Decomposition. (41%)](http://arxiv.org/abs/2103.09947)

Yaodong Yu, Zitong Yang, Edgar Dobriban, Jacob Steinhardt, Yi Ma


[Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots. (38%)](http://arxiv.org/abs/2103.09593)

Samson Tan, Shafiq Joty


[Cyber Intrusion Detection by Using Deep Neural Networks with Attack-sharing Loss. (13%)](http://arxiv.org/abs/2103.09713)

Boxiang Wendy Dong, Wendy Hui, Wang, Aparna S. Varde, Dawei Li, Bharath K. Samanthula, Weifeng Sun, Liang Zhao


## 2021-03-16

[Adversarial Driving: Attacking End-to-End Autonomous Driving. (93%)](http://arxiv.org/abs/2103.09151)

Han Wu, Syed Yunas, Sareh Rowlands, Wenjie Ruan, Johan Wahlstrom


[Adversarial YOLO: Defense Human Detection Patch Attacks via Detecting Adversarial Patches. (92%)](http://arxiv.org/abs/2103.08860)

Nan Ji, YanFei Feng, Haidong Xie, Xueshuang Xiang, Naijin Liu


[Anti-Adversarially Manipulated Attributions for Weakly and Semi-Supervised Semantic Segmentation. (75%)](http://arxiv.org/abs/2103.08896)

Jungbeom Lee, Eunji Kim, Sungroh Yoon


[Bio-inspired Robustness: A Review. (70%)](http://arxiv.org/abs/2103.09265)

Harshitha Machiraju, Oh-Hyeon Choung, Pascal Frossard, Michael. H Herzog


## 2021-03-15

[Constant Random Perturbations Provide Adversarial Robustness with Minimal Effect on Accuracy. (83%)](http://arxiv.org/abs/2103.08265)

Bronya Roni Chernyak, Bhiksha Raj, Tamir Hazan, Joseph Keshet


[Adversarial Training is Not Ready for Robot Learning. (67%)](http://arxiv.org/abs/2103.08187)

Mathias Lechner, Ramin Hasani, Radu Grosu, Daniela Rus, Thomas A. Henzinger


[HDTest: Differential Fuzz Testing of Brain-Inspired Hyperdimensional Computing. (64%)](http://arxiv.org/abs/2103.08668)

Dongning Ma, Jianmin Guo, Yu Jiang, Xun Jiao


[Understanding invariance via feedforward inversion of discriminatively trained classifiers. (10%)](http://arxiv.org/abs/2103.07470)

Piotr Teterwak, Chiyuan Zhang, Dilip Krishnan, Michael C. Mozer


[Meta-Solver for Neural Ordinary Differential Equations. (2%)](http://arxiv.org/abs/2103.08561)

Julia Gusak, Alexandr Katrutsa, Talgat Daulbaev, Andrzej Cichocki, Ivan Oseledets


## 2021-03-14

[Towards Robust Speech-to-Text Adversarial Attack. (99%)](http://arxiv.org/abs/2103.08095)

Mohammad Esmaeilpour, Patrick Cardinal, Alessandro Lameiras Koerich


[BreakingBED -- Breaking Binary and Efficient Deep Neural Networks by Adversarial Attacks. (98%)](http://arxiv.org/abs/2103.08031)

Manoj Rohit Vemparala, Alexander Frickenstein, Nael Fasfous, Lukas Frickenstein, Qi Zhao, Sabine Kuhn, Daniel Ehrhardt, Yuankai Wu, Christian Unger, Naveen Shankar Nagaraja, Walter Stechele


[Multi-Discriminator Sobolev Defense-GAN Against Adversarial Attacks for End-to-End Speech Systems. (82%)](http://arxiv.org/abs/2103.08086)

Mohammad Esmaeilpour, Patrick Cardinal, Alessandro Lameiras Koerich


[Membership Inference Attacks on Machine Learning: A Survey. (68%)](http://arxiv.org/abs/2103.07853)

Hongsheng Hu, Zoran Salcic, Lichao Sun, Gillian Dobbie, Philip S. Yu, Xuyun Zhang


## 2021-03-13

[Attack as Defense: Characterizing Adversarial Examples using Robustness. (99%)](http://arxiv.org/abs/2103.07633)

Zhe Zhao, Guangke Chen, Jingyi Wang, Yiwei Yang, Fu Song, Jun Sun


[Generating Unrestricted Adversarial Examples via Three Parameters. (99%)](http://arxiv.org/abs/2103.07640)

Hanieh Naderi, Leili Goli, Shohreh Kasaei


[Simeon -- Secure Federated Machine Learning Through Iterative Filtering. (12%)](http://arxiv.org/abs/2103.07704)

Nicholas Malecki, Hye-young Paik, Aleksandar Ignjatovic, Alan Blair, Elisa Bertino


## 2021-03-12

[Learning Defense Transformers for Counterattacking Adversarial Examples. (99%)](http://arxiv.org/abs/2103.07595)

Jincheng Li, Jiezhang Cao, Yifan Zhang, Jian Chen, Mingkui Tan


[Internal Wasserstein Distance for Adversarial Attack and Defense. (99%)](http://arxiv.org/abs/2103.07598)

Mingkui Tan, Shuhai Zhang, Jiezhang Cao, Jincheng Li, Yanwu Xu


[A Unified Game-Theoretic Interpretation of Adversarial Robustness. (98%)](http://arxiv.org/abs/2103.07364)

Jie Ren, Die Zhang, Yisen Wang, Lu Chen, Zhanpeng Zhou, Yiting Chen, Xu Cheng, Xin Wang, Meng Zhou, Jie Shi, Quanshi Zhang


[Adversarial Machine Learning Security Problems for 6G: mmWave Beam Prediction Use-Case. (82%)](http://arxiv.org/abs/2103.07268)

Evren Catak, Ferhat Ozgur Catak, Arild Moldsvor


[Network Environment Design for Autonomous Cyberdefense. (1%)](http://arxiv.org/abs/2103.07583)

Andres Molina-Markham, Cory Miniter, Becky Powell, Ahmad Ridley


## 2021-03-11

[Stochastic-HMDs: Adversarial Resilient Hardware Malware Detectors through Voltage Over-scaling. (99%)](http://arxiv.org/abs/2103.06936)

Md Shohidul Islam, Ihsen Alouani, Khaled N. Khasawneh


[Beta-CROWN: Efficient Bound Propagation with Per-neuron Split Constraints for Complete and Incomplete Neural Network Verification. (99%)](http://arxiv.org/abs/2103.06624)

Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, J. Zico Kolter


[Adversarial Laser Beam: Effective Physical-World Attack to DNNs in a Blink. (99%)](http://arxiv.org/abs/2103.06504)

Ranjie Duan, Xiaofeng Mao, A. K. Qin, Yun Yang, Yuefeng Chen, Shaokai Ye, Yuan He


[DAFAR: Detecting Adversaries by Feedback-Autoencoder Reconstruction. (99%)](http://arxiv.org/abs/2103.06487)

Haowen Liu, Ping Yi, Hsiao-Ying Lin, Jie Shi


[ReinforceBug: A Framework to Generate Adversarial Textual Examples. (97%)](http://arxiv.org/abs/2103.08306)

Bushra Sabir, M. Ali Babar, Raj Gaire


[Multi-Task Federated Reinforcement Learning with Adversaries. (15%)](http://arxiv.org/abs/2103.06473)

Aqeel Anwar, Arijit Raychowdhury


[BODAME: Bilevel Optimization for Defense Against Model Extraction. (8%)](http://arxiv.org/abs/2103.06797)

Yuto Mori, Atsushi Nitanda, Akiko Takeda


## 2021-03-10

[Improving Adversarial Robustness via Channel-wise Activation Suppressing. (99%)](http://arxiv.org/abs/2103.08307)

Yang Bai, Yuyuan Zeng, Yong Jiang, Shu-Tao Xia, Xingjun Ma, Yisen Wang


[TANTRA: Timing-Based Adversarial Network Traffic Reshaping Attack. (92%)](http://arxiv.org/abs/2103.06297)

Yam Sharon, David Berend, Yang Liu, Asaf Shabtai, Yuval Elovici


[VideoMoCo: Contrastive Video Representation Learning with Temporally Adversarial Examples. (67%)](http://arxiv.org/abs/2103.05905)

Tian Pan, Yibing Song, Tianyu Yang, Wenhao Jiang, Wei Liu


[Fine-tuning of Pre-trained End-to-end Speech Recognition with Generative Adversarial Networks. (1%)](http://arxiv.org/abs/2103.13329)

Md Akmal Haidar, Mehdi Rezagholizadeh


## 2021-03-09

[Stabilized Medical Image Attacks. (99%)](http://arxiv.org/abs/2103.05232)

Gege Qi, Lijun Gong, Yibing Song, Kai Ma, Yefeng Zheng


[Revisiting Model's Uncertainty and Confidences for Adversarial Example Detection. (99%)](http://arxiv.org/abs/2103.05354)

Ahmed Aldahdooh, Wassim Hamidouche, Olivier Déforges


[Practical Relative Order Attack in Deep Ranking. (99%)](http://arxiv.org/abs/2103.05248)

Mo Zhou, Le Wang, Zhenxing Niu, Qilin Zhang, Yinghui Xu, Nanning Zheng, Gang Hua


[BASAR:Black-box Attack on Skeletal Action Recognition. (99%)](http://arxiv.org/abs/2103.05266)

Yunfeng Diao, Tianjia Shao, Yong-Liang Yang, Kun Zhou, He Wang


[Understanding the Robustness of Skeleton-based Action Recognition under Adversarial Attack. (98%)](http://arxiv.org/abs/2103.05347)

He Wang, Feixiang He, Zhexi Peng, Tianjia Shao, Yong-Liang Yang, Kun Zhou, David Hogg


[Deep Learning for Android Malware Defenses: a Systematic Literature Review. (11%)](http://arxiv.org/abs/2103.05292)

Yue Liu, Chakkrit Tantithamthavorn, Li Li, Yepang Liu


[Robust Black-box Watermarking for Deep NeuralNetwork using Inverse Document Frequency. (10%)](http://arxiv.org/abs/2103.05590)

Mohammad Mehdi Yadollahi, Farzaneh Shoeleh, Sajjad Dadkhah, Ali A. Ghorbani


[Towards Strengthening Deep Learning-based Side Channel Attacks with Mixup. (2%)](http://arxiv.org/abs/2103.05833)

Zhimin Luo, Mengce Zheng, Ping Wang, Minhui Jin, Jiajia Zhang, Honggang Hu, Nenghai Yu


## 2021-03-08

[Enhancing Transformation-based Defenses against Adversarial Examples with First-Order Perturbations. (99%)](http://arxiv.org/abs/2103.04565)

Haimin Zhang, Min Xu


[Packet-Level Adversarial Network Traffic Crafting using Sequence Generative Adversarial Networks. (99%)](http://arxiv.org/abs/2103.04794)

Qiumei Cheng, Shiying Zhou, Yi Shen, Dezhang Kong, Chunming Wu


[Contemplating real-world object classification. (81%)](http://arxiv.org/abs/2103.05137)

Ali Borji


[Consistency Regularization for Adversarial Robustness. (50%)](http://arxiv.org/abs/2103.04623)

Jihoon Tack, Sihyun Yu, Jongheon Jeong, Minseon Kim, Sung Ju Hwang, Jinwoo Shin


[Prime+Probe 1, JavaScript 0: Overcoming Browser-based Side-Channel Defenses. (2%)](http://arxiv.org/abs/2103.04952)

Anatoly Shusterman, Ayush Agarwal, Sioli O'Connell, Daniel Genkin, Yossi Oren, Yuval Yarom


[Deeply Unsupervised Patch Re-Identification for Pre-training Object Detectors. (1%)](http://arxiv.org/abs/2103.04814)

Jian Ding, Enze Xie, Hang Xu, Chenhan Jiang, Zhenguo Li, Ping Luo, Gui-Song Xia


[Deep Model Intellectual Property Protection via Deep Watermarking. (1%)](http://arxiv.org/abs/2103.04980)

Jie Zhang, Dongdong Chen, Jing Liao, Weiming Zhang, Huamin Feng, Gang Hua, Nenghai Yu


## 2021-03-07

[Universal Adversarial Perturbations and Image Spam Classifiers. (99%)](http://arxiv.org/abs/2103.05469)

Andy Phung, Mark Stamp


[Detecting Adversarial Examples from Sensitivity Inconsistency of Spatial-Transform Domain. (99%)](http://arxiv.org/abs/2103.04302)

Jinyu Tian, Jiantao Zhou, Yuanman Li, Jia Duan


[Improving Global Adversarial Robustness Generalization With Adversarially Trained GAN. (99%)](http://arxiv.org/abs/2103.04513)

Desheng School of Electrical Engineering, Southwest Jiaotong University, Chengdu, P. R. China Wang, Weidong School of Electrical Engineering, Southwest Jiaotong University, Chengdu, P. R. China Jin, Yunpu School of Electrical Engineering, Southwest Jiaotong University, Chengdu, P. R. China Wu, Aamir School of Electrical Engineering, Southwest Jiaotong University, Chengdu, P. R. China Khan


[Insta-RS: Instance-wise Randomized Smoothing for Improved Robustness and Accuracy. (76%)](http://arxiv.org/abs/2103.04436)

Chen Chen, Kezhi Kong, Peihong Yu, Juan Luque, Tom Goldstein, Furong Huang


## 2021-03-06

[T-Miner: A Generative Approach to Defend Against Trojan Attacks on DNN-based Text Classification. (98%)](http://arxiv.org/abs/2103.04264)

Ahmadreza Azizi, Ibrahim Asadullah Tahmid, Asim Waheed, Neal Mangaokar, Jiameng Pu, Mobin Javed, Chandan K. Reddy, Bimal Viswanath


[Hidden Backdoor Attack against Semantic Segmentation Models. (93%)](http://arxiv.org/abs/2103.04038)

Yiming Li, Yanjie Li, Yalei Lv, Yong Jiang, Shu-Tao Xia


## 2021-03-05

[Cyber Threat Intelligence Model: An Evaluation of Taxonomies, Sharing Standards, and Ontologies within Cyber Threat Intelligence. (13%)](http://arxiv.org/abs/2103.03530)

Vasileios Mavroeidis, Siri Bromander


[Don't Forget to Sign the Gradients! (10%)](http://arxiv.org/abs/2103.03701)

Omid Aramoon, Pin-Yu Chen, Gang Qu


[Tor circuit fingerprinting defenses using adaptive padding. (1%)](http://arxiv.org/abs/2103.03831)

George Kadianakis, Theodoros Polyzos, Mike Perry, Kostas Chatzikokolakis


## 2021-03-04

[Hard-label Manifolds: Unexpected Advantages of Query Efficiency for Finding On-manifold Adversarial Examples. (99%)](http://arxiv.org/abs/2103.03325)

Washington Garcia, Pin-Yu Chen, Somesh Jha, Scott Clouse, Kevin R. B. Butler


[WaveGuard: Understanding and Mitigating Audio Adversarial Examples. (99%)](http://arxiv.org/abs/2103.03344)

Shehzeen Hussain, Paarth Neekhara, Shlomo Dubnov, Julian McAuley, Farinaz Koushanfar


[Towards Evaluating the Robustness of Deep Diagnostic Models by Adversarial Attack. (99%)](http://arxiv.org/abs/2103.03438)

Mengting Xu, Tao Zhang, Zhongnian Li, Mingxia Liu, Daoqiang Zhang


[QAIR: Practical Query-efficient Black-Box Attacks for Image Retrieval. (99%)](http://arxiv.org/abs/2103.02927)

Xiaodan Li, Jinfeng Li, Yuefeng Chen, Shaokai Ye, Yuan He, Shuhui Wang, Hang Su, Hui Xue


[SpectralDefense: Detecting Adversarial Attacks on CNNs in the Fourier Domain. (99%)](http://arxiv.org/abs/2103.03000)

Paula Harder, Franz-Josef Pfreundt, Margret Keuper, Janis Keuper


[Gradient-Guided Dynamic Efficient Adversarial Training. (96%)](http://arxiv.org/abs/2103.03076)

Fu Wang, Yanghao Zhang, Yanbin Zheng, Wenjie Ruan


[PointGuard: Provably Robust 3D Point Cloud Classification. (92%)](http://arxiv.org/abs/2103.03046)

Hongbin Liu, Jinyuan Jia, Neil Zhenqiang Gong


[Defending Medical Image Diagnostics against Privacy Attacks using Generative Methods. (12%)](http://arxiv.org/abs/2103.03078)

William Paul, Yinzhi Cao, Miaomiao Zhang, Phil Burlina


[A Novel Framework for Threat Analysis of Machine Learning-based Smart Healthcare Systems. (1%)](http://arxiv.org/abs/2103.03472)

Nur Imtiazul Haque, Mohammad Ashiqur Rahman, Md Hasan Shahriar, Alvi Ataur Khalil, Selcuk Uluagac


[On the privacy-utility trade-off in differentially private hierarchical text classification. (1%)](http://arxiv.org/abs/2103.02895)

Dominik Wunderlich, Daniel Bernau, Francesco Aldà, Javier Parra-Arnau, Thorsten Strufe


## 2021-03-03

[Structure-Preserving Progressive Low-rank Image Completion for Defending Adversarial Attacks. (99%)](http://arxiv.org/abs/2103.02781)

Zhiqun Zhao, Hengyou Wang, Hao Sun, Zhihai He


[A Modified Drake Equation for Assessing Adversarial Risk to Machine Learning Models. (89%)](http://arxiv.org/abs/2103.02718)

Josh Kalin, David Noever, Matthew Ciolino


[Shift Invariance Can Reduce Adversarial Robustness. (87%)](http://arxiv.org/abs/2103.02695)

Songwei Ge, Vasu Singla, Ronen Basri, David Jacobs


[A Robust Adversarial Network-Based End-to-End Communications System With Strong Generalization Ability Against Adversarial Attacks. (81%)](http://arxiv.org/abs/2103.02654)

Yudi Dong, Huaxia Wang, Yu-Dong Yao


[On the effectiveness of adversarial training against common corruptions. (67%)](http://arxiv.org/abs/2103.02325)

Klim Kireev, Maksym Andriushchenko, Nicolas Flammarion


[Formalizing Generalization and Robustness of Neural Networks to Weight Perturbations. (64%)](http://arxiv.org/abs/2103.02200)

Yu-Lin Tsai, Chia-Yi Hsu, Chia-Mu Yu, Pin-Yu Chen


## 2021-03-02

[Evaluating the Robustness of Geometry-Aware Instance-Reweighted Adversarial Training. (99%)](http://arxiv.org/abs/2103.01914)

Dorjan Hitaj, Giulio Pagnotta, Iacopo Masi, Luigi V. Mancini


[A Survey On Universal Adversarial Attack. (99%)](http://arxiv.org/abs/2103.01498)

Chaoning Zhang, Philipp Benz, Chenguo Lin, Adil Karjauv, Jing Wu, In So Kweon


[Online Adversarial Attacks. (99%)](http://arxiv.org/abs/2103.02014)

Andjela Mladenovic, Avishek Joey Bose, Hugo Berard, William L. Hamilton, Simon Lacoste-Julien, Pascal Vincent, Gauthier Gidel


[Adversarial Examples for Unsupervised Machine Learning Models. (98%)](http://arxiv.org/abs/2103.01895)

Chia-Yi Hsu, Pin-Yu Chen, Songtao Lu, Sijia Liu, Chia-Mu Yu


[DeepCert: Verification of Contextually Relevant Robustness for Neural Network Image Classifiers. (97%)](http://arxiv.org/abs/2103.01629)

Colin Paterson, Haoze Wu, John Grese, Radu Calinescu, Corina S. Pasareanu, Clark Barrett


[ActiveGuard: An Active DNN IP Protection Technique via Adversarial Examples. (97%)](http://arxiv.org/abs/2103.01527)

Mingfu Xue, Shichang Sun, Can He, Yushu Zhang, Jian Wang, Weiqiang Liu


[Fixing Data Augmentation to Improve Adversarial Robustness. (69%)](http://arxiv.org/abs/2103.01946)

Sylvestre-Alvise Rebuffi, Sven Gowal, Dan A. Calian, Florian Stimberg, Olivia Wiles, Timothy Mann


[A Brief Survey on Deep Learning Based Data Hiding. (54%)](http://arxiv.org/abs/2103.01607)

Chaoning Zhang, Chenguo Lin, Philipp Benz, Kejiang Chen, Weiming Zhang, In So Kweon


[Group-wise Inhibition based Feature Regularization for Robust Classification. (16%)](http://arxiv.org/abs/2103.02152)

Haozhe Liu, Haoqian Wu, Weicheng Xie, Feng Liu, Linlin Shen


[DP-InstaHide: Provably Defusing Poisoning and Backdoor Attacks with Differentially Private Data Augmentations. (1%)](http://arxiv.org/abs/2103.02079)

Eitan Borgnia, Jonas Geiping, Valeriia Cherepanova, Liam Fowl, Arjun Gupta, Amin Ghiasi, Furong Huang, Micah Goldblum, Tom Goldstein


## 2021-03-01

[Dual Attention Suppression Attack: Generate Adversarial Camouflage in Physical World. (99%)](http://arxiv.org/abs/2103.01050)

Jiakai Wang, Aishan Liu, Zixin Yin, Shunchang Liu, Shiyu Tang, Xianglong Liu


[Brain Programming is Immune to Adversarial Attacks: Towards Accurate and Robust Image Classification using Symbolic Learning. (99%)](http://arxiv.org/abs/2103.01359)

Gerardo Ibarra-Vazquez, Gustavo Olague, Mariana Chan-Ley, Cesar Puente, Carlos Soubervielle-Montalvo


[Smoothness Analysis of Adversarial Training. (98%)](http://arxiv.org/abs/2103.01400)

Sekitoshi Kanai, Masanori Yamada, Hiroshi Takahashi, Yuki Yamanaka, Yasutoshi Ida


[Explaining Adversarial Vulnerability with a Data Sparsity Hypothesis. (96%)](http://arxiv.org/abs/2103.00778)

Mahsa Paknezhad, Cuong Phuc Ngo, Amadeus Aristo Winarto, Alistair Cheong, Beh Chuen Yang, Wu Jiayang, Lee Hwee Kuan


[Mind the box: $l_1$-APGD for sparse adversarial attacks on image classifiers. (93%)](http://arxiv.org/abs/2103.01208)

Francesco Croce, Matthias Hein


[Adversarial training in communication constrained federated learning. (87%)](http://arxiv.org/abs/2103.01319)

Devansh Shah, Parijat Dube, Supriyo Chakraborty, Ashish Verma


[Counterfactual Explanations for Oblique Decision Trees: Exact, Efficient Algorithms. (82%)](http://arxiv.org/abs/2103.01096)

Miguel Á. Carreira-Perpiñán, Suryabhan Singh Hada


[Am I a Real or Fake Celebrity? Measuring Commercial Face Recognition Web APIs under Deepfake Impersonation Attack. (70%)](http://arxiv.org/abs/2103.00847)

Shahroz Tariq, Sowon Jeon, Simon S. Woo


[A Multiclass Boosting Framework for Achieving Fast and Provable Adversarial Robustness. (64%)](http://arxiv.org/abs/2103.01276)

Jacob Abernethy, Pranjal Awasthi, Satyen Kale


[Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation. (62%)](http://arxiv.org/abs/2103.03102)

Wei Dai, Daniel Berleant


## 2021-02-28

[Model-Agnostic Defense for Lane Detection against Adversarial Attack. (98%)](http://arxiv.org/abs/2103.00663)

Henry Xu, An Ju, David Wagner


[Robust learning under clean-label attack. (22%)](http://arxiv.org/abs/2103.00671)

Avrim Blum, Steve Hanneke, Jian Qian, Han Shao


## 2021-02-27

[Effective Universal Unrestricted Adversarial Attacks using a MOE Approach. (98%)](http://arxiv.org/abs/2103.00250)

A. E. Baia, Bari G. Di, V. Poggioni


[Tiny Adversarial Mulit-Objective Oneshot Neural Architecture Search. (93%)](http://arxiv.org/abs/2103.00363)

Guoyang Xie, Jinbao Wang, Guo Yu, Feng Zheng, Yaochu Jin


[End-to-end Uncertainty-based Mitigation of Adversarial Attacks to Automated Lane Centering. (73%)](http://arxiv.org/abs/2103.00345)

Ruochen Jiao, Hengyi Liang, Takami Sato, Junjie Shen, Qi Alfred Chen, Qi Zhu


[Adversarial Information Bottleneck. (33%)](http://arxiv.org/abs/2103.00381)

Pemhlong Zhai, Shihua Zhang


[Neuron Coverage-Guided Domain Generalization. (2%)](http://arxiv.org/abs/2103.00229)

Chris Xing Tian, Haoliang Li, Xiaofei Xie, Yang Liu, Shiqi Wang


## 2021-02-26

[What Doesn't Kill You Makes You Robust(er): Adversarial Training against Poisons and Backdoors.](http://arxiv.org/abs/2102.13624)

Jonas Geiping, Liam Fowl, Gowthami Somepalli, Micah Goldblum, Michael Moeller, Tom Goldstein


[NEUROSPF: A tool for the Symbolic Analysis of Neural Networks. (68%)](http://arxiv.org/abs/2103.00124)

Muhammad Usman, Yannic Noller, Corina Pasareanu, Youcheng Sun, Divya Gopinath


## 2021-02-25

[On Instabilities of Conventional Multi-Coil MRI Reconstruction to Small Adverserial Perturbations.](http://arxiv.org/abs/2102.13066)

Chi Zhang, Jinghan Jia, Burhaneddin Yaman, Steen Moeller, Sijia Liu, Mingyi Hong, Mehmet Akçakaya


[Do Input Gradients Highlight Discriminative Features?](http://arxiv.org/abs/2102.12781)

Harshay Shah, Prateek Jain, Praneeth Netrapalli


[Nonlinear Projection Based Gradient Estimation for Query Efficient Blackbox Attacks.](http://arxiv.org/abs/2102.13184)

Huichen Li, Linyi Li, Xiaojun Xu, Xiaolu Zhang, Shuang Yang, Bo Li


[Understanding Robustness in Teacher-Student Setting: A New Perspective.](http://arxiv.org/abs/2102.13170)

Zhuolin Yang, Zhaoxi Chen, Tiffany Cai, Xinyun Chen, Bo Li, Yuandong Tian


[Fast Minimum-norm Adversarial Attacks through Adaptive Norm Constraints.](http://arxiv.org/abs/2102.12827)

Maura Pintor, Fabio Roli, Wieland Brendel, Battista Biggio


[Cybersecurity Threats in Connected and Automated Vehicles based Federated Learning Systems.](http://arxiv.org/abs/2102.13256)

Ranwa Al Mallah, Godwin Badu-Marfo, Bilal Farooq


[A statistical framework for efficient out of distribution detection in deep neural networks. (1%)](http://arxiv.org/abs/2102.12967)

Matan Haroush, Tzviel Frostig, Ruth Heller, Daniel Soudry


## 2021-02-24

[Confidence Calibration with Bounded Error Using Transformations.](http://arxiv.org/abs/2102.12680)

Sooyong Jang, Radoslav Ivanov, Insup lee, James Weimer


[Sketching Curvature for Efficient Out-of-Distribution Detection for Deep Neural Networks.](http://arxiv.org/abs/2102.12567)

Apoorva Sharma, Navid Azizan, Marco Pavone


[Robust SleepNets.](http://arxiv.org/abs/2102.12555)

Yigit Alparslan, Edward Kim


[Multiplicative Reweighting for Robust Neural Network Optimization.](http://arxiv.org/abs/2102.12192)

Noga Bar, Tomer Koren, Raja Giryes


[Identifying Untrustworthy Predictions in Neural Networks by Geometric Gradient Analysis.](http://arxiv.org/abs/2102.12196)

Leo Schwinn, An Nguyen, René Raab, Leon Bungert, Daniel Tenbrinck, Dario Zanca, Martin Burger, Bjoern Eskofier


[Graphfool: Targeted Label Adversarial Attack on Graph Embedding.](http://arxiv.org/abs/2102.12284)

Jinyin Chen, Xiang Lin, Dunjie Zhang, Wenrong Jiang, Guohan Huang, Hui Xiong, Yun Xiang


## 2021-02-23

[The Sensitivity of Word Embeddings-based Author Detection Models to Semantic-preserving Adversarial Perturbations.](http://arxiv.org/abs/2102.11917)

Jeremiah Duncan, Fabian Fallas, Chris Gropp, Emily Herron, Maria Mahbub, Paula Olaya, Eduardo Ponce, Tabitha K. Samuel, Daniel Schultz, Sudarshan Srinivasan, Maofeng Tang, Viktor Zenkov, Quan Zhou, Edmon Begoli


[Rethinking Natural Adversarial Examples for Classification Models.](http://arxiv.org/abs/2102.11731)

Xiao Li, Jianmin Li, Ting Dai, Jie Shi, Jun Zhu, Xiaolin Hu


[Automated Discovery of Adaptive Attacks on Adversarial Defenses.](http://arxiv.org/abs/2102.11860)

Chengyuan Yao, Pavol Bielik, Petar Tsankov, Martin Vechev


[Adversarial Robustness with Non-uniform Perturbations.](http://arxiv.org/abs/2102.12002)

Ecenaz Erdemir, Jeffrey Bickford, Luca Melis, Sergul Aydore


[Non-Singular Adversarial Robustness of Neural Networks.](http://arxiv.org/abs/2102.11935)

Yu-Lin Tsai, Chia-Yi Hsu, Chia-Mu Yu, Pin-Yu Chen


[Enhancing Model Robustness By Incorporating Adversarial Knowledge Into Semantic Representation.](http://arxiv.org/abs/2102.11584)

Jinfeng Li, Tianyu Du, Xiangyu Liu, Rong Zhang, Hui Xue, Shouling Ji


[Adversarial Examples Detection beyond Image Space.](http://arxiv.org/abs/2102.11586)

Kejiang Chen, Yuefeng Chen, Hang Zhou, Chuan Qin, Xiaofeng Mao, Weiming Zhang, Nenghai Yu


[Oriole: Thwarting Privacy against Trustworthy Deep Learning Models.](http://arxiv.org/abs/2102.11502)

Liuqiao Chen, Hu Wang, Benjamin Zi Hao Zhao, Minhui Xue, Haifeng Qian


## 2021-02-22

[On the robustness of randomized classifiers to adversarial examples.](http://arxiv.org/abs/2102.10875)

Rafael Pinot, Laurent Meunier, Florian Yger, Cédric Gouy-Pailler, Yann Chevaleyre, Jamal Atif


[Resilience of Bayesian Layer-Wise Explanations under Adversarial Attacks.](http://arxiv.org/abs/2102.11010)

Ginevra Carbone, Guido Sanguinetti, Luca Bortolussi


[Man-in-The-Middle Attacks and Defense in a Power System Cyber-Physical Testbed.](http://arxiv.org/abs/2102.11455)

Patrick Wlazlo, Abhijeet Sahu, Zeyu Mao, Hao Huang, Ana Goulart, Katherine Davis, Saman Zonouz


[Sandwich Batch Normalization: A Drop-In Replacement for Feature Distribution Heterogeneity.](http://arxiv.org/abs/2102.11382)

Xinyu Gong, Wuyang Chen, Tianlong Chen, Zhangyang Wang


## 2021-02-21

[The Effects of Image Distribution and Task on Adversarial Robustness.](http://arxiv.org/abs/2102.10534)

Owen Kunhardt, Arturo Deza, Tomaso Poggio


[A Zeroth-Order Block Coordinate Descent Algorithm for Huge-Scale Black-Box Optimization.](http://arxiv.org/abs/2102.10707)

HanQin Cai, Yuchen Lou, Daniel McKenzie, Wotao Yin


[Constrained Optimization to Train Neural Networks on Critical and Under-Represented Classes. (1%)](http://arxiv.org/abs/2102.12894)

Sara Sangalli, Ertunc Erdil, Andreas Hoetker, Olivio Donati, Ender Konukoglu


## 2021-02-20

[On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning.](http://arxiv.org/abs/2102.10454)

Ren Wang, Kaidi Xu, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Chuang Gan, Meng Wang


[Measuring $\ell_\infty$ Attacks by the $\ell_2$ Norm.](http://arxiv.org/abs/2102.10343)

Sizhe Chen, Qinghua Tao, Zhixing Ye, Xiaolin Huang


## 2021-02-19

[A PAC-Bayes Analysis of Adversarial Robustness.](http://arxiv.org/abs/2102.11069)

Guillaume IRIT Vidot, Paul LHC Viallard, Amaury LHC Habrard, Emilie LHC Morvant


[Effective and Efficient Vote Attack on Capsule Networks.](http://arxiv.org/abs/2102.10055)

Jindong Gu, Baoyuan Wu, Volker Tresp


## 2021-02-18

[Random Projections for Improved Adversarial Robustness.](http://arxiv.org/abs/2102.09230)

Ginevra Carbone, Guido Sanguinetti, Luca Bortolussi


[Fortify Machine Learning Production Systems: Detect and Classify Adversarial Attacks.](http://arxiv.org/abs/2102.09695)

Matthew Ciolino, Josh Kalin, David Noever


[Make Sure You're Unsure: A Framework for Verifying Probabilistic Specifications.](http://arxiv.org/abs/2102.09479)

Leonard Berrada, Sumanth Dathathri, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Jonathan Uesato, Sven Gowal, M. Pawan Kumar


[Center Smoothing: Provable Robustness for Functions with Metric-Space Outputs.](http://arxiv.org/abs/2102.09701)

Aounon Kumar, Tom Goldstein


## 2021-02-17

[Improving Hierarchical Adversarial Robustness of Deep Neural Networks.](http://arxiv.org/abs/2102.09012)

Avery Ma, Aladin Virmaux, Kevin Scaman, Juwei Lu


[Consistent Non-Parametric Methods for Maximizing Robustness.](http://arxiv.org/abs/2102.09086)

Robi Bhattacharjee, Kamalika Chaudhuri


[Bridging the Gap Between Adversarial Robustness and Optimization Bias.](http://arxiv.org/abs/2102.08868)

Fartash Faghri, Sven Gowal, Cristina Vasconcelos, David J. Fleet, Fabian Pedregosa, Nicolas Le Roux


[Towards Adversarial-Resilient Deep Neural Networks for False Data Injection Attack Detection in Power Grids.](http://arxiv.org/abs/2102.09057)

Jiangnan Li, Yingyuan Yang, Jinyuan Stella Sun, Kevin Tomsovic, Hairong Qi


## 2021-02-16

[Globally-Robust Neural Networks.](http://arxiv.org/abs/2102.08452)

Klas Leino, Zifan Wang, Matt Fredrikson


[A Law of Robustness for Weight-bounded Neural Networks.](http://arxiv.org/abs/2102.08093)

Hisham Husain, Borja Balle


[Just Noticeable Difference for Machine Perception and Generation of Regularized Adversarial Images with Minimal Perturbation.](http://arxiv.org/abs/2102.08079)

Adil Kaan Akan, Emre Akbas, Fatos T. Yarman Vural


## 2021-02-15

[Data Profiling for Adversarial Training: On the Ruin of Problematic Data.](http://arxiv.org/abs/2102.07437)

Chengyu Dong, Liyuan Liu, Jingbo Shang


[Certified Robustness to Programmable Transformations in LSTMs.](http://arxiv.org/abs/2102.07818)

Yuhao Zhang, Aws Albarghouthi, Loris D'Antoni


[Generating Structured Adversarial Attacks Using Frank-Wolfe Method.](http://arxiv.org/abs/2102.07360)

Ehsan Kazemi, Thomas Kerdreux, Liquang Wang


[Universal Adversarial Examples and Perturbations for Quantum Classifiers.](http://arxiv.org/abs/2102.07788)

Weiyuan Gong, Dong-Ling Deng


[Low Curvature Activations Reduce Overfitting in Adversarial Training.](http://arxiv.org/abs/2102.07861)

Vasu Singla, Sahil Singla, David Jacobs, Soheil Feizi


[And/or trade-off in artificial neurons: impact on adversarial robustness.](http://arxiv.org/abs/2102.07389)

Alessandro Fontana


[Certifiably Robust Variational Autoencoders.](http://arxiv.org/abs/2102.07559)

Ben Barrett, Alexander Camuto, Matthew Willetts, Tom Rainforth


## 2021-02-14

[Guided Interpolation for Adversarial Training.](http://arxiv.org/abs/2102.07327)

Chen Chen, Jingfeng Zhang, Xilie Xu, Tianlei Hu, Gang Niu, Gang Chen, Masashi Sugiyama


[Resilient Machine Learning for Networked Cyber Physical Systems: A Survey for Machine Learning Security to Securing Machine Learning for CPS.](http://arxiv.org/abs/2102.07244)

Felix Olowononi, Danda B. Rawat, Chunmei Liu


[Exploring Adversarial Robustness of Deep Metric Learning.](http://arxiv.org/abs/2102.07265)

Thomas Kobber Panum, Zi Wang, Pengyu Kan, Earlence Fernandes, Somesh Jha


[Adversarial Attack on Network Embeddings via Supervised Network Poisoning.](http://arxiv.org/abs/2102.07164)

Viresh Gupta, Tanmoy Chakraborty


[Perceptually Constrained Adversarial Attacks.](http://arxiv.org/abs/2102.07140)

Muhammad Zaid Hameed, Andras Gyorgy


[CAP-GAN: Towards Adversarial Robustness with Cycle-consistent Attentional Purification.](http://arxiv.org/abs/2102.07304)

Mingu Kang, Trung Quang Tran, Seungju Cho, Daeyoung Kim


[Cross-modal Adversarial Reprogramming.](http://arxiv.org/abs/2102.07325)

Paarth Neekhara, Shehzeen Hussain, Jinglong Du, Shlomo Dubnov, Farinaz Koushanfar, Julian McAuley


## 2021-02-13

[Mixed Nash Equilibria in the Adversarial Examples Game.](http://arxiv.org/abs/2102.06905)

Laurent Meunier, Meyer Scetbon, Rafael Pinot, Jamal Atif, Yann Chevaleyre


[Adversarial defense for automatic speaker verification by cascaded self-supervised learning models.](http://arxiv.org/abs/2102.07047)

Haibin Wu, Xu Li, Andy T. Liu, Zhiyong Wu, Helen Meng, Hung-yi Lee


## 2021-02-12

[UAVs Path Deviation Attacks: Survey and Research Challenges.](http://arxiv.org/abs/2102.06638)

Francesco Betti Sorbelli, Mauro Conti, Cristina M. Pinotti, Giulio Rigoni


[Universal Adversarial Perturbations Through the Lens of Deep Steganography: Towards A Fourier Perspective.](http://arxiv.org/abs/2102.06479)

Chaoning Zhang, Philipp Benz, Adil Karjauv, In So Kweon


[Universal Adversarial Perturbations for Malware.](http://arxiv.org/abs/2102.06747)

Raphael Labaca-Castro, Luis Muñoz-González, Feargus Pendlebury, Gabi Dreo Rodosek, Fabio Pierazzi, Lorenzo Cavallaro


[On the Paradox of Certified Training. (13%)](http://arxiv.org/abs/2102.06700)

Nikola Jovanović, Mislav Balunović, Maximilian Baader, Martin Vechev


## 2021-02-11

[Adversarially robust deepfake media detection using fused convolutional neural network predictions.](http://arxiv.org/abs/2102.05950)

Sohail Ahmed Khan, Alessandro Artusi, Hang Dai


[Defuse: Harnessing Unrestricted Adversarial Examples for Debugging Models Beyond Test Accuracy.](http://arxiv.org/abs/2102.06162)

Dylan Slack, Nathalie Rauschmayr, Krishnaram Kenthapadi


[RobOT: Robustness-Oriented Testing for Deep Learning Systems.](http://arxiv.org/abs/2102.05913)

Jingyi Wang, Jialuo Chen, Youcheng Sun, Xingjun Ma, Dongxia Wang, Jun Sun, Peng Cheng


## 2021-02-10

[Meta Federated Learning.](http://arxiv.org/abs/2102.05561)

Omid Aramoon, Pin-Yu Chen, Gang Qu, Yuan Tian


[Adversarial Robustness: What fools you makes you stronger.](http://arxiv.org/abs/2102.05475)

Grzegorz Głuch, Rüdiger Urbanke


[CIFS: Improving Adversarial Robustness of CNNs via Channel-wise Importance-based Feature Selection.](http://arxiv.org/abs/2102.05311)

Hanshu Yan, Jingfeng Zhang, Gang Niu, Jiashi Feng, Vincent Y. F. Tan, Masashi Sugiyama


[Dompteur: Taming Audio Adversarial Examples.](http://arxiv.org/abs/2102.05431)

Thorsten Eisenhofer, Lea Schönherr, Joel Frank, Lars Speckemeier, Dorothea Kolossa, Thorsten Holz


[Enhancing Real-World Adversarial Patches through 3D Modeling of Complex Target Scenes.](http://arxiv.org/abs/2102.05334)

Yael Mathov, Lior Rokach, Yuval Elovici


[Towards Certifying L-infinity Robustness using Neural Networks with L-inf-dist Neurons.](http://arxiv.org/abs/2102.05363)

Bohang Zhang, Tianle Cai, Zhou Lu, Di He, Liwei Wang


[RoBIC: A benchmark suite for assessing classifiers robustness.](http://arxiv.org/abs/2102.05368)

Thibault Maho, Benoît Bonnet, Teddy Furon, Erwan Le Merrer


[Bayesian Inference with Certifiable Adversarial Robustness.](http://arxiv.org/abs/2102.05289)

Matthew Wicker, Luca Laurenti, Andrea Patane, Zhoutong Chen, Zheng Zhang, Marta Kwiatkowska


## 2021-02-09

[Target Training Does Adversarial Training Without Adversarial Samples.](http://arxiv.org/abs/2102.04836)

Blerta Lindqvist


[Security and Privacy for Artificial Intelligence: Opportunities and Challenges.](http://arxiv.org/abs/2102.04661)

Ayodeji Oseni, Nour Moustafa, Helge Janicke, Peng Liu, Zahir Tari, Athanasios Vasilakos


["What's in the box?!": Deflecting Adversarial Attacks by Randomly Deploying Adversarially-Disjoint Models.](http://arxiv.org/abs/2102.05104)

Sahar Abdelnabi, Mario Fritz


[Adversarial Perturbations Are Not So Weird: Entanglement of Robust and Non-Robust Features in Neural Network Classifiers.](http://arxiv.org/abs/2102.05110)

Jacob M. Springer, Melanie Mitchell, Garrett T. Kenyon


[Detecting Localized Adversarial Examples: A Generic Approach using Critical Region Analysis.](http://arxiv.org/abs/2102.05241)

Fengting Li, Xuankai Liu, Xiaoli Zhang, Qi Li, Kun Sun, Kang Li


[Making Paper Reviewing Robust to Bid Manipulation Attacks.](http://arxiv.org/abs/2102.06020)

Ruihan Wu, Chuan Guo, Felix Wu, Rahul Kidambi, der Maaten Laurens van, Kilian Q. Weinberger


[Towards Bridging the gap between Empirical and Certified Robustness against Adversarial Examples.](http://arxiv.org/abs/2102.05096)

Jay Nandy, Sudipan Saha, Wynne Hsu, Mong Li Lee, Xiao Xiang Zhu


## 2021-02-08

[Efficient Certified Defenses Against Patch Attacks on Image Classifiers.](http://arxiv.org/abs/2102.04154)

Jan Hendrik Metzen, Maksym Yatsura


[A Real-time Defense against Website Fingerprinting Attacks.](http://arxiv.org/abs/2102.04291)

Shawn Shan, Arjun Nitin Bhagoji, Haitao Zheng, Ben Y. Zhao


[Benford's law: what does it say on adversarial images?](http://arxiv.org/abs/2102.04615)

João G. Zago, Fabio L. Baldissera, Eric A. Antonelo, Rodrigo T. Saad


[Exploiting epistemic uncertainty of the deep learning models to generate adversarial samples.](http://arxiv.org/abs/2102.04150)

Omer Faruk Tuna, Ferhat Ozgur Catak, M. Taner Eskil


## 2021-02-07

[Adversarial example generation with AdaBelief Optimizer and Crop Invariance.](http://arxiv.org/abs/2102.03726)

Bo Yang, Hengwei Zhang, Yuchen Zhang, Kaiyong Xu, Jindong Wang


[Adversarial Imaging Pipelines.](http://arxiv.org/abs/2102.03728)

Buu Phan, Fahim Mannan, Felix Heide


## 2021-02-06

[SPADE: A Spectral Method for Black-Box Adversarial Robustness Evaluation.](http://arxiv.org/abs/2102.03716)

Wuxinlin Cheng, Chenhui Deng, Zhiqiang Zhao, Yaohui Cai, Zhiru Zhang, Zhuo Feng


## 2021-02-05

[Corner Case Generation and Analysis for Safety Assessment of Autonomous Vehicles.](http://arxiv.org/abs/2102.03483)

Haowei Sun, Shuo Feng, Xintao Yan, Henry X. Liu


[Model Agnostic Answer Reranking System for Adversarial Question Answering.](http://arxiv.org/abs/2102.03016)

Sagnik Majumder, Chinmoy Samant, Greg Durrett


[Robust Single-step Adversarial Training with Regularizer.](http://arxiv.org/abs/2102.03381)

Lehui Xie, Yaopeng Wang, Jia-Li Yin, Ximeng Liu


[Understanding the Interaction of Adversarial Training with Noisy Labels.](http://arxiv.org/abs/2102.03482)

Jianing Zhu, Jingfeng Zhang, Bo Han, Tongliang Liu, Gang Niu, Hongxia Yang, Mohan Kankanhalli, Masashi Sugiyama


[Optimal Transport as a Defense Against Adversarial Attacks.](http://arxiv.org/abs/2102.03156)

Quentin Bouniot, Romaric Audigier, Angélique Loesch


## 2021-02-04

[DetectorGuard: Provably Securing Object Detectors against Localized Patch Hiding Attacks.](http://arxiv.org/abs/2102.02956)

Chong Xiang, Prateek Mittal


[Adversarial Training Makes Weight Loss Landscape Sharper in Logistic Regression.](http://arxiv.org/abs/2102.02950)

Masanori Yamada, Sekitoshi Kanai, Tomoharu Iwata, Tomokatsu Takahashi, Yuki Yamanaka, Hiroshi Takahashi, Atsutoshi Kumagai


[Adversarial Robustness Study of Convolutional Neural Network for Lumbar Disk Shape Reconstruction from MR images.](http://arxiv.org/abs/2102.02885)

Jiasong Chen, Linchen Qian, Timur Urakov, Weiyong Gu, Liang Liang


[PredCoin: Defense against Query-based Hard-label Attack.](http://arxiv.org/abs/2102.02923)

Junfeng Guo, Yaswanth Yadlapalli, Thiele Lothar, Ang Li, Cong Liu


[Adversarial Attacks and Defenses in Physiological Computing: A Systematic Review.](http://arxiv.org/abs/2102.02729)

Dongrui Wu, Weili Fang, Yi Zhang, Liuqing Yang, Hanbin Luo, Lieyun Ding, Xiaodong Xu, Xiang Yu


[Audio Adversarial Examples: Attacks Using Vocal Masks.](http://arxiv.org/abs/2102.02417)

Lynnette Ng, Kai Yuan Tay, Wei Han Chua, Lucerne Loke, Danqi Ye, Melissa Chua


[ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models.](http://arxiv.org/abs/2102.02551)

Yugeng Liu, Rui Wen, Xinlei He, Ahmed Salem, Zhikun Zhang, Michael Backes, Cristofaro Emiliano De, Mario Fritz, Yang Zhang


## 2021-02-03

[Adversarially Robust Learning with Unknown Perturbation Sets.](http://arxiv.org/abs/2102.02145)

Omar Montasser, Steve Hanneke, Nathan Srebro


[IWA: Integrated Gradient based White-box Attacks for Fooling Deep Neural Networks.](http://arxiv.org/abs/2102.02128)

Yixiang Wang, Jiqiang Liu, Xiaolin Chang, Jelena Mišić, Vojislav B. Mišić


## 2021-02-02

[On Robustness of Neural Semantic Parsers.](http://arxiv.org/abs/2102.01563)

Shuo Huang, Zhuang Li, Lizhen Qu, Lei Pan


[Towards Robust Neural Networks via Close-loop Control.](http://arxiv.org/abs/2102.01862)

Zhuotong Chen, Qianxiao Li, Zheng Zhang


[Recent Advances in Adversarial Training for Adversarial Robustness.](http://arxiv.org/abs/2102.01356)

Tao Bai, Jinqi Luo, Jun Zhao, Bihan Wen, Qian Wang


[Probabilistic Trust Intervals for Out of Distribution Detection. (2%)](http://arxiv.org/abs/2102.01336)

Gagandeep Singh, Deepak Mishra


## 2021-02-01

[Fast Training of Provably Robust Neural Networks by SingleProp.](http://arxiv.org/abs/2102.01208)

Akhilan Boopathy, Tsui-Wei Weng, Sijia Liu, Pin-Yu Chen, Gaoyuan Zhang, Luca Daniel


[Towards Speeding up Adversarial Training in Latent Spaces.](http://arxiv.org/abs/2102.00662)

Yaguan Qian, Qiqi Shao, Tengteng Yao, Bin Wang, Shaoning Zeng, Zhaoquan Gu, Wassim Swaileh


[Robust Adversarial Attacks Against DNN-Based Wireless Communication Systems.](http://arxiv.org/abs/2102.00918)

Alireza Bahramali, Milad Nasr, Amir Houmansadr, Dennis Goeckel, Don Towsley


## 2021-01-31

[Deep Deterministic Information Bottleneck with Matrix-based Entropy Functional.](http://arxiv.org/abs/2102.00533)

Xi Yu, Shujian Yu, Jose C. Principe


[Towards Imperceptible Query-limited Adversarial Attacks with Perceptual Feature Fidelity Loss.](http://arxiv.org/abs/2102.00449)

Pengrui Quan, Ruiming Guo, Mani Srivastava


[Admix: Enhancing the Transferability of Adversarial Attacks.](http://arxiv.org/abs/2102.00436)

Xiaosen Wang, Xuanran He, Jingdong Wang, Kun He


## 2021-01-30

[Cortical Features for Defense Against Adversarial Audio Attacks.](http://arxiv.org/abs/2102.00313)

Ilya Kavalerov, Ruijie Zheng, Wojciech Czaja, Rama Chellappa


## 2021-01-29

[You Only Query Once: Effective Black Box Adversarial Attacks with Minimal Repeated Queries.](http://arxiv.org/abs/2102.00029)

Devin Willmott, Anit Kumar Sahu, Fatemeh Sheikholeslami, Filipe Condessa, Zico Kolter


## 2021-01-28

[Adversarial Machine Learning Attacks on Condition-Based Maintenance Capabilities.](http://arxiv.org/abs/2101.12097)

Hamidreza Habibollahi Najaf Abadi


[Adversarial Attacks on Deep Learning Based Power Allocation in a Massive MIMO Network.](http://arxiv.org/abs/2101.12090)

B. R. Manoj, Meysam Sadeghi, Erik G. Larsson


[Increasing the Confidence of Deep Neural Networks by Coverage Analysis.](http://arxiv.org/abs/2101.12100)

Giulio Rossolini, Alessandro Biondi, Giorgio Carlo Buttazzo


[Adversarial Learning with Cost-Sensitive Classes.](http://arxiv.org/abs/2101.12372)

Haojing Shen, Sihong Chen, Ran Wang, Xizhao Wang


## 2021-01-27

[Robust Android Malware Detection System against Adversarial Attacks using Q-Learning.](http://arxiv.org/abs/2101.12031)

Hemant Rathore, Sanjay K. Sahay, Piyush Nikam, Mohit Sewak


[Adversaries in Online Learning Revisited: with applications in Robust Optimization and Adversarial training.](http://arxiv.org/abs/2101.11443)

Sebastian Pokutta, Huan Xu


[Adversarial Stylometry in the Wild: Transferable Lexical Substitution Attacks on Author Profiling.](http://arxiv.org/abs/2101.11310)

Chris Emmery, Ákos Kádár, Grzegorz Chrupała


[Meta Adversarial Training against Universal Patches.](http://arxiv.org/abs/2101.11453)

Jan Hendrik Metzen, Nicole Finnie, Robin Hutmacher


[Detecting Adversarial Examples by Input Transformations, Defense Perturbations, and Voting.](http://arxiv.org/abs/2101.11466)

Federico Nesti, Alessandro Biondi, Giorgio Buttazzo


[Improving Neural Network Robustness through Neighborhood Preserving Layers.](http://arxiv.org/abs/2101.11766)

Bingyuan Liu, Christopher Malon, Lingzhou Xue, Erik Kruus


## 2021-01-26

[Blind Image Denoising and Inpainting Using Robust Hadamard Autoencoders.](http://arxiv.org/abs/2101.10876)

Rasika Karkare, Randy Paffenroth, Gunjan Mahindre


[Property Inference From Poisoning.](http://arxiv.org/abs/2101.11073)

Melissa Chase, Esha Ghosh, Saeed Mahloujifar


[Adversarial Vulnerability of Active Transfer Learning.](http://arxiv.org/abs/2101.10792)

Nicolas M. Müller, Konstantin Böttinger


[SkeletonVis: Interactive Visualization for Understanding Adversarial Attacks on Human Action Recognition Models.](http://arxiv.org/abs/2101.10586)

Haekyu Park, Zijie J. Wang, Nilaksh Das, Anindya S. Paul, Pruthvi Perumalla, Zhiyan Zhou, Duen Horng Chau


[The Effect of Class Definitions on the Transferability of Adversarial Attacks Against Forensic CNNs.](http://arxiv.org/abs/2101.11081)

Xinwei Zhao, Matthew C. Stamm


[Defenses Against Multi-Sticker Physical Domain Attacks on Classifiers.](http://arxiv.org/abs/2101.11060)

Xinwei Zhao, Matthew C. Stamm


[Investigating the significance of adversarial attacks and their relation to interpretability for radar-based human activity recognition systems.](http://arxiv.org/abs/2101.10562)

Utku Ozbulak, Baptist Vandersmissen, Azarakhsh Jalalvand, Ivo Couckuyt, Messem Arnout Van, Neve Wesley De


[Visual explanation of black-box model: Similarity Difference and Uniqueness (SIDU) method.](http://arxiv.org/abs/2101.10710)

Satya M. Muddamsetty, Mohammad N. S. Jahromi, Andreea E. Ciontos, Laura M. Fenoy, Thomas B. Moeslund


[Towards Universal Physical Attacks On Cascaded Camera-Lidar 3D Object Detection Models.](http://arxiv.org/abs/2101.10747)

Mazen Abdelfattah, Kaiwen Yuan, Z. Jane Wang, Rabab Ward


## 2021-01-25

[Diverse Adversaries for Mitigating Bias in Training.](http://arxiv.org/abs/2101.10001)

Xudong Han, Timothy Baldwin, Trevor Cohn


[They See Me Rollin': Inherent Vulnerability of the Rolling Shutter in CMOS Image Sensors.](http://arxiv.org/abs/2101.10011)

Sebastian Köhler, Giulio Lovisotto, Simon Birnbach, Richard Baker, Ivan Martinovic


[Generalizing Adversarial Examples by AdaBelief Optimizer.](http://arxiv.org/abs/2101.09930)

Yixiang Wang, Jiqiang Liu, Xiaolin Chang


[Towards Practical Robustness Analysis for DNNs based on PAC-Model Learning.](http://arxiv.org/abs/2101.10102)

Renjue Li, Pengfei Yang, Cheng-Chao Huang, Youcheng Sun, Bai Xue, Lijun Zhang


[Few-Shot Website Fingerprinting Attack.](http://arxiv.org/abs/2101.10063)

Mantun Chen, Yongjun Wang, Zhiquan Qin, Xiatian Zhu


[Understanding and Achieving Efficient Robustness with Adversarial Supervised Contrastive Learning.](http://arxiv.org/abs/2101.10027)

Anh Bui, Trung Le, He Zhao, Paul Montague, Seyit Camtepe, Dinh Phung


## 2021-01-23

[A Transferable Anti-Forensic Attack on Forensic CNNs Using A Generative Adversarial Network.](http://arxiv.org/abs/2101.09568)

Xinwei Zhao, Chen Chen, Matthew C. Stamm


[Error Diffusion Halftoning Against Adversarial Examples.](http://arxiv.org/abs/2101.09451)

Shao-Yuan Lo, Vishal M. Patel


[A Comprehensive Evaluation Framework for Deep Model Robustness.](http://arxiv.org/abs/2101.09617)

Jun Guo, Wei Bao, Jiakai Wang, Yuqing Ma, Xinghai Gao, Gang Xiao, Aishan Liu, Jian Dong, Xianglong Liu, Wenjun Wu


## 2021-01-22

[Online Adversarial Purification based on Self-Supervision.](http://arxiv.org/abs/2101.09387)

Changhao Shi, Chester Holtz, Gal Mishne


[Towards Optimal Branching of Linear and Semidefinite Relaxations for Neural Network Robustness Certification.](http://arxiv.org/abs/2101.09306)

Brendon G. Anderson, Ziye Ma, Jingqi Li, Somayeh Sojoudi


[Generating Black-Box Adversarial Examples in Sparse Domain.](http://arxiv.org/abs/2101.09324)

Hadi Zanddizari, Behnam Zeinali, J. Morris Chang


[Adaptive Neighbourhoods for the Discovery of Adversarial Examples.](http://arxiv.org/abs/2101.09108)

Jay Morgan, Adeline Paiement, Arno Pauly, Monika Seisenberger


## 2021-01-21

[Robust Reinforcement Learning on State Observations with Learned Optimal Adversary.](http://arxiv.org/abs/2101.08452)

Huan Zhang, Hongge Chen, Duane Boning, Cho-Jui Hsieh


[Adv-OLM: Generating Textual Adversaries via OLM.](http://arxiv.org/abs/2101.08523)

Vijit Malik, Ashwani Bhat, Ashutosh Modi


[Self-Adaptive Training: Bridging Supervised and Self-Supervised Learning.](http://arxiv.org/abs/2101.08732)

Lang Huang, Chao Zhang, Hongyang Zhang


[A Person Re-identification Data Augmentation Method with Adversarial Defense Effect.](http://arxiv.org/abs/2101.08783)

Yunpeng Gong, Zhiyong Zeng, Liwen Chen, Yifan Luo, Bin Weng, Feng Ye


[Adversarial Attacks and Defenses for Speaker Identification Systems.](http://arxiv.org/abs/2101.08909)

Sonal Joshi, Jesús Villalba, Piotr Żelasko, Laureano Moro-Velázquez, Najim Dehak


[A general multi-modal data learning method for Person Re-identification. (78%)](http://arxiv.org/abs/2101.08533)

Yunpeng Gong


## 2021-01-20

[Adversarial Attacks for Tabular Data: Application to Fraud Detection and Imbalanced Data.](http://arxiv.org/abs/2101.08030)

Francesco Cartella, Orlando Anunciacao, Yuki Funabiki, Daisuke Yamaguchi, Toru Akishita, Olivier Elshocht


[Invariance, encodings, and generalization: learning identity effects with neural networks.](http://arxiv.org/abs/2101.08386)

S. Brugiapaglia, M. Liu, P. Tupper


[Fooling thermal infrared pedestrian detectors in real world using small bulbs.](http://arxiv.org/abs/2101.08154)

Xiaopei Zhu, Xiao Li, Jianmin Li, Zheyao Wang, Xiaolin Hu


## 2021-01-19

[LowKey: Leveraging Adversarial Attacks to Protect Social Media Users from Facial Recognition.](http://arxiv.org/abs/2101.07922)

Valeriia Cherepanova, Micah Goldblum, Harrison Foley, Shiyuan Duan, John Dickerson, Gavin Taylor, Tom Goldstein


[A Search-Based Testing Framework for Deep Neural Networks of Source Code Embedding.](http://arxiv.org/abs/2101.07910)

Maryam Vahdat Pour, Zhuo Li, Lei Ma, Hadi Hemmati


[PICA: A Pixel Correlation-based Attentional Black-box Adversarial Attack.](http://arxiv.org/abs/2101.07538)

Jie Wang, Zhaoxia Yin, Jin Tang, Jing Jiang, Bin Luo


[Attention-Guided Black-box Adversarial Attacks with Large-Scale Multiobjective Evolutionary Optimization.](http://arxiv.org/abs/2101.07512)

Jie Wang, Zhaoxia Yin, Jing Jiang, Yang Du


## 2021-01-18

[What Do Deep Nets Learn? Class-wise Patterns Revealed in the Input Space.](http://arxiv.org/abs/2101.06898)

Shihao Zhao, Xingjun Ma, Yisen Wang, James Bailey, Bo Li, Yu-Gang Jiang


[Red Alarm for Pre-trained Models: Universal Vulnerability to Neuron-Level Backdoor Attacks. (1%)](http://arxiv.org/abs/2101.06969)

Zhengyan Zhang, Guangxuan Xiao, Yongwei Li, Tian Lv, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Xin Jiang, Maosong Sun


## 2021-01-17

[Adversarial Interaction Attack: Fooling AI to Misinterpret Human Intentions.](http://arxiv.org/abs/2101.06704)

Nodens Koren, Qiuhong Ke, Yisen Wang, James Bailey, Xingjun Ma


[GraphAttacker: A General Multi-Task GraphAttack Framework.](http://arxiv.org/abs/2101.06855)

Jinyin Chen, Dunjie Zhang, Zhaoyan Ming, Kejie Huang, Wenrong Jiang, Chen Cui


[Exploring Adversarial Robustness of Multi-Sensor Perception Systems in Self Driving.](http://arxiv.org/abs/2101.06784)

James Tu, Huichen Li, Xinchen Yan, Mengye Ren, Yun Chen, Ming Liang, Eilyan Bitar, Ersin Yumer, Raquel Urtasun


## 2021-01-16

[Multi-objective Search of Robust Neural Architectures against Multiple Types of Adversarial Attacks.](http://arxiv.org/abs/2101.06507)

Jia Liu, Yaochu Jin


[Adversarial Attacks On Multi-Agent Communication.](http://arxiv.org/abs/2101.06560)

James Tu, Tsunhsuan Wang, Jingkang Wang, Sivabalan Manivasagam, Mengye Ren, Raquel Urtasun


## 2021-01-15

[Fundamental Tradeoffs in Distributionally Adversarial Training.](http://arxiv.org/abs/2101.06309)

Mohammad Mehrabi, Adel Javanmard, Ryan A. Rossi, Anup Rao, Tung Mai


[Black-box Adversarial Attacks in Autonomous Vehicle Technology.](http://arxiv.org/abs/2101.06092)

K Naveen Kumar, C Vishnu, Reshmi Mitra, C Krishna Mohan


[Heating up decision boundaries: isocapacitory saturation, adversarial scenarios and generalization bounds.](http://arxiv.org/abs/2101.06061)

Bogdan Georgiev, Lukas Franken, Mayukh Mukherjee


[Mining Data Impressions from Deep Models as Substitute for the Unavailable Training Data.](http://arxiv.org/abs/2101.06069)

Gaurav Kumar Nayak, Konda Reddy Mopuri, Saksham Jain, Anirban Chakraborty


## 2021-01-14

[Context-Aware Image Denoising with Auto-Threshold Canny Edge Detection to Suppress Adversarial Perturbation.](http://arxiv.org/abs/2101.05833)

Li-Yun Wang, Yeganeh Jalalpour, Wu-chi Feng


[Robusta: Robust AutoML for Feature Selection via Reinforcement Learning.](http://arxiv.org/abs/2101.05950)

Xiaoyang Wang, Bo Li, Yibo Zhang, Bhavya Kailkhura, Klara Nahrstedt


[Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks.](http://arxiv.org/abs/2101.05930)

Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, Xingjun Ma


## 2021-01-13

[Untargeted, Targeted and Universal Adversarial Attacks and Defenses on Time Series.](http://arxiv.org/abs/2101.05639)

Pradeep Rathore, Arghya Basak, Sri Harsha Nistala, Venkataramana Runkana


[Image Steganography based on Iteratively Adversarial Samples of A Synchronized-directions Sub-image.](http://arxiv.org/abs/2101.05209)

Xinghong Qin, Shunquan Tan, Bin Li, Weixuan Tang, Jiwu Huang


## 2021-01-12

[Robustness Gym: Unifying the NLP Evaluation Landscape.](http://arxiv.org/abs/2101.04840)

Karan Goel, Nazneen Rajani, Jesse Vig, Samson Tan, Jason Wu, Stephan Zheng, Caiming Xiong, Mohit Bansal, Christopher Ré


[Robustness of on-device Models: Adversarial Attack to Deep Learning Models on Android Apps.](http://arxiv.org/abs/2101.04401)

Yujin Huang, Han Hu, Chunyang Chen


[Random Transformation of Image Brightness for Adversarial Attack.](http://arxiv.org/abs/2101.04321)

Bo Yang, Kaiyong Xu, Hengjun Wang, Hengwei Zhang


[On the Effectiveness of Small Input Noise for Defending Against Query-based Black-Box Attacks.](http://arxiv.org/abs/2101.04829)

Junyoung Byun, Hyojun Go, Changick Kim


## 2021-01-11

[The Vulnerability of Semantic Segmentation Networks to Adversarial Attacks in Autonomous Driving: Enhancing Extensive Environment Sensing.](http://arxiv.org/abs/2101.03924)

Andreas Bär, Jonas Löhdefink, Nikhil Kapoor, Serin J. Varghese, Fabian Hüger, Peter Schlicht, Tim Fingscheidt


## 2021-01-10

[Adversarially Robust and Explainable Model Compression with On-Device Personalization for Text Classification.](http://arxiv.org/abs/2101.05624)

Yao Qiang, Supriya Tumkur Suresh Kumar, Marco Brocanelli, Dongxiao Zhu


## 2021-01-08

[Adversarial Attack Attribution: Discovering Attributable Signals in Adversarial ML Attacks.](http://arxiv.org/abs/2101.02899)

Marissa Dotter, Sherry Xie, Keith Manville, Josh Harguess, Colin Busho, Mikel Rodriguez


[DiPSeN: Differentially Private Self-normalizing Neural Networks For Adversarial Robustness in Federated Learning.](http://arxiv.org/abs/2101.03218)

Olakunle Ibitoye, M. Omair Shafiq, Ashraf Matrawy


[Exploring Adversarial Fake Images on Face Manifold.](http://arxiv.org/abs/2101.03272)

Dongze Li, Wei Wang, Hongxing Fan, Jing Dong


## 2021-01-07

[The Effect of Prior Lipschitz Continuity on the Adversarial Robustness of Bayesian Neural Networks.](http://arxiv.org/abs/2101.02689)

Arno Blaas, Stephen J. Roberts


[Robust Text CAPTCHAs Using Adversarial Examples.](http://arxiv.org/abs/2101.02483)

Rulin Shao, Zhouxing Shi, Jinfeng Yi, Pin-Yu Chen, Cho-Jui Hsieh


## 2021-01-06

[Adversarial Robustness by Design through Analog Computing and Synthetic Gradients.](http://arxiv.org/abs/2101.02115)

Alessandro Cappelli, Ruben Ohana, Julien Launay, Laurent Meunier, Iacopo Poli, Florent Krzakala


[Understanding the Error in Evaluating Adversarial Robustness.](http://arxiv.org/abs/2101.02325)

Pengfei Xia, Ziqiang Li, Hongjing Niu, Bin Li


## 2021-01-05

[Noise Sensitivity-Based Energy Efficient and Robust Adversary Detection in Neural Networks.](http://arxiv.org/abs/2101.01543)

Rachel Sterneck, Abhishek Moitra, Priyadarshini Panda


## 2021-01-04

[Fooling Object Detectors: Adversarial Attacks by Half-Neighbor Masks.](http://arxiv.org/abs/2101.00989)

Yanghao Zhang, Fu Wang, Wenjie Ruan


[Local Competition and Stochasticity for Adversarial Robustness in Deep Learning.](http://arxiv.org/abs/2101.01121)

Konstantinos P. Panousis, Sotirios Chatzis, Antonios Alexos, Sergios Theodoridis


[Local Black-box Adversarial Attacks: A Query Efficient Approach.](http://arxiv.org/abs/2101.01032)

Tao Xiang, Hangcheng Liu, Shangwei Guo, Tianwei Zhang, Xiaofeng Liao


[Robust Machine Learning Systems: Challenges, Current Trends, Perspectives, and the Road Ahead.](http://arxiv.org/abs/2101.02559)

Muhammad Shafique, Mahum Naseer, Theocharis Theocharides, Christos Kyrkou, Onur Mutlu, Lois Orosa, Jungwook Choi


## 2021-01-02

[Improving DGA-Based Malicious Domain Classifiers for Malware Defense with Adversarial Machine Learning.](http://arxiv.org/abs/2101.00521)

Ibrahim Yilmaz, Ambareen Siraj, Denis Ulybyshev


## 2020-12-31

[Better Robustness by More Coverage: Adversarial Training with Mixup Augmentation for Robust Fine-tuning.](http://arxiv.org/abs/2012.15699)

Chenglei Si, Zhengyan Zhang, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Qun Liu, Maosong Sun


[Patch-wise++ Perturbation for Adversarial Targeted Attacks.](http://arxiv.org/abs/2012.15503)

Lianli Gao, Qilong Zhang, Jingkuan Song, Heng Tao Shen


## 2020-12-30

[Temporally-Transferable Perturbations: Efficient, One-Shot Adversarial Attacks for Online Visual Object Trackers.](http://arxiv.org/abs/2012.15183)

Krishna Kanth Nakka, Mathieu Salzmann


[Beating Attackers At Their Own Games: Adversarial Example Detection Using Adversarial Gradient Directions.](http://arxiv.org/abs/2012.15386)

Yuhang Wu, Sunpreet S. Arora, Yanhong Wu, Hao Yang


## 2020-12-29

[Black-box Adversarial Attacks on Monocular Depth Estimation Using Evolutionary Multi-objective Optimization.](http://arxiv.org/abs/2101.10452)

Renya Department of Information Science and Biomedical Engineering, Graduate School of Science and Engineering, Kagoshima University Daimo, Satoshi Department of Information Science and Biomedical Engineering, Graduate School of Science and Engineering, Kagoshima University Ono, Takahiro Department of Information Science and Biomedical Engineering, Graduate School of Science and Engineering, Kagoshima University Suzuki


[Generating Adversarial Examples in Chinese Texts Using Sentence-Pieces.](http://arxiv.org/abs/2012.14769)

Linyang Li, Yunfan Shao, Demin Song, Xipeng Qiu, Xuanjing Huang


[Improving Adversarial Robustness in Weight-quantized Neural Networks.](http://arxiv.org/abs/2012.14965)

Chang Song, Elias Fallon, Hai Li


[With False Friends Like These, Who Can Have Self-Knowledge?](http://arxiv.org/abs/2012.14738)

Lue Tao, Songcan Chen


[Generating Natural Language Attacks in a Hard Label Black Box Setting.](http://arxiv.org/abs/2012.14956)

Rishabh Maheshwary, Saket Maheshwary, Vikram Pudi


## 2020-12-28

[Enhanced Regularizers for Attributional Robustness.](http://arxiv.org/abs/2012.14395)

Anindya Sarkar, Anirban Sarkar, Vineeth N Balasubramanian


[Analysis of Dominant Classes in Universal Adversarial Perturbations.](http://arxiv.org/abs/2012.14352)

Jon Vadillo, Roberto Santana, Jose A. Lozano


## 2020-12-27

[Person Re-identification with Adversarial Triplet Embedding.](http://arxiv.org/abs/2012.14057)

Xinglu Wang


[My Teacher Thinks The World Is Flat! Interpreting Automatic Essay Scoring Mechanism.](http://arxiv.org/abs/2012.13872)

Swapnil Parekh, Yaman Kumar Singla, Changyou Chen, Junyi Jessy Li, Rajiv Ratn Shah


## 2020-12-26

[Sparse Adversarial Attack to Object Detection.](http://arxiv.org/abs/2012.13692)

Jiayu Bao


[Assessment of the Relative Importance of different hyper-parameters of LSTM for an IDS.](http://arxiv.org/abs/2012.14427)

Mohit Sewak, Sanjay K. Sahay, Hemant Rathore


## 2020-12-25

[Robustness, Privacy, and Generalization of Adversarial Training.](http://arxiv.org/abs/2012.13573)

Fengxiang He, Shaopeng Fu, Bohan Wang, Dacheng Tao


[A Simple Fine-tuning Is All You Need: Towards Robust Deep Learning Via Adversarial Fine-tuning.](http://arxiv.org/abs/2012.13628)

Ahmadreza Jeddi, Mohammad Javad Shafiee, Alexander Wong


## 2020-12-24

[A Context Aware Approach for Generating Natural Language Attacks.](http://arxiv.org/abs/2012.13339)

Rishabh Maheshwary, Saket Maheshwary, Vikram Pudi


[Exploring Adversarial Examples via Invertible Neural Networks.](http://arxiv.org/abs/2012.13111)

Ruqi Bai, Saurabh Bagchi, David I. Inouye


[Improving the Certified Robustness of Neural Networks via Consistency Regularization.](http://arxiv.org/abs/2012.13103)

Mengting Xu, Tao Zhang, Zhongnian Li, Daoqiang Zhang


[Adversarial Momentum-Contrastive Pre-Training.](http://arxiv.org/abs/2012.13154)

Cong Xu, Min Yang


[Learning Robust Representation for Clustering through Locality Preserving Variational Discriminative Network.](http://arxiv.org/abs/2012.13489)

Ruixuan Luo, Wei Li, Zhiyuan Zhang, Ruihan Bao, Keiko Harimoto, Xu Sun


## 2020-12-23

[The Translucent Patch: A Physical and Universal Attack on Object Detectors.](http://arxiv.org/abs/2012.12528)

Alon Zolfi, Moshe Kravchik, Yuval Elovici, Asaf Shabtai


[Gradient-Free Adversarial Attacks for Bayesian Neural Networks.](http://arxiv.org/abs/2012.12640)

Matthew Yuan, Matthew Wicker, Luca Laurenti


[SCOPE CPS: Secure Compiling of PLCs in Cyber-Physical Systems.](http://arxiv.org/abs/2012.12529)

Eyasu Getahun Chekole, Martin Ochoa, Sudipta Chattopadhyay


[Poisoning Attacks on Cyber Attack Detectors for Industrial Control Systems.](http://arxiv.org/abs/2012.15740)

Moshe Kravchik, Battista Biggio, Asaf Shabtai


## 2020-12-22

[Learning to Initialize Gradient Descent Using Gradient Descent.](http://arxiv.org/abs/2012.12141)

Kartik Ahuja, Amit Dhurandhar, Kush R. Varshney


[Unadversarial Examples: Designing Objects for Robust Vision.](http://arxiv.org/abs/2012.12235)

Hadi Salman, Andrew Ilyas, Logan Engstrom, Sai Vemprala, Aleksander Madry, Ashish Kapoor


[Multi-shot NAS for Discovering Adversarially Robust Convolutional Neural Architectures at Targeted Capacities.](http://arxiv.org/abs/2012.11835)

Xuefei Ning, Junbo Zhao, Wenshuo Li, Tianchen Zhao, Huazhong Yang, Yu Wang


[On Frank-Wolfe Optimization for Adversarial Robustness and Interpretability.](http://arxiv.org/abs/2012.12368)

Theodoros Tsiligkaridis, Jay Roberts


## 2020-12-21

[Genetic Adversarial Training of Decision Trees.](http://arxiv.org/abs/2012.11352)

Francesco Ranzato, Marco Zanella


[Incremental Verification of Fixed-Point Implementations of Neural Networks.](http://arxiv.org/abs/2012.11220)

Luiz Sena, Erickson Alves, Iury Bessa, Eddie Filho, Lucas Cordeiro


[Blurring Fools the Network -- Adversarial Attacks by Feature Peak Suppression and Gaussian Blurring.](http://arxiv.org/abs/2012.11442)

Chenchen Zhao, Hao Li


[Exploiting Vulnerability of Pooling in Convolutional Neural Networks by Strict Layer-Output Manipulation for Adversarial Attacks.](http://arxiv.org/abs/2012.11413)

Chenchen Zhao, Hao Li


[Deep Feature Space Trojan Attack of Neural Networks by Controlled Detoxification.](http://arxiv.org/abs/2012.11212)

Siyuan Cheng, Yingqi Liu, Shiqing Ma, Xiangyu Zhang


[Self-Progressing Robust Training.](http://arxiv.org/abs/2012.11769)

Minhao Cheng, Pin-Yu Chen, Sijia Liu, Shiyu Chang, Cho-Jui Hsieh, Payel Das


[Adjust-free adversarial example generation in speech recognition using evolutionary multi-objective optimization under black-box condition.](http://arxiv.org/abs/2012.11138)

Shoma Ishida, Satoshi Ono


[Defence against adversarial attacks using classical and quantum-enhanced Boltzmann machines.](http://arxiv.org/abs/2012.11619)

Aidan Kehoe, Peter Wittek, Yanbo Xue, Alejandro Pozas-Kerstjens


[On Success and Simplicity: A Second Look at Transferable Targeted Attacks.](http://arxiv.org/abs/2012.11207)

Zhengyu Zhao, Zhuoran Liu, Martha Larson


[Learning from What We Know: How to Perform Vulnerability Prediction using Noisy Historical Data. (1%)](http://arxiv.org/abs/2012.11701)

Aayush Garg, Renzo Degiovanni, Matthieu Jimenez, Maxime Cordy, Mike Papadakis, Yves Le Traon


## 2020-12-20

[Color Channel Perturbation Attacks for Fooling Convolutional Neural Networks and A Defense Against Such Attacks.](http://arxiv.org/abs/2012.14456)

Jayendra Kantipudi, Shiv Ram Dubey, Soumendu Chakraborty


## 2020-12-19

[Sample Complexity of Adversarially Robust Linear Classification on Separated Data.](http://arxiv.org/abs/2012.10794)

Robi Bhattacharjee, Somesh Jha, Kamalika Chaudhuri


## 2020-12-18

[Semantics and explanation: why counterfactual explanations produce adversarial examples in deep neural networks.](http://arxiv.org/abs/2012.10076)

Kieran Browne, Ben Swift


[ROBY: Evaluating the Robustness of a Deep Model by its Decision Boundaries.](http://arxiv.org/abs/2012.10282)

Jinyin Chen, Zhen Wang, Haibin Zheng, Jun Xiao, Zhaoyan Ming


[AdvExpander: Generating Natural Language Adversarial Examples by Expanding Text.](http://arxiv.org/abs/2012.10235)

Zhihong Shao, Zitao Liu, Jiyong Zhang, Zhongqin Wu, Minlie Huang


[Adversarially Robust Estimate and Risk Analysis in Linear Regression.](http://arxiv.org/abs/2012.10278)

Yue Xing, Ruizhi Zhang, Guang Cheng


[RAILS: A Robust Adversarial Immune-inspired Learning System.](http://arxiv.org/abs/2012.10485)

Ren Wang, Tianqi Chen, Stephen Lindsly, Alnawaz Rehemtulla, Alfred Hero, Indika Rajapakse


[Efficient Training of Robust Decision Trees Against Adversarial Examples.](http://arxiv.org/abs/2012.10438)

Daniël Vos, Sicco Verwer


[On the human-recognizability phenomenon of adversarially trained deep image classifiers.](http://arxiv.org/abs/2101.05219)

Jonathan Helland, Nathan VanHoudnos


## 2020-12-17

[Characterizing the Evasion Attackability of Multi-label Classifiers.](http://arxiv.org/abs/2012.09427)

Zhuo Yang, Yufei Han, Xiangliang Zhang


[A Hierarchical Feature Constraint to Camouflage Medical Adversarial Attacks.](http://arxiv.org/abs/2012.09501)

Qingsong Yao, Zecheng He, Yi Lin, Kai Ma, Yefeng Zheng, S. Kevin Zhou


## 2020-12-16

[On the Limitations of Denoising Strategies as Adversarial Defenses.](http://arxiv.org/abs/2012.09384)

Zhonghan Niu, Zhaoxi Chen, Linyi Li, Yubin Yang, Bo Li, Jinfeng Yi


## 2020-12-15

[FoggySight: A Scheme for Facial Lookup Privacy.](http://arxiv.org/abs/2012.08588)

Ivan Evtimov, Pascal Sturmfels, Tadayoshi Kohno


[FAWA: Fast Adversarial Watermark Attack on Optical Character Recognition (OCR) Systems.](http://arxiv.org/abs/2012.08096)

Lu Chen, Jiao Sun, Wei Xu


[Amata: An Annealing Mechanism for Adversarial Training Acceleration.](http://arxiv.org/abs/2012.08112)

Nanyang Ye, Qianxiao Li, Xiao-Yun Zhou, Zhanxing Zhu


## 2020-12-14

[Disentangled Information Bottleneck.](http://arxiv.org/abs/2012.07372)

Ziqi Pan, Li Niu, Jianfu Zhang, Liqing Zhang


[Adaptive Verifiable Training Using Pairwise Class Similarity.](http://arxiv.org/abs/2012.07887)

Shiqi Wang, Kevin Eykholt, Taesung Lee, Jiyong Jang, Ian Molloy


[Robustness Threats of Differential Privacy.](http://arxiv.org/abs/2012.07828)

Nurislam Tursynbek, Aleksandr Petiushko, Ivan Oseledets


[HaS-Nets: A Heal and Select Mechanism to Defend DNNs Against Backdoor Attacks for Data Collection Scenarios.](http://arxiv.org/abs/2012.07474)

Hassan Ali, Surya Nepal, Salil S. Kanhere, Sanjay Jha


[Improving Adversarial Robustness via Probabilistically Compact Loss with Logit Constraints.](http://arxiv.org/abs/2012.07688)

Xin Li, Xiangrui Li, Deng Pan, Dongxiao Zhu


[Binary Black-box Evasion Attacks Against Deep Learning-based Static Malware Detectors with Adversarial Byte-Level Language Model.](http://arxiv.org/abs/2012.07994)

Mohammadreza Ebrahimi, Ning Zhang, James Hu, Muhammad Taqi Raza, Hsinchun Chen


[Contrastive Learning with Adversarial Perturbations for Conditional Text Generation.](http://arxiv.org/abs/2012.07280)

Seanie Lee, Dong Bok Lee, Sung Ju Hwang


## 2020-12-13

[Achieving Adversarial Robustness Requires An Active Teacher.](http://arxiv.org/abs/2012.07233)

Chao Ma, Lexing Ying


## 2020-12-12

[Query-free Black-box Adversarial Attacks on Graphs.](http://arxiv.org/abs/2012.06757)

Jiarong Xu, Yizhou Sun, Xin Jiang, Yanhao Wang, Yang Yang, Chunping Wang, Jiangang Lu


## 2020-12-11

[Closeness and Uncertainty Aware Adversarial Examples Detection in Adversarial Machine Learning.](http://arxiv.org/abs/2012.06390)

Omer Faruk Tuna, Ferhat Ozgur Catak, M. Taner Eskil


[Attack Agnostic Detection of Adversarial Examples via Random Subspace Analysis.](http://arxiv.org/abs/2012.06405)

Nathan Drenkow, Neil Fendley, Philippe Burlina


[Analyzing and Improving Adversarial Training for Generative Modeling. (86%)](http://arxiv.org/abs/2012.06568)

Xuwang Yin, Shiying Li, Gustavo K. Rohde


## 2020-12-10

[GNNUnlock: Graph Neural Networks-based Oracle-less Unlocking Scheme for Provably Secure Logic Locking.](http://arxiv.org/abs/2012.05948)

Lilas Alrahis, Satwik Patnaik, Faiq Khalid, Muhammad Abdullah Hanif, Hani Saleh, Muhammad Shafique, Ozgur Sinanoglu


[Next Wave Artificial Intelligence: Robust, Explainable, Adaptable, Ethical, and Accountable.](http://arxiv.org/abs/2012.06058)

Odest Chadwicke Jenkins, Daniel Lopresti, Melanie Mitchell


[DSRNA: Differentiable Search of Robust Neural Architectures.](http://arxiv.org/abs/2012.06122)

Ramtin Hosseini, Xingyi Yang, Pengtao Xie


[I-GCN: Robust Graph Convolutional Network via Influence Mechanism.](http://arxiv.org/abs/2012.06110)

Haoxi Zhan, Xiaobing Pei


[An Empirical Review of Adversarial Defenses.](http://arxiv.org/abs/2012.06332)

Ayush Goel


[Robustness and Transferability of Universal Attacks on Compressed Models.](http://arxiv.org/abs/2012.06024)

Alberto G. Matachana, Kenneth T. Co, Luis Muñoz-González, David Martinez, Emil C. Lupu


[Geometric Adversarial Attacks and Defenses on 3D Point Clouds.](http://arxiv.org/abs/2012.05657)

Itai Lang, Uriel Kotlicki, Shai Avidan


[SPAA: Stealthy Projector-based Adversarial Attacks on Deep Image Classifiers.](http://arxiv.org/abs/2012.05858)

Bingyao Huang, Haibin Ling


## 2020-12-09

[Generating Out of Distribution Adversarial Attack using Latent Space Poisoning.](http://arxiv.org/abs/2012.05027)

Ujjwal Upadhyay, Prerana Mukherjee


[Detection of Adversarial Supports in Few-shot Classifiers Using Self-Similarity and Filtering.](http://arxiv.org/abs/2012.06330)

Yi Xiang Marcus Tan, Penny Chong, Jiamei Sun, Ngai-Man Cheung, Yuval Elovici, Alexander Binder


[Securing Deep Spiking Neural Networks against Adversarial Attacks through Inherent Structural Parameters.](http://arxiv.org/abs/2012.05321)

Rida El-Allami, Alberto Marchisio, Muhammad Shafique, Ihsen Alouani


[Composite Adversarial Attacks.](http://arxiv.org/abs/2012.05434)

Xiaofeng Mao, Yuefeng Chen, Shuhui Wang, Hang Su, Yuan He, Hui Xue


## 2020-12-08

[Provable Defense against Privacy Leakage in Federated Learning from Representation Perspective.](http://arxiv.org/abs/2012.06043)

Jingwei Sun, Ang Li, Binghui Wang, Huanrui Yang, Hai Li, Yiran Chen


[On 1/n neural representation and robustness.](http://arxiv.org/abs/2012.04729)

Josue Nassar, Piotr Aleksander Sokol, SueYeon Chung, Kenneth D. Harris, Il Memming Park


[Locally optimal detection of stochastic targeted universal adversarial perturbations.](http://arxiv.org/abs/2012.04692)

Amish Goel, Pierre Moulin


[A Deep Marginal-Contrastive Defense against Adversarial Attacks on 1D Models.](http://arxiv.org/abs/2012.04734)

Mohammed Hassanin, Nour Moustafa, Murat Tahtali


[Using Feature Alignment can Improve Clean Average Precision and Adversarial Robustness in Object Detection.](http://arxiv.org/abs/2012.04382)

Weipeng Xu, Hongcheng Huang


[EvaLDA: Efficient Evasion Attacks Towards Latent Dirichlet Allocation.](http://arxiv.org/abs/2012.04864)

Qi Zhou, Haipeng Chen, Yitao Zheng, Zhen Wang


[Overcomplete Representations Against Adversarial Videos.](http://arxiv.org/abs/2012.04262)

Shao-Yuan Lo, Jeya Maria Jose Valanarasu, Vishal M. Patel


[Mitigating the Impact of Adversarial Attacks in Very Deep Networks.](http://arxiv.org/abs/2012.04750)

Mohammed Hassanin, Ibrahim Radwan, Nour Moustafa, Murat Tahtali, Neeraj Kumar


[Reinforcement Based Learning on Classification Task Could Yield Better Generalization and Adversarial Accuracy.](http://arxiv.org/abs/2012.04353)

Shashi Kant Gupta


[Poisoning Semi-supervised Federated Learning via Unlabeled Data: Attacks and Defenses. (95%)](http://arxiv.org/abs/2012.04432)

Yi Liu, Xingliang Yuan, Ruihui Zhao, Cong Wang, Dusit Niyato, Yefeng Zheng


[Data Dependent Randomized Smoothing. (1%)](http://arxiv.org/abs/2012.04351)

Motasem Alfarra, Adel Bibi, Philip H. S. Torr, Bernard Ghanem


## 2020-12-07

[A Singular Value Perspective on Model Robustness.](http://arxiv.org/abs/2012.03516)

Malhar Jere, Maghav Kumar, Farinaz Koushanfar


[Backpropagating Linearly Improves Transferability of Adversarial Examples.](http://arxiv.org/abs/2012.03528)

Yiwen Guo, Qizhang Li, Hao Chen


[Learning to Separate Clusters of Adversarial Representations for Robust Adversarial Detection.](http://arxiv.org/abs/2012.03483)

Byunggill Joe, Jihun Hamm, Sung Ju Hwang, Sooel Son, Insik Shin


[Are DNNs fooled by extremely unrecognizable images?](http://arxiv.org/abs/2012.03843)

Soichiro Kumano, Hiroshi Kera, Toshihiko Yamasaki


[Reprogramming Language Models for Molecular Representation Learning.](http://arxiv.org/abs/2012.03460)

Ria Vinod, Pin-Yu Chen, Payel Das


## 2020-12-06

[Black-box Model Inversion Attribute Inference Attacks on Classification Models.](http://arxiv.org/abs/2012.03404)

Shagufta Mehnaz, Ninghui Li, Elisa Bertino


[PAC-Learning for Strategic Classification.](http://arxiv.org/abs/2012.03310)

Ravi Sundaram, Anil Vullikanti, Haifeng Xu, Fan Yao


## 2020-12-05

[Evaluating adversarial robustness in simulated cerebellum.](http://arxiv.org/abs/2012.02976)

Liu Yuezhang, Bo Li, Qifeng Chen


## 2020-12-04

[Advocating for Multiple Defense Strategies against Adversarial Examples.](http://arxiv.org/abs/2012.02632)

Alexandre Araujo, Laurent Meunier, Rafael Pinot, Benjamin Negrevergne


[Practical No-box Adversarial Attacks against DNNs.](http://arxiv.org/abs/2012.02525)

Qizhang Li, Yiwen Guo, Hao Chen


[Towards Natural Robustness Against Adversarial Examples.](http://arxiv.org/abs/2012.02452)

Haoyu Chu, Shikui Wei, Yao Zhao


[Unsupervised Adversarially-Robust Representation Learning on Graphs.](http://arxiv.org/abs/2012.02486)

Jiarong Xu, Yang Yang, Junru Chen, Chunping Wang, Xin Jiang, Jiangang Lu, Yizhou Sun


[Kernel-convoluted Deep Neural Networks with Data Augmentation.](http://arxiv.org/abs/2012.02521)

Minjin Kim, Young-geun Kim, Dongha Kim, Yongdai Kim, Myunghee Cho Paik


## 2020-12-03

[Ethical Testing in the Real World: Evaluating Physical Testing of Adversarial Machine Learning.](http://arxiv.org/abs/2012.02048)

Kendra Albert, Maggie Delano, Jonathon Penney, Afsaneh Rigot, Ram Shankar Siva Kumar


[FAT: Federated Adversarial Training.](http://arxiv.org/abs/2012.01791)

Giulio Zizzo, Ambrish Rawat, Mathieu Sinn, Beat Buesser


[An Empirical Study of Derivative-Free-Optimization Algorithms for Targeted Black-Box Attacks in Deep Neural Networks.](http://arxiv.org/abs/2012.01901)

Giuseppe Ughi, Vinayak Abrol, Jared Tanner


[Channel Effects on Surrogate Models of Adversarial Attacks against Wireless Signal Classifiers.](http://arxiv.org/abs/2012.02160)

Brian Kim, Yalin E. Sagduyu, Tugba Erpek, Kemal Davaslioglu, Sennur Ulukus


[Attribute-Guided Adversarial Training for Robustness to Natural Perturbations.](http://arxiv.org/abs/2012.01806)

Tejas Gokhale, Rushil Anirudh, Bhavya Kailkhura, Jayaraman J. Thiagarajan, Chitta Baral, Yezhou Yang


## 2020-12-02

[From a Fourier-Domain Perspective on Adversarial Examples to a Wiener Filter Defense for Semantic Segmentation.](http://arxiv.org/abs/2012.01558)

Nikhil Kapoor, Andreas Bär, Serin Varghese, Jan David Schneider, Fabian Hüger, Peter Schlicht, Tim Fingscheidt


[FenceBox: A Platform for Defeating Adversarial Examples with Data Augmentation Techniques.](http://arxiv.org/abs/2012.01701)

Han Qiu, Yi Zeng, Tianwei Zhang, Yong Jiang, Meikang Qiu


[Towards Defending Multiple $\ell_p$-norm Bounded Adversarial Perturbations via Gated Batch Normalization.](http://arxiv.org/abs/2012.01654)

Aishan Liu, Shiyu Tang, Xinyun Chen, Lei Huang, Haotong Qin, Xianglong Liu, Dacheng Tao


[Content-Adaptive Pixel Discretization to Improve Model Robustness.](http://arxiv.org/abs/2012.01699)

Ryan Feng, Wu-chi Feng, Atul Prakash


[How Robust are Randomized Smoothing based Defenses to Data Poisoning?](http://arxiv.org/abs/2012.01274)

Akshay Mehra, Bhavya Kailkhura, Pin-Yu Chen, Jihun Hamm


## 2020-12-01

[Adversarial Robustness Across Representation Spaces.](http://arxiv.org/abs/2012.00802)

Pranjal Awasthi, George Yu, Chun-Sung Ferng, Andrew Tomkins, Da-Cheng Juan


[Robustness Out of the Box: Compositional Representations Naturally Defend Against Black-Box Patch Attacks.](http://arxiv.org/abs/2012.00558)

Christian Cosgrove, Adam Kortylewski, Chenglin Yang, Alan Yuille


[Boosting Adversarial Attacks on Neural Networks with Better Optimizer.](http://arxiv.org/abs/2012.00567)

Heng Yin, Hengwei Zhang, Jindong Wang, Ruiyu Dou


[One-Pixel Attack Deceives Computer-Assisted Diagnosis of Cancer.](http://arxiv.org/abs/2012.00517)

Joni Korpihalkola, Tuomo Sipola, Samir Puuska, Tero Kokkonen


[Towards Imperceptible Adversarial Image Patches Based on Network Explanations.](http://arxiv.org/abs/2012.00909)

Yaguan Qian, Jiamin Wang, Bin Wang, Zhaoquan Gu, Xiang Ling, Chunming Wu


## 2020-11-30

[Guided Adversarial Attack for Evaluating and Enhancing Adversarial Defenses.](http://arxiv.org/abs/2011.14969)

Gaurang Sriramanan, Sravanti Addepalli, Arya Baburaj, R. Venkatesh Babu


[Just One Moment: Structural Vulnerability of Deep Action Recognition against One Frame Attack.](http://arxiv.org/abs/2011.14585)

Jaehui Hwang, Jun-Hyuk Kim, Jun-Ho Choi, Jong-Seok Lee


## 2020-11-29

[Architectural Adversarial Robustness: The Case for Deep Pursuit.](http://arxiv.org/abs/2011.14427)

George Cazenavette, Calvin Murdock, Simon Lucey


[A Targeted Universal Attack on Graph Convolutional Network.](http://arxiv.org/abs/2011.14365)

Jiazhu Dai, Weifeng Zhu, Xiangfeng Luo


[SwitchX: Gmin-Gmax Switching for Energy-Efficient and Robust Implementation of Binary Neural Networks on ReRAM Xbars.](http://arxiv.org/abs/2011.14498)

Abhiroop Bhattacharjee, Priyadarshini Panda


## 2020-11-28

[Cyberbiosecurity: DNA Injection Attack in Synthetic Biology.](http://arxiv.org/abs/2011.14224)

Dor Farbiash, Rami Puzis


[Deterministic Certification to Adversarial Attacks via Bernstein Polynomial Approximation.](http://arxiv.org/abs/2011.14085)

Ching-Chia Kao, Jhe-Bang Ko, Chun-Shien Lu


[FaceGuard: A Self-Supervised Defense Against Adversarial Face Images.](http://arxiv.org/abs/2011.14218)

Debayan Deb, Xiaoming Liu, Anil K. Jain


## 2020-11-27

[3D Invisible Cloak.](http://arxiv.org/abs/2011.13705)

Mingfu Xue, Can He, Zhiyu Wu, Jian Wang, Zhe Liu, Weiqiang Liu


[Fast and Complete: Enabling Complete Neural Network Verification with Rapid and Massively Parallel Incomplete Verifiers.](http://arxiv.org/abs/2011.13824)

Kaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, Cho-Jui Hsieh


[Voting based ensemble improves robustness of defensive models.](http://arxiv.org/abs/2011.14031)

Devvrit, Minhao Cheng, Cho-Jui Hsieh, Inderjit Dhillon


[Generalized Adversarial Examples: Attacks and Defenses.](http://arxiv.org/abs/2011.14045)

Haojing Shen, Sihong Chen, Ran Wang, Xizhao Wang


[Robust and Natural Physical Adversarial Examples for Object Detectors.](http://arxiv.org/abs/2011.13692)

Mingfu Xue, Chengxiang Yuan, Can He, Jian Wang, Weiqiang Liu


[SocialGuard: An Adversarial Example Based Privacy-Preserving Technique for Social Images.](http://arxiv.org/abs/2011.13560)

Mingfu Xue, Shichang Sun, Zhiyu Wu, Can He, Jian Wang, Weiqiang Liu


[Use the Spear as a Shield: A Novel Adversarial Example based Privacy-Preserving Technique against Membership Inference Attacks.](http://arxiv.org/abs/2011.13696)

Mingfu Xue, Chengxiang Yuan, Can He, Zhiyu Wu, Yushu Zhang, Zhe Liu, Weiqiang Liu


## 2020-11-26

[Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness.](http://arxiv.org/abs/2011.13538)

Yilun Jin, Lixin Fan, Kam Woh Ng, Ce Ju, Qiang Yang


[Exposing the Robustness and Vulnerability of Hybrid 8T-6T SRAM Memory Architectures to Adversarial Attacks in Deep Neural Networks.](http://arxiv.org/abs/2011.13392)

Abhishek Moitra, Priyadarshini Panda


[Robust Attacks on Deep Learning Face Recognition in the Physical World.](http://arxiv.org/abs/2011.13526)

Meng Shen, Hao Yu, Liehuang Zhu, Ke Xu, Qi Li, Xiaojiang Du


[Regularization with Latent Space Virtual Adversarial Training.](http://arxiv.org/abs/2011.13181)

Genki Osada, Budrul Ahsan, Revoti Prasad Bora, Takashi Nishide


[Invisible Perturbations: Physical Adversarial Examples Exploiting the Rolling Shutter Effect.](http://arxiv.org/abs/2011.13375)

Athena Sayles, Ashish Hooda, Mohit Gupta, Rahul Chatterjee, Earlence Fernandes


## 2020-11-25

[Adversarial Attack on Facial Recognition using Visible Light.](http://arxiv.org/abs/2011.12680)

Morgan Frearson, Kien Nguyen


[Adversarial Evaluation of Multimodal Models under Realistic Gray Box Assumption.](http://arxiv.org/abs/2011.12902)

Ivan Evtimov, Russel Howes, Brian Dolhansky, Hamed Firooz, Cristian Canton Ferrer


[SurFree: a fast surrogate-free black-box attack.](http://arxiv.org/abs/2011.12807)

Thibault Maho, Teddy Furon, Erwan Le Merrer


[Advancing diagnostic performance and clinical usability of neural networks via adversarial training and dual batch normalization.](http://arxiv.org/abs/2011.13011)

Tianyu Han, Sven Nebelung, Federico Pedersoli, Markus Zimmermann, Maximilian Schulze-Hagen, Michael Ho, Christoph Haarburger, Fabian Kiessling, Christiane Kuhl, Volkmar Schulz, Daniel Truhn


[Probing Model Signal-Awareness via Prediction-Preserving Input Minimization. (80%)](http://arxiv.org/abs/2011.14934)

Sahil Suneja, Yunhui Zheng, Yufan Zhuang, Jim Laredo, Alessandro Morari


## 2020-11-24

[Trust but Verify: Assigning Prediction Credibility by Counterfactual Constrained Learning.](http://arxiv.org/abs/2011.12344)

Luiz F. O. Chamon, Santiago Paternain, Alejandro Ribeiro


[Stochastic sparse adversarial attacks.](http://arxiv.org/abs/2011.12423)

Manon Césaire, Hatem Hajri, Sylvain Lamprier, Patrick Gallinari


[On the Adversarial Robustness of 3D Point Cloud Classification.](http://arxiv.org/abs/2011.11922)

Jiachen Sun, Karl Koenig, Yulong Cao, Qi Alfred Chen, Z. Morley Mao


[Towards Imperceptible Universal Attacks on Texture Recognition.](http://arxiv.org/abs/2011.11957)

Yingpeng Deng, Lina J. Karam


## 2020-11-23

[Omni: Automated Ensemble with Unexpected Models against Adversarial Evasion Attack.](http://arxiv.org/abs/2011.12720)

Rui Shu, Tianpei Xia, Laurie Williams, Tim Menzies


[Augmented Lagrangian Adversarial Attacks.](http://arxiv.org/abs/2011.11857)

Jérôme Rony, Eric Granger, Marco Pedersoli, Ismail Ben Ayed


## 2020-11-22

[Learnable Boundary Guided Adversarial Training.](http://arxiv.org/abs/2011.11164)

Jiequan Cui, Shu Liu, Liwei Wang, Jiaya Jia


[Nudge Attacks on Point-Cloud DNNs.](http://arxiv.org/abs/2011.11637)

Yiren Zhao, Ilia Shumailov, Robert Mullins, Ross Anderson


## 2020-11-21

[Spatially Correlated Patterns in Adversarial Images.](http://arxiv.org/abs/2011.10794)

Nandish Chattopadhyay, Lionell Yip En Zhi, Bryan Tan Bing Xing, Anupam Chattopadhyay


[A Neuro-Inspired Autoencoding Defense Against Adversarial Perturbations.](http://arxiv.org/abs/2011.10867)

Can Bakiskan, Metehan Cekic, Ahmet Dundar Sezer, Upamanyu Madhow


[Robust Data Hiding Using Inverse Gradient Attention. (2%)](http://arxiv.org/abs/2011.10850)

Honglei Zhang, Hu Wang, Yuanzhouhan Cao, Chunhua Shen, Yidong Li


## 2020-11-20

[Are Chess Discussions Racist? An Adversarial Hate Speech Data Set.](http://arxiv.org/abs/2011.10280)

Rupak Sarkar, Ashiqur R. KhudaBukhsh


[Detecting Universal Trigger's Adversarial Attack with Honeypot.](http://arxiv.org/abs/2011.10492)

Thai Le, Noseong Park, Dongwon Lee


## 2020-11-19

[An Experimental Study of Semantic Continuity for Deep Learning Models.](http://arxiv.org/abs/2011.09789)

Shangxi Wu, Jitao Sang, Xian Zhao, Lizhang Chen


[Adversarial Examples for $k$-Nearest Neighbor Classifiers Based on Higher-Order Voronoi Diagrams.](http://arxiv.org/abs/2011.09719)

Chawin Sitawarin, Evgenios M. Kornaropoulos, Dawn Song, David Wagner


[Adversarial Threats to DeepFake Detection: A Practical Perspective.](http://arxiv.org/abs/2011.09957)

Paarth Neekhara, Brian Dolhansky, Joanna Bitton, Cristian Canton Ferrer


[Multi-Task Adversarial Attack.](http://arxiv.org/abs/2011.09824)

Pengxin Guo, Yuancheng Xu, Baijiong Lin, Yu Zhang


[Latent Adversarial Debiasing: Mitigating Collider Bias in Deep Neural Networks.](http://arxiv.org/abs/2011.11486)

Luke Darlow, Stanisław Jastrzębski, Amos Storkey


## 2020-11-18

[Robustified Domain Adaptation.](http://arxiv.org/abs/2011.09563)

Jiajin Zhang, Hanqing Chao, Pingkun Yan


[Adversarial collision attacks on image hashing functions.](http://arxiv.org/abs/2011.09473)

Brian Dolhansky, Cristian Canton Ferrer


[Contextual Fusion For Adversarial Robustness.](http://arxiv.org/abs/2011.09526)

Aiswarya Akumalla, Seth Haney, Maksim Bazhenov


[Adversarial Turing Patterns from Cellular Automata.](http://arxiv.org/abs/2011.09393)

Nurislam Tursynbek, Ilya Vilkoviskiy, Maria Sindeeva, Ivan Oseledets


[Self-Gradient Networks.](http://arxiv.org/abs/2011.09364)

Hossein Aboutalebi, Mohammad Javad Shafiee Alexander Wong


[Adversarial Profiles: Detecting Out-Distribution & Adversarial Samples in Pre-trained CNNs.](http://arxiv.org/abs/2011.09123)

Arezoo Rajabi, Rakesh B. Bobba


## 2020-11-17

[FoolHD: Fooling speaker identification by Highly imperceptible adversarial Disturbances.](http://arxiv.org/abs/2011.08483)

Ali Shahin Shamsabadi, Francisco Sepúlveda Teixeira, Alberto Abad, Bhiksha Raj, Andrea Cavallaro, Isabel Trancoso


[SIENA: Stochastic Multi-Expert Neural Patcher.](http://arxiv.org/abs/2011.08908)

Thai Le, Noseong Park, Dongwon Lee


[Shaping Deep Feature Space towards Gaussian Mixture for Visual Classification.](http://arxiv.org/abs/2011.09066)

Weitao Wan, Jiansheng Chen, Cheng Yu, Tong Wu, Yuanyi Zhong, Ming-Hsuan Yang


[Generating universal language adversarial examples by understanding and enhancing the transferability across neural models.](http://arxiv.org/abs/2011.08558)

Liping Yuan, Xiaoqing Zheng, Yi Zhou, Cho-Jui Hsieh, Kai-wei Chang, Xuanjing Huang


[Probing Predictions on OOD Images via Nearest Categories. (75%)](http://arxiv.org/abs/2011.08485)

Yao-Yuan Yang, Cyrus Rashtchian, Ruslan Salakhutdinov, Kamalika Chaudhuri


## 2020-11-16

[MAAC: Novel Alert Correlation Method To Detect Multi-step Attack.](http://arxiv.org/abs/2011.07793)

Xiaoyu Wang, Lei Yu, Houhua He, Xiaorui Gong


[Enforcing robust control guarantees within neural network policies.](http://arxiv.org/abs/2011.08105)

Priya L. Donti, Melrose Roderick, Mahyar Fazlyab, J. Zico Kolter


[Adversarially Robust Classification based on GLRT.](http://arxiv.org/abs/2011.07835)

Bhagyashree Puranik, Upamanyu Madhow, Ramtin Pedarsani


[Combining GANs and AutoEncoders for Efficient Anomaly Detection.](http://arxiv.org/abs/2011.08102)

Fabio ISTI CNR, Pisa, Italy Carrara, Giuseppe ISTI CNR, Pisa, Italy Amato, Luca ISTI CNR, Pisa, Italy Brombin, Fabrizio ISTI CNR, Pisa, Italy Falchi, Claudio ISTI CNR, Pisa, Italy Gennaro


[Extreme Value Preserving Networks.](http://arxiv.org/abs/2011.08367)

Mingjie Sun, Jianguo Li, Changshui Zhang


## 2020-11-15

[Towards Understanding the Regularization of Adversarial Robustness on Neural Networks.](http://arxiv.org/abs/2011.07478)

Yuxin Wen, Shuai Li, Kui Jia


[Ensemble of Models Trained by Key-based Transformed Images for Adversarially Robust Defense Against Black-box Attacks.](http://arxiv.org/abs/2011.07697)

MaungMaung AprilPyone, Hitoshi Kiya


[Almost Tight L0-norm Certified Robustness of Top-k Predictions against Adversarial Perturbations.](http://arxiv.org/abs/2011.07633)

Jinyuan Jia, Binghui Wang, Xiaoyu Cao, Hongbin Liu, Neil Zhenqiang Gong


[Power Side-Channel Attacks on BNN Accelerators in Remote FPGAs. (1%)](http://arxiv.org/abs/2011.07603)

Shayan Moini, Shanquan Tian, Jakub Szefer, Daniel Holcomb, Russell Tessier


## 2020-11-14

[Audio-Visual Event Recognition through the lens of Adversary.](http://arxiv.org/abs/2011.07430)

Juncheng B Li, Kaixin Ma, Shuhui Qu, Po-Yao Huang, Florian Metze


## 2020-11-13

[Transformer-Encoder Detector Module: Using Context to Improve Robustness to Adversarial Attacks on Object Detection.](http://arxiv.org/abs/2011.06978)

Faisal Alamri, Sinan Kalkan, Nicolas Pugeault


[Query-based Targeted Action-Space Adversarial Policies on Deep Reinforcement Learning Agents.](http://arxiv.org/abs/2011.07114)

Xian Yeow Lee, Yasaman Esfandiari, Kai Liang Tan, Soumik Sarkar


## 2020-11-12

[Adversarial Robustness Against Image Color Transformation within Parametric Filter Space.](http://arxiv.org/abs/2011.06690)

Zhengyu Zhao, Zhuoran Liu, Martha Larson


[Sparse PCA: Algorithms, Adversarial Perturbations and Certificates.](http://arxiv.org/abs/2011.06585)

Tommaso d'Orsi, Pravesh K. Kothari, Gleb Novikov, David Steurer


## 2020-11-11

[Adversarial images for the primate brain.](http://arxiv.org/abs/2011.05623)

Li Yuan, Will Xiao, Gabriel Kreiman, Francis E. H. Tay, Jiashi Feng, Margaret S. Livingstone


[Detecting Adversarial Patches with Class Conditional Reconstruction Networks.](http://arxiv.org/abs/2011.05850)

Perry Deng, Mohammad Saidur Rahman, Matthew Wright


## 2020-11-10

[Efficient and Transferable Adversarial Examples from Bayesian Neural Networks.](http://arxiv.org/abs/2011.05074)

Martin Gubri, Maxime Cordy, Mike Papadakis, Yves Le Traon, Koushik Sen


## 2020-11-09

[Solving Inverse Problems With Deep Neural Networks -- Robustness Included?](http://arxiv.org/abs/2011.04268)

Martin Genzel, Jan Macdonald, Maximilian März


## 2020-11-07

[Adversarial Black-Box Attacks On Text Classifiers Using Multi-Objective Genetic Optimization Guided By Deep Networks.](http://arxiv.org/abs/2011.03901)

Alex Mathai, Shreya Khare, Srikanth Tamilselvam, Senthil Mani


[Bridging the Performance Gap between FGSM and PGD Adversarial Training.](http://arxiv.org/abs/2011.05157)

Tianjin Huang, Vlado Menkovski, Yulong Pei, Mykola Pechenizkiy


## 2020-11-06

[Single-Node Attacks for Fooling Graph Neural Networks.](http://arxiv.org/abs/2011.03574)

Ben Finkelshtein, Chaim Baskin, Evgenii Zheltonozhskii, Uri Alon


[A survey on practical adversarial examples for malware classifiers.](http://arxiv.org/abs/2011.05973)

Daniel Park, Bülent Yener


## 2020-11-05

[A Black-Box Attack Model for Visually-Aware Recommender Systems.](http://arxiv.org/abs/2011.02701)

Rami Cohen, Oren Sar Shalom, Dietmar Jannach, Amihood Amir


[Data Augmentation via Structured Adversarial Perturbations.](http://arxiv.org/abs/2011.03010)

Calvin Luo, Hossein Mobahi, Samy Bengio


[Defense-friendly Images in Adversarial Attacks: Dataset and Metrics forPerturbation Difficulty.](http://arxiv.org/abs/2011.02675)

Camilo Pestana, Wei Liu, David Glance, Ajmal Mian


[Dynamically Sampled Nonlocal Gradients for Stronger Adversarial Attacks.](http://arxiv.org/abs/2011.02707)

Leo Schwinn, An Nguyen, René Raab, Dario Zanca, Bjoern Eskofier, Daniel Tenbrinck, Martin Burger


## 2020-11-03

[You Do (Not) Belong Here: Detecting DPI Evasion Attacks with Context Learning.](http://arxiv.org/abs/2011.01514)

Shitong Zhu, Shasha Li, Zhongjie Wang, Xun Chen, Zhiyun Qian, Srikanth V. Krishnamurthy, Kevin S. Chan, Ananthram Swami


[Detecting Word Sense Disambiguation Biases in Machine Translation for Model-Agnostic Adversarial Attacks.](http://arxiv.org/abs/2011.01846)

Denis Emelin, Ivan Titov, Rico Sennrich


[Penetrating RF Fingerprinting-based Authentication with a Generative Adversarial Attack.](http://arxiv.org/abs/2011.01538)

Samurdhi Karunaratne, Enes Krijestorac, Danijela Cabric


[Recent Advances in Understanding Adversarial Robustness of Deep Neural Networks.](http://arxiv.org/abs/2011.01539)

Tao Bai, Jinqi Luo, Jun Zhao


[A Tunable Robust Pruning Framework Through Dynamic Network Rewiring of DNNs.](http://arxiv.org/abs/2011.03083)

Souvik Kundu, Mahdi Nazemi, Peter A. Beerel, Massoud Pedram


[MalFox: Camouflaged Adversarial Malware Example Generation Based on Conv-GANs Against Black-Box Detectors.](http://arxiv.org/abs/2011.01509)

Fangtian Zhong, Xiuzhen Cheng, Dongxiao Yu, Bei Gong, Shuaiwen Song, Jiguo Yu


## 2020-11-02

[Adversarial Examples in Constrained Domains.](http://arxiv.org/abs/2011.01183)

Ryan Sheatsley, Nicolas Papernot, Michael Weisman, Gunjan Verma, Patrick McDaniel


[Frequency-based Automated Modulation Classification in the Presence of Adversaries.](http://arxiv.org/abs/2011.01132)

Rajeev Sahay, Christopher G. Brinton, David J. Love


[Robust Algorithms for Online Convex Problems via Primal-Dual.](http://arxiv.org/abs/2011.01435)

Marco Molinaro


[Trustworthy AI.](http://arxiv.org/abs/2011.02272)

Richa Singh, Mayank Vatsa, Nalini Ratha


## 2020-11-01

[LG-GAN: Label Guided Adversarial Network for Flexible Targeted Attack of Point Cloud-based Deep Networks.](http://arxiv.org/abs/2011.00566)

Hang Zhou, Dongdong Chen, Jing Liao, Weiming Zhang, Kejiang Chen, Xiaoyi Dong, Kunlin Liu, Gang Hua, Nenghai Yu


[Vulnerability of the Neural Networks Against Adversarial Examples: A Survey.](http://arxiv.org/abs/2011.05976)

Rui Zhao


## 2020-10-31

[MAD-VAE: Manifold Awareness Defense Variational Autoencoder.](http://arxiv.org/abs/2011.01755)

Frederick Morlock, Dingsu Wang


## 2020-10-30

[Integer Programming-based Error-Correcting Output Code Design for Robust Classification.](http://arxiv.org/abs/2011.00144)

Samarth Gupta, Saurabh Amin


[Leveraging Extracted Model Adversaries for Improved Black Box Attacks.](http://arxiv.org/abs/2010.16336)

Naveen Jafer Nizar, Ari Kobren


[EEG-Based Brain-Computer Interfaces Are Vulnerable to Backdoor Attacks.](http://arxiv.org/abs/2011.00101)

Lubin Meng, Jian Huang, Zhigang Zeng, Xue Jiang, Shan Yu, Tzyy-Ping Jung, Chin-Teng Lin, Ricardo Chavarriaga, Dongrui Wu


[Adversarial Attacks on Optimization based Planners.](http://arxiv.org/abs/2011.00095)

Sai Vemprala, Ashish Kapoor


[Capture the Bot: Using Adversarial Examples to Improve CAPTCHA Robustness to Bot Attacks.](http://arxiv.org/abs/2010.16204)

Dorjan Hitaj, Briland Hitaj, Sushil Jajodia, Luigi V. Mancini


[Perception Improvement for Free: Exploring Imperceptible Black-box Adversarial Attacks on Image Classification.](http://arxiv.org/abs/2011.05254)

Yongwei Wang, Mingquan Feng, Rabab Ward, Z. Jane Wang, Lanjun Wang


[Adversarial Robust Training of Deep Learning MRI Reconstruction Models.](http://arxiv.org/abs/2011.00070)

Francesco Calivá, Kaiyang Cheng, Rutwik Shah, Valentina Pedoia


## 2020-10-29

[Volumetric Medical Image Segmentation: A 3D Deep Coarse-to-fine Framework and Its Adversarial Examples.](http://arxiv.org/abs/2010.16074)

Yingwei Li, Zhuotun Zhu, Yuyin Zhou, Yingda Xia, Wei Shen, Elliot K. Fishman, Alan L. Yuille


[Perception Matters: Exploring Imperceptible and Transferable Anti-forensics for GAN-generated Fake Face Imagery Detection.](http://arxiv.org/abs/2010.15886)

Yongwei Wang, Xin Ding, Li Ding, Rabab Ward, Z. Jane Wang


[Can the state of relevant neurons in a deep neural networks serve as indicators for detecting adversarial attacks?](http://arxiv.org/abs/2010.15974)

Roger Granda, Tinne Tuytelaars, Jose Oramas


[Reliable Graph Neural Networks via Robust Aggregation.](http://arxiv.org/abs/2010.15651)

Simon Geisler, Daniel Zügner, Stephan Günnemann


[Passport-aware Normalization for Deep Model Protection.](http://arxiv.org/abs/2010.15824)

Jie Zhang, Dongdong Chen, Jing Liao, Weiming Zhang, Gang Hua, Nenghai Yu


[Robustifying Binary Classification to Adversarial Perturbation.](http://arxiv.org/abs/2010.15391)

Fariborz Salehi, Babak Hassibi


[Beyond cross-entropy: learning highly separable feature distributions for robust and accurate classification.](http://arxiv.org/abs/2010.15487)

Arslan Ali, Andrea Migliorati, Tiziano Bianchi, Enrico Magli


[WaveTransform: Crafting Adversarial Examples via Input Decomposition.](http://arxiv.org/abs/2010.15773)

Divyam Anshumaan, Akshay Agarwal, Mayank Vatsa, Richa Singh


## 2020-10-28

[Most ReLU Networks Suffer from $\ell^2$ Adversarial Perturbations.](http://arxiv.org/abs/2010.14927)

Amit Daniely, Hadas Schacham


[Object Hider: Adversarial Patch Attack Against Object Detectors.](http://arxiv.org/abs/2010.14974)

Yusheng Zhao, Huanqian Yan, Xingxing Wei


[Evaluating Robustness of Predictive Uncertainty Estimation: Are Dirichlet-based Models Reliable?](http://arxiv.org/abs/2010.14986)

Anna-Kathrin Kopetzki, Bertrand Charpentier, Daniel Zügner, Sandhya Giri, Stephan Günnemann


[Transferable Universal Adversarial Perturbations Using Generative Models.](http://arxiv.org/abs/2010.14919)

Atiye Sadat Hashemi, Andreas Bär, Saeed Mozaffari, Tim Fingscheidt


## 2020-10-27

[Fast Local Attack: Generating Local Adversarial Examples for Object Detectors.](http://arxiv.org/abs/2010.14291)

Quanyu Liao, Xin Wang, Bin Kong, Siwei Lyu, Youbing Yin, Qi Song, Xi Wu


[Anti-perturbation of Online Social Networks by Graph Label Transition.](http://arxiv.org/abs/2010.14121)

Jun Zhuang, Mohammad Al Hasan


## 2020-10-26

[Robust and Verifiable Information Embedding Attacks to Deep Neural Networks via Error-Correcting Codes.](http://arxiv.org/abs/2010.13751)

Jinyuan Jia, Binghui Wang, Neil Zhenqiang Gong


[GreedyFool: Distortion-Aware Sparse Adversarial Attack.](http://arxiv.org/abs/2010.13773)

Xiaoyi Dong, Dongdong Chen, Jianmin Bao, Chuan Qin, Lu Yuan, Weiming Zhang, Nenghai Yu, Dong Chen


[Robust Pre-Training by Adversarial Contrastive Learning.](http://arxiv.org/abs/2010.13337)

Ziyu Jiang, Tianlong Chen, Ting Chen, Zhangyang Wang


[Versatile Verification of Tree Ensembles.](http://arxiv.org/abs/2010.13880)

Laurens Devos, Wannes Meert, Jesse Davis


[Robustness May Be at Odds with Fairness: An Empirical Study on Class-wise Accuracy.](http://arxiv.org/abs/2010.13365)

Philipp Benz, Chaoning Zhang, Adil Karjauv, In So Kweon


[Exploring the Security Boundary of Data Reconstruction via Neuron Exclusivity Analysis. (16%)](http://arxiv.org/abs/2010.13356)

Xudong Pan, Mi Zhang, Yifan Yan, Jiaming Zhu, Min Yang


## 2020-10-25

[Attack Agnostic Adversarial Defense via Visual Imperceptible Bound.](http://arxiv.org/abs/2010.13247)

Saheb Chhabra, Akshay Agarwal, Richa Singh, Mayank Vatsa


[Dynamic Adversarial Patch for Evading Object Detection Models.](http://arxiv.org/abs/2010.13070)

Shahar Hoory, Tzvika Shapira, Asaf Shabtai, Yuval Elovici


[Asymptotic Behavior of Adversarial Training in Binary Classification.](http://arxiv.org/abs/2010.13275)

Hossein Taheri, Ramtin Pedarsani, Christos Thrampoulidis


## 2020-10-24

[ATRO: Adversarial Training with a Rejection Option.](http://arxiv.org/abs/2010.12905)

Masahiro Kato, Zhenghang Cui, Yoshihiro Fukuhara


[Are Adversarial Examples Created Equal? A Learnable Weighted Minimax Risk for Robustness under Non-uniform Attacks.](http://arxiv.org/abs/2010.12989)

Huimin Zeng, Chen Zhu, Tom Goldstein, Furong Huang


[Stop Bugging Me! Evading Modern-Day Wiretapping Using Adversarial Perturbations.](http://arxiv.org/abs/2010.12809)

Yael Mathov, Tal Ben Senior, Asaf Shabtai, Yuval Elovici


## 2020-10-23

[Improving Robustness by Augmenting Training Sentences with Predicate-Argument Structures.](http://arxiv.org/abs/2010.12510)

Nafise Sadat Moosavi, Boer Marcel de, Prasetya Ajie Utama, Iryna Gurevych


[Towards Robust Neural Networks via Orthogonal Diversity.](http://arxiv.org/abs/2010.12190)

Kun Fang, Qinghua Tao, Yingwen Wu, Tao Li, Jia Cai, Feipeng Cai, Xiaolin Huang, Jie Yang


## 2020-10-22

[Contrastive Learning with Adversarial Examples.](http://arxiv.org/abs/2010.12050)

Chih-Hui Ho, Nuno Vasconcelos


[Adversarial Attacks on Binary Image Recognition Systems.](http://arxiv.org/abs/2010.11782)

Eric Balkanski, Harrison Chase, Kojin Oshiba, Alexander Rilee, Yaron Singer, Richard Wang


[Rewriting Meaningful Sentences via Conditional BERT Sampling and an application on fooling text classifiers.](http://arxiv.org/abs/2010.11869)

Lei Xu, Ivan Ramirez, Kalyan Veeramachaneni


[An Efficient Adversarial Attack for Tree Ensembles.](http://arxiv.org/abs/2010.11598)

Chong Zhang, Huan Zhang, Cho-Jui Hsieh


[Adversarial Robustness of Supervised Sparse Coding.](http://arxiv.org/abs/2010.12088)

Jeremias Sulam, Ramchandran Muthukumar, Raman Arora


[Enabling certification of verification-agnostic networks via memory-efficient semidefinite programming.](http://arxiv.org/abs/2010.11645)

Sumanth Dathathri, Krishnamurthy Dvijotham, Alexey Kurakin, Aditi Raghunathan, Jonathan Uesato, Rudy Bunel, Shreya Shankar, Jacob Steinhardt, Ian Goodfellow, Percy Liang, Pushmeet Kohli


[Defense-guided Transferable Adversarial Attacks.](http://arxiv.org/abs/2010.11535)

Zifei Zhang, Kai Qiao, Jian Chen, Ningning Liang


[Once-for-All Adversarial Training: In-Situ Tradeoff between Robustness and Accuracy for Free.](http://arxiv.org/abs/2010.11828)

Haotao Wang, Tianlong Chen, Shupeng Gui, Ting-Kuei Hu, Ji Liu, Zhangyang Wang


## 2020-10-21

[Adversarial Attacks on Deep Algorithmic Trading Policies.](http://arxiv.org/abs/2010.11388)

Yaser Faghan, Nancirose Piazza, Vahid Behzadan, Ali Fathi


[Maximum Mean Discrepancy is Aware of Adversarial Attacks.](http://arxiv.org/abs/2010.11415)

Ruize Gao, Feng Liu, Jingfeng Zhang, Bo Han, Tongliang Liu, Gang Niu, Masashi Sugiyama


[Precise Statistical Analysis of Classification Accuracies for Adversarial Training.](http://arxiv.org/abs/2010.11213)

Adel Javanmard, Mahdi Soltanolkotabi


[Learning Black-Box Attackers with Transferable Priors and Query Feedback.](http://arxiv.org/abs/2010.11742)

Jiancheng Yang, Yangzhou Jiang, Xiaoyang Huang, Bingbing Ni, Chenglong Zhao


[Class-Conditional Defense GAN Against End-to-End Speech Attacks.](http://arxiv.org/abs/2010.11352)

Mohammad Esmaeilpour, Patrick Cardinal, Alessandro Lameiras Koerich


[A Distributional Robustness Certificate by Randomized Smoothing.](http://arxiv.org/abs/2010.10987)

Jungang Yang, Liyao Xiang, Ruidong Chen, Yukun Wang, Wei Wang, Xinbing Wang


## 2020-10-20

[Preventing Personal Data Theft in Images with Adversarial ML.](http://arxiv.org/abs/2010.10242)

Thomas Cilloni, Wei Wang, Charles Walter, Charles Fleming


[Towards Understanding the Dynamics of the First-Order Adversaries.](http://arxiv.org/abs/2010.10650)

Zhun Deng, Hangfeng He, Jiaoyang Huang, Weijie J. Su


[Robust Neural Networks inspired by Strong Stability Preserving Runge-Kutta methods.](http://arxiv.org/abs/2010.10047)

Byungjoo Kim, Bryce Chudomelka, Jinyoung Park, Jaewoo Kang, Youngjoon Hong, Hyunwoo J. Kim


[Boosting Gradient for White-Box Adversarial Attacks.](http://arxiv.org/abs/2010.10712)

Hongying Liu, Zhenyu Zhou, Fanhua Shang, Xiaoyu Qi, Yuanyuan Liu, Licheng Jiao


[Tight Second-Order Certificates for Randomized Smoothing.](http://arxiv.org/abs/2010.10549)

Alexander Levine, Aounon Kumar, Thomas Goldstein, Soheil Feizi


## 2020-10-19

[A Survey of Machine Learning Techniques in Adversarial Image Forensics.](http://arxiv.org/abs/2010.09680)

Ehsan Nowroozi, Ali Dehghantanha, Reza M. Parizi, Kim-Kwang Raymond Choo


[Against All Odds: Winning the Defense Challenge in an Evasion Competition with Diversification.](http://arxiv.org/abs/2010.09569)

Erwin Quiring, Lukas Pirch, Michael Reimsbach, Daniel Arp, Konrad Rieck


[RobustBench: a standardized adversarial robustness benchmark.](http://arxiv.org/abs/2010.09670)

Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Nicolas Flammarion, Mung Chiang, Prateek Mittal, Matthias Hein


[Optimism in the Face of Adversity: Understanding and Improving Deep Learning through Adversarial Robustness.](http://arxiv.org/abs/2010.09624)

Guillermo Ortiz-Jimenez, Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard


[Verifying the Causes of Adversarial Examples.](http://arxiv.org/abs/2010.09633)

Honglin Li, Yifei Fan, Frieder Ganz, Anthony Yezzi, Payam Barnaghi


[When Bots Take Over the Stock Market: Evasion Attacks Against Algorithmic Traders.](http://arxiv.org/abs/2010.09246)

Elior Nehemya, Yael Mathov, Asaf Shabtai, Yuval Elovici


[FLAG: Adversarial Data Augmentation for Graph Neural Networks.](http://arxiv.org/abs/2010.09891)

Kezhi Kong, Guohao Li, Mucong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem, Gavin Taylor, Tom Goldstein


## 2020-10-18

[FADER: Fast Adversarial Example Rejection.](http://arxiv.org/abs/2010.09119)

Francesco Crecchi, Marco Melis, Angelo Sotgiu, Davide Bacciu, Battista Biggio


[Poisoned classifiers are not only backdoored, they are fundamentally broken.](http://arxiv.org/abs/2010.09080)

Mingjie Sun, Siddhant Agarwal, J. Zico Kolter


## 2020-10-17

[A Generative Model based Adversarial Security of Deep Learning and Linear Classifier Models.](http://arxiv.org/abs/2010.08546)

erhat Ozgur Catak, Samed Sivaslioglu, Kevser Sahinbas


[Finding Physical Adversarial Examples for Autonomous Driving with Fast and Differentiable Image Compositing.](http://arxiv.org/abs/2010.08844)

Jinghan Yang, Adith Boloor, Ayan Chakrabarti, Xuan Zhang, Yevgeniy Vorobeychik


[Weight-Covariance Alignment for Adversarially Robust Neural Networks.](http://arxiv.org/abs/2010.08852)

Panagiotis Eustratiadis, Henry Gouk, Da Li, Timothy Hospedales


## 2020-10-16

[DPAttack: Diffused Patch Attacks against Universal Object Detection.](http://arxiv.org/abs/2010.11679)

Shudeng Wu, Tao Dai, Shu-Tao Xia


[Mischief: A Simple Black-Box Attack Against Transformer Architectures.](http://arxiv.org/abs/2010.08542)

Wynter Adrian de


[Learning Robust Algorithms for Online Allocation Problems Using Adversarial Training.](http://arxiv.org/abs/2010.08418)

Goran Zuzic, Di Wang, Aranyak Mehta, D. Sivakumar


## 2020-10-15

[Adversarial Images through Stega Glasses.](http://arxiv.org/abs/2010.07542)

Benoît CRIStAL Bonnet, Teddy CRIStAL Furon, Patrick CRIStAL Bas


[A Hamiltonian Monte Carlo Method for Probabilistic Adversarial Attack and Learning.](http://arxiv.org/abs/2010.07849)

Hongjun Wang, Guanbin Li, Xiaobai Liu, Liang Lin


[Generalizing Universal Adversarial Attacks Beyond Additive Perturbations.](http://arxiv.org/abs/2010.07788)

Yanghao Zhang, Wenjie Ruan, Fu Wang, Xiaowei Huang


[Certifying Neural Network Robustness to Random Input Noise from Samples.](http://arxiv.org/abs/2010.07532)

Brendon G. Anderson, Somayeh Sojoudi


[Overfitting or Underfitting? Understand Robustness Drop in Adversarial Training.](http://arxiv.org/abs/2010.08034)

Zichao Li, Liyuan Liu, Chengyu Dong, Jingbo Shang


[Maximum-Entropy Adversarial Data Augmentation for Improved Generalization and Robustness.](http://arxiv.org/abs/2010.08001)

Long Zhao, Ting Liu, Xi Peng, Dimitris Metaxas


[Exploiting Vulnerabilities of Deep Learning-based Energy Theft Detection in AMI through Adversarial Attacks.](http://arxiv.org/abs/2010.09212)

Jiangnan Li, Yingyuan Yang, Jinyuan Stella Sun


[Progressive Defense Against Adversarial Attacks for Deep Learning as a Service in Internet of Things.](http://arxiv.org/abs/2010.11143)

Ling Wang, Cheng Zhang, Zejian Luo, Chenguang Liu, Jie Liu, Xi Zheng, Athanasios Vasilakos


## 2020-10-14

[Pair the Dots: Jointly Examining Training History and Test Stimuli for Model Interpretability.](http://arxiv.org/abs/2010.06943)

Yuxian Meng, Chun Fan, Zijun Sun, Eduard Hovy, Fei Wu, Jiwei Li


[Towards Resistant Audio Adversarial Examples.](http://arxiv.org/abs/2010.07190)

Tom Dörr, Karla Markert, Nicolas M. Müller, Konstantin Böttinger


[An Adversarial Attack against Stacked Capsule Autoencoder.](http://arxiv.org/abs/2010.07230)

Jiazhu Dai, Siwei Xiong


[Explain2Attack: Text Adversarial Attacks via Cross-Domain Interpretability.](http://arxiv.org/abs/2010.06812)

Mahmoud Hossam, Trung Le, He Zhao, Dinh Phung


[GreedyFool: Multi-Factor Imperceptibility and Its Application to Designing Black-box Adversarial Example Attack.](http://arxiv.org/abs/2010.06855)

Hui Liu, Bo Zhao, Jiabao Guo, Yang An, Peng Liu


## 2020-10-13

[Toward Few-step Adversarial Training from a Frequency Perspective.](http://arxiv.org/abs/2010.06545)

Hans Shih-Han Wang, Cory Cornelius, Brandon Edwards, Jason Martin


[Higher-Order Certification for Randomized Smoothing.](http://arxiv.org/abs/2010.06651)

Jeet Mohapatra, Ching-Yun Ko, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, Luca Daniel


[Linking average- and worst-case perturbation robustness via class selectivity and dimensionality.](http://arxiv.org/abs/2010.07693)

Matthew L. Leavitt, Ari Morcos


## 2020-10-12

[Universal Model for 3D Medical Image Analysis.](http://arxiv.org/abs/2010.06107)

Xiaoman Zhang, Ya Zhang, Xiaoyun Zhang, Yanfeng Wang


[To be Robust or to be Fair: Towards Fairness in Adversarial Training.](http://arxiv.org/abs/2010.06121)

Han Xu, Xiaorui Liu, Yaxin Li, Jiliang Tang


[Learning to Attack with Fewer Pixels: A Probabilistic Post-hoc Framework for Refining Arbitrary Dense Adversarial Attacks.](http://arxiv.org/abs/2010.06131)

He Zhao, Thanh Nguyen, Trung Le, Paul Montague, Vel Olivier De, Tamas Abraham, Dinh Phung


[Shape-Texture Debiased Neural Network Training.](http://arxiv.org/abs/2010.05981)

Yingwei Li, Qihang Yu, Mingxing Tan, Jieru Mei, Peng Tang, Wei Shen, Alan Yuille, Cihang Xie


[On the Power of Abstention and Data-Driven Decision Making for Adversarial Robustness.](http://arxiv.org/abs/2010.06154)

Maria-Florina Balcan, Avrim Blum, Dravyansh Sharma, Hongyang Zhang


[From Hero to Z\'eroe: A Benchmark of Low-Level Adversarial Attacks.](http://arxiv.org/abs/2010.05648)

Steffen Eger, Yannik Benz


[EFSG: Evolutionary Fooling Sentences Generator.](http://arxiv.org/abs/2010.05736)

Giovanni Marco Di, Marco Brambilla


[Contrast and Classify: Training Robust VQA Models. (2%)](http://arxiv.org/abs/2010.06087)

Yash Kant, Abhinav Moudgil, Dhruv Batra, Devi Parikh, Harsh Agrawal


## 2020-10-11

[Gradient-based Analysis of NLP Models is Manipulable.](http://arxiv.org/abs/2010.05419)

Junlin Wang, Jens Tuyls, Eric Wallace, Sameer Singh


[IF-Defense: 3D Adversarial Point Cloud Defense via Implicit Function based Restoration.](http://arxiv.org/abs/2010.05272)

Ziyi Wu, Yueqi Duan, He Wang, Qingnan Fan, Leonidas J. Guibas


## 2020-10-10

[Is It Time to Redefine the Classification Task for Deep Neural Networks?](http://arxiv.org/abs/2010.05125)

Keji Han, Yun Li


[Regularizing Neural Networks via Adversarial Model Perturbation. (1%)](http://arxiv.org/abs/2010.04925)

Yaowei Zheng, Richong Zhang, Yongyi Mao


## 2020-10-09

[Understanding Spatial Robustness of Deep Neural Networks.](http://arxiv.org/abs/2010.04821)

Ziyuan Zhong, Yuchi Tian, Baishakhi Ray


[How Does Mixup Help With Robustness and Generalization?](http://arxiv.org/abs/2010.04819)

Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, James Zou


## 2020-10-08

[Transcending Transcend: Revisiting Malware Classification with Conformal Evaluation.](http://arxiv.org/abs/2010.03856)

Federico Barbero, Feargus Pendlebury, Fabio Pierazzi, Lorenzo Cavallaro


[Improve Adversarial Robustness via Weight Penalization on Classification Layer.](http://arxiv.org/abs/2010.03844)

Cong Xu, Dan Li, Min Yang


[A Unified Approach to Interpreting and Boosting Adversarial Transferability.](http://arxiv.org/abs/2010.04055)

Xin Wang, Jie Ren, Shuyun Lin, Xiangming Zhu, Yisen Wang, Quanshi Zhang


[Improved Techniques for Model Inversion Attacks.](http://arxiv.org/abs/2010.04092)

Si Chen, Ruoxi Jia, Guo-Jun Qi


[Affine-Invariant Robust Training.](http://arxiv.org/abs/2010.04216)

Oriol Barbany Mayor


[Targeted Attention Attack on Deep Learning Models in Road Sign Recognition.](http://arxiv.org/abs/2010.04331)

Xinghao Yang, Weifeng Liu, Shengli Zhang, Wei Liu, Dacheng Tao


[Gaussian MRF Covariance Modeling for Efficient Black-Box Adversarial Attacks.](http://arxiv.org/abs/2010.04205)

Anit Kumar Sahu, Satya Narayan Shukla, J. Zico Kolter


## 2020-10-07

[Hiding the Access Pattern is Not Enough: Exploiting Search Pattern Leakage in Searchable Encryption.](http://arxiv.org/abs/2010.03465)

Simon Oya, Florian Kerschbaum


[Learning Clusterable Visual Features for Zero-Shot Recognition.](http://arxiv.org/abs/2010.03245)

Jingyi Xu, Zhixin Shu, Dimitris Samaras


[Don't Trigger Me! A Triggerless Backdoor Attack Against Deep Neural Networks.](http://arxiv.org/abs/2010.03282)

Ahmed Salem, Michael Backes, Yang Zhang


[Revisiting Batch Normalization for Improving Corruption Robustness.](http://arxiv.org/abs/2010.03630)

Philipp Benz, Chaoning Zhang, Adil Karjauv, In So Kweon


[Batch Normalization Increases Adversarial Vulnerability: Disentangling Usefulness and Robustness of Model Features.](http://arxiv.org/abs/2010.03316)

Philipp Benz, Chaoning Zhang, In So Kweon


[Decamouflage: A Framework to Detect Image-Scaling Attacks on Convolutional Neural Networks.](http://arxiv.org/abs/2010.03735)

Bedeuro Kim, Alsharif Abuadbba, Yansong Gao, Yifeng Zheng, Muhammad Ejaz Ahmed, Hyoungshick Kim, Surya Nepal


[Global Optimization of Objective Functions Represented by ReLU Networks.](http://arxiv.org/abs/2010.03258)

Christopher A. Strong, Haoze Wu, Aleksandar Zeljić, Kyle D. Julian, Guy Katz, Clark Barrett, Mykel J. Kochenderfer


[CD-UAP: Class Discriminative Universal Adversarial Perturbation.](http://arxiv.org/abs/2010.03300)

Chaoning Zhang, Philipp Benz, Tooba Imtiaz, In So Kweon


[Not All Datasets Are Born Equal: On Heterogeneous Data and Adversarial Examples.](http://arxiv.org/abs/2010.03180)

Eden Levy, Yael Mathov, Ziv Katzir, Asaf Shabtai, Yuval Elovici


[Double Targeted Universal Adversarial Perturbations.](http://arxiv.org/abs/2010.03288)

Philipp Benz, Chaoning Zhang, Tooba Imtiaz, In So Kweon


[Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples.](http://arxiv.org/abs/2010.03593)

Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, Pushmeet Kohli


[Adversarial Attacks to Machine Learning-Based Smart Healthcare Systems.](http://arxiv.org/abs/2010.03671)

AKM Iqtidar Newaz, Nur Imtiazul Haque, Amit Kumar Sikder, Mohammad Ashiqur Rahman, A. Selcuk Uluagac


[Adversarial attacks on audio source separation.](http://arxiv.org/abs/2010.03164)

Naoya Takahashi, Shota Inoue, Yuki Mitsufuji


## 2020-10-06

[Visualizing Color-wise Saliency of Black-Box Image Classification Models.](http://arxiv.org/abs/2010.02468)

Yuhki SenseTime Japan Hatakeyama, Hiroki SenseTime Japan Sakuma, Yoshinori SenseTime Japan Konishi, Kohei Kyoto University Suenaga


[Constraining Logits by Bounded Function for Adversarial Robustness.](http://arxiv.org/abs/2010.02558)

Sekitoshi Kanai, Masanori Yamada, Shin'ya Yamaguchi, Hiroshi Takahashi, Yasutoshi Ida


[Adversarial Patch Attacks on Monocular Depth Estimation Networks.](http://arxiv.org/abs/2010.03072)

Koichiro Yamanaka, Ryutaroh Matsumoto, Keita Takahashi, Toshiaki Fujii


[BAAAN: Backdoor Attacks Against Autoencoder and GAN-Based Machine Learning Models.](http://arxiv.org/abs/2010.03007)

Ahmed Salem, Yannick Sautter, Michael Backes, Mathias Humbert, Yang Zhang


## 2020-10-05

[Detecting Misclassification Errors in Neural Networks with a Gaussian Process Model.](http://arxiv.org/abs/2010.02065)

Xin Qiu, Risto Miikkulainen


[Adversarial Boot Camp: label free certified robustness in one epoch.](http://arxiv.org/abs/2010.02508)

Ryan Campbell, Chris Finlay, Adam M Oberman


[Understanding Classifier Mistakes with Generative Models.](http://arxiv.org/abs/2010.02364)

Laëtitia Shao, Yang Song, Stefano Ermon


[CAT-Gen: Improving Robustness in NLP Models via Controlled Adversarial Text Generation.](http://arxiv.org/abs/2010.02338)

Tianlu Wang, Xuezhi Wang, Yao Qin, Ben Packer, Kang Li, Jilin Chen, Alex Beutel, Ed Chi


[Second-Order NLP Adversarial Examples.](http://arxiv.org/abs/2010.01770)

John X. Morris


[A Panda? No, It's a Sloth: Slowdown Attacks on Adaptive Multi-Exit Neural Network Inference.](http://arxiv.org/abs/2010.02432)

Sanghyun Hong, Yiğitcan Kaya, Ionuţ-Vlad Modoranu, Tudor Dumitraş


[InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective.](http://arxiv.org/abs/2010.02329)

Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li, Jingjing Liu


[Understanding Catastrophic Overfitting in Single-step Adversarial Training.](http://arxiv.org/abs/2010.01799)

Hoki Kim, Woojin Lee, Jaewook Lee


[Downscaling Attack and Defense: Turning What You See Back Into What You Get.](http://arxiv.org/abs/2010.02456)

Andrew J. Lohn


[Metadata-Based Detection of Child Sexual Abuse Material. (1%)](http://arxiv.org/abs/2010.02387)

Mayana Pereira, Rahul Dodhia, Hyrum Anderson, Richard Brown


## 2020-10-04

[TextAttack: Lessons learned in designing Python frameworks for NLP.](http://arxiv.org/abs/2010.01724)

John X. Morris, Jin Yong Yoo, Yanjun Qi


[A Study for Universal Adversarial Attacks on Texture Recognition.](http://arxiv.org/abs/2010.01506)

Yingpeng Deng, Lina J. Karam


[Adversarial Attack and Defense of Structured Prediction Models.](http://arxiv.org/abs/2010.01610)

Wenjuan Han, Liwen Zhang, Yong Jiang, Kewei Tu


[Geometry-aware Instance-reweighted Adversarial Training.](http://arxiv.org/abs/2010.01736)

Jingfeng Zhang, Jianing Zhu, Gang Niu, Bo Han, Masashi Sugiyama, Mohan Kankanhalli


[Unknown Presentation Attack Detection against Rational Attackers.](http://arxiv.org/abs/2010.01592)

Ali Khodabakhsh, Zahid Akhtar


## 2020-10-03

[Adversarial and Natural Perturbations for General Robustness.](http://arxiv.org/abs/2010.01401)

Sadaf Gulshad, Jan Hendrik Metzen, Arnold Smeulders


[Multi-Step Adversarial Perturbations on Recommender Systems Embeddings.](http://arxiv.org/abs/2010.01329)

Vito Walter Anelli, Alejandro Bellogín, Yashar Deldjoo, Noia Tommaso Di, Felice Antonio Merra


[A Geometry-Inspired Attack for Generating Natural Language Adversarial Examples.](http://arxiv.org/abs/2010.01345)

Zhao Meng, Roger Wattenhofer


[Efficient Robust Training via Backward Smoothing.](http://arxiv.org/abs/2010.01278)

Jinghui Chen, Yu Cheng, Zhe Gan, Quanquan Gu, Jingjing Liu


[Do Wider Neural Networks Really Help Adversarial Robustness?](http://arxiv.org/abs/2010.01279)

Boxi Wu, Jinghui Chen, Deng Cai, Xiaofei He, Quanquan Gu


## 2020-10-02

[Note: An alternative proof of the vulnerability of $k$-NN classifiers in high intrinsic dimensionality regions.](http://arxiv.org/abs/2010.00990)

Teddy Furon


[An Empirical Study of DNNs Robustification Inefficacy in Protecting Visual Recommenders.](http://arxiv.org/abs/2010.00984)

Vito Walter Anelli, Noia Tommaso Di, Daniele Malitesta, Felice Antonio Merra


[Block-wise Image Transformation with Secret Key for Adversarially Robust Defense.](http://arxiv.org/abs/2010.00801)

MaungMaung AprilPyone, Hitoshi Kiya


[Query complexity of adversarial attacks.](http://arxiv.org/abs/2010.01039)

Grzegorz Głuch, Rüdiger Urbanke


[CorrAttack: Black-box Adversarial Attack with Structured Search.](http://arxiv.org/abs/2010.01250)

Zhichao Huang, Yaowei Huang, Tong Zhang


[A Deep Genetic Programming based Methodology for Art Media Classification Robust to Adversarial Perturbations.](http://arxiv.org/abs/2010.01238)

Gustavo Olague, Gerardo Ibarra-Vazquez, Mariana Chan-Ley, Cesar Puente, Carlos Soubervielle-Montalvo, Axel Martinez


[Data-Driven Certification of Neural Networks with Random Input Noise. (16%)](http://arxiv.org/abs/2010.01171)

Brendon G. Anderson, Somayeh Sojoudi


## 2020-10-01

[Assessing Robustness of Text Classification through Maximal Safe Radius Computation.](http://arxiv.org/abs/2010.02004)

Malfa Emanuele La, Min Wu, Luca Laurenti, Benjie Wang, Anthony Hartshorn, Marta Kwiatkowska


[Bag of Tricks for Adversarial Training.](http://arxiv.org/abs/2010.00467)

Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, Jun Zhu


## 2020-09-30

[Erratum Concerning the Obfuscated Gradients Attack on Stochastic Activation Pruning.](http://arxiv.org/abs/2010.00071)

Guneet S. Dhillon, Nicholas Carlini


[Accurate and Robust Feature Importance Estimation under Distribution Shifts.](http://arxiv.org/abs/2009.14454)

Jayaraman J. Thiagarajan, Vivek Narayanaswamy, Rushil Anirudh, Peer-Timo Bremer, Andreas Spanias


[Uncertainty-Matching Graph Neural Networks to Defend Against Poisoning Attacks.](http://arxiv.org/abs/2009.14455)

Uday Shankar Shanthamallu, Jayaraman J. Thiagarajan, Andreas Spanias


[DVERGE: Diversifying Vulnerabilities for Enhanced Robust Generation of Ensembles.](http://arxiv.org/abs/2009.14720)

Huanrui Yang, Jingyang Zhang, Hongliang Dong, Nathan Inkawhich, Andrew Gardner, Andrew Touchet, Wesley Wilkes, Heath Berry, Hai Li


## 2020-09-29

[Neural Topic Modeling with Cycle-Consistent Adversarial Training.](http://arxiv.org/abs/2009.13971)

Xuemeng Hu, Rui Wang, Deyu Zhou, Yuxuan Xiong


[Fast Fr\'echet Inception Distance.](http://arxiv.org/abs/2009.14075)

Alexander Mathiasen, Frederik Hvilshøj


## 2020-09-28

[Adversarial Attacks Against Deep Learning Systems for ICD-9 Code Assignment.](http://arxiv.org/abs/2009.13720)

Sharan Raja, Rudraksh Tuwani


[STRATA: Building Robustness with a Simple Method for Generating Black-box Adversarial Attacks for Models of Code.](http://arxiv.org/abs/2009.13562)

Jacob M. Springer, Bryn Marie Reinstadler, Una-May O'Reilly


[Graph Adversarial Networks: Protecting Information against Adversarial Attacks.](http://arxiv.org/abs/2009.13504)

Peiyuan Liao, Han Zhao, Keyulu Xu, Tommi Jaakkola, Geoffrey Gordon, Stefanie Jegelka, Ruslan Salakhutdinov


[Adversarial Robustness of Stabilized NeuralODEs Might be from Obfuscated Gradients.](http://arxiv.org/abs/2009.13145)

Yifei Huang, Yaodong Yu, Hongyang Zhang, Yi Ma, Yuan Yao


[Generating End-to-End Adversarial Examples for Malware Classifiers Using Explainability.](http://arxiv.org/abs/2009.13243)

Ishai Rosenberg, Shai Meir, Jonathan Berrebi, Ilay Gordon, Guillaume Sicard, Eli David


[Learning to Generate Image Source-Agnostic Universal Adversarial Perturbations. (92%)](http://arxiv.org/abs/2009.13714)

Pu Zhao, Parikshit Ram, Songtao Lu, Yuguang Yao, Djallel Bouneffouf, Xue Lin, Sijia Liu


## 2020-09-27

[Learning to Improve Image Compression without Changing the Standard Decoder.](http://arxiv.org/abs/2009.12927)

Yannick Strümpler, Ren Yang, Radu Timofte


[RoGAT: a robust GNN combined revised GAT with adjusted graphs.](http://arxiv.org/abs/2009.13038)

Xianchen Zhou, Yaoyun Zeng, Hongxia Wang


[Where Does the Robustness Come from? A Study of the Transformation-based Ensemble Defence.](http://arxiv.org/abs/2009.13033)

Chang Liao, Yao Cheng, Chengfang Fang, Jie Shi


## 2020-09-26

[Differentially Private Adversarial Robustness Through Randomized Perturbations.](http://arxiv.org/abs/2009.12718)

Nan Xu, Oluwaseyi Feyisetan, Abhinav Aggarwal, Zekun Xu, Nathanael Teissier


[Beneficial Perturbations Network for Defending Adversarial Examples.](http://arxiv.org/abs/2009.12724)

Shixian Wen, Amanda Rios, Laurent Itti


## 2020-09-25

[Training CNNs in Presence of JPEG Compression: Multimedia Forensics vs Computer Vision.](http://arxiv.org/abs/2009.12088)

Sara Mandelli, Nicolò Bonettini, Paolo Bestagini, Stefano Tubaro


[Attention Meets Perturbations: Robust and Interpretable Attention with Adversarial Training.](http://arxiv.org/abs/2009.12064)

Shunsuke Kitada, Hitoshi Iyatomi


## 2020-09-24

[Advancing the Research and Development of Assured Artificial Intelligence and Machine Learning Capabilities.](http://arxiv.org/abs/2009.13250)

Tyler J. Shipp, Daniel J. Clouse, Lucia Michael J. De, Metin B. Ahiskali, Kai Steverson, Jonathan M. Mullin, Nathaniel D. Bastian


[Adversarial Examples in Deep Learning for Multivariate Time Series Regression.](http://arxiv.org/abs/2009.11911)

Gautam Raj Mode, Khaza Anuarul Hoque


[Improving Query Efficiency of Black-box Adversarial Attack.](http://arxiv.org/abs/2009.11508)

Yang Bai, Yuyuan Zeng, Yong Jiang, Yisen Wang, Shu-Tao Xia, Weiwei Guo


## 2020-09-23

[Enhancing Mixup-based Semi-Supervised Learning with Explicit Lipschitz Regularization.](http://arxiv.org/abs/2009.11416)

Prashnna Kumar Gyawali, Sandesh Ghimire, Linwei Wang


[Improving Dialog Evaluation with a Multi-reference Adversarial Dataset and Large Scale Pretraining.](http://arxiv.org/abs/2009.11321)

Ananya B. Sai, Akash Kumar Mohankumar, Siddhartha Arora, Mitesh M. Khapra


[Adversarial robustness via stochastic regularization of neural activation sensitivity.](http://arxiv.org/abs/2009.11349)

Gil Fidel, Ron Bitton, Ziv Katzir, Asaf Shabtai


[A Partial Break of the Honeypots Defense to Catch Adversarial Attacks.](http://arxiv.org/abs/2009.10975)

Nicholas Carlini


[Semantics-Preserving Adversarial Training.](http://arxiv.org/abs/2009.10978)

Wonseok Lee, Hanbit Lee, Sang-goo Lee


[Robustification of Segmentation Models Against Adversarial Perturbations In Medical Imaging.](http://arxiv.org/abs/2009.11090)

Hanwool Park, Amirhossein Bayat, Mohammad Sabokrou, Jan S. Kirschke, Bjoern H. Menze


[Detection of Iterative Adversarial Attacks via Counter Attack.](http://arxiv.org/abs/2009.11397)

Matthias Rottmann, Kira Maag, Mathis Peyron, Natasa Krejic, Hanno Gottschalk


[Torchattacks: A PyTorch Repository for Adversarial Attacks.](http://arxiv.org/abs/2010.01950)

Hoki Kim


## 2020-09-22

[What Do You See? Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors.](http://arxiv.org/abs/2009.10639)

Yi-Shan Lin, Wen-Chuan Lee, Z. Berkay Celik


[Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time.](http://arxiv.org/abs/2009.10623)

Ferran Alet, Kenji Kawaguchi, Tomas Lozano-Perez, Leslie Pack Kaelbling


[Adversarial Attack Based Countermeasures against Deep Learning Side-Channel Attacks.](http://arxiv.org/abs/2009.10568)

Ruizhe Gu, Ping Wang, Mengce Zheng, Honggang Hu, Nenghai Yu


## 2020-09-21

[Uncertainty-aware Attention Graph Neural Network for Defending Adversarial Attacks.](http://arxiv.org/abs/2009.10235)

Boyuan Feng, Yuke Wang, Zheng Wang, Yufei Ding


[Scalable Adversarial Attack on Graph Neural Networks with Alternating Direction Method of Multipliers.](http://arxiv.org/abs/2009.10233)

Boyuan Feng, Yuke Wang, Xu Li, Yufei Ding


[Generating Adversarial yet Inconspicuous Patches with a Single Image.](http://arxiv.org/abs/2009.09774)

Jinqi Luo, Tao Bai, Jun Zhao, Bo Li


[Adversarial Training with Stochastic Weight Average.](http://arxiv.org/abs/2009.10526)

Joong-Won Hwang, Youngwan Lee, Sungchan Oh, Yuseok Bae


[Improving Ensemble Robustness by Collaboratively Promoting and Demoting Adversarial Robustness.](http://arxiv.org/abs/2009.09612)

Anh Bui, Trung Le, He Zhao, Paul Montague, Olivier deVel, Tamas Abraham, Dinh Phung


[DeepDyve: Dynamic Verification for Deep Neural Networks.](http://arxiv.org/abs/2009.09663)

Yu Li, Min Li, Bo Luo, Ye Tian, Qiang Xu


[Feature Distillation With Guided Adversarial Contrastive Learning.](http://arxiv.org/abs/2009.09922)

Tao Bai, Jinnan Chen, Jun Zhao, Bihan Wen, Xudong Jiang, Alex Kot


[Crafting Adversarial Examples for Deep Learning Based Prognostics (Extended Version).](http://arxiv.org/abs/2009.10149)

Gautam Raj Mode, Khaza Anuarul Hoque


[Stereopagnosia: Fooling Stereo Networks with Adversarial Perturbations.](http://arxiv.org/abs/2009.10142)

Alex Wong, Mukund Mundhra, Stefano Soatto


[Optimal Provable Robustness of Quantum Classification via Quantum Hypothesis Testing.](http://arxiv.org/abs/2009.10064)

Maurice Weber, Nana Liu, Bo Li, Ce Zhang, Zhikuan Zhao


[Password Strength Signaling: A Counter-Intuitive Defense Against Password Cracking. (1%)](http://arxiv.org/abs/2009.10060)

Wenjie Bai, Jeremiah Blocki, Ben Harsha


## 2020-09-20

[Improving Robustness and Generality of NLP Models Using Disentangled Representations.](http://arxiv.org/abs/2009.09587)

Jiawei Wu, Xiaoya Li, Xiang Ao, Yuxian Meng, Fei Wu, Jiwei Li


## 2020-09-19

[Efficient Certification of Spatial Robustness.](http://arxiv.org/abs/2009.09318)

Anian Ruoss, Maximilian Baader, Mislav Balunović, Martin Vechev


[OpenAttack: An Open-source Textual Adversarial Attack Toolkit.](http://arxiv.org/abs/2009.09191)

Guoyang Zeng, Fanchao Qi, Qianrui Zhou, Tingji Zhang, Bairu Hou, Yuan Zang, Zhiyuan Liu, Maosong Sun


[Making Images Undiscoverable from Co-Saliency Detection.](http://arxiv.org/abs/2009.09258)

Ruijun Gao, Qing Guo, Felix Juefei-Xu, Hongkai Yu, Xuhong Ren, Wei Feng, Song Wang


[Adversarial Exposure Attack on Diabetic Retinopathy Imagery.](http://arxiv.org/abs/2009.09231)

Yupeng Cheng, Felix Juefei-Xu, Qing Guo, Huazhu Fu, Xiaofei Xie, Shang-Wei Lin, Weisi Lin, Yang Liu


[Bias Field Poses a Threat to DNN-based X-Ray Recognition.](http://arxiv.org/abs/2009.09247)

Binyu Tian, Qing Guo, Felix Juefei-Xu, Wen Le Chan, Yupeng Cheng, Xiaohong Li, Xiaofei Xie, Shengchao Qin


[Learning to Attack: Towards Textual Adversarial Attacking in Real-world Situations.](http://arxiv.org/abs/2009.09192)

Yuan Zang, Bairu Hou, Fanchao Qi, Zhiyuan Liu, Xiaojun Meng, Maosong Sun


[Adversarial Rain Attack and Defensive Deraining for DNN Perception.](http://arxiv.org/abs/2009.09205)

Liming Zhai, Felix Juefei-Xu, Qing Guo, Xiaofei Xie, Lei Ma, Wei Feng, Shengchao Qin, Yang Liu


[EI-MTD:Moving Target Defense for Edge Intelligence against Adversarial Attacks.](http://arxiv.org/abs/2009.10537)

Yaguan Qian, Qiqi Shao, Jiamin Wang, Xiang Lin, Yankai Guo, Zhaoquan Gu, Bin Wang, Chunming Wu


## 2020-09-18

[Robust Decentralized Learning for Neural Networks.](http://arxiv.org/abs/2009.09026)

Yao Zhou, Jun Wu, Jingrui He


[MIRAGE: Mitigating Conflict-Based Cache Attacks with a Practical Fully-Associative Design. (1%)](http://arxiv.org/abs/2009.09090)

Gururaj Saileshwar, Moinuddin Qureshi


## 2020-09-17

[Certifying Confidence via Randomized Smoothing.](http://arxiv.org/abs/2009.08061)

Aounon Kumar, Alexander Levine, Soheil Feizi, Tom Goldstein


[Generating Label Cohesive and Well-Formed Adversarial Claims.](http://arxiv.org/abs/2009.08205)

Pepa Atanasova, Dustin Wright, Isabelle Augenstein


[Vax-a-Net: Training-time Defence Against Adversarial Patch Attacks.](http://arxiv.org/abs/2009.08194)

T. Gittings, S. Schneider, J. Collomosse


[Label Smoothing and Adversarial Robustness.](http://arxiv.org/abs/2009.08233)

Chaohao Fu, Hongbin Chen, Na Ruan, Weijia Jia


[Online Alternate Generator against Adversarial Attacks.](http://arxiv.org/abs/2009.08110)

Haofeng Li, Yirui Zeng, Guanbin Li, Liang Lin, Yizhou Yu


[MultAV: Multiplicative Adversarial Videos.](http://arxiv.org/abs/2009.08058)

Shao-Yuan Lo, Vishal M. Patel


[On the Transferability of Minimal Prediction Preserving Inputs in Question Answering.](http://arxiv.org/abs/2009.08070)

Shayne Longpre, Yi Lu, Christopher DuBois


[Large Norms of CNN Layers Do Not Hurt Adversarial Robustness.](http://arxiv.org/abs/2009.08435)

Youwei Liang, Dong Huang


## 2020-09-16

[Multimodal Safety-Critical Scenarios Generation for Decision-Making Algorithms Evaluation.](http://arxiv.org/abs/2009.08311)

Wenhao Ding, Baiming Chen, Bo Li, Kim Ji Eun, Ding Zhao


[Analysis of Generalizability of Deep Neural Networks Based on the Complexity of Decision Boundary.](http://arxiv.org/abs/2009.07974)

Shuyue Guan, Murray Loew


[Malicious Network Traffic Detection via Deep Learning: An Information Theoretic View.](http://arxiv.org/abs/2009.07753)

Erick Galinkin


[Contextualized Perturbation for Textual Adversarial Attack.](http://arxiv.org/abs/2009.07502)

Dianqi Li, Yizhe Zhang, Hao Peng, Liqun Chen, Chris Brockett, Ming-Ting Sun, Bill Dolan


## 2020-09-15

[Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup.](http://arxiv.org/abs/2009.06962)

Jang-Hyun Kim, Wonho Choo, Hyun Oh Song


[Light Can Hack Your Face! Black-box Backdoor Attack on Face Recognition Systems.](http://arxiv.org/abs/2009.06996)

Haoliang Nanyang Technological University, Singapore Li, Yufei Nanyang Technological University, Singapore Wang, Xiaofei Nanyang Technological University, Singapore Xie, Yang Nanyang Technological University, Singapore Liu, Shiqi City University of Hong Kong Wang, Renjie Nanyang Technological University, Singapore Wan, Lap-Pui Nanyang Technological University, Singapore Chau, Alex C. Nanyang Technological University, Singapore Kot


[Switching Gradient Directions for Query-Efficient Black-Box Adversarial Attacks.](http://arxiv.org/abs/2009.07191)

Chen Ma, Shuyu Cheng, Li Chen, Junhai Yong


[Decision-based Universal Adversarial Attack.](http://arxiv.org/abs/2009.07024)

Jing Wu, Mingyi Zhou, Shuaicheng Liu, Yipeng Liu, Ce Zhu


## 2020-09-14

[A Game Theoretic Analysis of Additive Adversarial Attacks and Defenses.](http://arxiv.org/abs/2009.06530)

Ambar Pal, René Vidal


[Input Hessian Regularization of Neural Networks.](http://arxiv.org/abs/2009.06571)

Waleed Mustafa, Robert A. Vandermeulen, Marius Kloft


[Robust Deep Learning Ensemble against Deception.](http://arxiv.org/abs/2009.06589)

Wenqi Wei, Ling Liu


[Hold Tight and Never Let Go: Security of Deep Learning based Automated Lane Centering under Physical-World Attack.](http://arxiv.org/abs/2009.06701)

Takami Sato, Junjie Shen, Ningfei Wang, Yunhan Jack Jia, Xue Lin, Qi Alfred Chen


## 2020-09-13

[Manifold attack.](http://arxiv.org/abs/2009.05965)

Khanh-Hung Tran, Fred-Maurice Ngole-Mboula, Jean-Luc Starck


[Towards the Quantification of Safety Risks in Deep Neural Networks.](http://arxiv.org/abs/2009.06114)

Peipei Xu, Wenjie Ruan, Xiaowei Huang


## 2020-09-12

[Certified Robustness of Graph Classification against Topology Attack with Randomized Smoothing.](http://arxiv.org/abs/2009.05872)

Zhidong Gao, Rui Hu, Yanmin Gong


## 2020-09-11

[Defending Against Multiple and Unforeseen Adversarial Videos.](http://arxiv.org/abs/2009.05244)

Shao-Yuan Lo, Vishal M. Patel


[Robust Neural Machine Translation: Modeling Orthographic and Interpunctual Variation.](http://arxiv.org/abs/2009.05460)

Toms Bergmanis, Artūrs Stafanovičs, Mārcis Pinnis


[Achieving Adversarial Robustness via Sparsity.](http://arxiv.org/abs/2009.05423)

Shufan Wang, Ningyi Liao, Liyao Xiang, Nanyang Ye, Quanshi Zhang


[The Intriguing Relation Between Counterfactual Explanations and Adversarial Examples.](http://arxiv.org/abs/2009.05487)

Timo Freiesleben


[Semantic-preserving Reinforcement Learning Attack Against Graph Neural Networks for Malware Detection.](http://arxiv.org/abs/2009.05602)

Lan Zhang, Peng Liu, Yoon-Ho Choi


## 2020-09-10

[Second Order Optimization for Adversarial Robustness and Interpretability.](http://arxiv.org/abs/2009.04923)

Theodoros Tsiligkaridis, Jay Roberts


[Quantifying the Preferential Direction of the Model Gradient in Adversarial Training With Projected Gradient Descent.](http://arxiv.org/abs/2009.04709)

Ricardo Bigolin Lanfredi, Joyce D. Schroeder, Tolga Tasdizen


## 2020-09-09

[End-to-end Kernel Learning via Generative Random Fourier Features.](http://arxiv.org/abs/2009.04614)

Kun Fang, Xiaolin Huang, Fanghui Liu, Jie Yang


[Searching for a Search Method: Benchmarking Search Algorithms for Generating NLP Adversarial Examples.](http://arxiv.org/abs/2009.06368)

Jin Yong Yoo, John X. Morris, Eli Lifland, Yanjun Qi


[A black-box adversarial attack for poisoning clustering.](http://arxiv.org/abs/2009.05474)

Antonio Emanuele Cinà, Alessandro Torcinovich, Marcello Pelillo


[SoK: Certified Robustness for Deep Neural Networks.](http://arxiv.org/abs/2009.04131)

Linyi Li, Tao Xie, Bo Li


## 2020-09-08

[Fuzzy Unique Image Transformation: Defense Against Adversarial Attacks On Deep COVID-19 Models.](http://arxiv.org/abs/2009.04004)

Achyut Mani Tripathi, Ashish Mishra


[Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective.](http://arxiv.org/abs/2009.03728)

Gabriel Resende Machado, Eugênio Silva, Ronaldo Ribeiro Goldschmidt


## 2020-09-07

[Adversarial attacks on deep learning models for fatty liver disease classification by modification of ultrasound image reconstruction method.](http://arxiv.org/abs/2009.03364)

Michal Byra, Grzegorz Styczynski, Cezary Szmigielski, Piotr Kalinowski, Lukasz Michalowski, Rafal Paluszkiewicz, Bogna Ziarkiewicz-Wroblewska, Krzysztof Zieniewicz, Andrzej Nowicki


[Adversarial Attack on Large Scale Graph.](http://arxiv.org/abs/2009.03488)

Jintang Li, Tao Xie, Liang Chen, Fenfang Xie, Xiangnan He, Zibin Zheng


[Black Box to White Box: Discover Model Characteristics Based on Strategic Probing.](http://arxiv.org/abs/2009.03136)

Josh Kalin, Matthew Ciolino, David Noever, Gerry Dozier


## 2020-09-06

[A Game Theoretic Analysis of LQG Control under Adversarial Attack.](http://arxiv.org/abs/2009.02877)

Zuxing Li, György Dán, Dong Liu


[Dynamically Computing Adversarial Perturbations for Recurrent Neural Networks.](http://arxiv.org/abs/2009.02874)

Shankar A. Deka, Dušan M. Stipanović, Claire J. Tomlin


[Detection Defense Against Adversarial Attacks with Saliency Map.](http://arxiv.org/abs/2009.02738)

Dengpan Ye, Chuanxi Chen, Changrui Liu, Hao Wang, Shunzhi Jiang


## 2020-09-05

[Bluff: Interactively Deciphering Adversarial Attacks on Deep Neural Networks.](http://arxiv.org/abs/2009.02608)

Nilaksh Polo Das, Haekyu Polo Park, Zijie J. Polo Wang, Fred Polo Hohman, Robert Polo Firstman, Emily Polo Rogers, Duen Polo Horng, Chau


[Dual Manifold Adversarial Robustness: Defense against Lp and non-Lp Adversarial Attacks.](http://arxiv.org/abs/2009.02470)

Wei-An Lin, Chun Pong Lau, Alexander Levine, Rama Chellappa, Soheil Feizi


## 2020-09-03

[MIPGAN -- Generating Strong and High Quality Morphing Attacks Using Identity Prior Driven GAN. (10%)](http://arxiv.org/abs/2009.01729)

Haoyu Zhang, Sushma Venkatesh, Raghavendra Ramachandra, Kiran Raja, Naser Damer, Christoph Busch


## 2020-09-02

[Yet Meta Learning Can Adapt Fast, It Can Also Break Easily.](http://arxiv.org/abs/2009.01672)

Han Xu, Yaxin Li, Xiaorui Liu, Hui Liu, Jiliang Tang


[Perceptual Deep Neural Networks: Adversarial Robustness through Input Recreation.](http://arxiv.org/abs/2009.01110)

Danilo Vasconcellos Vargas, Bingli Liao, Takahiro Kanzaki


[Open-set Adversarial Defense.](http://arxiv.org/abs/2009.00814)

Rui Shao, Pramuditha Perera, Pong C. Yuen, Vishal M. Patel


[Adversarially Robust Neural Architectures.](http://arxiv.org/abs/2009.00902)

Minjing Dong, Yanxi Li, Yunhe Wang, Chang Xu


[Flow-based detection and proxy-based evasion of encrypted malware C2 traffic.](http://arxiv.org/abs/2009.01122)

Carlos University of Porto and INESC TEC Novo, Ricardo University of Porto and INESC TEC Morla


[Adversarial Attacks on Deep Learning Systems for User Identification based on Motion Sensors.](http://arxiv.org/abs/2009.01109)

Cezara Benegui, Radu Tudor Ionescu


[Simulating Unknown Target Models for Query-Efficient Black-box Attacks.](http://arxiv.org/abs/2009.00960)

Chen Ma, Li Chen, Jun-Hai Yong


## 2020-09-01

[Defending against substitute model black box adversarial attacks with the 01 loss.](http://arxiv.org/abs/2009.09803)

Yunzhe Xue, Meiyan Xie, Usman Roshan


## 2020-08-31

[Adversarial Patch Camouflage against Aerial Detection.](http://arxiv.org/abs/2008.13671)

Ajaya Adhikari, Richard den Hollander, Ioannis Tolios, Bekkum Michael van, Anneloes Bal, Stijn Hendriks, Maarten Kruithof, Dennis Gross, Nils Jansen, Guillermo Pérez, Kit Buurman, Stephan Raaijmakers


[Evasion Attacks to Graph Neural Networks via Influence Function.](http://arxiv.org/abs/2009.00203)

Binghui Wang, Tianxiang Zhou, Minhua Lin, Pan Zhou, Ang Li, Meng Pang, Cai Fu, Hai Li, Yiran Chen


[MALCOM: Generating Malicious Comments to Attack Neural Fake News Detection Models.](http://arxiv.org/abs/2009.01048)

Thai Le, Suhang Wang, Dongwon Lee


## 2020-08-30

[Benchmarking adversarial attacks and defenses for time-series data.](http://arxiv.org/abs/2008.13261)

Shoaib Ahmed Siddiqui, Andreas Dengel, Sheraz Ahmed


[An Integrated Approach to Produce Robust Models with High Efficiency.](http://arxiv.org/abs/2008.13305)

Zhijian Li, Bao Wang, Jack Xin


[Shape Defense Against Adversarial Attacks.](http://arxiv.org/abs/2008.13336)

Ali Borji


## 2020-08-29

[Improving Resistance to Adversarial Deformations by Regularizing Gradients.](http://arxiv.org/abs/2008.12997)

Pengfei Xia, Bin Li


## 2020-08-27

[A Scene-Agnostic Framework with Adversarial Training for Abnormal Event Detection in Video.](http://arxiv.org/abs/2008.12328)

Mariana-Iuliana Georgescu, Radu Tudor Ionescu, Fahad Shahbaz Khan, Marius Popescu, Mubarak Shah


[GhostBuster: Looking Into Shadows to Detect Ghost Objects in Autonomous Vehicle 3D Sensing.](http://arxiv.org/abs/2008.12008)

Zhongyuan Hau, Soteris Demetriou, Luis Muñoz-González, Emil C. Lupu


[Minimal Adversarial Examples for Deep Learning on 3D Point Clouds.](http://arxiv.org/abs/2008.12066)

Jaeyeon Kim, Binh-Son Hua, Duc Thanh Nguyen, Sai-Kit Yeung


[On the Intrinsic Robustness of NVM Crossbars Against Adversarial Attacks.](http://arxiv.org/abs/2008.12016)

Deboleena Roy, Indranil Chakraborty, Timur Ibrayev, Kaushik Roy


[Adversarial Eigen Attack on Black-Box Models.](http://arxiv.org/abs/2009.00097)

Linjun Zhou, Peng Cui, Yinan Jiang, Shiqiang Yang


[Color and Edge-Aware Adversarial Image Perturbations.](http://arxiv.org/abs/2008.12454)

Robert Bassett, Mitchell Graves, Patrick Reilly


[Adversarially Robust Learning via Entropic Regularization.](http://arxiv.org/abs/2008.12338)

Gauri Jagatap, Ameya Joshi, Animesh Basak Chowdhury, Siddharth Garg, Chinmay Hegde


## 2020-08-26

[Adversarially Training for Audio Classifiers.](http://arxiv.org/abs/2008.11618)

Raymel Alfonso Sallo, Mohammad Esmaeilpour, Patrick Cardinal


## 2020-08-25

[Likelihood Landscapes: A Unifying Principle Behind Many Adversarial Defenses.](http://arxiv.org/abs/2008.11300)

Fu Lin, Rohit Mittapalli, Prithvijit Chattopadhyay, Daniel Bolya, Judy Hoffman


[Two Sides of the Same Coin: White-box and Black-box Attacks for Transfer Learning.](http://arxiv.org/abs/2008.11089)

Yinghua Zhang, Yangqiu Song, Jian Liang, Kun Bai, Qiang Yang


[Rethinking Non-idealities in Memristive Crossbars for Adversarial Robustness in Neural Networks.](http://arxiv.org/abs/2008.11298)

Abhiroop Bhattacharjee, Priyadarshini Panda


[An Adversarial Attack Defending System for Securing In-Vehicle Networks.](http://arxiv.org/abs/2008.11278)

Yi Li, Jing Lin, Kaiqi Xiong


## 2020-08-24

[Certified Robustness of Graph Neural Networks against Adversarial Structural Perturbation.](http://arxiv.org/abs/2008.10715)

Binghui Wang, Jinyuan Jia, Xiaoyu Cao, Neil Zhenqiang Gong


## 2020-08-23

[Developing and Defeating Adversarial Examples.](http://arxiv.org/abs/2008.10106)

Ian McDiarmid-Sterling, Allan Moser


[Ptolemy: Architecture Support for Robust Deep Learning.](http://arxiv.org/abs/2008.09954)

Yiming Gan, Yuxian Qiu, Jingwen Leng, Minyi Guo, Yuhao Zhu


[PermuteAttack: Counterfactual Explanation of Machine Learning Credit Scorecards.](http://arxiv.org/abs/2008.10138)

Masoud Hashemi, Ali Fathi


## 2020-08-22

[Self-Competitive Neural Networks.](http://arxiv.org/abs/2008.09824)

Iman Saberi, Fathiyeh Faghih


## 2020-08-21

[A Survey on Assessing the Generalization Envelope of Deep Neural Networks: Predictive Uncertainty, Out-of-distribution and Adversarial Samples.](http://arxiv.org/abs/2008.09381)

Julia Lust, Alexandru Paul Condurache


## 2020-08-20

[Towards adversarial robustness with 01 loss neural networks.](http://arxiv.org/abs/2008.09148)

Yunzhe Xue, Meiyan Xie, Usman Roshan


[On Attribution of Deepfakes.](http://arxiv.org/abs/2008.09194)

Baiwu Zhang, Jin Peng Zhou, Ilia Shumailov, Nicolas Papernot


[$\beta$-Variational Classifiers Under Attack.](http://arxiv.org/abs/2008.09010)

Marco Maggipinto, Matteo Terzi, Gian Antonio Susto


[Yet Another Intermediate-Level Attack.](http://arxiv.org/abs/2008.08847)

Qizhang Li, Yiwen Guo, Hao Chen


## 2020-08-19

[Prototype-based interpretation of the functionality of neurons in winner-take-all neural networks.](http://arxiv.org/abs/2008.08750)

Ramin Zarei Sabzevar, Kamaledin Ghiasi-Shirazi, Ahad Harati


[Addressing Neural Network Robustness with Mixup and Targeted Labeling Adversarial Training.](http://arxiv.org/abs/2008.08384)

Alfred Laugros, Alice Caplier, Matthieu Ospici


[On $\ell_p$-norm Robustness of Ensemble Stumps and Trees.](http://arxiv.org/abs/2008.08755)

Yihan Wang, Huan Zhang, Hongge Chen, Duane Boning, Cho-Jui Hsieh


## 2020-08-18

[Improving adversarial robustness of deep neural networks by using semantic information.](http://arxiv.org/abs/2008.07838)

Lina Wang, Rui Tang, Yawei Yue, Xingshu Chen, Wei Wang, Yi Zhu, Xuemei Zeng


[Direct Adversarial Training for GANs.](http://arxiv.org/abs/2008.09041)

Ziqiang Li


[Accelerated Zeroth-Order and First-Order Momentum Methods from Mini to Minimax Optimization.](http://arxiv.org/abs/2008.08170)

Feihu Huang, Shangqian Gao, Jian Pei, Heng Huang


## 2020-08-17

[A Deep Dive into Adversarial Robustness in Zero-Shot Learning.](http://arxiv.org/abs/2008.07651)

Mehmet Kerim Yucel, Ramazan Gokberk Cinbis, Pinar Duygulu


[Adversarial Attack and Defense Strategies for Deep Speaker Recognition Systems.](http://arxiv.org/abs/2008.07685)

Arindam Jati, Chin-Cheng Hsu, Monisankha Pal, Raghuveer Peri, Wael AbdAlmageed, Shrikanth Narayanan


[Adversarial EXEmples: A Survey and Experimental Evaluation of Practical Attacks on Machine Learning for Windows Malware Detection.](http://arxiv.org/abs/2008.07125)

Luca Demetrio, Scott E. Coull, Battista Biggio, Giovanni Lagorio, Alessandro Armando, Fabio Roli


[Robustness Verification of Quantum Classifiers. (81%)](http://arxiv.org/abs/2008.07230)

Ji Guan, Wang Fang, Mingsheng Ying


## 2020-08-16

[TextDecepter: Hard Label Black Box Attack on Text Classifiers.](http://arxiv.org/abs/2008.06860)

Sachin Saxena


[Adversarial Concurrent Training: Optimizing Robustness and Accuracy Trade-off of Deep Neural Networks.](http://arxiv.org/abs/2008.07015)

Elahe Arani, Fahad Sarfraz, Bahram Zonooz


## 2020-08-15

[Relevance Attack on Detectors.](http://arxiv.org/abs/2008.06822)

Sizhe Chen, Fan He, Xiaolin Huang, Kun Zhang


## 2020-08-14

[Defending Adversarial Attacks without Adversarial Attacks in Deep Reinforcement Learning.](http://arxiv.org/abs/2008.06199)

Xinghua Qu, Yew-Soon Ong, Abhishek Gupta, Zhu Sun


[Generating Image Adversarial Examples by Embedding Digital Watermarks.](http://arxiv.org/abs/2009.05107)

Yuexin Xiang, Tiantian Li, Wei Ren, Tianqing Zhu, Kim-Kwang Raymond Choo


[On the Generalization Properties of Adversarial Training.](http://arxiv.org/abs/2008.06631)

Yue Xing, Qifan Song, Guang Cheng


## 2020-08-13

[Adversarial Training and Provable Robustness: A Tale of Two Objectives.](http://arxiv.org/abs/2008.06081)

Jiameng Fan, Wenchao Li


[Semantically Adversarial Learnable Filters.](http://arxiv.org/abs/2008.06069)

Ali Shahin Shamsabadi, Changjae Oh, Andrea Cavallaro


[Continuous Patrolling Games. (45%)](http://arxiv.org/abs/2008.07369)

Steve Alpern, Thuy Bui, Thomas Lidbetter, Katerina Papadaki


## 2020-08-12

[Learning to Learn from Mistakes: Robust Optimization for Adversarial Noise.](http://arxiv.org/abs/2008.05247)

Alex Serban, Erik Poll, Joost Visser


[Defending Adversarial Examples via DNN Bottleneck Reinforcement.](http://arxiv.org/abs/2008.05230)

Wenqing Liu, Miaojing Shi, Teddy Furon, Li Li


[Feature Binding with Category-Dependant MixUp for Semantic Segmentation and Adversarial Robustness.](http://arxiv.org/abs/2008.05667)

Md Amirul Islam, Matthew Kowal, Konstantinos G. Derpanis, Neil D. B. Bruce


[Semantics-preserving adversarial attacks in NLP.](http://arxiv.org/abs/2008.05536)

Rahul Singh, Tarun Joshi, Vijayan N. Nair, Agus Sudjianto


## 2020-08-11

[Revisiting Adversarially Learned Injection Attacks Against Recommender Systems.](http://arxiv.org/abs/2008.04876)

Jiaxi Tang, Hongyi Wen, Ke Wang


## 2020-08-10

[Informative Dropout for Robust Representation Learning: A Shape-bias Perspective.](http://arxiv.org/abs/2008.04254)

Baifeng Shi, Dinghuai Zhang, Qi Dai, Zhanxing Zhu, Yadong Mu, Jingdong Wang


[FireBERT: Hardening BERT-based classifiers against adversarial attack.](http://arxiv.org/abs/2008.04203)

Gunnar Mein, Kevin Hartman, Andrew Morris


## 2020-08-09

[Enhancing Robustness Against Adversarial Examples in Network Intrusion Detection Systems.](http://arxiv.org/abs/2008.03677)

Mohammad J. Hashemi, Eric Keller


[Adversarial Training with Fast Gradient Projection Method against Synonym Substitution based Text Attacks.](http://arxiv.org/abs/2008.03709)

Xiaosen Wang, Yichen Yang, Yihe Deng, Kun He


## 2020-08-08

[Enhance CNN Robustness Against Noises for Classification of 12-Lead ECG with Variable Length.](http://arxiv.org/abs/2008.03609)

Linhai Ma, Liang Liang


## 2020-08-07

[Visual Attack and Defense on Text.](http://arxiv.org/abs/2008.10356)

Shengjun Liu, Ningkang Jiang, Yuanbin Wu


[Optimizing Information Loss Towards Robust Neural Networks.](http://arxiv.org/abs/2008.03072)

Philip Sperl, Konstantin Böttinger


[Adversarial Examples on Object Recognition: A Comprehensive Survey.](http://arxiv.org/abs/2008.04094)

Alex Serban, Erik Poll, Joost Visser


## 2020-08-06

[Stronger and Faster Wasserstein Adversarial Attacks.](http://arxiv.org/abs/2008.02883)

Kaiwen Wu, Allen Houze Wang, Yaoliang Yu


[Improve Generalization and Robustness of Neural Networks via Weight Scale Shifting Invariant Regularizations.](http://arxiv.org/abs/2008.02965)

Ziquan Liu, Yufei Cui, Antoni B. Chan


## 2020-08-05

[One word at a time: adversarial attacks on retrieval models.](http://arxiv.org/abs/2008.02197)

Nisarg Raval, Manisha Verma


[Robust Deep Reinforcement Learning through Adversarial Loss.](http://arxiv.org/abs/2008.01976)

Tuomas Oikarinen, Wang Zhang, Alexandre Megretski, Luca Daniel, Tsui-Wei Weng


## 2020-08-04

[Adv-watermark: A Novel Watermark Perturbation for Adversarial Examples.](http://arxiv.org/abs/2008.01919)

Xiaojun Jia, Xingxing Wei, Xiaochun Cao, Xiaoguang Han


[TREND: Transferability based Robust ENsemble Design.](http://arxiv.org/abs/2008.01524)

Deepak Ravikumar, Sangamesh Kodge, Isha Garg, Kaushik Roy


[Can Adversarial Weight Perturbations Inject Neural Backdoors?](http://arxiv.org/abs/2008.01761)

Siddhant Garg, Adarsh Kumar, Vibhor Goel, Yingyu Liang


[Entropy Guided Adversarial Model for Weakly Supervised Object Localization.](http://arxiv.org/abs/2008.01786)

Sabrina Narimene Benassou, Wuzhen Shi, Feng Jiang


## 2020-08-03

[Hardware Accelerator for Adversarial Attacks on Deep Learning Neural Networks.](http://arxiv.org/abs/2008.01219)

Haoqiang Guo, Lu Peng, Jian Zhang, Fang Qi, Lide Duan


[Anti-Bandit Neural Architecture Search for Model Defense.](http://arxiv.org/abs/2008.00698)

Hanlin Chen, Baochang Zhang, Song Xue, Xuan Gong, Hong Liu, Rongrong Ji, David Doermann


## 2020-08-01

[Efficient Adversarial Attacks for Visual Object Tracking.](http://arxiv.org/abs/2008.00217)

Siyuan Liang, Xingxing Wei, Siyuan Yao, Xiaochun Cao


[Trojaning Language Models for Fun and Profit.](http://arxiv.org/abs/2008.00312)

Xinyang Zhang, Zheng Zhang, Shouling Ji, Ting Wang


## 2020-07-31

[Vulnerability Under Adversarial Machine Learning: Bias or Variance?](http://arxiv.org/abs/2008.00138)

Hossein Aboutalebi, Mohammad Javad Shafiee, Michelle Karg, Christian Scharfenberger, Alexander Wong


[Physical Adversarial Attack on Vehicle Detector in the Carla Simulator.](http://arxiv.org/abs/2007.16118)

Tong Wu, Xuefei Ning, Wenshuo Li, Ranran Huang, Huazhong Yang, Yu Wang


[Adversarial Attacks with Multiple Antennas Against Deep Learning-Based Modulation Classifiers.](http://arxiv.org/abs/2007.16204)

Brian Kim, Yalin E. Sagduyu, Tugba Erpek, Kemal Davaslioglu, Sennur Ulukus


[TEAM: We Need More Powerful Adversarial Examples for DNNs.](http://arxiv.org/abs/2007.15836)

Yaguan Qian, Ximin Zhang, Bin Wang, Wei Li, Zhaoquan Gu, Haijiang Wang, Wassim Swaileh


## 2020-07-30

[Black-box Adversarial Sample Generation Based on Differential Evolution.](http://arxiv.org/abs/2007.15310)

Junyu Lin, Lei Xu, Yingqi Liu, Xiangyu Zhang


[A Data Augmentation-based Defense Method Against Adversarial Attacks in Neural Networks.](http://arxiv.org/abs/2007.15290)

Yi Zeng, Han Qiu, Gerard Memmi, Meikang Qiu


[vWitness: Certifying Web Page Interactions with Computer Vision. (83%)](http://arxiv.org/abs/2007.15805)

He Shuang, Lianying Zhao, David Lie


## 2020-07-29

[End-to-End Adversarial White Box Attacks on Music Instrument Classification.](http://arxiv.org/abs/2007.14714)

Katharina Johannes Kepler University Linz Prinz, Arthur Johannes Kepler University Linz Flexer


[Adversarial Robustness for Machine Learning Cyber Defenses Using Log Data.](http://arxiv.org/abs/2007.14983)

Kai Steverson, Jonathan Mullin, Metin Ahiskali


[Generative Classifiers as a Basis for Trustworthy Computer Vision.](http://arxiv.org/abs/2007.15036)

Radek Mackowiak, Lynton Ardizzone, Ullrich Köthe, Carsten Rother


[Stylized Adversarial Defense.](http://arxiv.org/abs/2007.14672)

Muzammal Naseer, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Fatih Porikli


[Detecting Anomalous Inputs to DNN Classifiers By Joint Statistical Testing at the Layers.](http://arxiv.org/abs/2007.15147)

Jayaram Raghuram, Varun Chandrasekaran, Somesh Jha, Suman Banerjee


## 2020-07-28

[Cassandra: Detecting Trojaned Networks from Adversarial Perturbations.](http://arxiv.org/abs/2007.14433)

Xiaoyu Zhang, Ajmal Mian, Rohit Gupta, Nazanin Rahnavard, Mubarak Shah


[Derivation of Information-Theoretically Optimal Adversarial Attacks with Applications to Robust Machine Learning.](http://arxiv.org/abs/2007.14042)

Jirong Yi, Raghu Mudumbai, Weiyu Xu


[Reachable Sets of Classifiers and Regression Models: (Non-)Robustness Analysis and Robust Training.](http://arxiv.org/abs/2007.14120)

Anna-Kathrin Kopetzki, Stephan Günnemann


[Label-Only Membership Inference Attacks.](http://arxiv.org/abs/2007.14321)

Christopher A. Choquette-Choo, Florian Tramer, Nicholas Carlini, Nicolas Papernot


## 2020-07-27

[Attacking and Defending Machine Learning Applications of Public Cloud.](http://arxiv.org/abs/2008.02076)

Dou Goodman, Hao Xin


[KOVIS: Keypoint-based Visual Servoing with Zero-Shot Sim-to-Real Transfer for Robotics Manipulation.](http://arxiv.org/abs/2007.13960)

En Yen Puang, Keng Peng Tee, Wei Jing


[From Sound Representation to Model Robustness.](http://arxiv.org/abs/2007.13703)

Mohamad Esmaeilpour, Patrick Cardinal, Alessandro Lameiras Koerich


[Towards Accuracy-Fairness Paradox: Adversarial Example-based Data Augmentation for Visual Debiasing.](http://arxiv.org/abs/2007.13632)

Yi Zhang, Jitao Sang


## 2020-07-26

[RANDOM MASK: Towards Robust Convolutional Neural Networks.](http://arxiv.org/abs/2007.14249)

Tiange Luo, Tianle Cai, Mengxiao Zhang, Siyu Chen, Liwei Wang


[Robust Collective Classification against Structural Attacks.](http://arxiv.org/abs/2007.13073)

Kai Zhou, Yevgeniy Vorobeychik


[Train Like a (Var)Pro: Efficient Training of Neural Networks with Variable Projection. (1%)](http://arxiv.org/abs/2007.13171)

Elizabeth Newman, Lars Ruthotto, Joseph Hart, Bart van Bloemen Waanders


## 2020-07-25

[MirrorNet: Bio-Inspired Adversarial Attack for Camouflaged Object Segmentation.](http://arxiv.org/abs/2007.12881)

Jinnan Yan, Trung-Nghia Le, Khanh-Duy Nguyen, Minh-Triet Tran, Thanh-Toan Do, Tam V. Nguyen


[Adversarial Privacy-preserving Filter.](http://arxiv.org/abs/2007.12861)

Jiaming Zhang, Jitao Sang, Xian Zhao, Xiaowen Huang, Yanfeng Sun, Yongli Hu


[MP3 Compression To Diminish Adversarial Noise in End-to-End Speech Recognition.](http://arxiv.org/abs/2007.12892)

Iustina Andronic, Ludwig Kürzinger, Edgar Ricardo Chavez Rosas, Gerhard Rigoll, Bernhard U. Seeber


## 2020-07-24

[Deep Co-Training with Task Decomposition for Semi-Supervised Domain Adaptation. (1%)](http://arxiv.org/abs/2007.12684)

Luyu Yang, Yan Wang, Mingfei Gao, Abhinav Shrivastava, Kilian Q. Weinberger, Wei-Lun Chao, Ser-Nam Lim


## 2020-07-23

[Provably Robust Adversarial Examples.](http://arxiv.org/abs/2007.12133)

Dimitar I. Dimitrov, Gagandeep Singh, Timon Gehr, Martin Vechev


## 2020-07-22

[SOCRATES: Towards a Unified Platform for Neural Network Verification.](http://arxiv.org/abs/2007.11206)

Long H. Pham, Jiaying Li, Jun Sun


[Adversarial Training Reduces Information and Improves Transferability.](http://arxiv.org/abs/2007.11259)

Matteo Terzi, Alessandro Achille, Marco Maggipinto, Gian Antonio Susto


[Robust Machine Learning via Privacy/Rate-Distortion Theory.](http://arxiv.org/abs/2007.11693)

Ye Wang, Shuchin Aeron, Adnan Siraj Rakin, Toshiaki Koike-Akino, Pierre Moulin


[Threat of Adversarial Attacks on Face Recognition: A Comprehensive Survey.](http://arxiv.org/abs/2007.11709)

Fatemeh Vakhshiteh, Raghavendra Ramachandra, Ahmad Nickabadi


## 2020-07-21

[Audio Adversarial Examples for Robust Hybrid CTC/Attention Speech Recognition.](http://arxiv.org/abs/2007.10723)

Ludwig Kürzinger, Edgar Ricardo Chavez Rosas, Lujun Li, Tobias Watzel, Gerhard Rigoll


[Towards Visual Distortion in Black-Box Attacks.](http://arxiv.org/abs/2007.10593)

Nannan Li, Zhenzhong Chen


## 2020-07-20

[DeepNNK: Explaining deep models and their generalization using polytope interpolation.](http://arxiv.org/abs/2007.10505)

Sarath Shekkizhar, Antonio Ortega


[Evaluating a Simple Retraining Strategy as a Defense Against Adversarial Attacks.](http://arxiv.org/abs/2007.09916)

Nupur Thakur, Yuzhen Ding, Baoxin Li


[Robust Tracking against Adversarial Attacks.](http://arxiv.org/abs/2007.09919)

Shuai Jia, Chao Ma, Yibing Song, Xiaokang Yang


[Scaling Polyhedral Neural Network Verification on GPUs.](http://arxiv.org/abs/2007.10868)

Christoph Müller, François Serre, Gagandeep Singh, Markus Püschel, Martin Vechev


[AdvFoolGen: Creating Persistent Troubles for Deep Classifiers.](http://arxiv.org/abs/2007.10485)

Yuzhen Ding, Nupur Thakur, Baoxin Li


## 2020-07-19

[Semantic Equivalent Adversarial Data Augmentation for Visual Question Answering.](http://arxiv.org/abs/2007.09592)

Ruixue Tang, Chao Ma, Wei Emma Zhang, Qi Wu, Xiaokang Yang


[Exploiting vulnerabilities of deep neural networks for privacy protection.](http://arxiv.org/abs/2007.09766)

Ricardo Sanchez-Matilla, Chau Yi Li, Ali Shahin Shamsabadi, Riccardo Mazzon, Andrea Cavallaro


[Connecting the Dots: Detecting Adversarial Perturbations Using Context Inconsistency.](http://arxiv.org/abs/2007.09763)

Shasha Li, Shitong Zhu, Sudipta Paul, Amit Roy-Chowdhury, Chengyu Song, Srikanth Krishnamurthy, Ananthram Swami, Kevin S Chan


[Adversarial Immunization for Improving Certifiable Robustness on Graphs.](http://arxiv.org/abs/2007.09647)

Shuchang Tao, Huawei Shen, Qi Cao, Liang Hou, Xueqi Cheng


## 2020-07-18

[DDR-ID: Dual Deep Reconstruction Networks Based Image Decomposition for Anomaly Detection.](http://arxiv.org/abs/2007.09431)

Dongyun Lin, Yiqun Li, Shudong Xie, Tin Lay Nwe, Sheng Dong


[Towards Quantum-Secure Authentication and Key Agreement via Abstract Multi-Agent Interaction. (1%)](http://arxiv.org/abs/2007.09327)

Ibrahim H. Ahmed, Josiah P. Hanna, Elliot Fosong, Stefano V. Albrecht


## 2020-07-17

[Anomaly Detection in Unsupervised Surveillance Setting Using Ensemble of Multimodal Data with Adversarial Defense.](http://arxiv.org/abs/2007.10812)

Sayeed Shafayet Chowdhury, Kaji Mejbaul Islam, Rouhan Noor


[Neural Networks with Recurrent Generative Feedback.](http://arxiv.org/abs/2007.09200)

Yujia Huang, James Gornet, Sihui Dai, Zhiding Yu, Tan Nguyen, Doris Y. Tsao, Anima Anandkumar


## 2020-07-16

[Understanding and Diagnosing Vulnerability under Adversarial Attacks.](http://arxiv.org/abs/2007.08716)

Haizhong Zheng, Ziqi Zhang, Honglak Lee, Atul Prakash


[Transfer Learning without Knowing: Reprogramming Black-box Machine Learning Models with Scarce Data and Limited Resources.](http://arxiv.org/abs/2007.08714)

Yun-Yun Tsai, Pin-Yu Chen, Tsung-Yi Ho


[Accelerated Stochastic Gradient-free and Projection-free Methods.](http://arxiv.org/abs/2007.12625)

Feihu Huang, Lue Tao, Songcan Chen


[Provable Worst Case Guarantees for the Detection of Out-of-Distribution Data.](http://arxiv.org/abs/2007.08473)

Julian Bitterwolf, Alexander Meinke, Matthias Hein


[An Empirical Study on the Robustness of NAS based Architectures.](http://arxiv.org/abs/2007.08428)

Chaitanya Devaguptapu, Devansh Agarwal, Gaurav Mittal, Vineeth N Balasubramanian


[Do Adversarially Robust ImageNet Models Transfer Better?](http://arxiv.org/abs/2007.08489)

Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, Aleksander Madry


[Learning perturbation sets for robust machine learning.](http://arxiv.org/abs/2007.08450)

Eric Wong, J. Zico Kolter


[On Robustness and Transferability of Convolutional Neural Networks. (1%)](http://arxiv.org/abs/2007.08558)

Josip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D'Amour, Dan Moldovan, Sylvain Gelly, Neil Houlsby, Xiaohua Zhai, Mario Lucic


[Less is More: A privacy-respecting Android malware classifier using Federated Learning. (1%)](http://arxiv.org/abs/2007.08319)

Rafa Gálvez, Veelasha Moonsamy, Claudia Diaz


## 2020-07-15

[A Survey of Privacy Attacks in Machine Learning.](http://arxiv.org/abs/2007.07646)

Maria Rigaki, Sebastian Garcia


[Accelerating Robustness Verification of Deep Neural Networks Guided by Target Labels.](http://arxiv.org/abs/2007.08520)

Wenjie Wan, Zhaodi Zhang, Yiwei Zhu, Min Zhang, Fu Song


[A Survey on Security Attacks and Defense Techniques for Connected and Autonomous Vehicles.](http://arxiv.org/abs/2007.08041)

Minh Pham, Kaiqi Xiong


## 2020-07-14

[Towards robust sensing for Autonomous Vehicles: An adversarial perspective.](http://arxiv.org/abs/2007.10115)

Apostolos Modas, Ricardo Sanchez-Matilla, Pascal Frossard, Andrea Cavallaro


[Robustifying Reinforcement Learning Agents via Action Space Adversarial Training.](http://arxiv.org/abs/2007.07176)

Kai Liang Tan, Yasaman Esfandiari, Xian Yeow Lee, Aakanksha, Soumik Sarkar


[Bounding The Number of Linear Regions in Local Area for Neural Networks with ReLU Activations.](http://arxiv.org/abs/2007.06803)

Rui Zhu, Bo Lin, Haixu Tang


[Multitask Learning Strengthens Adversarial Robustness.](http://arxiv.org/abs/2007.07236)

Chengzhi Mao, Amogh Gupta, Vikram Nitin, Baishakhi Ray, Shuran Song, Junfeng Yang, Carl Vondrick


[Adversarial Examples and Metrics.](http://arxiv.org/abs/2007.06993)

Nico Döttling, Kathrin Grosse, Michael Backes, Ian Molloy


[AdvFlow: Inconspicuous Black-box Adversarial Attacks using Normalizing Flows.](http://arxiv.org/abs/2007.07435)

Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie


[Pasadena: Perceptually Aware and Stealthy Adversarial Denoise Attack.](http://arxiv.org/abs/2007.07097)

Yupeng Cheng, Qing Guo, Felix Juefei-Xu, Wei Feng, Shang-Wei Lin, Weisi Lin, Yang Liu


[Adversarial Attacks against Neural Networks in Audio Domain: Exploiting Principal Components.](http://arxiv.org/abs/2007.07001)

Ken Alparslan, Yigit Alparslan, Matthew Burlick


[Towards a Theoretical Understanding of the Robustness of Variational Autoencoders.](http://arxiv.org/abs/2007.07365)

Alexander Camuto, Matthew Willetts, Stephen Roberts, Chris Holmes, Tom Rainforth


## 2020-07-13

[A simple defense against adversarial attacks on heatmap explanations.](http://arxiv.org/abs/2007.06381)

Laura Rieger, Lars Kai Hansen


[Understanding Adversarial Examples from the Mutual Influence of Images and Perturbations.](http://arxiv.org/abs/2007.06189)

Chaoning Zhang, Philipp Benz, Tooba Imtiaz, In-So Kweon


[Adversarial robustness via robust low rank representations.](http://arxiv.org/abs/2007.06555)

Pranjal Awasthi, Himanshu Jain, Ankit Singh Rawat, Aravindan Vijayaraghavan


[Security and Machine Learning in the Real World.](http://arxiv.org/abs/2007.07205)

Ivan Evtimov, Weidong Cui, Ece Kamar, Emre Kiciman, Tadayoshi Kohno, Jerry Li


[Hard Label Black-box Adversarial Attacks in Low Query Budget Regimes.](http://arxiv.org/abs/2007.07210)

Satya Narayan Shukla, Anit Kumar Sahu, Devin Willmott, J. Zico Kolter


[Calling Out Bluff: Attacking the Robustness of Automatic Scoring Systems with Simple Adversarial Testing.](http://arxiv.org/abs/2007.06796)

Yaman Kumar, Mehar Bhatia, Anubha Kabra, Jessy Junyi Li, Di Jin, Rajiv Ratn Shah


[SoK: The Faults in our ASRs: An Overview of Attacks against Automatic Speech Recognition and Speaker Identification Systems.](http://arxiv.org/abs/2007.06622)

Hadi Abdullah, Kevin Warren, Vincent Bindschaedler, Nicolas Papernot, Patrick Traynor


[Patch-wise Attack for Fooling Deep Neural Network.](http://arxiv.org/abs/2007.06765)

Lianli Gao, Qilong Zhang, Jingkuan Song, Xianglong Liu, Heng Tao Shen


## 2020-07-12

[Generating Fluent Adversarial Examples for Natural Languages.](http://arxiv.org/abs/2007.06174)

Huangzhao Zhang, Hao Zhou, Ning Miao, Lei Li


[Adversarial jamming attacks and defense strategies via adaptive deep reinforcement learning.](http://arxiv.org/abs/2007.06055)

Feng Wang, Chen Zhong, M. Cenk Gursoy, Senem Velipasalar


[Probabilistic Jacobian-based Saliency Maps Attacks.](http://arxiv.org/abs/2007.06032)

Théo Combey, António Loison, Maxime Faucher, Hatem Hajri


## 2020-07-11

[Understanding Object Detection Through An Adversarial Lens.](http://arxiv.org/abs/2007.05828)

Ka-Ho Chow, Ling Liu, Mehmet Emre Gursoy, Stacey Truex, Wenqi Wei, Yanzhao Wu


[ManiGen: A Manifold Aided Black-box Generator of Adversarial Examples.](http://arxiv.org/abs/2007.05817)

Guanxiong Liu, Issa Khalil, Abdallah Khreishah, Abdulelah Algosaibi, Adel Aldalbahi, Mohammed Alaneem, Abdulaziz Alhumam, Mohammed Anan


[Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification. (15%)](http://arxiv.org/abs/2007.05869)

Francisco Utrera, Evan Kravitz, N. Benjamin Erichson, Rajiv Khanna, Michael W. Mahoney


## 2020-07-10

[Improved Detection of Adversarial Images Using Deep Neural Networks.](http://arxiv.org/abs/2007.05573)

Yutong Gao, Yi Pan


[Miss the Point: Targeted Adversarial Attack on Multiple Landmark Detection.](http://arxiv.org/abs/2007.05225)

Qingsong Yao, Zecheng He, Hu Han, S. Kevin Zhou


[Generating Adversarial Inputs Using A Black-box Differential Technique.](http://arxiv.org/abs/2007.05315)

João Batista Pereira Matos Juúnior, Lucas Carvalho Cordeiro, Marcelo d'Amorim, Xiaowei Huang


## 2020-07-09

[Improving Adversarial Robustness by Enforcing Local and Global Compactness.](http://arxiv.org/abs/2007.05123)

Anh Bui, Trung Le, He Zhao, Paul Montague, Olivier deVel, Tamas Abraham, Dinh Phung


[Boundary thickness and robustness in learning models.](http://arxiv.org/abs/2007.05086)

Yaoqing Yang, Rajiv Khanna, Yaodong Yu, Amir Gholami, Kurt Keutzer, Joseph E. Gonzalez, Kannan Ramchandran, Michael W. Mahoney


[Node Copying for Protection Against Graph Neural Network Topology Attacks.](http://arxiv.org/abs/2007.06704)

Florence Regol, Soumyasundar Pal, Mark Coates


[Efficient detection of adversarial images.](http://arxiv.org/abs/2007.04564)

Darpan Kumar Yadav, Kartik Mundra, Rahul Modpur, Arpan Chattopadhyay, Indra Narayan Kar


## 2020-07-08

[How benign is benign overfitting?](http://arxiv.org/abs/2007.04028)

Amartya Sanyal, Puneet K Dokania, Varun Kanade, Philip H. S. Torr


[SLAP: Improving Physical Adversarial Examples with Short-Lived Adversarial Perturbations.](http://arxiv.org/abs/2007.04137)

Giulio Lovisotto, Henry Turner, Ivo Sluganovic, Martin Strohmeier, Ivan Martinovic


[RobFR: Benchmarking Adversarial Robustness on Face Recognition.](http://arxiv.org/abs/2007.04118)

Xiao Yang, Dingcheng Yang, Yinpeng Dong, Hang Su, Wenjian Yu, Jun Zhu


[A Critical Evaluation of Open-World Machine Learning.](http://arxiv.org/abs/2007.04391)

Liwei Song, Vikash Sehwag, Arjun Nitin Bhagoji, Prateek Mittal


[On the relationship between class selectivity, dimensionality, and robustness.](http://arxiv.org/abs/2007.04440)

Matthew L. Leavitt, Ari S. Morcos


[Evaluation of Adversarial Training on Different Types of Neural Networks in Deep Learning-based IDSs.](http://arxiv.org/abs/2007.04472)

Rana Abou Khamis, Ashraf Matrawy


## 2020-07-07

[Robust Learning with Frequency Domain Regularization.](http://arxiv.org/abs/2007.03244)

Weiyu Guo, Yidong Ouyang


[Regional Image Perturbation Reduces $L_p$ Norms of Adversarial Examples While Maintaining Model-to-model Transferability.](http://arxiv.org/abs/2007.03198)

Utku Ozbulak, Jonathan Peck, Neve Wesley De, Bart Goossens, Yvan Saeys, Messem Arnout Van


[Fast Training of Deep Neural Networks Robust to Adversarial Perturbations.](http://arxiv.org/abs/2007.03832)

Justin Goodwin, Olivia Brown, Victoria Helus


[Making Adversarial Examples More Transferable and Indistinguishable.](http://arxiv.org/abs/2007.03838)

Junhua Zou, Yexin Duan, Boyu Li, Wu Zhang, Yu Pan, Zhisong Pan


[Detection as Regression: Certified Object Detection by Median Smoothing.](http://arxiv.org/abs/2007.03730)

Ping-yeh Chiang, Michael J. Curry, Ahmed Abdelkader, Aounon Kumar, John Dickerson, Tom Goldstein


## 2020-07-06

[Certifying Decision Trees Against Evasion Attacks by Program Analysis.](http://arxiv.org/abs/2007.02771)

Stefano Calzavara, Pietro Ferrara, Claudio Lucchese


[On Data Augmentation and Adversarial Risk: An Empirical Analysis.](http://arxiv.org/abs/2007.02650)

Hamid Eghbal-zadeh, Khaled Koutini, Paul Primus, Verena Haunschmid, Michal Lewandowski, Werner Zellinger, Bernhard A. Moser, Gerhard Widmer


[Understanding and Improving Fast Adversarial Training.](http://arxiv.org/abs/2007.02617)

Maksym Andriushchenko, Nicolas Flammarion


[Black-box Adversarial Example Generation with Normalizing Flows.](http://arxiv.org/abs/2007.02734)

Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie


## 2020-07-05

[Adversarial Learning in the Cyber Security Domain.](http://arxiv.org/abs/2007.02407)

Ihai Rosenberg, Asaf Shabtai, Yuval Elovici, Lior Rokach


## 2020-07-04

[On Connections between Regularizations for Improving DNN Robustness.](http://arxiv.org/abs/2007.02209)

Yiwen Guo, Long Chen, Yurong Chen, Changshui Zhang


[Relationship between manifold smoothness and adversarial vulnerability in deep learning with local errors.](http://arxiv.org/abs/2007.02047)

Zijian Jiang, Jianwen Zhou, Haiping Huang


[Deep Active Learning via Open Set Recognition. (1%)](http://arxiv.org/abs/2007.02196)

Jaya Krishna Mandivarapu, Blake Camp, Rolando Estrada


## 2020-07-03

[Towards Robust Deep Learning with Ensemble Networks and Noisy Layers.](http://arxiv.org/abs/2007.01507)

Yuting Liang, Reza Samavi


## 2020-07-02

[Efficient Proximal Mapping of the 1-path-norm of Shallow Networks.](http://arxiv.org/abs/2007.01003)

Fabian Latorre, Paul Rolland, Nadav Hallak, Volkan Cevher


[Deep Learning Defenses Against Adversarial Examples for Dynamic Risk Assessment.](http://arxiv.org/abs/2007.01017)

Xabier Echeberria-Barrio, Amaia Gil-Lerchundi, Ines Goicoechea-Telleria, Raul Orduna-Urrutia


[Decoder-free Robustness Disentanglement without (Additional) Supervision.](http://arxiv.org/abs/2007.01356)

Yifei Wang, Dan Peng, Furui Liu, Zhenguo Li, Zhitang Chen, Jiansheng Yang


[Increasing Trustworthiness of Deep Neural Networks via Accuracy Monitoring.](http://arxiv.org/abs/2007.01472)

Zhihui Shao, Jianyi Yang, Shaolei Ren


[Trace-Norm Adversarial Examples.](http://arxiv.org/abs/2007.01855)

Ehsan Kazemi, Thomas Kerdreux, Liqiang Wang


[Generating Adversarial Examples withControllable Non-transferability.](http://arxiv.org/abs/2007.01299)

Renzhi Wang, Tianwei Zhang, Xiaofei Xie, Lei Ma, Cong Tian, Felix Juefei-Xu, Yang Liu


## 2020-07-01

[Unifying Model Explainability and Robustness via Machine-Checkable Concepts.](http://arxiv.org/abs/2007.00251)

Vedant Nanda, Till Speicher, John P. Dickerson, Krishna P. Gummadi, Muhammad Bilal Zafar


[Measuring Robustness to Natural Distribution Shifts in Image Classification.](http://arxiv.org/abs/2007.00644)

Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, Ludwig Schmidt


[Determining Sequence of Image Processing Technique (IPT) to Detect Adversarial Attacks.](http://arxiv.org/abs/2007.00337)

Kishor Datta Gupta, Dipankar Dasgupta, Zahid Akhtar


[Query-Free Adversarial Transfer via Undertrained Surrogates.](http://arxiv.org/abs/2007.00806)

Chris Miller, Soroush Vosoughi


[Adversarial Example Games.](http://arxiv.org/abs/2007.00720)

Avishek Joey Bose, Gauthier Gidel, Hugo Berard, Andre Cianflone, Pascal Vincent, Simon Lacoste-Julien, William L. Hamilton


[Robustness against Relational Adversary.](http://arxiv.org/abs/2007.00772)

Yizhen Wang, Xiaozhu Meng, Ke Wang, Mihai Christodorescu, Somesh Jha


[A Le Cam Type Bound for Adversarial Learning and Applications.](http://arxiv.org/abs/2007.00289)

Qiuling Xu, Kevin Bello, Jean Honorio


[Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey.](http://arxiv.org/abs/2007.00753)

Samuel Henrique Silva, Peyman Najafirad


## 2020-06-30

[Towards Robust LiDAR-based Perception in Autonomous Driving: General Black-box Adversarial Sensor Attack and Countermeasures.](http://arxiv.org/abs/2006.16974)

Jiachen Sun, Yulong Cao, Qi Alfred Chen, Z. Morley Mao


[Adversarial Deep Ensemble: Evasion Attacks and Defenses for Malware Detection.](http://arxiv.org/abs/2006.16545)

Deqiang Li, Qianmu Li


[Black-box Certification and Learning under Adversarial Perturbations.](http://arxiv.org/abs/2006.16520)

Hassan Ashtiani, Vinayak Pathak, Ruth Urner


[Neural Network Virtual Sensors for Fuel Injection Quantities with Provable Performance Specifications.](http://arxiv.org/abs/2007.00147)

Eric Wong, Tim Schneider, Joerg Schmitt, Frank R. Schmidt, J. Zico Kolter


[Generating Adversarial Examples with an Optimized Quality.](http://arxiv.org/abs/2007.00146)

Aminollah Khormali, DaeHun Nyang, David Mohaisen


## 2020-06-29

[Harnessing Adversarial Distances to Discover High-Confidence Errors.](http://arxiv.org/abs/2006.16055)

Walter Bennette, Karsten Maurer, Sean Sisti


[Sharp Statistical Guarantees for Adversarially Robust Gaussian Classification.](http://arxiv.org/abs/2006.16384)

Chen Dan, Yuting Wei, Pradeep Ravikumar


[Legal Risks of Adversarial Machine Learning Research.](http://arxiv.org/abs/2006.16179)

Ram Shankar Siva Kumar, Jonathon Penney, Bruce Schneier, Kendra Albert


[Biologically Inspired Mechanisms for Adversarial Robustness.](http://arxiv.org/abs/2006.16427)

Manish V. Reddy, Andrzej Banburski, Nishka Pant, Tomaso Poggio


[Improving Uncertainty Estimates through the Relationship with Adversarial Robustness.](http://arxiv.org/abs/2006.16375)

Yao Qin, Xuezhi Wang, Alex Beutel, Ed H. Chi


## 2020-06-28

[FDA3 : Federated Defense Against Adversarial Attacks for Cloud-Based IIoT Applications.](http://arxiv.org/abs/2006.15632)

Yunfei Song, Tian Liu, Tongquan Wei, Xiangfeng Wang, Zhe Tao, Mingsong Chen


[Geometry-Inspired Top-k Adversarial Perturbations.](http://arxiv.org/abs/2006.15669)

Nurislam Tursynbek, Aleksandr Petiushko, Ivan Oseledets


## 2020-06-26

[Orthogonal Deep Models As Defense Against Black-Box Attacks.](http://arxiv.org/abs/2006.14856)

Mohammad A. A. K. Jalwana, Naveed Akhtar, Mohammed Bennamoun, Ajmal Mian


[Informative Outlier Matters: Robustifying Out-of-distribution Detection Using Outlier Mining.](http://arxiv.org/abs/2006.15207)

Jiefeng Chen, Yixuan Li, Xi Wu, Yingyu Liang, Somesh Jha


[Diverse Knowledge Distillation (DKD): A Solution for Improving The Robustness of Ensemble Models Against Adversarial Attacks.](http://arxiv.org/abs/2006.15127)

Ali Mirzaeian, Jana Kosecka, Houman Homayoun, Tinoosh Mohsenin, Avesta Sasan


[Can We Mitigate Backdoor Attack Using Adversarial Detection Methods?](http://arxiv.org/abs/2006.14871)

Kaidi Jin, Tianwei Zhang, Chao Shen, Yufei Chen, Ming Fan, Chenhao Lin, Ting Liu


## 2020-06-25

[Smooth Adversarial Training.](http://arxiv.org/abs/2006.14536)

Cihang Xie, Mingxing Tan, Boqing Gong, Alan Yuille, Quoc V. Le


[Proper Network Interpretability Helps Adversarial Robustness in Classification.](http://arxiv.org/abs/2006.14748)

Akhilan Boopathy, Sijia Liu, Gaoyuan Zhang, Cynthia Liu, Pin-Yu Chen, Shiyu Chang, Luca Daniel


[Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability.](http://arxiv.org/abs/2006.14512)

Kaizhao Liang, Jacky Y. Zhang, Boxin Wang, Zhuolin Yang, Oluwasanmi Koyejo, Bo Li


[Can 3D Adversarial Logos Cloak Humans?](http://arxiv.org/abs/2006.14655)

Yi Wang, Jingyang Zhou, Tianlong Chen, Sijia Liu, Shiyu Chang, Chandrajit Bajaj, Zhangyang Wang


## 2020-06-24

[Defending against adversarial attacks on medical imaging AI system, classification or detection?](http://arxiv.org/abs/2006.13555)

Xin Li, Deng Pan, Dongxiao Zhu


[Compositional Explanations of Neurons.](http://arxiv.org/abs/2006.14032)

Jesse Mu, Jacob Andreas


[Blacklight: Defending Black-Box Adversarial Attacks on Deep Neural Networks.](http://arxiv.org/abs/2006.14042)

Huiying Li, Shawn Shan, Emily Wenger, Jiayun Zhang, Haitao Zheng, Ben Y. Zhao


[Imbalanced Gradients: A Subtle Cause of Overestimated Adversarial Robustness.](http://arxiv.org/abs/2006.13726)

Xingjun Ma, Linxi Jiang, Hanxun Huang, Zejia Weng, James Bailey, Yu-Gang Jiang


## 2020-06-23

[RayS: A Ray Searching Method for Hard-label Adversarial Attack.](http://arxiv.org/abs/2006.12792)

Jinghui Chen, Quanquan Gu


[Sparse-RS: a versatile framework for query-efficient sparse black-box adversarial attacks.](http://arxiv.org/abs/2006.12834)

Francesco Croce, Maksym Andriushchenko, Naman D. Singh, Nicolas Flammarion, Matthias Hein


[Adversarial Robustness of Deep Sensor Fusion Models.](http://arxiv.org/abs/2006.13192)

Shaojie Wang, Tong Wu, Ayan Chakrabarti, Yevgeniy Vorobeychik


## 2020-06-22

[Learning to Generate Noise for Multi-Attack Robustness.](http://arxiv.org/abs/2006.12135)

Divyam Madaan, Jinwoo Shin, Sung Ju Hwang


[Perceptual Adversarial Robustness: Defense Against Unseen Threat Models.](http://arxiv.org/abs/2006.12655)

Cassidy Laidlaw, Sahil Singla, Soheil Feizi


## 2020-06-21

[Network Moments: Extensions and Sparse-Smooth Attacks.](http://arxiv.org/abs/2006.11776)

Modar Alfadly, Adel Bibi, Emilio Botero, Salman Alsubaihi, Bernard Ghanem


## 2020-06-20

[How do SGD hyperparameters in natural training affect adversarial robustness?](http://arxiv.org/abs/2006.11604)

Sandesh Kamath, Amit Deshpande, K V Subrahmanyam


[Defense against Adversarial Attacks in NLP via Dirichlet Neighborhood Ensemble.](http://arxiv.org/abs/2006.11627)

Yi Zhou, Xiaoqing Zheng, Cho-Jui Hsieh, Kai-wei Chang, Xuanjing Huang


[Stochastic Shortest Path with Adversarially Changing Costs. (1%)](http://arxiv.org/abs/2006.11561)

Aviv Rosenberg, Yishay Mansour


## 2020-06-19

[Local Convolutions Cause an Implicit Bias towards High Frequency Adversarial Examples.](http://arxiv.org/abs/2006.11440)

Josue Ortega Caro, Yilong Ju, Ryan Pyle, Sourav Dey, Wieland Brendel, Fabio Anselmi, Ankit Patel


[A general framework for defining and optimizing robustness.](http://arxiv.org/abs/2006.11122)

Alessandro Tibo, Manfred Jaeger, Kim G. Larsen


[Analyzing the Real-World Applicability of DGA Classifiers.](http://arxiv.org/abs/2006.11103)

Arthur Drichel, Ulrike Meyer, Samuel Schüppen, Dominik Teubert


[Towards an Adversarially Robust Normalization Approach.](http://arxiv.org/abs/2006.11007)

Muhammad Awais, Fahad Shamshad, Sung-Ho Bae


[Differentiable Language Model Adversarial Attacks on Categorical Sequence Classifiers.](http://arxiv.org/abs/2006.11078)

I. Fursov, A. Zaytsev, N. Kluchnikov, A. Kravchenko, E. Burnaev


[Adversarial Attacks for Multi-view Deep Models.](http://arxiv.org/abs/2006.11004)

Xuli Sun, Shiliang Sun


## 2020-06-18

[Local Competition and Uncertainty for Adversarial Robustness in Deep Learning.](http://arxiv.org/abs/2006.10620)

Antonios Alexos, Konstantinos P. Panousis, Sotirios Chatzis


[Dissecting Deep Networks into an Ensemble of Generative Classifiers for Robust Predictions.](http://arxiv.org/abs/2006.10679)

Lokender Tiwari, Anish Madan, Saket Anand, Subhashis Banerjee


[The Dilemma Between Dimensionality Reduction and Adversarial Robustness.](http://arxiv.org/abs/2006.10885)

Sheila Alemany, Niki Pissinou


[Beware the Black-Box: on the Robustness of Recent Defenses to Adversarial Examples.](http://arxiv.org/abs/2006.10876)

Kaleel Mahmood, Deniz Gurevin, Dijk Marten van, Phuong Ha Nguyen


## 2020-06-17

[Noise or Signal: The Role of Image Backgrounds in Object Recognition.](http://arxiv.org/abs/2006.09994)

Kai Xiao, Logan Engstrom, Andrew Ilyas, Aleksander Madry


[Adversarial Examples Detection and Analysis with Layer-wise Autoencoders.](http://arxiv.org/abs/2006.10013)

Bartosz Wójcik, Paweł Morawiecki, Marek Śmieja, Tomasz Krzyżek, Przemysław Spurek, Jacek Tabor


[Adversarial Defense by Latent Style Transformations.](http://arxiv.org/abs/2006.09701)

Shuo Wang, Surya Nepal, Alsharif Abuadbba, Carsten Rudolph, Marthie Grobler


[Disrupting Deepfakes with an Adversarial Attack that Survives Training.](http://arxiv.org/abs/2006.12247)

Eran Segalis


[Universal Lower-Bounds on Classification Error under Adversarial Attacks and Random Corruption.](http://arxiv.org/abs/2006.09989)

Elvis Dohmatob


[Fairness Through Robustness: Investigating Robustness Disparity in Deep Learning.](http://arxiv.org/abs/2006.12621)

Vedant Nanda, Samuel Dooley, Sahil Singla, Soheil Feizi, John P. Dickerson


## 2020-06-16

[Calibrating Deep Neural Network Classifiers on Out-of-Distribution Datasets.](http://arxiv.org/abs/2006.08914)

Zhihui Shao, Jianyi Yang, Shaolei Ren


[SPLASH: Learnable Activation Functions for Improving Accuracy and Adversarial Robustness.](http://arxiv.org/abs/2006.08947)

Mohammadamin Tavakoli, Forest Agostinelli, Pierre Baldi


[Debona: Decoupled Boundary Network Analysis for Tighter Bounds and Faster Adversarial Robustness Proofs.](http://arxiv.org/abs/2006.09040)

Christopher Brix, Thomas Noll


[On sparse connectivity, adversarial robustness, and a novel model of the artificial neuron.](http://arxiv.org/abs/2006.09510)

Sergey Bochkanov


[AdvMind: Inferring Adversary Intent of Black-Box Attacks.](http://arxiv.org/abs/2006.09539)

Ren Pang, Xinyang Zhang, Shouling Ji, Xiapu Luo, Ting Wang


[The shape and simplicity biases of adversarially robust ImageNet-trained CNNs.](http://arxiv.org/abs/2006.09373)

Peijie Chen, Chirag Agarwal, Anh Nguyen


## 2020-06-15

[Total Deep Variation: A Stable Regularizer for Inverse Problems.](http://arxiv.org/abs/2006.08789)

Erich Kobler, Alexander Effland, Karl Kunisch, Thomas Pock


[DefenseVGAE: Defending against Adversarial Attacks on Graph Data via a Variational Graph Autoencoder.](http://arxiv.org/abs/2006.08900)

Ao Zhang, Jinwen Ma


[Improving Adversarial Robustness via Unlabeled Out-of-Domain Data.](http://arxiv.org/abs/2006.08476)

Zhun Deng, Linjun Zhang, Amirata Ghorbani, James Zou


[Fast & Accurate Method for Bounding the Singular Values of Convolutional Layers with Application to Lipschitz Regularization.](http://arxiv.org/abs/2006.08391)

Alexandre Araujo, Benjamin Negrevergne, Yann Chevaleyre, Jamal Atif


[GNNGuard: Defending Graph Neural Networks against Adversarial Attacks.](http://arxiv.org/abs/2006.08149)

Xiang Zhang, Marinka Zitnik


[CG-ATTACK: Modeling the Conditional Distribution of Adversarial Perturbations to Boost Black-Box Attack.](http://arxiv.org/abs/2006.08538)

Yan Feng, Baoyuan Wu, Yanbo Fan, Li Liu, Zhifeng Li, Shutao Xia


[Multiscale Deep Equilibrium Models.](http://arxiv.org/abs/2006.08656)

Shaojie Bai, Vladlen Koltun, J. Zico Kolter


## 2020-06-14

[GradAug: A New Regularization Method for Deep Neural Networks.](http://arxiv.org/abs/2006.07989)

Taojiannan Yang, Sijie Zhu, Chen Chen


[PatchUp: A Regularization Technique for Convolutional Neural Networks.](http://arxiv.org/abs/2006.07794)

Mojtaba Faramarzi, Mohammad Amini, Akilesh Badrinaaraayanan, Vikas Verma, Sarath Chandar


[On Saliency Maps and Adversarial Robustness.](http://arxiv.org/abs/2006.07828)

Puneet Mangla, Vedant Singh, Vineeth N Balasubramanian


[On the transferability of adversarial examples between convex and 01 loss models.](http://arxiv.org/abs/2006.07800)

Yunzhe Xue, Meiyan Xie, Usman Roshan


[Adversarial Attacks and Detection on Reinforcement Learning-Based Interactive Recommender Systems.](http://arxiv.org/abs/2006.07934)

Yuanjiang Cao, Xiaocong Chen, Lina Yao, Xianzhi Wang, Wei Emma Zhang


[Sparsity Turns Adversarial: Energy and Latency Attacks on Deep Neural Networks.](http://arxiv.org/abs/2006.08020)

Sarada Krithivasan, Sanchari Sen, Anand Raghunathan


[Duplicity Games for Deception Design with an Application to Insider Threat Mitigation. (11%)](http://arxiv.org/abs/2006.07942)

Linan Huang, Quanyan Zhu


## 2020-06-13

[The Pitfalls of Simplicity Bias in Neural Networks.](http://arxiv.org/abs/2006.07710)

Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, Praneeth Netrapalli


[Adversarial Self-Supervised Contrastive Learning.](http://arxiv.org/abs/2006.07589)

Minseon Kim, Jihoon Tack, Sung Ju Hwang


[Rethinking Clustering for Robustness.](http://arxiv.org/abs/2006.07682)

Motasem Alfarra, Juan C. Pérez, Adel Bibi, Ali Thabet, Pablo Arbeláez, Bernard Ghanem


[Defensive Approximation: Securing CNNs using Approximate Computing.](http://arxiv.org/abs/2006.07700)

Amira Guesmi, Ihsen Alouani, Khaled Khasawneh, Mouna Baklouti, Tarek Frikha, Mohamed Abid, Nael Abu-Ghazaleh


## 2020-06-12

[Provably Robust Metric Learning.](http://arxiv.org/abs/2006.07024)

Lu Wang, Xuanqing Liu, Jinfeng Yi, Yuan Jiang, Cho-Jui Hsieh


[Defending against GAN-based Deepfake Attacks via Transformation-aware Adversarial Faces.](http://arxiv.org/abs/2006.07421)

Chaofei Yang, Lei Ding, Yiran Chen, Hai Li


[D-square-B: Deep Distribution Bound for Natural-looking Adversarial Attack.](http://arxiv.org/abs/2006.07258)

Qiuling Xu, Guanhong Tao, Xiangyu Zhang


[Targeted Adversarial Perturbations for Monocular Depth Prediction.](http://arxiv.org/abs/2006.08602)

Alex Wong, Safa Cicek, Stefano Soatto


## 2020-06-11

[Large-Scale Adversarial Training for Vision-and-Language Representation Learning.](http://arxiv.org/abs/2006.06195)

Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, Jingjing Liu


[Smoothed Geometry for Robust Attribution.](http://arxiv.org/abs/2006.06643)

Zifan Wang, Haofan Wang, Shakul Ramkumar, Matt Fredrikson, Piotr Mardziel, Anupam Datta


[Protecting Against Image Translation Deepfakes by Leaking Universal Perturbations from Black-Box Neural Networks.](http://arxiv.org/abs/2006.06493)

Nataniel Ruiz, Sarah Adel Bargal, Stan Sclaroff


[Investigating Robustness of Adversarial Samples Detection for Automatic Speaker Verification.](http://arxiv.org/abs/2006.06186)

Xu Li, Na Li, Jinghua Zhong, Xixin Wu, Xunying Liu, Dan Su, Dong Yu, Helen Meng


[Robustness to Adversarial Attacks in Learning-Enabled Controllers.](http://arxiv.org/abs/2006.06861)

Zikang Xiong, Joe Eappen, He Zhu, Suresh Jagannathan


[On the Tightness of Semidefinite Relaxations for Certifying Robustness to Adversarial Examples.](http://arxiv.org/abs/2006.06759)

Richard Y. Zhang


[Adversarial Attack Vulnerability of Medical Image Analysis Systems: Unexplored Factors.](http://arxiv.org/abs/2006.06356)

Suzanne C. Wetstein, Cristina González-Gonzalo, Gerda Bortsova, Bart Liefers, Florian Dubost, Ioannis Katramados, Laurens Hogeweg, Ginneken Bram van, Josien P. W. Pluim, Bruijne Marleen de, Clara I. Sánchez, Mitko Veta


[Achieving robustness in classification using optimal transport with hinge regularization.](http://arxiv.org/abs/2006.06520)

Mathieu Serrurier, Franck Mamalet, Alberto González-Sanz, Thibaut Boissin, Jean-Michel Loubes, Barrio Eustasio del


[Backdoor Smoothing: Demystifying Backdoor Attacks on Deep Neural Networks. (96%)](http://arxiv.org/abs/2006.06721)

Kathrin Grosse, Taesung Lee, Battista Biggio, Youngja Park, Michael Backes, Ian Molloy


## 2020-06-10

[Evaluating Graph Vulnerability and Robustness using TIGER.](http://arxiv.org/abs/2006.05648)

Scott Freitas, Duen Horng Chau


[Towards Robust Fine-grained Recognition by Maximal Separation of Discriminative Features.](http://arxiv.org/abs/2006.06028)

Krishna Kanth Nakka, Mathieu Salzmann


[Deterministic Gaussian Averaged Neural Networks.](http://arxiv.org/abs/2006.06061)

Ryan Campbell, Chris Finlay, Adam M Oberman


[Interpolation between Residual and Non-Residual Networks.](http://arxiv.org/abs/2006.05749)

Zonghan Yang, Yang Liu, Chenglong Bao, Zuoqiang Shi


[Towards Certified Robustness of Metric Learning.](http://arxiv.org/abs/2006.05945)

Xiaochen Yang, Yiwen Guo, Mingzhi Dong, Jing-Hao Xue


## 2020-06-09

[Towards an Intrinsic Definition of Robustness for a Classifier.](http://arxiv.org/abs/2006.05095)

Théo Giraudon, Vincent Gripon, Matthias Löwe, Franck Vermet


[Black-Box Adversarial Attacks on Graph Neural Networks with Limited Node Access.](http://arxiv.org/abs/2006.05057)

Jiaqi Ma, Shuangrui Ding, Qiaozhu Mei


[GAP++: Learning to generate target-conditioned adversarial examples.](http://arxiv.org/abs/2006.05097)

Xiaofeng Mao, Yuefeng Chen, Yuhong Li, Yuan He, Hui Xue


[Adversarial Attacks on Brain-Inspired Hyperdimensional Computing-Based Classifiers.](http://arxiv.org/abs/2006.05594)

Fangfang Yang, Shaolei Ren


[Provable tradeoffs in adversarially robust classification.](http://arxiv.org/abs/2006.05161)

Edgar Dobriban, Hamed Hassani, David Hong, Alexander Robey


[Distributional Robust Batch Contextual Bandits. (1%)](http://arxiv.org/abs/2006.05630)

Nian Si, Fan Zhang, Zhengyuan Zhou, Jose Blanchet


## 2020-06-08

[Calibrated neighborhood aware confidence measure for deep metric learning.](http://arxiv.org/abs/2006.04935)

Maryna Karpusha, Sunghee Yun, Istvan Fehervari


[A Self-supervised Approach for Adversarial Robustness.](http://arxiv.org/abs/2006.04924)

Muzammal Naseer, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Fatih Porikli


[Distributional Robustness with IPMs and links to Regularization and GANs.](http://arxiv.org/abs/2006.04349)

Hisham Husain


[On Universalized Adversarial and Invariant Perturbations.](http://arxiv.org/abs/2006.04449)

Sandesh Kamath, Amit Deshpande, K V Subrahmanyam


[Tricking Adversarial Attacks To Fail.](http://arxiv.org/abs/2006.04504)

Blerta Lindqvist


[Global Robustness Verification Networks.](http://arxiv.org/abs/2006.04403)

Weidi Sun, Yuteng Lu, Xiyue Zhang, Zhanxing Zhu, Meng Sun


[Trade-offs between membership privacy & adversarially robust learning.](http://arxiv.org/abs/2006.04622)

Jamie Hayes


[Adversarial Feature Desensitization.](http://arxiv.org/abs/2006.04621)

Pouya Bashivan, Reza Bayat, Adam Ibrahim, Kartik Ahuja, Mojtaba Faramarzi, Touraj Laleh, Blake Aaron Richards, Irina Rish


## 2020-06-07

[Extensions and limitations of randomized smoothing for robustness guarantees.](http://arxiv.org/abs/2006.04208)

Jamie Hayes


[Uncertainty-Aware Deep Classifiers using Generative Models.](http://arxiv.org/abs/2006.04183)

Murat Sensoy, Lance Kaplan, Federico Cerutti, Maryam Saleki


## 2020-06-06

[Unique properties of adversarially trained linear classifiers on Gaussian data.](http://arxiv.org/abs/2006.03873)

Jamie Hayes


[Can Domain Knowledge Alleviate Adversarial Attacks in Multi-Label Classifiers?](http://arxiv.org/abs/2006.03833)

Stefano Melacci, Gabriele Ciravegna, Angelo Sotgiu, Ambra Demontis, Battista Biggio, Marco Gori, Fabio Roli


## 2020-06-05

[Adversarial Image Generation and Training for Deep Convolutional Neural Networks.](http://arxiv.org/abs/2006.03243)

Ronghua Shi, Hai Shu, Hongtu Zhu, Ziqi Chen


[Lipschitz Bounds and Provably Robust Training by Laplacian Smoothing.](http://arxiv.org/abs/2006.03712)

Vishaal Krishnan, Abed AlRahman Al Makdah, Fabio Pasqualetti


[Sponge Examples: Energy-Latency Attacks on Neural Networks.](http://arxiv.org/abs/2006.03463)

Ilia Shumailov, Yiren Zhao, Daniel Bates, Nicolas Papernot, Robert Mullins, Ross Anderson


## 2020-06-04

[Characterizing the Weight Space for Different Learning Models.](http://arxiv.org/abs/2006.02724)

Saurav Musunuru, Jay N. Paranjape, Rahul Kumar Dubey, Vijendran G. Venkoparao


[Towards Understanding Fast Adversarial Training.](http://arxiv.org/abs/2006.03089)

Bai Li, Shiqi Wang, Suman Jana, Lawrence Carin


[Defense for Black-box Attacks on Anti-spoofing Models by Self-Supervised Learning.](http://arxiv.org/abs/2006.03214)

Haibin Wu, Andy T. Liu, Hung-yi Lee


[Pick-Object-Attack: Type-Specific Adversarial Attack for Object Detection.](http://arxiv.org/abs/2006.03184)

Omid Mohamad Nezami, Akshay Chaturvedi, Mark Dras, Utpal Garain


## 2020-06-02

[SaliencyMix: A Saliency Guided Data Augmentation Strategy for Better Regularization.](http://arxiv.org/abs/2006.01791)

A. F. M. Shahab Uddin, Mst. Sirazam Monira, Wheemyung Shin, TaeChoong Chung, Sung-Ho Bae


[Exploring the role of Input and Output Layers of a Deep Neural Network in Adversarial Defense.](http://arxiv.org/abs/2006.01408)

Jay N. Paranjape, Rahul Kumar Dubey, Vijendran V Gopalan


[Perturbation Analysis of Gradient-based Adversarial Attacks.](http://arxiv.org/abs/2006.01456)

Utku Ozbulak, Manvel Gasparyan, Neve Wesley De, Messem Arnout Van


[Adversarial Item Promotion: Vulnerabilities at the Core of Top-N Recommenders that Use Images to Address Cold Start.](http://arxiv.org/abs/2006.01888)

Zhuoran Liu, Martha Larson


[Detecting Audio Attacks on ASR Systems with Dropout Uncertainty.](http://arxiv.org/abs/2006.01906)

Tejas Jayashankar, Jonathan Le Roux, Pierre Moulin


## 2020-06-01

[Second-Order Provable Defenses against Adversarial Attacks.](http://arxiv.org/abs/2006.00731)

Sahil Singla, Soheil Feizi


[Adversarial Attacks on Reinforcement Learning based Energy Management Systems of Extended Range Electric Delivery Vehicles.](http://arxiv.org/abs/2006.00817)

Pengyue Wang, Yan Li, Shashi Shekhar, William F. Northrop


[Adversarial Attacks on Classifiers for Eye-based User Modelling.](http://arxiv.org/abs/2006.00860)

Inken CISPA Helmholtz Center for Information Security Hagestedt, Michael CISPA Helmholtz Center for Information Security Backes, Andreas University of Stuttgart Bulling


[Rethinking Empirical Evaluation of Adversarial Robustness Using First-Order Attack Methods.](http://arxiv.org/abs/2006.01304)

Kyungmi Lee, Anantha P. Chandrakasan


## 2020-05-31

[Evaluations and Methods for Explanation through Robustness Analysis.](http://arxiv.org/abs/2006.00442)

Cheng-Yu Hsieh, Chih-Kuan Yeh, Xuanqing Liu, Pradeep Ravikumar, Seungyeon Kim, Sanjiv Kumar, Cho-Jui Hsieh


[Estimating Principal Components under Adversarial Perturbations.](http://arxiv.org/abs/2006.00602)

Pranjal Awasthi, Xue Chen, Aravindan Vijayaraghavan


## 2020-05-30

[Exploring Model Robustness with Adaptive Networks and Improved Adversarial Training.](http://arxiv.org/abs/2006.00387)

Zheng Xu, Ali Shafahi, Tom Goldstein


## 2020-05-29

[SAFER: A Structure-free Approach for Certified Robustness to Adversarial Word Substitutions.](http://arxiv.org/abs/2005.14424)

Mao Ye, Chengyue Gong, Qiang Liu


## 2020-05-28

[Monocular Depth Estimators: Vulnerabilities and Attacks.](http://arxiv.org/abs/2005.14302)

Alwyn Mathew, Aditya Prakash Patra, Jimson Mathew


[QEBA: Query-Efficient Boundary-Based Blackbox Attack.](http://arxiv.org/abs/2005.14137)

Huichen Li, Xiaojun Xu, Xiaolu Zhang, Shuang Yang, Bo Li


[Adversarial Attacks and Defense on Texts: A Survey.](http://arxiv.org/abs/2005.14108)

Aminul Huq, Mst. Tasnim Pervin


[Adversarial Robustness of Deep Convolutional Candlestick Learner.](http://arxiv.org/abs/2006.03686)

Jun-Hao Chen, Samuel Yen-Chi Chen, Yun-Cheng Tsai, Chih-Shiang Shur


## 2020-05-27

[Enhancing Resilience of Deep Learning Networks by Means of Transferable Adversaries.](http://arxiv.org/abs/2005.13293)

Moritz Seiler, Heike Trautmann, Pascal Kerschke


[Mitigating Advanced Adversarial Attacks with More Advanced Gradient Obfuscation Techniques.](http://arxiv.org/abs/2005.13712)

Han Qiu, Yi Zeng, Qinkai Zheng, Tianwei Zhang, Meikang Qiu, Gerard Memmi


[Stochastic Security: Adversarial Defense Using Long-Run Dynamics of Energy-Based Models.](http://arxiv.org/abs/2005.13525)

Mitch Hill, Jonathan Mitchell, Song-Chun Zhu


[Calibrated Surrogate Losses for Adversarially Robust Classification.](http://arxiv.org/abs/2005.13748)

Han Bao, Clayton Scott, Masashi Sugiyama


## 2020-05-26

[Effects of Forward Error Correction on Communications Aware Evasion Attacks.](http://arxiv.org/abs/2005.13123)

Matthew DelVecchio, Bryse Flowers, William C. Headley


[Investigating a Spectral Deception Loss Metric for Training Machine Learning-based Evasion Attacks.](http://arxiv.org/abs/2005.13124)

Matthew DelVecchio, Vanessa Arndorfer, William C. Headley


[Generating Semantically Valid Adversarial Questions for TableQA.](http://arxiv.org/abs/2005.12696)

Yi Zhu, Menglin Xia, Yiwei Zhou


## 2020-05-25

[Adversarial Feature Selection against Evasion Attacks.](http://arxiv.org/abs/2005.12154)

Fei Zhang, Patrick P. K. Chan, Battista Biggio, Daniel S. Yeung, Fabio Roli


## 2020-05-24

[Detecting Adversarial Examples for Speech Recognition via Uncertainty Quantification.](http://arxiv.org/abs/2005.14611)

Sina Däubener, Lea Schönherr, Asja Fischer, Dorothea Kolossa


[SoK: Arms Race in Adversarial Malware Detection.](http://arxiv.org/abs/2005.11671)

Deqiang Li, Qianmu Li, Yanfang Ye, Shouhuai Xu


[Adaptive Adversarial Logits Pairing.](http://arxiv.org/abs/2005.11904)

Shangxi Wu, Jitao Sang, Kaiyuan Xu, Guanhua Zheng, Changsheng Xu


## 2020-05-23

[ShapeAdv: Generating Shape-Aware Adversarial 3D Point Clouds.](http://arxiv.org/abs/2005.11626)

Kibok Lee, Zhuoyuan Chen, Xinchen Yan, Raquel Urtasun, Ersin Yumer


[Adversarial Attack on Hierarchical Graph Pooling Neural Networks.](http://arxiv.org/abs/2005.11560)

Haoteng Tang, Guixiang Ma, Yurong Chen, Lei Guo, Wei Wang, Bo Zeng, Liang Zhan


[Frontal Attack: Leaking Control-Flow in SGX via the CPU Frontend. (1%)](http://arxiv.org/abs/2005.11516)

Ivan Puddu, Moritz Schneider, Miro Haller, Srdjan Čapkun


## 2020-05-22

[Vulnerability of deep neural networks for detecting COVID-19 cases from chest X-ray images to universal adversarial attacks.](http://arxiv.org/abs/2005.11061)

Hokuto Hirano, Kazuki Koga, Kazuhiro Takemoto


## 2020-05-21

[Revisiting Role of Autoencoders in Adversarial Settings.](http://arxiv.org/abs/2005.10750)

Byeong Cheon Kim, Jung Uk Kim, Hakmin Lee, Yong Man Ro


[Robust Ensemble Model Training via Random Layer Sampling Against Adversarial Attack.](http://arxiv.org/abs/2005.10757)

Hakmin Lee, Hong Joo Lee, Seong Tae Kim, Yong Man Ro


[Inaudible Adversarial Perturbations for Targeted Attack in Speaker Recognition.](http://arxiv.org/abs/2005.10637)

Qing Wang, Pengcheng Guo, Lei Xie


[Investigating Vulnerability to Adversarial Examples on Multimodal Data Fusion in Deep Learning.](http://arxiv.org/abs/2005.10987)

Youngjoon Yu, Hong Joo Lee, Byeong Cheon Kim, Jung Uk Kim, Yong Man Ro


## 2020-05-20

[Graph Structure Learning for Robust Graph Neural Networks.](http://arxiv.org/abs/2005.10203)

Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, Jiliang Tang


[Model-Based Robust Deep Learning: Generalizing to Natural, Out-of-Distribution Data.](http://arxiv.org/abs/2005.10247)

Alexander Robey, Hamed Hassani, George J. Pappas


[An Adversarial Approach for Explaining the Predictions of Deep Neural Networks.](http://arxiv.org/abs/2005.10284)

Arash Rahnama, Andrew Tseng


[A survey on Adversarial Recommender Systems: from Attack/Defense strategies to Generative Adversarial Networks.](http://arxiv.org/abs/2005.10322)

Yashar Deldjoo, Noia Tommaso Di, Felice Antonio Merra


[Feature Purification: How Adversarial Training Performs Robust Deep Learning.](http://arxiv.org/abs/2005.10190)

Zeyuan Allen-Zhu, Yuanzhi Li


## 2020-05-19

[Synthesizing Unrestricted False Positive Adversarial Objects Using Generative Models.](http://arxiv.org/abs/2005.09294)

Martin Kotuliak, Sandro E. Schoenborn, Andrei Dan


[Bias-based Universal Adversarial Patch Attack for Automatic Check-out.](http://arxiv.org/abs/2005.09257)

Aishan Liu, Jiakai Wang, Xianglong Liu, Bowen Cao, Chongzhi Zhang, Hang Yu


## 2020-05-18

[Universalization of any adversarial attack using very few test examples.](http://arxiv.org/abs/2005.08632)

Sandesh Kamath, Amit Deshpande, K V Subrahmanyam


[On Intrinsic Dataset Properties for Adversarial Machine Learning.](http://arxiv.org/abs/2005.09170)

Jeffrey Z. Pan, Nicholas Zufelt


[Defending Your Voice: Adversarial Attack on Voice Conversion.](http://arxiv.org/abs/2005.08781)

Chien-yu Huang, Yist Y. Lin, Hung-yi Lee, Lin-shan Lee


[Reliability and Robustness analysis of Machine Learning based Phishing URL Detectors.](http://arxiv.org/abs/2005.08454)

Bushra University of Adelaide, CREST - The Centre for Research on Engineering Software Technologies, CSIROs Data61 Sabir, M. Ali University of Adelaide, CREST - The Centre for Research on Engineering Software Technologies Babar, Raj CSIROs Data61 Gaire, Alsharif CSIROs DATA61 Abuadbba


[Improve robustness of DNN for ECG signal classification:a noise-to-signal ratio perspective.](http://arxiv.org/abs/2005.09134)

Linhai Ma, Liang Liang


[Increasing-Margin Adversarial (IMA) Training to Improve Adversarial Robustness of Neural Networks.](http://arxiv.org/abs/2005.09147)

Linhai Ma, Liang Liang


[Spatiotemporal Attacks for Embodied Agents.](http://arxiv.org/abs/2005.09161)

Aishan Liu, Tairan Huang, Xianglong Liu, Yitao Xu, Yuqing Ma, Xinyun Chen, Stephen J. Maybank, Dacheng Tao


## 2020-05-17

[Toward Adversarial Robustness by Diversity in an Ensemble of Specialized Deep Neural Networks.](http://arxiv.org/abs/2005.08321)

Mahdieh Abbasi, Arezoo Rajabi, Christian Gagne, Rakesh B. Bobba


## 2020-05-16

[Universal Adversarial Perturbations: A Survey.](http://arxiv.org/abs/2005.08087)

Ashutosh Chaubey, Nikhil Agrawal, Kavya Barnwal, Keerat K. Guliani, Pramod Mehta


[Encryption Inspired Adversarial Defense for Visual Classification.](http://arxiv.org/abs/2005.07998)

MaungMaung AprilPyone, Hitoshi Kiya


[PatchGuard: Provable Defense against Adversarial Patches Using Masks on Small Receptive Fields.](http://arxiv.org/abs/2005.10884)

Chong Xiang, Arjun Nitin Bhagoji, Vikash Sehwag, Prateek Mittal


## 2020-05-15

[How to Make 5G Communications "Invisible": Adversarial Machine Learning for Wireless Privacy.](http://arxiv.org/abs/2005.07675)

Brian Kim, Yalin E. Sagduyu, Kemal Davaslioglu, Tugba Erpek, Sennur Ulukus


[Practical Traffic-space Adversarial Attacks on Learning-based NIDSs.](http://arxiv.org/abs/2005.07519)

Dongqi Han, Zhiliang Wang, Ying Zhong, Wenqi Chen, Jiahai Yang, Shuqiang Lu, Xingang Shi, Xia Yin


[Initializing Perturbations in Multiple Directions for Fast Adversarial Training.](http://arxiv.org/abs/2005.07606)

Xunguang Wang, Ship Peng Xu, Eric Ke Wang


## 2020-05-14

[Stealthy and Efficient Adversarial Attacks against Deep Reinforcement Learning.](http://arxiv.org/abs/2005.07099)

Jianwen Sun, Tianwei Zhang, Xiaofei Xie, Lei Ma, Yan Zheng, Kangjie Chen, Yang Liu


[Towards Assessment of Randomized Mechanisms for Certifying Adversarial Robustness.](http://arxiv.org/abs/2005.07347)

Tianhang Zheng, Di Wang, Baochun Li, Jinhui Xu


[A Deep Learning-based Fine-grained Hierarchical Learning Approach for Robust Malware Classification.](http://arxiv.org/abs/2005.07145)

Ahmed Abusnaina, Mohammed Abuhamad, Hisham Alasmary, Afsah Anwar, Rhongho Jang, Saeed Salem, DaeHun Nyang, David Mohaisen


## 2020-05-13

[DeepRobust: A PyTorch Library for Adversarial Attacks and Defenses.](http://arxiv.org/abs/2005.06149)

Yaxin Li, Wei Jin, Han Xu, Jiliang Tang


## 2020-05-12

[Effective and Robust Detection of Adversarial Examples via Benford-Fourier Coefficients.](http://arxiv.org/abs/2005.05552)

Chengcheng Ma, Baoyuan Wu, Shibiao Xu, Yanbo Fan, Yong Zhang, Xiaopeng Zhang, Zhifeng Li


[Evaluating Ensemble Robustness Against Adversarial Attacks.](http://arxiv.org/abs/2005.05750)

George Adam, Romain Speciel


[Increased-confidence adversarial examples for improved transferability of Counter-Forensic attacks.](http://arxiv.org/abs/2005.06023)

Wenjie Li, Benedetta Tondi, Rongrong Ni, Mauro Barni


[Adversarial examples are useful too!](http://arxiv.org/abs/2005.06107)

Ali Borji


## 2020-05-11

[Channel-Aware Adversarial Attacks Against Deep Learning-Based Wireless Signal Classifiers.](http://arxiv.org/abs/2005.05321)

Brian Kim, Yalin E. Sagduyu, Kemal Davaslioglu, Tugba Erpek, Sennur Ulukus


[Spanning Attack: Reinforce Black-box Attacks with Unlabeled Data.](http://arxiv.org/abs/2005.04871)

Lu Wang, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, Yuan Jiang


## 2020-05-09

[It's Morphin' Time! Combating Linguistic Discrimination with Inflectional Perturbations.](http://arxiv.org/abs/2005.04364)

Samson Tan, Shafiq Joty, Min-Yen Kan, Richard Socher


[Class-Aware Domain Adaptation for Improving Adversarial Robustness.](http://arxiv.org/abs/2005.04564)

Xianxu Hou, Jingxin Liu, Bolei Xu, Xiaolong Wang, Bozhi Liu, Guoping Qiu


## 2020-05-08

[Towards Robustness against Unsuspicious Adversarial Examples.](http://arxiv.org/abs/2005.04272)

Liang Tong, Minzhe Guo, Atul Prakash, Yevgeniy Vorobeychik


## 2020-05-07

[Efficient Exact Verification of Binarized Neural Networks.](http://arxiv.org/abs/2005.03597)

Kai Jia, Martin Rinard


[Projection & Probability-Driven Black-Box Attack.](http://arxiv.org/abs/2005.03837)

Jie Li, Rongrong Ji, Hong Liu, Jianzhuang Liu, Bineng Zhong, Cheng Deng, Qi Tian


[Defending Hardware-based Malware Detectors against Adversarial Attacks.](http://arxiv.org/abs/2005.03644)

Abraham Peedikayil Kuruvila, Shamik Kundu, Kanad Basu


## 2020-05-06

[GraCIAS: Grassmannian of Corrupted Images for Adversarial Security.](http://arxiv.org/abs/2005.02936)

Ankita Shukla, Pavan Turaga, Saket Anand


[Training robust neural networks using Lipschitz bounds.](http://arxiv.org/abs/2005.02929)

Patricia Pauli, Anne Koch, Julian Berberich, Paul Kohler, Frank Allgöwer


## 2020-05-05

[Enhancing Intrinsic Adversarial Robustness via Feature Pyramid Decoder.](http://arxiv.org/abs/2005.02552)

Guanlin Li, Shuya Ding, Jun Luo, Chang Liu


[Hacking the Waveform: Generalized Wireless Adversarial Deep Learning.](http://arxiv.org/abs/2005.02270)

Francesco Restuccia, Salvatore D'Oro, Amani Al-Shawabka, Bruno Costa Rendon, Kaushik Chowdhury, Stratis Ioannidis, Tommaso Melodia


[Adversarial Training against Location-Optimized Adversarial Patches.](http://arxiv.org/abs/2005.02313)

Sukrut Rao, David Stutz, Bernt Schiele


[Measuring Adversarial Robustness using a Voronoi-Epsilon Adversary.](http://arxiv.org/abs/2005.02540)

Hyeongji Kim, Pekka Parviainen, Ketil Malde


## 2020-05-04

[On the Benefits of Models with Perceptually-Aligned Gradients.](http://arxiv.org/abs/2005.01499)

Gunjan Aggarwal, Abhishek Sinha, Nupur Kumari, Mayank Singh


[Do Gradient-based Explanations Tell Anything About Adversarial Robustness to Android Malware?](http://arxiv.org/abs/2005.01452)

Marco Melis, Michele Scalas, Ambra Demontis, Davide Maiorca, Battista Biggio, Giorgio Giacinto, Fabio Roli


## 2020-05-03

[Robust Encodings: A Framework for Combating Adversarial Typos.](http://arxiv.org/abs/2005.01229)

Erik Jones, Robin Jia, Aditi Raghunathan, Percy Liang


## 2020-05-02

[On the Generalization Effects of Linear Transformations in Data Augmentation. (1%)](http://arxiv.org/abs/2005.00695)

Sen Wu, Hongyang R. Zhang, Gregory Valiant, Christopher Ré


## 2020-05-01

[Jacks of All Trades, Masters Of None: Addressing Distributional Shift and Obtrusiveness via Transparent Patch Attacks.](http://arxiv.org/abs/2005.00656)

Neil Fendley, Max Lennon, I-Jeng Wang, Philippe Burlina, Nathan Drenkow


[Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models.](http://arxiv.org/abs/2005.00683)

Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, Xiang Ren


[Robust Deep Learning as Optimal Control: Insights and Convergence Guarantees.](http://arxiv.org/abs/2005.00616)

Jacob H. Seidman, Mahyar Fazlyab, Victor M. Preciado, George J. Pappas


[Defense of Word-level Adversarial Attacks via Random Substitution Encoding.](http://arxiv.org/abs/2005.00446)

Zhaoyang Wang, Hongtao Wang


## 2020-04-30

[Evaluating Neural Machine Comprehension Model Robustness to Noisy Inputs and Adversarial Attacks.](http://arxiv.org/abs/2005.00190)

Winston Wu, Dustin Arendt, Svitlana Volkova


[Imitation Attacks and Defenses for Black-box Machine Translation Systems.](http://arxiv.org/abs/2004.15015)

Eric Wallace, Mitchell Stern, Dawn Song


[Universal Adversarial Attacks with Natural Triggers for Text Classification.](http://arxiv.org/abs/2005.00174)

Liwei Song, Xinwei Yu, Hsuan-Tung Peng, Karthik Narasimhan


[Bridging Mode Connectivity in Loss Landscapes and Adversarial Robustness.](http://arxiv.org/abs/2005.00060)

Pu Zhao, Pin-Yu Chen, Payel Das, Karthikeyan Natesan Ramamurthy, Xue Lin


## 2020-04-29

[Perturbing Across the Feature Hierarchy to Improve Standard and Strict Blackbox Attack Transferability.](http://arxiv.org/abs/2004.14861)

Nathan Inkawhich, Kevin J Liang, Binghui Wang, Matthew Inkawhich, Lawrence Carin, Yiran Chen


[TAVAT: Token-Aware Virtual Adversarial Training for Language Understanding.](http://arxiv.org/abs/2004.14543)

Linyang Li, Xipeng Qiu


[TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP.](http://arxiv.org/abs/2005.05909)

John X. Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, Yanjun Qi


## 2020-04-28

[Adversarial Learning Guarantees for Linear Hypotheses and Neural Networks.](http://arxiv.org/abs/2004.13617)

Pranjal Awasthi, Natalie Frank, Mehryar Mohri


[Minority Reports Defense: Defending Against Adversarial Patches.](http://arxiv.org/abs/2004.13799)

Michael McCoyd, Won Park, Steven Chen, Neil Shah, Ryan Roggenkemper, Minjune Hwang, Jason Xinyu Liu, David Wagner


## 2020-04-27

[DeSePtion: Dual Sequence Prediction and Adversarial Examples for Improved Fact-Checking.](http://arxiv.org/abs/2004.12864)

Christopher Hidey, Tuhin Chakrabarty, Tariq Alhindi, Siddharth Varia, Kriste Krstovski, Mona Diab, Smaranda Muresan


[Adversarial Fooling Beyond "Flipping the Label".](http://arxiv.org/abs/2004.12771)

Konda Reddy Mopuri, Vaisakh Shaj, R. Venkatesh Babu


["Call me sexist, but...": Revisiting Sexism Detection Using Psychological Scales and Adversarial Samples. (81%)](http://arxiv.org/abs/2004.12764)

Mattia Samory, Indira Sen, Julian Kohne, Fabian Floeck, Claudia Wagner


## 2020-04-26

[Transferable Perturbations of Deep Feature Distributions.](http://arxiv.org/abs/2004.12519)

Nathan Inkawhich, Kevin J Liang, Lawrence Carin, Yiran Chen


[Towards Feature Space Adversarial Attack.](http://arxiv.org/abs/2004.12385)

Qiuling Xu, Guanhong Tao, Siyuan Cheng, Xiangyu Zhang


[Printing and Scanning Attack for Image Counter Forensics.](http://arxiv.org/abs/2005.02160)

Hailey James, Otkrist Gupta, Dan Raviv


[Improved Image Wasserstein Attacks and Defenses.](http://arxiv.org/abs/2004.12478)

Edward J. Hu, Adith Swaminathan, Hadi Salman, Greg Yang


## 2020-04-25

[Improved Adversarial Training via Learned Optimizer.](http://arxiv.org/abs/2004.12227)

Yuanhao Xiong, Cho-Jui Hsieh


[Enabling Fast and Universal Audio Adversarial Attack Using Generative Model.](http://arxiv.org/abs/2004.12261)

Yi Xie, Zhuohang Li, Cong Shi, Jian Liu, Yingying Chen, Bo Yuan


[Harnessing adversarial examples with a surprisingly simple defense.](http://arxiv.org/abs/2004.13013)

Ali Borji


## 2020-04-24

[Towards Characterizing Adversarial Defects of Deep Learning Software from the Lens of Uncertainty.](http://arxiv.org/abs/2004.11573)

Xiyue Zhang, Xiaofei Xie, Lei Ma, Xiaoning Du, Qiang Hu, Yang Liu, Jianjun Zhao, Meng Sun


[A Black-box Adversarial Attack Strategy with Adjustable Sparsity and Generalizability for Deep Image Classifiers.](http://arxiv.org/abs/2004.13002)

Arka Ghosh, Sankha Subhra Mullick, Shounak Datta, Swagatam Das, Rammohan Mallipeddi, Asit Kr. Das


[Reevaluating Adversarial Examples in Natural Language.](http://arxiv.org/abs/2004.14174)

John X. Morris, Eli Lifland, Jack Lanchantin, Yangfeng Ji, Yanjun Qi


## 2020-04-23

[Adversarial Machine Learning in Network Intrusion Detection Systems.](http://arxiv.org/abs/2004.11898)

Elie Alhajjar, Paul Maxwell, Nathaniel D. Bastian


[Adversarial Attacks and Defenses: An Interpretation Perspective.](http://arxiv.org/abs/2004.11488)

Ninghao Liu, Mengnan Du, Ruocheng Guo, Huan Liu, Xia Hu


[Evaluating Adversarial Robustness for Deep Neural Network Interpretability using fMRI Decoding.](http://arxiv.org/abs/2004.11114)

Patrick McClure, Dustin Moraczewski, Ka Chun Lam, Adam Thomas, Francisco Pereira


[On Adversarial Examples for Biomedical NLP Tasks.](http://arxiv.org/abs/2004.11157)

Vladimir Araujo, Andres Carvallo, Carlos Aspillaga, Denis Parra


[Ensemble Generative Cleaning with Feedback Loops for Defending Adversarial Attacks.](http://arxiv.org/abs/2004.11273)

Jianhe Yuan, Zhihai He


[Improved Noise and Attack Robustness for Semantic Segmentation by Using Multi-Task Training with Self-Supervised Depth Estimation.](http://arxiv.org/abs/2004.11072)

Marvin Klingner, Andreas Bär, Tim Fingscheidt


[RAIN: A Simple Approach for Robust and Accurate Image Classification Networks.](http://arxiv.org/abs/2004.14798)

Jiawei Du, Hanshu Yan, Vincent Y. F. Tan, Joey Tianyi Zhou, Rick Siow Mong Goh, Jiashi Feng


## 2020-04-22

[CodNN -- Robust Neural Networks From Coded Classification.](http://arxiv.org/abs/2004.10700)

Netanel Andrew Raviv, Siddharth Andrew Jain, Pulakesh Andrew Upadhyaya, Jehoshua Andrew Bruck, Andrew Anxiao, Jiang


[Provably robust deep generative models.](http://arxiv.org/abs/2004.10608)

Filipe Condessa, Zico Kolter


[QUANOS- Adversarial Noise Sensitivity Driven Hybrid Quantization of Neural Networks.](http://arxiv.org/abs/2004.11233)

Priyadarshini Panda


[Adversarial examples and where to find them.](http://arxiv.org/abs/2004.10882)

Niklas Risse, Christina Göpfert, Jan Philip Göpfert


## 2020-04-21

[Scalable Attack on Graph Data by Injecting Vicious Nodes.](http://arxiv.org/abs/2004.13825)

Jihong Wang, Minnan Luo, Fnu Suya, Jundong Li, Zijiang Yang, Qinghua Zheng


[Certifying Joint Adversarial Robustness for Model Ensembles.](http://arxiv.org/abs/2004.10250)

Mainuddin Ahmad Jonas, David Evans


[Probabilistic Safety for Bayesian Neural Networks.](http://arxiv.org/abs/2004.10281)

Matthew Wicker, Luca Laurenti, Andrea Patane, Marta Kwiatkowska


[BERT-ATTACK: Adversarial Attack Against BERT Using BERT.](http://arxiv.org/abs/2004.09984)

Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, Xipeng Qiu


[EMPIR: Ensembles of Mixed Precision Deep Networks for Increased Robustness against Adversarial Attacks.](http://arxiv.org/abs/2004.10162)

Sanchari Sen, Balaraman Ravindran, Anand Raghunathan


## 2020-04-20

[GraN: An Efficient Gradient-Norm Based Detector for Adversarial and Misclassified Examples.](http://arxiv.org/abs/2004.09179)

Julia Lust, Alexandru Paul Condurache


[Approximate exploitability: Learning a best response in large games. (74%)](http://arxiv.org/abs/2004.09677)

Finbarr Timbers, Nolan Bard, Edward Lockhart, Marc Lanctot, Martin Schmid, Neil Burch, Julian Schrittwieser, Thomas Hubert, Michael Bowling


## 2020-04-19

[Dynamic Knowledge Graph-based Dialogue Generation with Improved Adversarial Meta-Learning.](http://arxiv.org/abs/2004.08833)

Hongcai Xu, Junpeng Bao, Gaojie Zhang


[Adversarial Training for Large Neural Language Models.](http://arxiv.org/abs/2004.08994)

Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, Jianfeng Gao


[Headless Horseman: Adversarial Attacks on Transfer Learning Models.](http://arxiv.org/abs/2004.09007)

Ahmed Abdelkader, Michael J. Curry, Liam Fowl, Tom Goldstein, Avi Schwarzschild, Manli Shu, Christoph Studer, Chen Zhu


## 2020-04-18

[Protecting Classifiers From Attacks. A Bayesian Approach.](http://arxiv.org/abs/2004.08705)

Victor Gallego, Roi Naveiro, Alberto Redondo, David Rios Insua, Fabrizio Ruggeri


[Single-step Adversarial training with Dropout Scheduling.](http://arxiv.org/abs/2004.08628)

Vivek B. S., R. Venkatesh Babu


## 2020-04-17

[Adversarial Attack on Deep Learning-Based Splice Localization.](http://arxiv.org/abs/2004.08443)

Andras Rozsa, Zheng Zhong, Terrance E. Boult


## 2020-04-16

[Shortcut Learning in Deep Neural Networks.](http://arxiv.org/abs/2004.07780)

Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix A. Wichmann


## 2020-04-15

[Targeted Attack for Deep Hashing based Retrieval.](http://arxiv.org/abs/2004.07955)

Jiawang Bai, Bin Chen, Yiming Li, Dongxian Wu, Weiwei Guo, Shu-tao Xia, En-hui Yang


[A Framework for Enhancing Deep Neural Networks Against Adversarial Malware.](http://arxiv.org/abs/2004.07919)

Deqiang Li, Qianmu Li, Yanfang Ye, Shouhuai Xu


[Advanced Evasion Attacks and Mitigations on Practical ML-Based Phishing Website Classifiers.](http://arxiv.org/abs/2004.06954)

Yusi Lei, Sen Chen, Lingling Fan, Fu Song, Yang Liu


## 2020-04-14

[On the Optimal Interaction Range for Multi-Agent Systems Under Adversarial Attack.](http://arxiv.org/abs/2004.06562)

Saad J Saleh


[Extending Adversarial Attacks to Produce Adversarial Class Probability Distributions.](http://arxiv.org/abs/2004.06383)

Jon Vadillo, Roberto Santana, Jose A. Lozano


## 2020-04-13

[Adversarial Robustness Guarantees for Random Deep Neural Networks.](http://arxiv.org/abs/2004.05923)

Palma Giacomo De, Bobak T. Kiani, Seth Lloyd


[Frequency-Guided Word Substitutions for Detecting Textual Adversarial Examples.](http://arxiv.org/abs/2004.05887)

Maximilian Mozes, Pontus Stenetorp, Bennett Kleinberg, Lewis D. Griffin


[Adversarial Weight Perturbation Helps Robust Generalization.](http://arxiv.org/abs/2004.05884)

Dongxian Wu, Shu-tao Xia, Yisen Wang


[Adversarial Augmentation Policy Search for Domain and Cross-Lingual Generalization in Reading Comprehension.](http://arxiv.org/abs/2004.06076)

Adyasha Maharana, Mohit Bansal


[Towards Robust Classification with Image Quality Assessment.](http://arxiv.org/abs/2004.06288)

Yeli Feng, Yiyu Cai


[Towards Transferable Adversarial Attack against Deep Face Recognition.](http://arxiv.org/abs/2004.05790)

Yaoyao Zhong, Weihong Deng


## 2020-04-12

[PatchAttack: A Black-box Texture-based Attack with Reinforcement Learning.](http://arxiv.org/abs/2004.05682)

Chenglin Yang, Adam Kortylewski, Cihang Xie, Yinzhi Cao, Alan Yuille


## 2020-04-11

[Domain Adaptive Transfer Attack (DATA)-based Segmentation Networks for Building Extraction from Aerial Images.](http://arxiv.org/abs/2004.11819)

Younghwan Na, Jun Hee Kim, Kyungsu Lee, Juhum Park, Jae Youn Hwang, Jihwan P. Choi


[Certified Adversarial Robustness for Deep Reinforcement Learning.](http://arxiv.org/abs/2004.06496)

Michael Everett, Bjorn Lutjens, Jonathan P. How


[Robust Large-Margin Learning in Hyperbolic Space.](http://arxiv.org/abs/2004.05465)

Melanie Weber, Manzil Zaheer, Ankit Singh Rawat, Aditya Menon, Sanjiv Kumar


[Verification of Deep Convolutional Neural Networks Using ImageStars.](http://arxiv.org/abs/2004.05511)

Hoang-Dung Tran, Stanley Bak, Weiming Xiang, Taylor T. Johnson


## 2020-04-10

[Adversarial Attacks on Machine Learning Cybersecurity Defences in Industrial Control Systems.](http://arxiv.org/abs/2004.05005)

Eirini Anthi, Lowri Williams, Matilda Rhode, Pete Burnap, Adam Wedgbury


[Luring of transferable adversarial perturbations in the black-box paradigm.](http://arxiv.org/abs/2004.04919)

Rémi Bernhard, Pierre-Alain Moellic, Jean-Max Dutertre


## 2020-04-09

[Blind Adversarial Training: Balance Accuracy and Robustness.](http://arxiv.org/abs/2004.05914)

Haidong Xie, Xueshuang Xiang, Naijin Liu, Bin Dong


[Blind Adversarial Pruning: Balance Accuracy, Efficiency and Robustness.](http://arxiv.org/abs/2004.05913)

Haidong Xie, Lixin Qian, Xueshuang Xiang, Naijin Liu


[On Adversarial Examples and Stealth Attacks in Artificial Intelligence Systems.](http://arxiv.org/abs/2004.04479)

Ivan Y. Tyukin, Desmond J. Higham, Alexander N. Gorban


## 2020-04-08

[Transferable, Controllable, and Inconspicuous Adversarial Attacks on Person Re-identification With Deep Mis-Ranking.](http://arxiv.org/abs/2004.04199)

Hongjun Wang, Guangrun Wang, Ya Li, Dongyu Zhang, Liang Lin


## 2020-04-07

[Towards Evaluating the Robustness of Chinese BERT Classifiers.](http://arxiv.org/abs/2004.03742)

Boxin Wang, Boyuan Pan, Xin Li, Bo Li


[Feature Partitioning for Robust Tree Ensembles and their Certification in Adversarial Scenarios.](http://arxiv.org/abs/2004.03295)

Stefano Calzavara, Claudio Lucchese, Federico Marcuzzi, Salvatore Orlando


[Learning to fool the speaker recognition.](http://arxiv.org/abs/2004.03434)

Jiguo Li, Xinfeng Zhang, Jizheng Xu, Li Zhang, Yue Wang, Siwei Ma, Wen Gao


[Universal Adversarial Perturbations Generative Network for Speaker Recognition.](http://arxiv.org/abs/2004.03428)

Jiguo Li, Xinfeng Zhang, Chuanmin Jia, Jizheng Xu, Li Zhang, Yue Wang, Siwei Ma, Wen Gao


## 2020-04-05

[Approximate Manifold Defense Against Multiple Adversarial Perturbations.](http://arxiv.org/abs/2004.02183)

Jay Nandy, Wynne Hsu, Mong Li Lee


## 2020-04-04

[Understanding (Non-)Robust Feature Disentanglement and the Relationship Between Low- and High-Dimensional Adversarial Attacks.](http://arxiv.org/abs/2004.01903)

Zuowen Wang, Leo Horne


[BAE: BERT-based Adversarial Examples for Text Classification.](http://arxiv.org/abs/2004.01970)

Siddhant Garg, Goutham Ramakrishnan


## 2020-04-03

[Adversarial Robustness through Regularization: A Second-Order Approach.](http://arxiv.org/abs/2004.01832)

Avery Ma, Fartash Faghri, Amir-massoud Farahmand


## 2020-04-01

[Evading Deepfake-Image Detectors with White- and Black-Box Attacks.](http://arxiv.org/abs/2004.00622)

Nicholas Carlini, Hany Farid


[Towards Achieving Adversarial Robustness by Enforcing Feature Consistency Across Bit Planes.](http://arxiv.org/abs/2004.00306)

Sravanti Addepalli, Vivek B. S., Arya Baburaj, Gaurang Sriramanan, R. Venkatesh Babu


[Physically Realizable Adversarial Examples for LiDAR Object Detection.](http://arxiv.org/abs/2004.00543)

James Tu, Mengye Ren, Siva Manivasagam, Ming Liang, Bin Yang, Richard Du, Frank Cheng, Raquel Urtasun


## 2020-03-31

[A Thorough Comparison Study on Adversarial Attacks and Defenses for Common Thorax Disease Classification in Chest X-rays.](http://arxiv.org/abs/2003.13969)

Chendi Rao, Jiezhang Cao, Runhao Zeng, Qi Chen, Huazhu Fu, Yanwu Xu, Mingkui Tan


## 2020-03-30

[Characterizing Speech Adversarial Examples Using Self-Attention U-Net Enhancement.](http://arxiv.org/abs/2003.13917)

Chao-Han Huck Yang, Jun Qi, Pin-Yu Chen, Xiaoli Ma, Chin-Hui Lee


[Adversarial Attacks on Multivariate Time Series.](http://arxiv.org/abs/2004.00410)

Samuel Harford, Fazle Karim, Houshang Darabi


[Improved Gradient based Adversarial Attacks for Quantized Networks.](http://arxiv.org/abs/2003.13511)

Kartik Gupta, Thalaiyasingam Ajanthan


[Towards Deep Learning Models Resistant to Large Perturbations.](http://arxiv.org/abs/2003.13370)

Amirreza Shaeiri, Rozhin Nobahari, Mohammad Hossein Rohban


[Efficient Black-box Optimization of Adversarial Windows Malware with Constrained Manipulations.](http://arxiv.org/abs/2003.13526)

Luca Demetrio, Battista Biggio, Giovanni Lagorio, Fabio Roli, Alessandro Armando


## 2020-03-28

[Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning.](http://arxiv.org/abs/2003.12862)

Tianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, Zhangyang Wang


[DaST: Data-free Substitute Training for Adversarial Attacks.](http://arxiv.org/abs/2003.12703)

Mingyi Zhou, Jing Wu, Yipeng Liu, Shuaicheng Liu, Ce Zhu


[Adversarial Imitation Attack.](http://arxiv.org/abs/2003.12760)

Mingyi Zhou, Jing Wu, Yipeng Liu, Shuaicheng Liu, Xiang Zhang, Ce Zhu


## 2020-03-26

[Do Deep Minds Think Alike? Selective Adversarial Attacks for Fine-Grained Manipulation of Multiple Deep Neural Networks.](http://arxiv.org/abs/2003.11816)

Zain Khan, Jirong Yi, Raghu Mudumbai, Xiaodong Wu, Weiyu Xu


[Challenging the adversarial robustness of DNNs based on error-correcting output codes.](http://arxiv.org/abs/2003.11855)

Bowen Zhang, Benedetta Tondi, Xixiang Lv, Mauro Barni


## 2020-03-25

[Plausible Counterfactuals: Auditing Deep Learning Classifiers with Realistic Adversarial Examples.](http://arxiv.org/abs/2003.11323)

Alejandro Barredo-Arrieta, Ser Javier Del


## 2020-03-24

[Adversarial Light Projection Attacks on Face Recognition Systems: A Feasibility Study.](http://arxiv.org/abs/2003.11145)

Luan Nguyen, Sunpreet S. Arora, Yuhang Wu, Hao Yang


## 2020-03-23

[Defense Through Diverse Directions.](http://arxiv.org/abs/2003.10602)

Christopher M. Bender, Yang Li, Yifeng Shi, Michael K. Reiter, Junier B. Oliva


[Adversarial Attacks on Monocular Depth Estimation.](http://arxiv.org/abs/2003.10315)

Ziqi Zhang, Xinge Zhu, Yingwei Li, Xiangqun Chen, Yao Guo


[Inherent Adversarial Robustness of Deep Spiking Neural Networks: Effects of Discrete Input Encoding and Non-Linear Activations.](http://arxiv.org/abs/2003.10399)

Saima Sharmin, Nitin Rathi, Priyadarshini Panda, Kaushik Roy


[Adversarial Perturbations Fool Deepfake Detectors.](http://arxiv.org/abs/2003.10596)

Apurva Gandhi, Shomik Jain


## 2020-03-22

[Understanding the robustness of deep neural network classifiers for breast cancer screening.](http://arxiv.org/abs/2003.10041)

Witold Oleszkiewicz, Taro Makino, Stanisław Jastrzębski, Tomasz Trzciński, Linda Moy, Kyunghyun Cho, Laura Heacock, Krzysztof J. Geras


[Architectural Resilience to Foreground-and-Background Adversarial Noise.](http://arxiv.org/abs/2003.10045)

Carl Cheng, Evan Hu


## 2020-03-21

[Detecting Adversarial Examples in Learning-Enabled Cyber-Physical Systems using Variational Autoencoder for Regression.](http://arxiv.org/abs/2003.10804)

Feiyang Cai, Jiani Li, Xenofon Koutsoukos


[Robust Out-of-distribution Detection in Neural Networks.](http://arxiv.org/abs/2003.09711)

Jiefeng Chen, Yixuan Li, Xi Wu, Yingyu Liang, Somesh Jha


[Cooling-Shrinking Attack: Blinding the Tracker with Imperceptible Noises.](http://arxiv.org/abs/2003.09595)

Bin Yan, Dong Wang, Huchuan Lu, Xiaoyun Yang


## 2020-03-20

[Adversarial Examples and the Deeper Riddle of Induction: The Need for a Theory of Artifacts in Deep Learning.](http://arxiv.org/abs/2003.11917)

Cameron Buckner


[Investigating Image Applications Based on Spatial-Frequency Transform and Deep Learning Techniques.](http://arxiv.org/abs/2004.02756)

Qinkai Zheng, Han Qiu, Gerard Memmi, Isabelle Bloch


[Quantum noise protects quantum classifiers against adversaries.](http://arxiv.org/abs/2003.09416)

Yuxuan Du, Min-Hsiu Hsieh, Tongliang Liu, Dacheng Tao, Nana Liu


[One Neuron to Fool Them All.](http://arxiv.org/abs/2003.09372)

Anshuman Suri, David Evans


[Adversarial Robustness on In- and Out-Distribution Improves Explainability.](http://arxiv.org/abs/2003.09461)

Maximilian Augustin, Alexander Meinke, Matthias Hein


## 2020-03-19

[Breaking certified defenses: Semantic adversarial examples with spoofed robustness certificates.](http://arxiv.org/abs/2003.08937)

Amin Ghiasi, Ali Shafahi, Tom Goldstein


[Face-Off: Adversarial Face Obfuscation.](http://arxiv.org/abs/2003.08861)

Varun Chandrasekaran, Chuhan Gao, Brian Tang, Kassem Fawaz, Somesh Jha, Suman Banerjee


[Robust Deep Reinforcement Learning against Adversarial Perturbations on State Observations.](http://arxiv.org/abs/2003.08938)

Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, Cho-Jui Hsieh


[Overinterpretation reveals image classification model pathologies. (81%)](http://arxiv.org/abs/2003.08907)

Brandon Carter, Siddhartha Jain, Jonas Mueller, David Gifford


## 2020-03-18

[Vulnerabilities of Connectionist AI Applications: Evaluation and Defence.](http://arxiv.org/abs/2003.08837)

Christian Berghoff, Matthias Neu, Twickel Arndt von


[Generating Socially Acceptable Perturbations for Efficient Evaluation of Autonomous Vehicles.](http://arxiv.org/abs/2003.08034)

Songan Zhang, Huei Peng, Subramanya Nageshrao, H. Eric Tseng


[Solving Non-Convex Non-Differentiable Min-Max Games using Proximal Gradient Method.](http://arxiv.org/abs/2003.08093)

Babak Barazandeh, Meisam Razaviyayn


[SAT: Improving Adversarial Training via Curriculum-Based Loss Smoothing.](http://arxiv.org/abs/2003.09347)

Chawin Sitawarin, Supriyo Chakraborty, David Wagner


## 2020-03-17

[Motion-Excited Sampler: Video Adversarial Attack with Sparked Prior.](http://arxiv.org/abs/2003.07637)

Hu Zhang, Linchao Zhu, Yi Zhu, Yi Yang


[Heat and Blur: An Effective and Fast Defense Against Adversarial Examples.](http://arxiv.org/abs/2003.07573)

Haya Brama, Tal Grinshpoun


[Adversarial Transferability in Wearable Sensor Systems.](http://arxiv.org/abs/2003.07982)

Ramesh Kumar Sah, Hassan Ghasemzadeh


## 2020-03-15

[Output Diversified Initialization for Adversarial Attacks.](http://arxiv.org/abs/2003.06878)

Yusuke Tashiro, Yang Song, Stefano Ermon


[Anomalous Example Detection in Deep Learning: A Survey.](http://arxiv.org/abs/2003.06979)

Saikiran Bulusu, Bhavya Kailkhura, Bo Li, Pramod K. Varshney, Dawn Song


[Towards Face Encryption by Generating Adversarial Identity Masks.](http://arxiv.org/abs/2003.06814)

Xiao Yang, Yinpeng Dong, Tianyu Pang, Hang Su, Jun Zhu, Yuefeng Chen, Hui Xue


[Toward Adversarial Robustness via Semi-supervised Robust Training.](http://arxiv.org/abs/2003.06974)

Yiming Li, Baoyuan Wu, Yan Feng, Yanbo Fan, Yong Jiang, Zhifeng Li, Shutao Xia


## 2020-03-14

[Minimum-Norm Adversarial Examples on KNN and KNN-Based Models.](http://arxiv.org/abs/2003.06559)

Chawin Sitawarin, David Wagner


[Certified Defenses for Adversarial Patches.](http://arxiv.org/abs/2003.06693)

Ping-Yeh Chiang, Renkun Ni, Ahmed Abdelkader, Chen Zhu, Christoph Studer, Tom Goldstein


[Dynamic Divide-and-Conquer Adversarial Training for Robust Semantic Segmentation.](http://arxiv.org/abs/2003.06555)

Xiaogang Xu, Hengshuang Zhao, Jiaya Jia


[On the benefits of defining vicinal distributions in latent space.](http://arxiv.org/abs/2003.06566)

Puneet Mangla, Vedant Singh, Shreyas Jayant Havaldar, Vineeth N Balasubramanian


## 2020-03-13

[Towards a Resilient Machine Learning Classifier -- a Case Study of Ransomware Detection.](http://arxiv.org/abs/2003.06428)

Chih-Yuan Yang, Ravi Sahita


[GeoDA: a geometric framework for black-box adversarial attacks.](http://arxiv.org/abs/2003.06468)

Ali Rahmati, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard, Huaiyu Dai


[When are Non-Parametric Methods Robust?](http://arxiv.org/abs/2003.06121)

Robi Bhattacharjee, Kamalika Chaudhuri


## 2020-03-12

[Topological Effects on Attacks Against Vertex Classification.](http://arxiv.org/abs/2003.05822)

Benjamin A. Miller, Mustafa Çamurcu, Alexander J. Gomez, Kevin Chan, Tina Eliassi-Rad


[Inline Detection of DGA Domains Using Side Information.](http://arxiv.org/abs/2003.05703)

Raaghavi Sivaguru, Jonathan Peck, Femi Olumofin, Anderson Nascimento, Cock Martine De


[ARAE: Adversarially Robust Training of Autoencoders Improves Novelty Detection.](http://arxiv.org/abs/2003.05669)

Mohammadreza Salehi, Atrin Arya, Barbod Pajoum, Mohammad Otoofi, Amirreza Shaeiri, Mohammad Hossein Rohban, Hamid R. Rabiee


[ConAML: Constrained Adversarial Machine Learning for Cyber-Physical Systems.](http://arxiv.org/abs/2003.05631)

Jiangnan Li, Yingyuan Yang, Jinyuan Stella Sun, Kevin Tomsovic, Hairong Qi


## 2020-03-11

[Frequency-Tuned Universal Adversarial Attacks.](http://arxiv.org/abs/2003.05549)

Yingpeng Deng, Lina J. Karam


## 2020-03-10

[SAD: Saliency-based Defenses Against Adversarial Examples.](http://arxiv.org/abs/2003.04820)

Richard Tran, David Patrick, Michael Geyer, Amanda Fernandez


[Using an ensemble color space model to tackle adversarial examples.](http://arxiv.org/abs/2003.05005)

Shreyank N Gowda, Chun Yuan


[Cryptanalytic Extraction of Neural Network Models.](http://arxiv.org/abs/2003.04884)

Nicholas Carlini, Matthew Jagielski, Ilya Mironov


[A Survey of Adversarial Learning on Graphs.](http://arxiv.org/abs/2003.05730)

Liang Chen, Jintang Li, Jiaying Peng, Tao Xie, Zengxu Cao, Kun Xu, Xiangnan He, Zibin Zheng


## 2020-03-09

[Domain Adaptation with Conditional Distribution Matching and Generalized Label Shift.](http://arxiv.org/abs/2003.04475)

Remi Tachet des Combes, Han Zhao, Yu-Xiang Wang, Geoff Gordon


[Towards Probabilistic Verification of Machine Unlearning.](http://arxiv.org/abs/2003.04247)

David Marco Sommer, Liwei Song, Sameer Wagh, Prateek Mittal


[Manifold Regularization for Locally Stable Deep Neural Networks.](http://arxiv.org/abs/2003.04286)

Charles Jin, Martin Rinard


[Generating Natural Language Adversarial Examples on a Large Scale with Generative Models.](http://arxiv.org/abs/2003.10388)

Yankun Ren, Jianbin Lin, Siliang Tang, Jun Zhou, Shuang Yang, Yuan Qi, Xiang Ren


[Gradient-based adversarial attacks on categorical sequence models via traversing an embedded world.](http://arxiv.org/abs/2003.04173)

Ivan Fursov, Alexey Zaytsev, Nikita Kluchnikov, Andrey Kravchenko, Evgeny Burnaev


## 2020-03-08

[Security of Distributed Machine Learning: A Game-Theoretic Approach to Design Secure DSVM.](http://arxiv.org/abs/2003.04735)

Rui Zhang, Quanyan Zhu


[An Empirical Evaluation on Robustness and Uncertainty of Regularization Methods.](http://arxiv.org/abs/2003.03879)

Sanghyuk Chun, Seong Joon Oh, Sangdoo Yun, Dongyoon Han, Junsuk Choe, Youngjoon Yoo


[On the Robustness of Cooperative Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2003.03722)

Jieyu Lin, Kristina Dzeparoska, Sai Qian Zhang, Alberto Leon-Garcia, Nicolas Papernot


[Adversarial Attacks on Probabilistic Autoregressive Forecasting Models.](http://arxiv.org/abs/2003.03778)

Raphaël Dang-Nhu, Gagandeep Singh, Pavol Bielik, Martin Vechev


[Adversarial Camouflage: Hiding Physical-World Attacks with Natural Styles.](http://arxiv.org/abs/2003.08757)

Ranjie Duan, Xingjun Ma, Yisen Wang, James Bailey, A. K. Qin, Yun Yang


[No Surprises: Training Robust Lung Nodule Detection for Low-Dose CT Scans by Augmenting with Adversarial Attacks.](http://arxiv.org/abs/2003.03824)

Siqi Liu, Arnaud Arindra Adiyoso Setio, Florin C. Ghesu, Eli Gibson, Sasa Grbic, Bogdan Georgescu, Dorin Comaniciu


## 2020-03-07

[Dynamic Backdoor Attacks Against Machine Learning Models.](http://arxiv.org/abs/2003.03675)

Ahmed Salem, Rui Wen, Michael Backes, Shiqing Ma, Yang Zhang


## 2020-03-06

[Defense against adversarial attacks on spoofing countermeasures of ASV.](http://arxiv.org/abs/2003.03065)

Haibin Wu, Songxiang Liu, Helen Meng, Hung-yi Lee


[Triple Memory Networks: a Brain-Inspired Method for Continual Learning.](http://arxiv.org/abs/2003.03143)

Liyuan Wang, Bo Lei, Qian Li, Hang Su, Jun Zhu, Yi Zhong


[MAB-Malware: A Reinforcement Learning Framework for Attacking Static Malware Classifiers.](http://arxiv.org/abs/2003.03100)

Wei Song, Xuezixiang Li, Sadia Afroz, Deepali Garg, Dmitry Kuznetsov, Heng Yin


## 2020-03-05

[Towards Practical Lottery Ticket Hypothesis for Adversarial Training.](http://arxiv.org/abs/2003.05733)

Bai Li, Shiqi Wang, Yunhan Jia, Yantao Lu, Zhenyu Zhong, Lawrence Carin, Suman Jana


[Exploiting Verified Neural Networks via Floating Point Numerical Error.](http://arxiv.org/abs/2003.03021)

Kai Jia, Martin Rinard


[Detection and Recovery of Adversarial Attacks with Injected Attractors.](http://arxiv.org/abs/2003.02732)

Jiyi Zhang, Ee-Chien Chang, Hwee Kuan Lee


[Adversarial Robustness Through Local Lipschitzness.](http://arxiv.org/abs/2003.02460)

Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Ruslan Salakhutdinov, Kamalika Chaudhuri


[Adversarial Vertex Mixup: Toward Better Adversarially Robust Generalization.](http://arxiv.org/abs/2003.02484)

Saehyung Lee, Hyungyu Lee, Sungroh Yoon


[Search Space of Adversarial Perturbations against Image Filters.](http://arxiv.org/abs/2003.02750)

Dang Duy Thang, Toshihiro Matsui


## 2020-03-04

[Real-time, Universal, and Robust Adversarial Attacks Against Speaker Recognition Systems.](http://arxiv.org/abs/2003.02301)

Yi Xie, Cong Shi, Zhuohang Li, Jian Liu, Yingying Chen, Bo Yuan


[Colored Noise Injection for Training Adversarially Robust Neural Networks.](http://arxiv.org/abs/2003.02188)

Evgenii Zheltonozhskii, Chaim Baskin, Yaniv Nemcovsky, Brian Chmiel, Avi Mendelson, Alex M. Bronstein


[Double Backpropagation for Training Autoencoders against Adversarial Attack.](http://arxiv.org/abs/2003.01895)

Chengjin Sun, Sizhe Chen, Xiaolin Huang


[Black-box Smoothing: A Provable Defense for Pretrained Classifiers.](http://arxiv.org/abs/2003.01908)

Hadi Salman, Mingjie Sun, Greg Yang, Ashish Kapoor, J. Zico Kolter


[Metrics and methods for robustness evaluation of neural networks with generative models.](http://arxiv.org/abs/2003.01993)

Igor Buzhinsky, Arseny Nerinovsky, Stavros Tripakis


## 2020-03-03

[Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks.](http://arxiv.org/abs/2003.01690)

Francesco Croce, Matthias Hein


[Analyzing Accuracy Loss in Randomized Smoothing Defenses.](http://arxiv.org/abs/2003.01595)

Yue Gao, Harrison Rosenberg, Kassem Fawaz, Somesh Jha, Justin Hsu


[Discriminative Multi-level Reconstruction under Compact Latent Space for One-Class Novelty Detection.](http://arxiv.org/abs/2003.01665)

Jaewoo Park, Yoon Gyo Jung, Andrew Beng Jin Teoh


[Security of Deep Learning based Lane Keeping System under Physical-World Adversarial Attack.](http://arxiv.org/abs/2003.01782)

Takami Sato, Junjie Shen, Ningfei Wang, Yunhan Jack Jia, Xue Lin, Qi Alfred Chen


[Type I Attack for Generative Models.](http://arxiv.org/abs/2003.01872)

Chengjin Sun, Sizhe Chen, Jia Cai, Xiaolin Huang


## 2020-03-02

[Data-Free Adversarial Perturbations for Practical Black-Box Attack.](http://arxiv.org/abs/2003.01295)

ZhaoXin Huan, Yulong Wang, Xiaolu Zhang, Lin Shang, Chilin Fu, Jun Zhou


[Learn2Perturb: an End-to-end Feature Perturbation Learning to Improve Adversarial Robustness.](http://arxiv.org/abs/2003.01090)

Ahmadreza Jeddi, Mohammad Javad Shafiee, Michelle Karg, Christian Scharfenberger, Alexander Wong


[Disrupting Deepfakes: Adversarial Attacks Against Conditional Image Translation Networks and Facial Manipulation Systems.](http://arxiv.org/abs/2003.01279)

Nataniel Ruiz, Sarah Adel Bargal, Stan Sclaroff


[Hidden Cost of Randomized Smoothing.](http://arxiv.org/abs/2003.01249)

Jeet Lily Mohapatra, Ching-Yun Lily Ko, Lily Tsui-Wei, Weng, Sijia Liu, Pin-Yu Chen, Luca Daniel


[Adversarial Network Traffic: Towards Evaluating the Robustness of Deep Learning-Based Network Traffic Classification.](http://arxiv.org/abs/2003.01261)

Amir Mahdi Sadeghzadeh, Saeed Shiravi, Rasool Jalili


## 2020-03-01

[Adversarial Attacks and Defenses on Graphs: A Review, A Tool and Empirical Studies.](http://arxiv.org/abs/2003.00653)

Wei Jin, Yaxin Li, Han Xu, Yiqi Wang, Shuiwang Ji, Charu Aggarwal, Jiliang Tang


## 2020-02-29

[Understanding the Intrinsic Robustness of Image Distributions using Conditional Generative Models.](http://arxiv.org/abs/2003.00378)

Xiao Zhang, Jinghui Chen, Quanquan Gu, David Evans


[Why is the Mahalanobis Distance Effective for Anomaly Detection?](http://arxiv.org/abs/2003.00402)

Ryo Kamoi, Kei Kobayashi


## 2020-02-28

[Improving Certified Robustness via Statistical Learning with Logical Reasoning.](http://arxiv.org/abs/2003.00120)

Zhuolin Yang, Zhikuan Zhao, Boxin Wang, Jiawei Zhang, Linyi Li, Hengzhi Pei, Bojan Karlas, Ji Liu, Heng Guo, Ce Zhang, Bo Li


[Applying Tensor Decomposition to image for Robustness against Adversarial Attack.](http://arxiv.org/abs/2002.12913)

Seungju Cho, Tae Joon Jun, Mingu Kang, Daeyoung Kim


## 2020-02-27

[Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT.](http://arxiv.org/abs/2003.04985)

Lichao Sun, Kazuma Hashimoto, Wenpeng Yin, Akari Asai, Jia Li, Philip Yu, Caiming Xiong


[Detecting Patch Adversarial Attacks with Image Residuals.](http://arxiv.org/abs/2002.12504)

Marius Arvinte, Ahmed Tewfik, Sriram Vishwanath


[Certified Defense to Image Transformations via Randomized Smoothing.](http://arxiv.org/abs/2002.12463)

Marc Fischer, Maximilian Baader, Martin Vechev


[Are L2 adversarial examples intrinsically different?](http://arxiv.org/abs/2002.12527)

Mingxuan Li, Jingyuan Wang, Yufan Wu


[TSS: Transformation-Specific Smoothing for Robustness Certification.](http://arxiv.org/abs/2002.12398)

Linyi Li, Maurice Weber, Xiaojun Xu, Luka Rimanic, Bhavya Kailkhura, Tao Xie, Ce Zhang, Bo Li


[On Isometry Robustness of Deep 3D Point Cloud Models under Adversarial Attacks.](http://arxiv.org/abs/2002.12222)

Yue Zhao, Yuwei Wu, Caihua Chen, Andrew Lim


[Utilizing Network Properties to Detect Erroneous Inputs.](http://arxiv.org/abs/2002.12520)

Matt Gorbett, Nathaniel Blanchard


[FMix: Enhancing Mixed Sample Data Augmentation. (22%)](http://arxiv.org/abs/2002.12047)

Ethan Harris, Antonia Marcu, Matthew Painter, Mahesan Niranjan, Adam Prügel-Bennett, Jonathon Hare


## 2020-02-26

[Revisiting Ensembles in an Adversarial Context: Improving Natural Accuracy.](http://arxiv.org/abs/2002.11572)

Aditya Saligrama, Guillaume Leclerc


[Invariance vs. Robustness of Neural Networks.](http://arxiv.org/abs/2002.11318)

Sandesh Kamath, Amit Deshpande, K V Subrahmanyam


[Overfitting in adversarially robust deep learning.](http://arxiv.org/abs/2002.11569)

Leslie Rice, Eric Wong, J. Zico Kolter


[MGA: Momentum Gradient Attack on Network.](http://arxiv.org/abs/2002.11320)

Jinyin Chen, Yixian Chen, Haibin Zheng, Shijing Shen, Shanqing Yu, Dan Zhang, Qi Xuan


[Improving Robustness of Deep-Learning-Based Image Reconstruction.](http://arxiv.org/abs/2002.11821)

Ankit Raj, Yoram Bresler, Bo Li


[Defense-PointNet: Protecting PointNet Against Adversarial Attacks.](http://arxiv.org/abs/2002.11881)

Yu Zhang, Gongbo Liang, Tawfiq Salem, Nathan Jacobs


[Adversarial Attack on Deep Product Quantization Network for Image Retrieval.](http://arxiv.org/abs/2002.11374)

Yan Feng, Bin Chen, Tao Dai, Shutao Xia


[Randomization matters. How to defend against strong adversarial attacks.](http://arxiv.org/abs/2002.11565)

Rafael Pinot, Raphael Ettedgui, Geovani Rizk, Yann Chevaleyre, Jamal Atif


[Learning Adversarially Robust Representations via Worst-Case Mutual Information Maximization.](http://arxiv.org/abs/2002.11798)

Sicheng Zhu, Xiao Zhang, David Evans


## 2020-02-25

[Understanding and Mitigating the Tradeoff Between Robustness and Accuracy.](http://arxiv.org/abs/2002.10716)

Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John Duchi, Percy Liang


[The Curious Case of Adversarially Robust Models: More Data Can Help, Double Descend, or Hurt Generalization.](http://arxiv.org/abs/2002.11080)

Yifei Min, Lin Chen, Amin Karbasi


[G\"odel's Sentence Is An Adversarial Example But Unsolvable.](http://arxiv.org/abs/2002.10703)

Xiaodong Qi, Lansheng Han


[Towards an Efficient and General Framework of Robust Training for Graph Neural Networks.](http://arxiv.org/abs/2002.10947)

Kaidi Xu, Sijia Liu, Pin-Yu Chen, Mengshu Sun, Caiwen Ding, Bhavya Kailkhura, Xue Lin


[(De)Randomized Smoothing for Certifiable Defense against Patch Attacks.](http://arxiv.org/abs/2002.10733)

Alexander Levine, Soheil Feizi


[Attacks Which Do Not Kill Training Make Adversarial Learning Stronger.](http://arxiv.org/abs/2002.11242)

Jingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, Mohan Kankanhalli


[Adversarial Ranking Attack and Defense.](http://arxiv.org/abs/2002.11293)

Mo Zhou, Zhenxing Niu, Le Wang, Qilin Zhang, Gang Hua


## 2020-02-24

[A Model-Based Derivative-Free Approach to Black-Box Adversarial Examples: BOBYQA.](http://arxiv.org/abs/2002.10349)

Giuseppe Ughi, Vinayak Abrol, Jared Tanner


[Utilizing a null class to restrict decision spaces and defend against neural network adversarial attacks.](http://arxiv.org/abs/2002.10084)

Matthew J. Roos


[Adversarial Perturbations Prevail in the Y-Channel of the YCbCr Color Space.](http://arxiv.org/abs/2003.00883)

Camilo Pestana, Naveed Akhtar, Wei Liu, David Glance, Ajmal Mian


[Towards Rapid and Robust Adversarial Training with One-Step Attacks.](http://arxiv.org/abs/2002.10097)

Leo Schwinn, René Raab, Björn Eskofier


[Precise Tradeoffs in Adversarial Training for Linear Regression.](http://arxiv.org/abs/2002.10477)

Adel Javanmard, Mahdi Soltanolkotabi, Hamed Hassani


[HYDRA: Pruning Adversarially Robust Neural Networks.](http://arxiv.org/abs/2002.10509)

Vikash Sehwag, Shiqi Wang, Prateek Mittal, Suman Jana


## 2020-02-23

[Adversarial Attack on DL-based Massive MIMO CSI Feedback.](http://arxiv.org/abs/2002.09896)

Qing Liu, Jiajia Guo, Chao-Kai Wen, Shi Jin


[Triple Wins: Boosting Accuracy, Robustness and Efficiency Together by Enabling Input-Adaptive Inference.](http://arxiv.org/abs/2002.10025)

Ting-Kuei Hu, Tianlong Chen, Haotao Wang, Zhangyang Wang


## 2020-02-22

[Non-Intrusive Detection of Adversarial Deep Learning Attacks via Observer Networks.](http://arxiv.org/abs/2002.09772)

Kirthi Shankar Sivamani, Rajeev Sahay, Aly El Gamal


[Temporal Sparse Adversarial Attack on Sequence-based Gait Recognition.](http://arxiv.org/abs/2002.09674)

Ziwen He, Wei Wang, Jing Dong, Tieniu Tan


[Real-Time Detectors for Digital and Physical Adversarial Inputs to Perception Systems.](http://arxiv.org/abs/2002.09792)

Yiannis Kantaros, Taylor Carpenter, Kaustubh Sridhar, Yahan Yang, Insup Lee, James Weimer


[Using Single-Step Adversarial Training to Defend Iterative Adversarial Examples.](http://arxiv.org/abs/2002.09632)

Guanxiong Liu, Issa Khalil, Abdallah Khreishah


## 2020-02-21

[Polarizing Front Ends for Robust CNNs.](http://arxiv.org/abs/2002.09580)

Can Bakiskan, Soorya Gopalakrishnan, Metehan Cekic, Upamanyu Madhow, Ramtin Pedarsani


[Robustness from Simple Classifiers.](http://arxiv.org/abs/2002.09422)

Sharon Qian, Dimitris Kalimeris, Gal Kaplun, Yaron Singer


[Adversarial Detection and Correction by Matching Prediction Distributions.](http://arxiv.org/abs/2002.09364)

Giovanni Vacanti, Looveren Arnaud Van


[UnMask: Adversarial Detection and Defense Through Robust Feature Alignment.](http://arxiv.org/abs/2002.09576)

Scott Freitas, Shang-Tse Chen, Zijie J. Wang, Duen Horng Chau


[Robustness to Programmable String Transformations via Augmented Abstract Training.](http://arxiv.org/abs/2002.09579)

Yuhao Zhang, Aws Albarghouthi, Loris D'Antoni


[Black-Box Certification with Randomized Smoothing: A Functional Optimization Based Framework.](http://arxiv.org/abs/2002.09169)

Dinghuai Zhang, Mao Ye, Chengyue Gong, Zhanxing Zhu, Qiang Liu


[Adversarial Attacks on Machine Learning Systems for High-Frequency Trading.](http://arxiv.org/abs/2002.09565)

Micah Goldblum, Avi Schwarzschild, Ankit B. Patel, Tom Goldstein


## 2020-02-20

[Enhanced Adversarial Strategically-Timed Attacks against Deep Reinforcement Learning.](http://arxiv.org/abs/2002.09027)

Chao-Han Huck Yang, Jun Qi, Pin-Yu Chen, Yi Ouyang, I-Te Danny Hung, Chin-Hui Lee, Xiaoli Ma


[On the Decision Boundaries of Deep Neural Networks: A Tropical Geometry Perspective.](http://arxiv.org/abs/2002.08838)

Motasem Alfarra, Adel Bibi, Hasan Hammoud, Mohamed Gaafar, Bernard Ghanem


[A Bayes-Optimal View on Adversarial Examples.](http://arxiv.org/abs/2002.08859)

Eitan Richardson, Yair Weiss


[Towards Certifiable Adversarial Sample Detection.](http://arxiv.org/abs/2002.08740)

Ilia Shumailov, Yiren Zhao, Robert Mullins, Ross Anderson


[Boosting Adversarial Training with Hypersphere Embedding.](http://arxiv.org/abs/2002.08619)

Tianyu Pang, Xiao Yang, Yinpeng Dong, Kun Xu, Hang Su, Jun Zhu


[Byzantine-resilient Decentralized Stochastic Gradient Descent. (5%)](http://arxiv.org/abs/2002.08569)

Shangwei Guo, Tianwei Zhang, Han Yu, Xiaofei Xie, Lei Ma, Tao Xiang, Yang Liu


## 2020-02-19

[Bayes-TrEx: Model Transparency by Example.](http://arxiv.org/abs/2002.10248)

Serena Booth, Yilun Zhou, Ankit Shah, Julie Shah


[AdvMS: A Multi-source Multi-cost Defense Against Adversarial Attacks.](http://arxiv.org/abs/2002.08439)

Xiao Wang, Siyue Wang, Pin-Yu Chen, Xue Lin, Peter Chin


[NAttack! Adversarial Attacks to bypass a GAN based classifier trained to detect Network intrusion.](http://arxiv.org/abs/2002.08527)

Aritran Piplai, Sai Sree Laya Chukkapalli, Anupam Joshi


[On Adaptive Attacks to Adversarial Example Defenses.](http://arxiv.org/abs/2002.08347)

Florian Tramer, Nicholas Carlini, Wieland Brendel, Aleksander Madry


[Indirect Adversarial Attacks via Poisoning Neighbors for Graph Convolutional Networks.](http://arxiv.org/abs/2002.08012)

Tsubasa Takahashi


[Randomized Smoothing of All Shapes and Sizes.](http://arxiv.org/abs/2002.08118)

Greg Yang, Tony Duan, J. Edward Hu, Hadi Salman, Ilya Razenshteyn, Jerry Li


## 2020-02-18

[Action-Manipulation Attacks Against Stochastic Bandits: Attacks and Defense.](http://arxiv.org/abs/2002.08000)

Guanlin Liu, Lifeng lai


[Deflecting Adversarial Attacks.](http://arxiv.org/abs/2002.07405)

Yao Qin, Nicholas Frosst, Colin Raffel, Garrison Cottrell, Geoffrey Hinton


[Towards Query-Efficient Black-Box Adversary with Zeroth-Order Natural Gradient Descent.](http://arxiv.org/abs/2002.07891)

Pu Zhao, Pin-Yu Chen, Siyue Wang, Xue Lin


[Block Switching: A Stochastic Approach for Deep Learning Security.](http://arxiv.org/abs/2002.07920)

Xiao Wang, Siyue Wang, Pin-Yu Chen, Xue Lin, Peter Chin


## 2020-02-17

[TensorShield: Tensor-based Defense Against Adversarial Attacks on Images.](http://arxiv.org/abs/2002.10252)

Negin Entezari, Evangelos E. Papalexakis


[On the Similarity of Deep Learning Representations Across Didactic and Adversarial Examples.](http://arxiv.org/abs/2002.06816)

Pamela K. Douglas, Farzad Vasheghani Farahani


[Scalable Quantitative Verification For Deep Neural Networks.](http://arxiv.org/abs/2002.06864)

Teodora Baluta, Zheng Leong Chua, Kuldeep S. Meel, Prateek Saxena


[CAT: Customized Adversarial Training for Improved Robustness.](http://arxiv.org/abs/2002.06789)

Minhao Cheng, Qi Lei, Pin-Yu Chen, Inderjit Dhillon, Cho-Jui Hsieh


[On the Matrix-Free Generation of Adversarial Perturbations for Black-Box Attacks.](http://arxiv.org/abs/2002.07317)

Hisaichi Shibata, Shouhei Hanaoka, Yukihiro Nomura, Naoto Hayashi, Osamu Abe


[Robust Stochastic Bandit Algorithms under Probabilistic Unbounded Adversarial Attack.](http://arxiv.org/abs/2002.07214)

Ziwei Guan, Kaiyi Ji, Donald J Jr Bucci, Timothy Y Hu, Joseph Palombo, Michael Liston, Yingbin Liang


[Regularized Training and Tight Certification for Randomized Smoothed Classifier with Provable Robustness.](http://arxiv.org/abs/2002.07246)

Huijie Feng, Chunpeng Wu, Guoyang Chen, Weifeng Zhang, Yang Ning


[GRAPHITE: A Practical Framework for Generating Automatic Physical Adversarial Machine Learning Attacks.](http://arxiv.org/abs/2002.07088)

Ryan Feng, Neal Mangaokar, Jiefeng Chen, Earlence Fernandes, Somesh Jha, Atul Prakash


## 2020-02-16

[Over-parameterized Adversarial Training: An Analysis Overcoming the Curse of Dimensionality.](http://arxiv.org/abs/2002.06668)

Yi Zhang, Orestis Plevrakis, Simon S. Du, Xingguo Li, Zhao Song, Sanjeev Arora


## 2020-02-15

[Undersensitivity in Neural Reading Comprehension.](http://arxiv.org/abs/2003.04808)

Johannes Welbl, Pasquale Minervini, Max Bartolo, Pontus Stenetorp, Sebastian Riedel


[Hold me tight! Influence of discriminative features on deep network boundaries.](http://arxiv.org/abs/2002.06349)

Guillermo Ortiz-Jimenez, Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard


[Blind Adversarial Network Perturbations.](http://arxiv.org/abs/2002.06495)

Milad Nasr, Alireza Bahramali, Amir Houmansadr


## 2020-02-14

[Skip Connections Matter: On the Transferability of Adversarial Examples Generated with ResNets.](http://arxiv.org/abs/2002.05990)

Dongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, Xingjun Ma


[Adversarial Distributional Training for Robust Deep Learning.](http://arxiv.org/abs/2002.05999)

Yinpeng Dong, Zhijie Deng, Tianyu Pang, Hang Su, Jun Zhu


## 2020-02-13

[Recurrent Attention Model with Log-Polar Mapping is Robust against Adversarial Attacks.](http://arxiv.org/abs/2002.05388)

Taro Kiritani, Koji Ono


[The Conditional Entropy Bottleneck.](http://arxiv.org/abs/2002.05379)

Ian Fischer


[Identifying Audio Adversarial Examples via Anomalous Pattern Detection.](http://arxiv.org/abs/2002.05463)

Victor Akinwande, Celia Cintas, Skyler Speakman, Srihari Sridharan


## 2020-02-12

[Stabilizing Differentiable Architecture Search via Perturbation-based Regularization.](http://arxiv.org/abs/2002.05283)

Xiangning Chen, Cho-Jui Hsieh


[Over-the-Air Adversarial Flickering Attacks against Video Recognition Networks.](http://arxiv.org/abs/2002.05123)

Roi Pony, Itay Naeh, Shie Mannor


## 2020-02-11

[Adversarial Robustness for Code.](http://arxiv.org/abs/2002.04694)

Pavol Bielik, Martin Vechev


[Fundamental Tradeoffs between Invariance and Sensitivity to Adversarial Perturbations.](http://arxiv.org/abs/2002.04599)

Florian Tramèr, Jens Behrmann, Nicholas Carlini, Nicolas Papernot, Jörn-Henrik Jacobsen


[Robustness of Bayesian Neural Networks to Gradient-Based Attacks.](http://arxiv.org/abs/2002.04359)

Ginevra Carbone, Matthew Wicker, Luca Laurenti, Andrea Patane, Luca Bortolussi, Guido Sanguinetti


[Improving the affordability of robustness training for DNNs.](http://arxiv.org/abs/2002.04237)

Sidharth Gupta, Parijat Dube, Ashish Verma


[Fast Geometric Projections for Local Robustness Certification.](http://arxiv.org/abs/2002.04742)

Aymeric Fromherz, Klas Leino, Matt Fredrikson, Bryan Parno, Corina Păsăreanu


[Graph Universal Adversarial Attacks: A Few Bad Actors Ruin Graph Learning Models.](http://arxiv.org/abs/2002.04784)

Xiao Zang, Yi Xie, Jie Chen, Bo Yuan


[More Data Can Expand the Generalization Gap Between Adversarially Robust and Standard Models.](http://arxiv.org/abs/2002.04725)

Lin Chen, Yifei Min, Mingrui Zhang, Amin Karbasi


## 2020-02-10

[Playing to Learn Better: Repeated Games for Adversarial Learning with Multiple Classifiers.](http://arxiv.org/abs/2002.03924)

Prithviraj Dasgupta, Joseph B. Collins, Michael McCarrick


[Adversarial Data Encryption.](http://arxiv.org/abs/2002.03793)

Yingdong Hu, Liang Zhang, Wei Shan, Xiaoxiao Qin, Jing Qi, Zhenzhou Wu, Yang Yuan


[Generalised Lipschitz Regularisation Equals Distributional Robustness.](http://arxiv.org/abs/2002.04197)

Zac Cranko, Zhan Shi, Xinhua Zhang, Richard Nock, Simon Kornblith


## 2020-02-09

[MDEA: Malware Detection with Evolutionary Adversarial Learning.](http://arxiv.org/abs/2002.03331)

Xiruo Wang, Risto Miikkulainen


[Input Validation for Neural Networks via Runtime Local Robustness Verification.](http://arxiv.org/abs/2002.03339)

Jiangchao Liu, Liqian Chen, Antoine Mine, Ji Wang


[Robust binary classification with the 01 loss.](http://arxiv.org/abs/2002.03444)

Yunzhe Xue, Meiyan Xie, Usman Roshan


[Watch out! Motion is Blurring the Vision of Your Deep Neural Networks.](http://arxiv.org/abs/2002.03500)

Qing Guo, Felix Juefei-Xu, Xiaofei Xie, Lei Ma, Jian Wang, Bing Yu, Wei Feng, Yang Liu


[Feature-level Malware Obfuscation in Deep Learning.](http://arxiv.org/abs/2002.05517)

Keith Dillon


[Adversarial Deepfakes: Evaluating Vulnerability of Deepfake Detectors to Adversarial Examples.](http://arxiv.org/abs/2002.12749)

Paarth Neekhara, Shehzeen Hussain, Malhar Jere, Farinaz Koushanfar, Julian McAuley


[Category-wise Attack: Transferable Adversarial Examples for Anchor Free Object Detection.](http://arxiv.org/abs/2003.04367)

Quanyu Liao, Xin Wang, Bin Kong, Siwei Lyu, Youbing Yin, Qi Song, Xi Wu


[Certified Robustness of Community Detection against Adversarial Structural Perturbation via Randomized Smoothing.](http://arxiv.org/abs/2002.03421)

Jinyuan Jia, Binghui Wang, Xiaoyu Cao, Neil Zhenqiang Gong


[Random Smoothing Might be Unable to Certify $\ell_\infty$ Robustness for High-Dimensional Images.](http://arxiv.org/abs/2002.03517)

Avrim Blum, Travis Dick, Naren Manoj, Hongyang Zhang


## 2020-02-08

[Attacking Optical Character Recognition (OCR) Systems with Adversarial Watermarks.](http://arxiv.org/abs/2002.03095)

Lu Chen, Wei Xu


[Curse of Dimensionality on Randomized Smoothing for Certifiable Robustness.](http://arxiv.org/abs/2002.03239)

Aounon Kumar, Alexander Levine, Tom Goldstein, Soheil Feizi


## 2020-02-07

[Renofeation: A Simple Transfer Learning Method for Improved Adversarial Robustness.](http://arxiv.org/abs/2002.02998)

Ting-Wu Chin, Cha Zhang, Diana Marculescu


[Analysis of Random Perturbations for Robust Convolutional Neural Networks.](http://arxiv.org/abs/2002.03080)

Adam Dziedzic, Sanjay Krishnan


[RAID: Randomized Adversarial-Input Detection for Neural Networks.](http://arxiv.org/abs/2002.02776)

Hasan Ferit Eniser, Maria Christakis, Valentin Wüstholz


[Assessing the Adversarial Robustness of Monte Carlo and Distillation Methods for Deep Bayesian Neural Network Classification.](http://arxiv.org/abs/2002.02842)

Meet P. Vadera, Satya Narayan Shukla, Brian Jalaian, Benjamin M. Marlin


[Semantic Robustness of Models of Source Code.](http://arxiv.org/abs/2002.03043)

Goutham Ramakrishnan, Jordan Henkel, Zi Wang, Aws Albarghouthi, Somesh Jha, Thomas Reps


## 2020-02-06

[Reliability Validation of Learning Enabled Vehicle Tracking.](http://arxiv.org/abs/2002.02424)

Youcheng Sun, Yifan Zhou, Simon Maskell, James Sharp, Xiaowei Huang


[An Analysis of Adversarial Attacks and Defenses on Autonomous Driving Models.](http://arxiv.org/abs/2002.02175)

Yao Deng, Xi Zheng, Tianyi Zhang, Chen Chen, Guannan Lou, Miryung Kim


[AI-GAN: Attack-Inspired Generation of Adversarial Examples.](http://arxiv.org/abs/2002.02196)

Tao Bai, Jun Zhao, Jinlin Zhu, Shoudong Han, Jiefeng Chen, Bo Li, Alex Kot


## 2020-02-05

[Over-the-Air Adversarial Attacks on Deep Learning Based Modulation Classifier over Wireless Channels.](http://arxiv.org/abs/2002.02400)

Brian Kim, Yalin E. Sagduyu, Kemal Davaslioglu, Tugba Erpek, Sennur Ulukus


[Understanding the Decision Boundary of Deep Neural Networks: An Empirical Study.](http://arxiv.org/abs/2002.01810)

David Mickisch, Felix Assion, Florens Greßner, Wiebke Günther, Mariele Motta


## 2020-02-04

[Adversarially Robust Frame Sampling with Bounded Irregularities.](http://arxiv.org/abs/2002.01147)

Hanhan Li, Pin Wang


[Adversarial Attacks to Scale-Free Networks: Testing the Robustness of Physical Criteria.](http://arxiv.org/abs/2002.01249)

Qi Xuan, Yalu Shan, Jinhuan Wang, Zhongyuan Ruan, Guanrong Chen


[Minimax Defense against Gradient-based Adversarial Attacks.](http://arxiv.org/abs/2002.01256)

Blerta Lindqvist, Rauf Izmailov


## 2020-02-03

[A Differentiable Color Filter for Generating Unrestricted Adversarial Images.](http://arxiv.org/abs/2002.01008)

Zhengyu Zhao, Zhuoran Liu, Martha Larson


[Regularizers for Single-step Adversarial Training.](http://arxiv.org/abs/2002.00614)

B. S. Vivek, R. Venkatesh Babu


[Defending Adversarial Attacks via Semantic Feature Manipulation.](http://arxiv.org/abs/2002.02007)

Shuo Wang, Tianle Chen, Surya Nepal, Carsten Rudolph, Marthie Grobler, Shangyu Chen


## 2020-02-02

[Robust saliency maps with decoy-enhanced saliency score.](http://arxiv.org/abs/2002.00526)

Yang Lu, Wenbo Guo, Xinyu Xing, William Stafford Noble


## 2020-02-01

[Towards Sharper First-Order Adversary with Quantized Gradients.](http://arxiv.org/abs/2002.02372)

Zhuanghua Liu, Ivor W. Tsang


[AdvJND: Generating Adversarial Examples with Just Noticeable Difference.](http://arxiv.org/abs/2002.00179)

Zifei Zhang, Kai Qiao, Lingyun Jiang, Linyuan Wang, Bin Yan


## 2020-01-31

[Additive Tree Ensembles: Reasoning About Potential Instances.](http://arxiv.org/abs/2001.11905)

Laurens Devos, Wannes Meert, Jesse Davis


[Politics of Adversarial Machine Learning.](http://arxiv.org/abs/2002.05648)

Kendra Albert, Jonathon Penney, Bruce Schneier, Ram Shankar Siva Kumar


[FastWordBug: A Fast Method To Generate Adversarial Text Against NLP Applications.](http://arxiv.org/abs/2002.00760)

Dou Goodman, Lv Zhonghou, Wang minghua


## 2020-01-30

[Tiny Noise Can Make an EEG-Based Brain-Computer Interface Speller Output Anything.](http://arxiv.org/abs/2001.11569)

Xiao Zhang, Dongrui Wu, Lieyun Ding, Hanbin Luo, Chin-Teng Lin, Tzyy-Ping Jung, Ricardo Chavarriaga


## 2020-01-29

[A4 : Evading Learning-based Adblockers.](http://arxiv.org/abs/2001.10999)

Shitong Zhu, Zhongjie Wang, Xun Chen, Shasha Li, Umar Iqbal, Zhiyun Qian, Kevin S. Chan, Srikanth V. Krishnamurthy, Zubair Shafiq


[D2M: Dynamic Defense and Modeling of Adversarial Movement in Networks.](http://arxiv.org/abs/2001.11108)

Scott Freitas, Andrew Wicker, Duen Horng Chau, Joshua Neil


[Just Noticeable Difference for Machines to Generate Adversarial Images.](http://arxiv.org/abs/2001.11064)

Adil Kaan Akan, Mehmet Ali Genc, Fatos T. Yarman Vural


[Semantic Adversarial Perturbations using Learnt Representations.](http://arxiv.org/abs/2001.11055)

Isaac Dunn, Tom Melham, Daniel Kroening


[Adversarial Attacks on Convolutional Neural Networks in Facial Recognition Domain.](http://arxiv.org/abs/2001.11137)

Yigit Alparslan, Ken Alparslan, Jeremy Keim-Shenk, Shweta Khade, Rachel Greenstadt


## 2020-01-28

[Modelling and Quantifying Membership Information Leakage in Machine Learning.](http://arxiv.org/abs/2001.10648)

Farhad Farokhi, Mohamed Ali Kaafar


## 2020-01-27

[Interpreting Machine Learning Malware Detectors Which Leverage N-gram Analysis.](http://arxiv.org/abs/2001.10916)

William Briguglio, Sherif Saad


[Generating Natural Adversarial Hyperspectral examples with a modified Wasserstein GAN.](http://arxiv.org/abs/2001.09993)

Jean-Christophe OBELIX Burnel, Kilian OBELIX Fatras, Nicolas OBELIX Courty


[FakeLocator: Robust Localization of GAN-Based Face Manipulations via Semantic Segmentation Networks with Bells and Whistles.](http://arxiv.org/abs/2001.09598)

Yihao Huang, Felix Juefei-Xu, Run Wang, Xiaofei Xie, Lei Ma, Jianwen Li, Weikai Miao, Yang Liu, Geguang Pu


[Challenges and Countermeasures for Adversarial Attacks on Deep Reinforcement Learning.](http://arxiv.org/abs/2001.09684)

Inaam Ilahi, Muhammad Usama, Junaid Qadir, Muhammad Umar Janjua, Ala Al-Fuqaha, Dinh Thai Hoang, Dusit Niyato


[Practical Fast Gradient Sign Attack against Mammographic Image Classifier.](http://arxiv.org/abs/2001.09610)

Ibrahim Yilmaz


## 2020-01-26

[Ensemble Noise Simulation to Handle Uncertainty about Gradient-based Adversarial Attacks.](http://arxiv.org/abs/2001.09486)

Rehana Mahfuz, Rajeev Sahay, Aly El Gamal


## 2020-01-25

[Weighted Average Precision: Adversarial Example Detection in the Visual Perception of Autonomous Vehicles.](http://arxiv.org/abs/2002.03751)

Yilan Li, Senem Velipasalar


[AI-Powered GUI Attack and Its Defensive Methods.](http://arxiv.org/abs/2001.09388)

Ning Yu, Zachary Tuttle, Carl Jake Thurnau, Emmanuel Mireku


[Analyzing the Noise Robustness of Deep Neural Networks.](http://arxiv.org/abs/2001.09395)

Kelei Cao, Mengchen Liu, Hang Su, Jing Wu, Jun Zhu, Shixia Liu


## 2020-01-24

[When Wireless Security Meets Machine Learning: Motivation, Challenges, and Research Directions.](http://arxiv.org/abs/2001.08883)

Yalin E. Sagduyu, Yi Shi, Tugba Erpek, William Headley, Bryse Flowers, George Stantchev, Zhuo Lu


## 2020-01-23

[Privacy for All: Demystify Vulnerability Disparity of Differential Privacy against Membership Inference Attack.](http://arxiv.org/abs/2001.08855)

Bo Zhang, Ruotong Yu, Haipei Sun, Yanying Li, Jun Xu, Hui Wang


[Towards Robust DNNs: An Taylor Expansion-Based Method for Generating Powerful Adversarial Examples.](http://arxiv.org/abs/2001.08389)

Ya-guan Qian, Xi-Ming Zhang, Bin Wang, Wei Li, Jian-Hai Chen, Wu-Jie Zhou, Jing-Sheng Lei


[On the human evaluation of audio adversarial examples.](http://arxiv.org/abs/2001.08444)

Jon Vadillo, Roberto Santana


## 2020-01-22

[Adversarial Attack on Community Detection by Hiding Individuals.](http://arxiv.org/abs/2001.07933)

Jia Li, Honglei Zhang, Zhichao Han, Yu Rong, Hong Cheng, Junzhou Huang


## 2020-01-21

[SAUNet: Shape Attentive U-Net for Interpretable Medical Image Segmentation.](http://arxiv.org/abs/2001.07645)

Jesse Sun, Fatemeh Darbeha, Mark Zaidi, Bo Wang


[Secure and Robust Machine Learning for Healthcare: A Survey.](http://arxiv.org/abs/2001.08103)

Adnan Qayyum, Junaid Qadir, Muhammad Bilal, Ala Al-Fuqaha


[FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence.](http://arxiv.org/abs/2001.07685)

Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Han Zhang, Colin Raffel


[GhostImage: Perception Domain Attacks against Vision-based Object Classification Systems.](http://arxiv.org/abs/2001.07792)

Yanmao Man, Ming Li, Ryan Gerdes


[Generate High-Resolution Adversarial Samples by Identifying Effective Features.](http://arxiv.org/abs/2001.07631)

Sizhe Chen, Peidong Zhang, Chengjin Sun, Jia Cai, Xiaolin Huang


[Massif: Interactive Interpretation of Adversarial Attacks on Deep Learning.](http://arxiv.org/abs/2001.07769)

Nilaksh Polo Das, Haekyu Polo Park, Zijie J. Polo Wang, Fred Polo Hohman, Robert Polo Firstman, Emily Polo Rogers, Duen Polo Horng, Chau


[Elephant in the Room: An Evaluation Framework for Assessing Adversarial Examples in NLP.](http://arxiv.org/abs/2001.07820)

Ying Xu, Xu Zhong, Antonio Jose Jimeno Yepes, Jey Han Lau


## 2020-01-17

[Cyber Attack Detection thanks to Machine Learning Algorithms.](http://arxiv.org/abs/2001.06309)

Antoine Delplace, Sheryl Hermoso, Kristofer Anandita


## 2020-01-16

[Code-Bridged Classifier (CBC): A Low or Negative Overhead Defense for Making a CNN Classifier Robust Against Adversarial Attacks.](http://arxiv.org/abs/2001.06099)

Farnaz Behnia, Ali Mirzaeian, Mohammad Sabokrou, Sai Manoj, Tinoosh Mohsenin, Khaled N. Khasawneh, Liang Zhao, Houman Homayoun, Avesta Sasan


[A Little Fog for a Large Turn.](http://arxiv.org/abs/2001.05873)

Harshitha Machiraju, Vineeth N Balasubramanian


[The gap between theory and practice in function approximation with deep neural networks.](http://arxiv.org/abs/2001.07523)

Ben Adcock, Nick Dexter


[Universal Adversarial Attack on Attention and the Resulting Dataset DAmageNet.](http://arxiv.org/abs/2001.06325)

Sizhe Chen, Zhengbao He, Chengjin Sun, Jie Yang, Xiaolin Huang


[Increasing the robustness of DNNs against image corruptions by playing the Game of Noise.](http://arxiv.org/abs/2001.06057)

Evgenia Rusak, Lukas Schott, Roland S. Zimmermann, Julian Bitterwolf, Oliver Bringmann, Matthias Bethge, Wieland Brendel


## 2020-01-14

[Noisy Machines: Understanding Noisy Neural Networks and Enhancing Robustness to Analog Hardware Errors Using Distillation.](http://arxiv.org/abs/2001.04974)

Chuteng Zhou, Prad Kadambi, Matthew Mattina, Paul N. Whatmough


## 2020-01-13

[Advbox: a toolbox to generate adversarial examples that fool neural networks.](http://arxiv.org/abs/2001.05574)

Dou Goodman, Hao Xin, Wang Yang, Wu Yuesheng, Xiong Junfeng, Zhang Huan


## 2020-01-12

[Membership Inference Attacks Against Object Detection Models.](http://arxiv.org/abs/2001.04011)

Yeachan Park, Myungjoo Kang


[An Adversarial Approach for the Robust Classification of Pneumonia from Chest Radiographs.](http://arxiv.org/abs/2001.04051)

Joseph D. Janizek, Gabriel Erion, Alex J. DeGrave, Su-In Lee


[Fast is better than free: Revisiting adversarial training.](http://arxiv.org/abs/2001.03994)

Eric Wong, Leslie Rice, J. Zico Kolter


## 2020-01-11

[Exploring and Improving Robustness of Multi Task Deep Neural Networks via Domain Agnostic Defenses.](http://arxiv.org/abs/2001.05286)

Kashyap Coimbatore Murali


[Sparse Black-box Video Attack with Reinforcement Learning.](http://arxiv.org/abs/2001.03754)

Huanqian Yan, Xingxing Wei, Bo Li


## 2020-01-10

[ReluDiff: Differential Verification of Deep Neural Networks.](http://arxiv.org/abs/2001.03662)

Brandon Paulsen, Jingbo Wang, Chao Wang


[Guess First to Enable Better Compression and Adversarial Robustness.](http://arxiv.org/abs/2001.03311)

Sicheng Zhu, Bang An, Shiyu Niu


## 2020-01-08

[To Transfer or Not to Transfer: Misclassification Attacks Against Transfer Learned Text Classifiers.](http://arxiv.org/abs/2001.02438)

Bijeeta Pal, Shruti Tople


[MACER: Attack-free and Scalable Robust Training via Maximizing Certified Radius.](http://arxiv.org/abs/2001.02378)

Runtian Zhai, Chen Dan, Di He, Huan Zhang, Boqing Gong, Pradeep Ravikumar, Cho-Jui Hsieh, Liwei Wang


[Transferability of Adversarial Examples to Attack Cloud-based Image Classifier Service.](http://arxiv.org/abs/2001.03460)

Dou Goodman


## 2020-01-07

[Softmax-based Classification is k-means Clustering: Formal Proof, Consequences for Adversarial Attacks, and Improvement through Centroid Based Tailoring.](http://arxiv.org/abs/2001.01987)

Sibylle Hess, Wouter Duivesteijn, Decebal Mocanu


## 2020-01-06

[Deceiving Image-to-Image Translation Networks for Autonomous Driving with Adversarial Perturbations.](http://arxiv.org/abs/2001.01506)

Lin Wang, Wonjune Cho, Kuk-Jin Yoon


[Generating Semantic Adversarial Examples via Feature Manipulation.](http://arxiv.org/abs/2001.02297)

Shuo Wang, Surya Nepal, Carsten Rudolph, Marthie Grobler, Shangyu Chen, Tianle Chen


## 2020-01-05

[The Human Visual System and Adversarial AI.](http://arxiv.org/abs/2001.01172)

Yaoshiang Ho, Samuel Wookey


## 2020-01-02

[Reject Illegal Inputs with Generative Classifier Derived from Any Discriminative Classifier.](http://arxiv.org/abs/2001.00483)

Xin Wang


## 2020-01-01

[Exploring Adversarial Attack in Spiking Neural Networks with Spike-Compatible Gradient.](http://arxiv.org/abs/2001.01587)

Ling Liang, Xing Hu, Lei Deng, Yujie Wu, Guoqi Li, Yufei Ding, Peng Li, Yuan Xie


[Ensembles of Many Diverse Weak Defenses can be Strong: Defending Deep Neural Networks Against Adversarial Attacks.](http://arxiv.org/abs/2001.00308)

Ying Meng, Jianhai Su, Jason O'Kane, Pooyan Jamshidi


## 2019-12-31

[Automated Testing for Deep Learning Systems with Differential Behavior Criteria.](http://arxiv.org/abs/1912.13258)

Yuan Gao, Yiqiang Han


[Protecting GANs against privacy attacks by preventing overfitting.](http://arxiv.org/abs/2001.00071)

Sumit Mukherjee, Yixi Xu, Anusua Trivedi, Juan Lavista Ferres


[Erase and Restore: Simple, Accurate and Resilient Detection of $L_2$ Adversarial Examples.](http://arxiv.org/abs/2001.00116)

Fei Zuo, Qiang Zeng


[Quantum Adversarial Machine Learning.](http://arxiv.org/abs/2001.00030)

Sirui Lu, Lu-Ming Duan, Dong-Ling Deng


## 2019-12-30

[Adversarial Example Generation using Evolutionary Multi-objective Optimization.](http://arxiv.org/abs/2001.05844)

Takahiro Suzuki, Shingo Takeshita, Satoshi Ono


[Defending from adversarial examples with a two-stream architecture.](http://arxiv.org/abs/1912.12859)

Hao Ge, Xiaoguang Tu, Mei Xie, Zheng Ma


## 2019-12-28

[Detecting Out-of-Distribution Examples with In-distribution Examples and Gram Matrices.](http://arxiv.org/abs/1912.12510)

Chandramouli Shama Sastry, Sageev Oore


[Search Based Repair of Deep Neural Networks.](http://arxiv.org/abs/1912.12463)

Jeongju Sohn, Sungmin Kang, Shin Yoo


## 2019-12-26

[Benchmarking Adversarial Robustness.](http://arxiv.org/abs/1912.11852)

Yinpeng Dong, Qi-An Fu, Xiao Yang, Tianyu Pang, Hang Su, Zihao Xiao, Jun Zhu


[Efficient Adversarial Training with Transferable Adversarial Examples.](http://arxiv.org/abs/1912.11969)

Haizhong Zheng, Ziqi Zhang, Juncheng Gu, Honglak Lee, Atul Prakash


## 2019-12-24

[Attack-Resistant Federated Learning with Residual-based Reweighting.](http://arxiv.org/abs/1912.11464)

Shuhao Fu, Chulin Xie, Bo Li, Qifeng Chen


[Analysis of Moving Target Defense Against False Data Injection Attacks on Power Grid.](http://arxiv.org/abs/1912.11372)

Zhenyong Zhang, Ruilong Deng, Member, IEEE, David K. Y. Yau, Senior Member, IEEE, Peng Cheng, Member, IEEE, Jiming Chen, Fellow, IEEE


[Cronus: Robust and Heterogeneous Collaborative Learning with Black-Box Knowledge Transfer.](http://arxiv.org/abs/1912.11279)

Hongyan Chang, Virat Shejwalkar, Reza Shokri, Amir Houmansadr


[Characterizing the Decision Boundary of Deep Neural Networks.](http://arxiv.org/abs/1912.11460)

Hamid Karimi, Tyler Derr, Jiliang Tang


## 2019-12-23

[White Noise Analysis of Neural Networks.](http://arxiv.org/abs/1912.12106)

Ali Borji, Sikun Lin


[Adversarial AutoAugment.](http://arxiv.org/abs/1912.11188)

Xinyu Zhang, Qiang Wang, Jian Zhang, Zhao Zhong


[Geometry-aware Generation of Adversarial and Cooperative Point Clouds.](http://arxiv.org/abs/1912.11171)

Yuxin Wen, Jiehong Lin, Ke Chen, Kui Jia


## 2019-12-21

[T3: Tree-Autoencoder Constrained Adversarial Text Generation for Targeted Attack.](http://arxiv.org/abs/1912.10375)

Boxin Wang, Hengzhi Pei, Boyuan Pan, Qian Chen, Shuohang Wang, Bo Li


## 2019-12-20

[Measuring Dataset Granularity.](http://arxiv.org/abs/1912.10154)

Yin Cui, Zeqi Gu, Dhruv Mahajan, der Maaten Laurens van, Serge Belongie, Ser-Nam Lim


[Certified Robustness for Top-k Predictions against Adversarial Perturbations via Randomized Smoothing.](http://arxiv.org/abs/1912.09899)

Jinyuan Jia, Xiaoyu Cao, Binghui Wang, Neil Zhenqiang Gong


[secml: A Python Library for Secure and Explainable Machine Learning.](http://arxiv.org/abs/1912.10013)

Maura Pintor, Luca Demetrio, Angelo Sotgiu, Marco Melis, Ambra Demontis, Battista Biggio


[Jacobian Adversarially Regularized Networks for Robustness.](http://arxiv.org/abs/1912.10185)

Alvin Chan, Yi Tay, Yew Soon Ong, Jie Fu


[Explainability and Adversarial Robustness for RNNs.](http://arxiv.org/abs/1912.09855)

Alexander Hartl, Maximilian Bachl, Joachim Fabini, Tanja Zseby


[Adversarial symmetric GANs: bridging adversarial samples and adversarial networks.](http://arxiv.org/abs/1912.09670)

Faqiang Liu, Mingkun Xu, Guoqi Li, Jing Pei, Luping Shi, Rong Zhao


## 2019-12-19

[Does Symbolic Knowledge Prevent Adversarial Fooling?](http://arxiv.org/abs/1912.10834)

Stefano Teso


[A New Ensemble Method for Concessively Targeted Multi-model Attack.](http://arxiv.org/abs/1912.10833)

Ziwen He, Wei Wang, Xinsheng Xuan, Jing Dong, Tieniu Tan


[Mitigating large adversarial perturbations on X-MAS (X minus Moving Averaged Samples).](http://arxiv.org/abs/1912.12170)

Woohyung Chun, Sung-Min Hong, Junho Huh, Inyup Kang


[Optimization-Guided Binary Diversification to Mislead Neural Networks for Malware Detection.](http://arxiv.org/abs/1912.09064)

Mahmood Sharif, Keane Lucas, Lujo Bauer, Michael K. Reiter, Saurabh Shintre


[$n$-ML: Mitigating Adversarial Examples via Ensembles of Topologically Manipulated Classifiers.](http://arxiv.org/abs/1912.09059)

Mahmood Sharif, Lujo Bauer, Michael K. Reiter


[Towards Verifying Robustness of Neural Networks Against Semantic Perturbations.](http://arxiv.org/abs/1912.09533)

Jeet Lily Mohapatra, Lily Tsui-Wei, Weng, Pin-Yu Chen, Sijia Liu, Luca Daniel


[Perturbations on the Perceptual Ball.](http://arxiv.org/abs/1912.09405)

Andrew Elliott, Stephen Law, Chris Russell


## 2019-12-18

[Identifying Adversarial Sentences by Analyzing Text Complexity.](http://arxiv.org/abs/1912.08981)

Hoang-Quoc Nguyen-Son, Tran Phuong Thao, Seira Hidano, Shinsaku Kiyomoto


[An Adversarial Perturbation Oriented Domain Adaptation Approach for Semantic Segmentation.](http://arxiv.org/abs/1912.08954)

Jihan Yang, Ruijia Xu, Ruiyu Li, Xiaojuan Qi, Xiaoyong Shen, Guanbin Li, Liang Lin


[Adversarial VC-dimension and Sample Complexity of Neural Networks.](http://arxiv.org/abs/1912.08865)

Zetong Qi, T. J. Wilder


[SIGMA : Strengthening IDS with GAN and Metaheuristics Attacks.](http://arxiv.org/abs/1912.09303)

Simon Msika, Alejandro Quintero, Foutse Khomh


[Detecting Adversarial Attacks On Audio-Visual Speech Recognition.](http://arxiv.org/abs/1912.08639)

Pingchuan Ma, Stavros Petridis, Maja Pantic


## 2019-12-17

[APRICOT: A Dataset of Physical Adversarial Attacks on Object Detection.](http://arxiv.org/abs/1912.08166)

A. Braunegg, Amartya Chakraborty, Michael Krumdick, Nicole Lape, Sara Leary, Keith Manville, Elizabeth Merkhofer, Laura Strickhart, Matthew Walmer


## 2019-12-16

[CAG: A Real-time Low-cost Enhanced-robustness High-transferability Content-aware Adversarial Attack Generator.](http://arxiv.org/abs/1912.07742)

Huy Phan, Yi Xie, Siyu Liao, Jie Chen, Bo Yuan


[MimicGAN: Robust Projection onto Image Manifolds with Corruption Mimicking.](http://arxiv.org/abs/1912.07748)

Rushil Anirudh, Jayaraman J. Thiagarajan, Bhavya Kailkhura, Timo Bremer


[On-manifold Adversarial Data Augmentation Improves Uncertainty Calibration.](http://arxiv.org/abs/1912.07458)

Kanil Patel, William Beluch, Dan Zhang, Michael Pfeiffer, Bin Yang


[Constructing a provably adversarially-robust classifier from a high accuracy one.](http://arxiv.org/abs/1912.07561)

Grzegorz Głuch, Rüdiger Urbanke


## 2019-12-15

[DAmageNet: A Universal Adversarial Dataset.](http://arxiv.org/abs/1912.07160)

Sizhe Chen, Xiaolin Huang, Zhengbao He, Chengjin Sun


## 2019-12-14

[What Else Can Fool Deep Learning? Addressing Color Constancy Errors on Deep Neural Network Performance.](http://arxiv.org/abs/1912.06960)

Mahmoud Afifi, Michael S Brown


[Towards Robust Toxic Content Classification.](http://arxiv.org/abs/1912.06872)

Keita Kurita, Anna Belova, Antonios Anastasopoulos


## 2019-12-13

[Potential adversarial samples for white-box attacks.](http://arxiv.org/abs/1912.06409)

Amir Nazemi, Paul Fieguth


## 2019-12-11

[Learning to Model Aspects of Hearing Perception Using Neural Loss Functions.](http://arxiv.org/abs/1912.05683)

Prateek Verma, Jonathan Berger


[Gabor Layers Enhance Network Robustness.](http://arxiv.org/abs/1912.05661)

Juan C. Pérez, Motasem Alfarra, Guillaume Jeanneret, Adel Bibi, Ali Thabet, Bernard Ghanem, Pablo Arbeláez


[An Efficient Approach for Using Expectation Maximization Algorithm in Capsule Networks.](http://arxiv.org/abs/1912.05333)

Moein Hasani, Amin Nasim Saravi, Hassan Khotanlou


[Detecting and Correcting Adversarial Images Using Image Processing Operations and Convolutional Neural Networks.](http://arxiv.org/abs/1912.05391)

Huy H. Nguyen, Minoru Kuribayashi, Junichi Yamagishi, Isao Echizen


[What it Thinks is Important is Important: Robustness Transfers through Input Gradients.](http://arxiv.org/abs/1912.05699)

Alvin Chan, Yi Tay, Yew-Soon Ong


## 2019-12-10

[Towards a Robust Classifier: An MDL-Based Method for Generating Adversarial Examples.](http://arxiv.org/abs/1912.05945)

Behzad Asadi, Vijay Varadharajan


[Appending Adversarial Frames for Universal Video Attack.](http://arxiv.org/abs/1912.04538)

Zhikai Chen, Lingxi Xie, Shanmin Pang, Yong He, Qi Tian


[Training Provably Robust Models by Polyhedral Envelope Regularization.](http://arxiv.org/abs/1912.04792)

Chen Liu, Mathieu Salzmann, Sabine Süsstrunk


[Statistically Robust Neural Network Classification. (22%)](http://arxiv.org/abs/1912.04884)

Benjie Wang, Stefan Webb, Tom Rainforth


## 2019-12-09

[Feature Losses for Adversarial Robustness.](http://arxiv.org/abs/1912.04497)

Kirthi Shankar Sivamani


## 2019-12-08

[Hardening Random Forest Cyber Detectors Against Adversarial Attacks.](http://arxiv.org/abs/1912.03790)

Giovanni Apruzzese, Mauro Andreolini, Michele Colajanni, Mirco Marchetti


[Amora: Black-box Adversarial Morphing Attack.](http://arxiv.org/abs/1912.03829)

Run Wang, Felix Juefei-Xu, Xiaofei Xie, Lei Ma, Yihao Huang, Yang Liu


## 2019-12-07

[Exploring the Back Alleys: Analysing The Robustness of Alternative Neural Network Architectures against Adversarial Attacks.](http://arxiv.org/abs/1912.03609)

Yi Xiang Marcus Tan, Yuval Elovici, Alexander Binder


## 2019-12-06

[Achieving Robustness in the Wild via Adversarial Mixing with Disentangled Representations.](http://arxiv.org/abs/1912.03192)

Sven Gowal, Chongli Qin, Po-Sen Huang, Taylan Cemgil, Krishnamurthy Dvijotham, Timothy Mann, Pushmeet Kohli


[Principal Component Properties of Adversarial Samples.](http://arxiv.org/abs/1912.03406)

Malhar Jere, Sandro Herbig, Christine Lind, Farinaz Koushanfar


[Training Deep Neural Networks for Interpretability and Adversarial Robustness.](http://arxiv.org/abs/1912.03430)

Adam Noack, Isaac Ahern, Dejing Dou, Boyang Li


## 2019-12-05

[Detection of Face Recognition Adversarial Attacks.](http://arxiv.org/abs/1912.02918)

Fabio Valerio Massoli, Fabio Carrara, Giuseppe Amato, Fabrizio Falchi


[The Search for Sparse, Robust Neural Networks.](http://arxiv.org/abs/1912.02386)

Justin Cosentino, Federico Zaiter, Dan Pei, Jun Zhu


[Region-Wise Attack: On Efficient Generation of Robust Physical Adversarial Examples.](http://arxiv.org/abs/1912.02598)

Bo Luo, Qiang Xu


## 2019-12-04

[Learning with Multiplicative Perturbations.](http://arxiv.org/abs/1912.01810)

Xiulong Yang, Shihao Ji


[A Survey of Game Theoretic Approaches for Adversarial Machine Learning in Cybersecurity Tasks.](http://arxiv.org/abs/1912.02258)

Prithviraj Dasgupta, Joseph B. Collins


[Walking on the Edge: Fast, Low-Distortion Adversarial Examples.](http://arxiv.org/abs/1912.02153)

Hanwei Zhang, Yannis Avrithis, Teddy Furon, Laurent Amsaleg


[Towards Robust Image Classification Using Sequential Attention Models.](http://arxiv.org/abs/1912.02184)

Daniel Zoran, Mike Chrzanowski, Po-Sen Huang, Sven Gowal, Alex Mott, Pushmeet Kohl


[Scratch that! An Evolution-based Adversarial Attack against Neural Networks.](http://arxiv.org/abs/1912.02316)

Malhar Jere, Briland Hitaj, Gabriela Ciocarlie, Farinaz Koushanfar


## 2019-12-03

[A Survey of Black-Box Adversarial Attacks on Computer Vision Models.](http://arxiv.org/abs/1912.01667)

Siddhant Bhambri, Sumanyu Muku, Avinash Tulasi, Arun Balaji Buduru


[FANNet: Formal Analysis of Noise Tolerance, Training Bias and Input Sensitivity in Neural Networks.](http://arxiv.org/abs/1912.01978)

Mahum Naseer, Mishal Fatima Minhas, Faiq Khalid, Muhammad Abdullah Hanif, Osman Hasan, Muhammad Shafique


## 2019-12-02

[Cost-Aware Robust Tree Ensembles for Security Applications.](http://arxiv.org/abs/1912.01149)

Yizheng Chen, Shiqi Wang, Weifan Jiang, Asaf Cidon, Suman Jana


[Deep Neural Network Fingerprinting by Conferrable Adversarial Examples.](http://arxiv.org/abs/1912.00888)

Nils Lukas, Yuxuan Zhang, Florian Kerschbaum


[Universal Adversarial Perturbations for CNN Classifiers in EEG-Based BCIs.](http://arxiv.org/abs/1912.01171)

Zihan Liu, Xiao Zhang, Lubin Meng, Dongrui Wu


## 2019-12-01

[Adversary A3C for Robust Reinforcement Learning.](http://arxiv.org/abs/1912.00330)

Zhaoyuan Gu, Zhenzhong Jia, Howie Choset


[A Method for Computing Class-wise Universal Adversarial Perturbations.](http://arxiv.org/abs/1912.00466)

Tejus Gupta, Abhishek Sinha, Nupur Kumari, Mayank Singh, Balaji Krishnamurthy


[AdvPC: Transferable Adversarial Perturbations on 3D Point Clouds.](http://arxiv.org/abs/1912.00461)

Abdullah Hamdi, Sara Rojas, Ali Thabet, Bernard Ghanem


## 2019-11-30

[Design and Interpretation of Universal Adversarial Patches in Face Detection.](http://arxiv.org/abs/1912.05021)

Xiao Yang, Fangyun Wei, Hongyang Zhang, Jun Zhu


[Error-Correcting Neural Network.](http://arxiv.org/abs/1912.00181)

Yang Song, Qiyu Kang, Wee Peng Tay


## 2019-11-29

[Square Attack: a query-efficient black-box adversarial attack via random search.](http://arxiv.org/abs/1912.00049)

Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, Matthias Hein


## 2019-11-28

[Towards Privacy and Security of Deep Learning Systems: A Survey.](http://arxiv.org/abs/1911.12562)

Yingzhe He, Guozhu Meng, Kai Chen, Xingbo Hu, Jinwen He


## 2019-11-26

[Survey of Attacks and Defenses on Edge-Deployed Neural Networks.](http://arxiv.org/abs/1911.11932)

Mihailo Isakov, Vijay Gadepally, Karen M. Gettings, Michel A. Kinsy


[An Adaptive View of Adversarial Robustness from Test-time Smoothing Defense.](http://arxiv.org/abs/1911.11881)

Chao Tang, Yifei Fan, Anthony Yezzi


[Can Attention Masks Improve Adversarial Robustness?](http://arxiv.org/abs/1911.11946)

Pratik Vaishnavi, Tianji Cong, Kevin Eykholt, Atul Prakash, Amir Rahmati


[Defending Against Adversarial Machine Learning.](http://arxiv.org/abs/1911.11746)

Alison Jenkins


[Using Depth for Pixel-Wise Detection of Adversarial Attacks in Crowd Counting.](http://arxiv.org/abs/1911.11484)

Weizhe Liu, Mathieu Salzmann, Pascal Fua


## 2019-11-25

[Playing it Safe: Adversarial Robustness with an Abstain Option.](http://arxiv.org/abs/1911.11253)

Cassidy Laidlaw, Soheil Feizi


[ColorFool: Semantic Adversarial Colorization.](http://arxiv.org/abs/1911.10891)

Ali Shahin Shamsabadi, Ricardo Sanchez-Matilla, Andrea Cavallaro


[Adversarial Attack with Pattern Replacement.](http://arxiv.org/abs/1911.10875)

Ziang Dong, Liang Mao, Shiliang Sun


[One Man's Trash is Another Man's Treasure: Resisting Adversarial Examples by Adversarial Examples.](http://arxiv.org/abs/1911.11219)

Chang Xiao, Changxi Zheng


## 2019-11-24

[When NAS Meets Robustness: In Search of Robust Architectures against Adversarial Attacks.](http://arxiv.org/abs/1911.10695)

Minghao Guo, Yuzhe Yang, Rui Xu, Ziwei Liu, Dahua Lin


[Time-aware Gradient Attack on Dynamic Network Link Prediction.](http://arxiv.org/abs/1911.10561)

Jinyin Chen, Jian Zhang, Zhi Chen, Min Du, Feifei Li, Qi Xuan


## 2019-11-23

[Robust Assessment of Real-World Adversarial Examples.](http://arxiv.org/abs/1911.10435)

Brett Jefferson, Carlos Ortiz Marrero


[Universal Adversarial Robustness of Texture and Shape-Biased Models.](http://arxiv.org/abs/1911.10364)

Kenneth T. Co, Luis Muñoz-González, Leslie Kanthan, Ben Glocker, Emil C. Lupu


## 2019-11-22

[Bounding Singular Values of Convolution Layers.](http://arxiv.org/abs/1911.10258)

Sahil Singla, Soheil Feizi


[Enhancing Cross-task Black-Box Transferability of Adversarial Examples with Dispersion Reduction.](http://arxiv.org/abs/1911.11616)

Yantao Lu, Yunhan Jia, Jianyu Wang, Bai Li, Weiheng Chai, Lawrence Carin, Senem Velipasalar


[Attack Agnostic Statistical Method for Adversarial Detection.](http://arxiv.org/abs/1911.10008)

Sambuddha Saha, Aashish Kumar, Pratyush Sahay, George Jose, Srinivas Kruthiventi, Harikrishna Muralidhara


[Universal adversarial examples in speech command classification.](http://arxiv.org/abs/1911.10182)

Jon Vadillo, Roberto Santana


[Invert and Defend: Model-based Approximate Inversion of Generative Adversarial Networks for Secure Inference.](http://arxiv.org/abs/1911.10291)

Wei-An Lin, Yogesh Balaji, Pouya Samangouei, Rama Chellappa


## 2019-11-21

[Heuristic Black-box Adversarial Attacks on Video Recognition Models.](http://arxiv.org/abs/1911.09449)

Zhipeng Wei, Jingjing Chen, Xingxing Wei, Linxi Jiang, Tat-Seng Chua, Fengfeng Zhou, Yu-Gang Jiang


[Adversarial Examples Improve Image Recognition.](http://arxiv.org/abs/1911.09665)

Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan Yuille, Quoc V. Le


## 2019-11-20

[Robustness Certificates for Sparse Adversarial Attacks by Randomized Ablation.](http://arxiv.org/abs/1911.09272)

Alexander Levine, Soheil Feizi


[Analysis of Deep Networks for Monocular Depth Estimation Through Adversarial Attacks with Proposal of a Defense Method.](http://arxiv.org/abs/1911.08790)

Junjie Hu, Takayuki Okatani


[Fine-grained Synthesis of Unrestricted Adversarial Examples.](http://arxiv.org/abs/1911.09058)

Omid Poursaeed, Tianxing Jiang, Harry Yang, Serge Belongie, Ser-Nam Lim


[Deep Minimax Probability Machine.](http://arxiv.org/abs/1911.08723)

Lirong He, Ziyi Guo, Kaizhu Huang, Zenglin Xu


## 2019-11-19

[Logic-inspired Deep Neural Networks.](http://arxiv.org/abs/1911.08635)

Minh Le


[Where is the Bottleneck of Adversarial Learning with Unlabeled Data?](http://arxiv.org/abs/1911.08696)

Jingfeng Zhang, Bo Han, Gang Niu, Tongliang Liu, Masashi Sugiyama


[Adversarial Robustness of Flow-Based Generative Models.](http://arxiv.org/abs/1911.08654)

Phillip Pope, Yogesh Balaji, Soheil Feizi


[Defective Convolutional Layers Learn Robust CNNs.](http://arxiv.org/abs/1911.08432)

Tiange Luo, Tianle Cai, Mengxiao Zhang, Siyu Chen, Di He, Liwei Wang


[Generate (non-software) Bugs to Fool Classifiers.](http://arxiv.org/abs/1911.08644)

Hiromu Yakura, Youhei Akimoto, Jun Sakuma


## 2019-11-18

[A New Ensemble Adversarial Attack Powered by Long-term Gradient Memories.](http://arxiv.org/abs/1911.07682)

Zhaohui Che, Ali Borji, Guangtao Zhai, Suiyi Ling, Jing Li, Patrick Le Callet


[A novel method for identifying the deep neural network model with the Serial Number.](http://arxiv.org/abs/1911.08053)

XiangRui Xu, YaQin Li, Cao Yuan


[Adversarial Attacks on Grid Events Classification: An Adversarial Machine Learning Approach.](http://arxiv.org/abs/1911.08011)

Iman Niazazari, Hanif Livani


[WITCHcraft: Efficient PGD attacks with random step size.](http://arxiv.org/abs/1911.07989)

Ping-Yeh Chiang, Jonas Geiping, Micah Goldblum, Tom Goldstein, Renkun Ni, Steven Reich, Ali Shafahi


[Deep Detector Health Management under Adversarial Campaigns.](http://arxiv.org/abs/1911.08090)

Javier Echauz, Keith Kenemer, Sarfaraz Hussein, Jay Dhaliwal, Saurabh Shintre, Slawomir Grzonkowski, Andrew Gardner


## 2019-11-17

[Countering Inconsistent Labelling by Google's Vision API for Rotated Images.](http://arxiv.org/abs/1911.07201)

Aman Apte, Aritra Bandyopadhyay, K Akhilesh Shenoy, Jason Peter Andrews, Aditya Rathod, Manish Agnihotri, Aditya Jajodia


[Deep Verifier Networks: Verification of Deep Discriminative Models with Deep Generative Models.](http://arxiv.org/abs/1911.07421)

Tong Che, Xiaofeng Liu, Site Li, Yubin Ge, Ruixiang Zhang, Caiming Xiong, Yoshua Bengio


[Smoothed Inference for Adversarially-Trained Models.](http://arxiv.org/abs/1911.07198)

Yaniv Nemcovsky, Evgenii Zheltonozhskii, Chaim Baskin, Brian Chmiel, Maxim Fishman, Alex M. Bronstein, Avi Mendelson


## 2019-11-16

[SMART: Skeletal Motion Action Recognition aTtack.](http://arxiv.org/abs/1911.07107)

He Wang, Feixiang He, Zexi Peng, Yongliang Yang, Tianjia Shao, Kun Zhou, David Hogg


[Suspicion-Free Adversarial Attacks on Clustering Algorithms.](http://arxiv.org/abs/1911.07015)

Anshuman Chhabra, Abhishek Roy, Prasant Mohapatra


[Black-Box Adversarial Attack with Transferable Model-based Embedding.](http://arxiv.org/abs/1911.07140)

Zhichao Huang, Tong Zhang


[Defensive Few-shot Learning.](http://arxiv.org/abs/1911.06968)

Wenbin Li, Lei Wang, Xingxing Zhang, Lei Qi, Jing Huo, Yang Gao, Jiebo Luo


## 2019-11-15

[Learning To Characterize Adversarial Subspaces.](http://arxiv.org/abs/1911.06587)

Xiaofeng Mao, Yuefeng Chen, Yuhong Li, Yuan He, Hui Xue


[On Model Robustness Against Adversarial Examples.](http://arxiv.org/abs/1911.06479)

Shufei Zhang, Kaizhu Huang, Zenglin Xu


[Simple iterative method for generating targeted universal adversarial perturbations.](http://arxiv.org/abs/1911.06502)

Hokuto Hirano, Kazuhiro Takemoto


[AdvKnn: Adversarial Attacks On K-Nearest Neighbor Classifiers With Approximate Gradients.](http://arxiv.org/abs/1911.06591)

Xiaodan Li, Yuefeng Chen, Yuan He, Hui Xue


## 2019-11-14

[Adversarial Embedding: A robust and elusive Steganography and Watermarking technique.](http://arxiv.org/abs/1912.01487)

Salah Ghamizi, Maxime Cordy, Mike Papadakis, Yves Le Traon


[Self-supervised Adversarial Training.](http://arxiv.org/abs/1911.06470)

Kejiang Chen, Hang Zhou, Yuefeng Chen, Xiaofeng Mao, Yuhong Li, Yuan He, Hui Xue, Weiming Zhang, Nenghai Yu


[DomainGAN: Generating Adversarial Examples to Attack Domain Generation Algorithm Classifiers.](http://arxiv.org/abs/1911.06285)

Isaac Corley, Jonathan Lwowski, Justin Hoffman


[CAGFuzz: Coverage-Guided Adversarial Generative Fuzzing Testing of Deep Learning Systems.](http://arxiv.org/abs/1911.07931)

Pengcheng Zhang, Qiyin Dai, Patrizio Pelliccione


## 2019-11-13

[There is Limited Correlation between Coverage and Robustness for Deep Neural Networks.](http://arxiv.org/abs/1911.05904)

Yizhen Dong, Peixin Zhang, Jingyi Wang, Shuang Liu, Jun Sun, Jianye Hao, Xinyu Wang, Li Wang, Jin Song Dong, Dai Ting


[Adversarial Margin Maximization Networks.](http://arxiv.org/abs/1911.05916)

Ziang Yan, Yiwen Guo, Changshui Zhang


## 2019-11-12

[Improving Robustness of Task Oriented Dialog Systems.](http://arxiv.org/abs/1911.05153)

Arash Einolghozati, Sonal Gupta, Mrinal Mohit, Rushin Shah


[On Robustness to Adversarial Examples and Polynomial Optimization.](http://arxiv.org/abs/1911.04681)

Pranjal Awasthi, Abhratanu Dutta, Aravindan Vijayaraghavan


[Adversarial Examples in Modern Machine Learning: A Review.](http://arxiv.org/abs/1911.05268)

Rey Reza Wiyatno, Anqi Xu, Ousmane Dia, Berker Archy de


## 2019-11-11

[Few-Features Attack to Fool Machine Learning Models through Mask-Based GAN.](http://arxiv.org/abs/1911.06269)

Feng Chen, Yunkai Shang, Bo Xu, Jincheng Hu


[RNN-Test: Towards Adversarial Testing for Recurrent Neural Network Systems.](http://arxiv.org/abs/1911.06155)

Jianmin Guo, Yue Zhao, Quan Zhang, Yu Jiang


[Learning From Brains How to Regularize Machines.](http://arxiv.org/abs/1911.05072)

Zhe Li, Wieland Brendel, Edgar Y. Walker, Erick Cobos, Taliah Muhammad, Jacob Reimer, Matthias Bethge, Fabian H. Sinz, Xaq Pitkow, Andreas S. Tolias


[Robust Design of Deep Neural Networks against Adversarial Attacks based on Lyapunov Theory.](http://arxiv.org/abs/1911.04636)

Arash Rahnama, Andre T. Nguyen, Edward Raff


[CALPA-NET: Channel-pruning-assisted Deep Residual Network for Steganalysis of Digital Images.](http://arxiv.org/abs/1911.04657)

Shunquan Tan, Weilong Wu, Zilong Shao, Qiushi Li, Bin Li, Jiwu Huang


[GraphDefense: Towards Robust Graph Convolutional Networks.](http://arxiv.org/abs/1911.04429)

Xiaoyun Wang, Xuanqing Liu, Cho-Jui Hsieh


## 2019-11-09

[A Reinforced Generation of Adversarial Samples for Neural Machine Translation.](http://arxiv.org/abs/1911.03677)

Wei Zou, Shujian Huang, Jun Xie, Xinyu Dai, Jiajun Chen


[Improving Machine Reading Comprehension via Adversarial Training.](http://arxiv.org/abs/1911.03614)

Ziqing Yang, Yiming Cui, Wanxiang Che, Ting Liu, Shijin Wang, Guoping Hu


[Adaptive versus Standard Descent Methods and Robustness Against Adversarial Examples.](http://arxiv.org/abs/1911.03784)

Marc Khoury


[Minimalistic Attacks: How Little it Takes to Fool a Deep Reinforcement Learning Policy.](http://arxiv.org/abs/1911.03849)

Xinghua Qu, Zhu Sun, Yew-Soon Ong, Abhishek Gupta, Pengfei Wei


## 2019-11-08

[Adversarial Attacks on Time-Series Intrusion Detection for Industrial Control Systems.](http://arxiv.org/abs/1911.04278)

Giulio Zizzo, Chris Hankin, Sergio Maffeis, Kevin Jones


[Patch augmentation: Towards efficient decision boundaries for neural networks.](http://arxiv.org/abs/1911.07922)

Marcus D. Bloice, Andreas Holzinger


[Domain Robustness in Neural Machine Translation.](http://arxiv.org/abs/1911.03109)

Mathias Müller, Annette Rios, Rico Sennrich


[Adversarial Attacks on GMM i-vector based Speaker Verification Systems.](http://arxiv.org/abs/1911.03078)

Xu Li, Jinghua Zhong, Xixin Wu, Jianwei Yu, Xunying Liu, Helen Meng


[Imperceptible Adversarial Attacks on Tabular Data.](http://arxiv.org/abs/1911.03274)

Vincent Ballet, Xavier Renard, Jonathan Aigrain, Thibault Laugel, Pascal Frossard, Marcin Detyniecki


## 2019-11-07

[White-Box Target Attack for EEG-Based BCI Regression Problems.](http://arxiv.org/abs/1911.04606)

Lubin Meng, Chin-Teng Lin, Tzyy-Ring Jung, Dongrui Wu


[Active Learning for Black-Box Adversarial Attacks in EEG-Based Brain-Computer Interfaces.](http://arxiv.org/abs/1911.04338)

Xue Jiang, Xiao Zhang, Dongrui Wu


## 2019-11-06

[Towards Large yet Imperceptible Adversarial Image Perturbations with Perceptual Color Distance.](http://arxiv.org/abs/1911.02466)

Zhengyu Zhao, Zhuoran Liu, Martha Larson


[Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods.](http://arxiv.org/abs/1911.02508)

Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, Himabindu Lakkaraju


[The Threat of Adversarial Attacks on Machine Learning in Network Security -- A Survey.](http://arxiv.org/abs/1911.02621)

Olakunle Ibitoye, Rana Abou-Khamis, Ashraf Matrawy, M. Omair Shafiq


[Reversible Adversarial Example based on Reversible Image Transformation.](http://arxiv.org/abs/1911.02360)

Zhaoxia Yin, Hua Wang, Weiming Zhang


## 2019-11-05

[Adversarial Enhancement for Community Detection in Complex Networks.](http://arxiv.org/abs/1911.01670)

Jiajun Zhou, Zhi Chen, Min Du, Lihong Chen, Shanqing Yu, Feifei Li, Guanrong Chen, Qi Xuan


[DLA: Dense-Layer-Analysis for Adversarial Example Detection.](http://arxiv.org/abs/1911.01921)

Philip Sperl, Ching-Yu Kao, Peng Chen, Konstantin Böttinger


[Intriguing Properties of Adversarial ML Attacks in the Problem Space.](http://arxiv.org/abs/1911.02142)

Fabio Pierazzi, Feargus Pendlebury, Jacopo Cortellazzi, Lorenzo Cavallaro


[Coverage Guided Testing for Recurrent Neural Networks.](http://arxiv.org/abs/1911.01952)

Wei Huang, Youcheng Sun, Xingyu Zhao, James Sharp, Wenjie Ruan, Jie Meng, Xiaowei Huang


## 2019-11-04

[Persistency of Excitation for Robustness of Neural Networks.](http://arxiv.org/abs/1911.01043)

Kamil Nar, S. Shankar Sastry


[Fast-UAP: An Algorithm for Speeding up Universal Adversarial Perturbation Generation with Orientation of Perturbation Vectors.](http://arxiv.org/abs/1911.01172)

Jiazhu Dai, Le Shu


[A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models.](http://arxiv.org/abs/1911.01559)

Ren Pang, Hua Shen, Xinyang Zhang, Shouling Ji, Yevgeniy Vorobeychik, Xiapu Luo, Alex Liu, Ting Wang


## 2019-11-03

[Who is Real Bob? Adversarial Attacks on Speaker Recognition Systems.](http://arxiv.org/abs/1911.01840)

Guangke Chen, Sen Chen, Lingling Fan, Xiaoning Du, Zhe Zhao, Fu Song, Yang Liu


[MadNet: Using a MAD Optimization for Defending Against Adversarial Attacks.](http://arxiv.org/abs/1911.00870)

Shai Rozenberg, Gal Elidan, Ran El-Yaniv


## 2019-11-02

[Automatic Detection of Generated Text is Easiest when Humans are Fooled.](http://arxiv.org/abs/1911.00650)

Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck


[Security of Facial Forensics Models Against Adversarial Attacks.](http://arxiv.org/abs/1911.00660)

Rong Huang, Fuming Fang, Huy H. Nguyen, Junichi Yamagishi, Isao Echizen


## 2019-10-31

[Enhancing Certifiable Robustness via a Deep Model Ensemble.](http://arxiv.org/abs/1910.14655)

Huan Zhang, Minhao Cheng, Cho-Jui Hsieh


[Certifiable Robustness to Graph Perturbations.](http://arxiv.org/abs/1910.14356)

Aleksandar Bojchevski, Stephan Günnemann


[Adversarial Music: Real World Audio Adversary Against Wake-word Detection System.](http://arxiv.org/abs/1911.00126)

Juncheng B. Li, Shuhui Qu, Xinjian Li, Joseph Szurley, J. Zico Kolter, Florian Metze


## 2019-10-30

[Investigating Resistance of Deep Learning-based IDS against Adversaries using min-max Optimization.](http://arxiv.org/abs/1910.14107)

Rana Abou Khamis, Omair Shafiq, Ashraf Matrawy


[Beyond Universal Person Re-ID Attack.](http://arxiv.org/abs/1910.14184)

Wenjie Ding, Xing Wei, Rongrong Ji, Xiaopeng Hong, Qi Tian, Yihong Gong


## 2019-10-29

[Adversarial Example in Remote Sensing Image Recognition.](http://arxiv.org/abs/1910.13222)

Li Chen, Guowei Zhu, Qi Li, Haifeng Li


## 2019-10-28

[Active Subspace of Neural Networks: Structural Analysis and Universal Attacks.](http://arxiv.org/abs/1910.13025)

Chunfeng Cui, Kaiqi Zhang, Talgat Daulbaev, Julia Gusak, Ivan Oseledets, Zheng Zhang


[Certified Adversarial Robustness for Deep Reinforcement Learning.](http://arxiv.org/abs/1910.12908)

Björn Lütjens, Michael Everett, Jonathan P. How


## 2019-10-27

[Word-level Textual Adversarial Attacking as Combinatorial Optimization.](http://arxiv.org/abs/1910.12196)

Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu, Maosong Sun


[EdgeFool: An Adversarial Image Enhancement Filter.](http://arxiv.org/abs/1910.12227)

Ali Shahin Shamsabadi, Changjae Oh, Andrea Cavallaro


[Spot Evasion Attacks: Adversarial Examples for License Plate Recognition Systems with Convolutional Neural Networks.](http://arxiv.org/abs/1911.00927)

Ya-guan Qian, Dan-feng Ma, Bin Wang, Jun Pan, Jia-min Wang, Jian-hai Chen, Wu-jie Zhou, Jing-sheng Lei


## 2019-10-26

[Detection of Adversarial Attacks and Characterization of Adversarial Subspace.](http://arxiv.org/abs/1910.12084)

Mohammad Esmaeilpour, Patrick Cardinal, Alessandro Lameiras Koerich


[Understanding and Quantifying Adversarial Examples Existence in Linear Classification.](http://arxiv.org/abs/1910.12163)

Xupeng Shi, A. Adam Ding


[Adversarial Defense Via Local Flatness Regularization.](http://arxiv.org/abs/1910.12165)

Jia Xu, Yiming Li, Yong Jiang, Shu-Tao Xia


## 2019-10-25

[Effectiveness of random deep feature selection for securing image manipulation detectors against adversarial examples.](http://arxiv.org/abs/1910.12392)

Mauro Barni, Ehsan Nowroozi, Benedetta Tondi, Bowen Zhang


[MediaEval 2019: Concealed FGSM Perturbations for Privacy Preservation.](http://arxiv.org/abs/1910.11603)

Panagiotis Linardos, Suzanne Little, Kevin McGuinness


[Label Smoothing and Logit Squeezing: A Replacement for Adversarial Training?](http://arxiv.org/abs/1910.11585)

Ali Shafahi, Amin Ghiasi, Furong Huang, Tom Goldstein


## 2019-10-24

[ATZSL: Defensive Zero-Shot Recognition in the Presence of Adversaries.](http://arxiv.org/abs/1910.10994)

Xingxing Zhang, Shupeng Gui, Zhenfeng Zhu, Yao Zhao, Ji Liu


## 2019-10-23

[A Useful Taxonomy for Adversarial Robustness of Neural Networks.](http://arxiv.org/abs/1910.10679)

Leslie N. Smith


[Wasserstein Smoothing: Certified Robustness against Wasserstein Adversarial Attacks.](http://arxiv.org/abs/1910.10783)

Alexander Levine, Soheil Feizi


## 2019-10-22

[Attacking Optical Flow.](http://arxiv.org/abs/1910.10053)

Anurag Ranjan, Joel Janai, Andreas Geiger, Michael J. Black


[Adversarial Example Detection by Classification for Deep Speech Recognition.](http://arxiv.org/abs/1910.10013)

Saeid Samizade, Zheng-Hua Tan, Chao Shen, Xiaohong Guan


[Cross-Representation Transferability of Adversarial Attacks: From Spectrograms to Audio Waveforms.](http://arxiv.org/abs/1910.10106)

Karl M. Koerich, Mohammad Esmailpour, Sajjad Abdoli, Alceu S. Jr. Britto, Alessandro L. Koerich


[Structure Matters: Towards Generating Transferable Adversarial Images.](http://arxiv.org/abs/1910.09821)

Dan Peng, Zizhan Zheng, Linhao Luo, Xiaofeng Zhang


## 2019-10-21

[Recovering Localized Adversarial Attacks.](http://arxiv.org/abs/1910.09239)

Jan Philip Göpfert, Heiko Wersing, Barbara Hammer


[Learning to Learn by Zeroth-Order Oracle.](http://arxiv.org/abs/1910.09464)

Yangjun Ruan, Yuanhao Xiong, Sashank Reddi, Sanjiv Kumar, Cho-Jui Hsieh


[An Alternative Surrogate Loss for PGD-based Adversarial Testing.](http://arxiv.org/abs/1910.09338)

Sven Gowal, Jonathan Uesato, Chongli Qin, Po-Sen Huang, Timothy Mann, Pushmeet Kohli


## 2019-10-20

[Enhancing Recurrent Neural Networks with Sememes.](http://arxiv.org/abs/1910.08910)

Yujia Qin, Fanchao Qi, Sicong Ouyang, Zhiyuan Liu, Cheng Yang, Yasheng Wang, Qun Liu, Maosong Sun


## 2019-10-19

[Adversarial Attacks on Spoofing Countermeasures of automatic speaker verification.](http://arxiv.org/abs/1910.08716)

Songxiang Liu, Haibin Wu, Hung-yi Lee, Helen Meng


## 2019-10-18

[Toward Metrics for Differentiating Out-of-Distribution Sets.](http://arxiv.org/abs/1910.08650)

Mahdieh Abbasi, Changjian Shui, Arezoo Rajabi, Christian Gagne, Rakesh Bobba


[Are Perceptually-Aligned Gradients a General Property of Robust Classifiers?](http://arxiv.org/abs/1910.08640)

Simran Kaur, Jeremy Cohen, Zachary C. Lipton


[Spatial-aware Online Adversarial Perturbations Against Visual Object Tracking.](http://arxiv.org/abs/1910.08681)

Qing Guo, Xiaofei Xie, Lei Ma, Zhongguo Li, Wei Feng, Yang Liu


[A Fast Saddle-Point Dynamical System Approach to Robust Deep Learning.](http://arxiv.org/abs/1910.08623)

Yasaman Esfandiari, Aditya Balu, Keivan Ebrahimi, Umesh Vaidya, Nicola Elia, Soumik Sarkar


## 2019-10-17

[Instance adaptive adversarial training: Improved accuracy tradeoffs in neural nets.](http://arxiv.org/abs/1910.08051)

Yogesh Balaji, Tom Goldstein, Judy Hoffman


[Enforcing Linearity in DNN succours Robustness and Adversarial Image Generation.](http://arxiv.org/abs/1910.08108)

Anindya Sarkar, Nikhil Kumar Gupta, Raghu Iyengar


[LanCe: A Comprehensive and Lightweight CNN Defense Methodology against Physical Adversarial Attacks on Embedded Multimedia Applications.](http://arxiv.org/abs/1910.08536)

Zirui Xu, Fuxun Yu, Xiang Chen


[Adversarial T-shirt! Evading Person Detectors in A Physical World.](http://arxiv.org/abs/1910.11099)

Kaidi Xu, Gaoyuan Zhang, Sijia Liu, Quanfu Fan, Mengshu Sun, Hongge Chen, Pin-Yu Chen, Yanzhi Wang, Xue Lin


## 2019-10-16

[A New Defense Against Adversarial Images: Turning a Weakness into a Strength.](http://arxiv.org/abs/1910.07629)

Tao Yu, Shengyuan Hu, Chuan Guo, Wei-Lun Chao, Kilian Q. Weinberger


## 2019-10-15

[Improving Robustness of time series classifier with Neural ODE guided gradient based data augmentation.](http://arxiv.org/abs/1910.06813)

Anindya Sarkar, Anirudh Sunder Raj, Raghu Sesha Iyengar


[Understanding Misclassifications by Attributes.](http://arxiv.org/abs/1910.07416)

Sadaf Gulshad, Zeynep Akata, Jan Hendrik Metzen, Arnold Smeulders


[Adversarial Examples for Models of Code.](http://arxiv.org/abs/1910.07517)

Noam Yefet, Uri Alon, Eran Yahav


[On adversarial patches: real-world attack on ArcFace-100 face recognition system.](http://arxiv.org/abs/1910.07067)

Mikhail Pautov, Grigorii Melnikov, Edgar Kaziakhmedov, Klim Kireev, Aleksandr Petiushko


## 2019-10-14

[DeepSearch: Simple and Effective Blackbox Fuzzing of Deep Neural Networks.](http://arxiv.org/abs/1910.06296)

Fuyuan Zhang, Sankalan Pal Chowdhury, Maria Christakis


[Confidence-Calibrated Adversarial Training: Generalizing to Unseen Attacks.](http://arxiv.org/abs/1910.06259)

David Stutz, Matthias Hein, Bernt Schiele


[ZO-AdaMM: Zeroth-Order Adaptive Momentum Method for Black-Box Optimization.](http://arxiv.org/abs/1910.06513)

Xiangyi Chen, Sijia Liu, Kaidi Xu, Xingguo Li, Xue Lin, Mingyi Hong, David Cox


[Man-in-the-Middle Attacks against Machine Learning Classifiers via Malicious Generative Models.](http://arxiv.org/abs/1910.06838)

Derek Derui, Wang, Chaoran Li, Sheng Wen, Surya Nepal, Yang Xiang


[Real-world adversarial attack on MTCNN face detection system.](http://arxiv.org/abs/1910.06261)

Edgar Kaziakhmedov, Klim Kireev, Grigorii Melnikov, Mikhail Pautov, Aleksandr Petiushko


## 2019-10-12

[On Robustness of Neural Ordinary Differential Equations.](http://arxiv.org/abs/1910.05513)

Hanshu Yan, Jiawei Du, Vincent Y. F. Tan, Jiashi Feng


## 2019-10-11

[Hear "No Evil", See "Kenansville": Efficient and Transferable Black-Box Attacks on Speech Recognition and Voice Identification Systems.](http://arxiv.org/abs/1910.05262)

Hadi Abdullah, Muhammad Sajidur Rahman, Washington Garcia, Logan Blue, Kevin Warren, Anurag Swarnim Yadav, Tom Shrimpton, Patrick Traynor


[Verification of Neural Networks: Specifying Global Robustness using Generative Models.](http://arxiv.org/abs/1910.05018)

Nathanaël Fijalkow, Mohit Kumar Gupta


## 2019-10-10

[Universal Adversarial Perturbation for Text Classification.](http://arxiv.org/abs/1910.04618)

Hang Gao, Tim Oates


[Information Aware Max-Norm Dirichlet Networks for Predictive Uncertainty Estimation.](http://arxiv.org/abs/1910.04819)

Theodoros Tsiligkaridis


## 2019-10-09

[Learning deep forest with multi-scale Local Binary Pattern features for face anti-spoofing.](http://arxiv.org/abs/1910.03850)

Rizhao Cai, Changsheng Chen


[Adversarial Learning of Deepfakes in Accounting.](http://arxiv.org/abs/1910.03810)

Marco Schreyer, Timur Sattarov, Bernd Reimer, Damian Borth


[Deep Latent Defence.](http://arxiv.org/abs/1910.03916)

Giulio Zizzo, Chris Hankin, Sergio Maffeis, Kevin Jones


[Adversarial Training: embedding adversarial perturbations into the parameter space of a neural network to build a robust system.](http://arxiv.org/abs/1910.04279)

Shixian Wen, Laurent Itti


## 2019-10-08

[Directional Adversarial Training for Cost Sensitive Deep Learning Classification Applications.](http://arxiv.org/abs/1910.03468)

Matteo Terzi, Gian Antonio Susto, Pratik Chaudhari


[SmoothFool: An Efficient Framework for Computing Smooth Adversarial Perturbations.](http://arxiv.org/abs/1910.03624)

Ali Dabouei, Sobhan Soleymani, Fariborz Taherkhani, Jeremy Dawson, Nasser M. Nasrabadi


## 2019-10-07

[Interpretable Disentanglement of Neural Networks by Extracting Class-Specific Subnetwork.](http://arxiv.org/abs/1910.02673)

Yulong Wang, Xiaolin Hu, Hang Su


## 2019-10-05

[Unrestricted Adversarial Attacks for Semantic Segmentation.](http://arxiv.org/abs/1910.02354)

Guangyu Shen, Chengzhi Mao, Junfeng Yang, Baishakhi Ray


[Yet another but more efficient black-box adversarial attack: tiling and evolution strategies.](http://arxiv.org/abs/1910.02244)

Laurent Meunier, Jamal Atif, Olivier Teytaud


## 2019-10-04

[Requirements for Developing Robust Neural Networks.](http://arxiv.org/abs/1910.02125)

John S. Hyatt, Michael S. Lee


[Adversarial Examples for Cost-Sensitive Classifiers.](http://arxiv.org/abs/1910.02095)

Gavin S. Hartnett, Andrew J. Lohn, Alexander P. Sedlack


## 2019-10-03

[Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions.](http://arxiv.org/abs/1910.01329)

He Zhao, Trung Le, Paul Montague, Vel Olivier De, Tamas Abraham, Dinh Phung


[BUZz: BUffer Zones for defending adversarial examples in image classification.](http://arxiv.org/abs/1910.02785)

Kaleel Mahmood, Phuong Ha Nguyen, Lam M. Nguyen, Thanh Nguyen, Dijk Marten van


[Verification of Neural Network Behaviour: Formal Guarantees for Power System Applications.](http://arxiv.org/abs/1910.01624)

Andreas Venzke, Spyros Chatzivasileiadis


## 2019-10-02

[Attacking Vision-based Perception in End-to-End Autonomous Driving Models.](http://arxiv.org/abs/1910.01907)

Adith Boloor, Karthik Garimella, Xin He, Christopher Gill, Yevgeniy Vorobeychik, Xuan Zhang


[Adversarially Robust Few-Shot Learning: A Meta-Learning Approach.](http://arxiv.org/abs/1910.00982)

Micah Goldblum, Liam Fowl, Tom Goldstein


## 2019-10-01

[Boosting Image Recognition with Non-differentiable Constraints.](http://arxiv.org/abs/1910.00736)

Xuan Li, Yuchen Lu, Peng Xu, Jizong Peng, Christian Desrosiers, Xue Liu


[Generating Semantic Adversarial Examples with Differentiable Rendering.](http://arxiv.org/abs/1910.00727)

Lakshya Jain, Wilson Wu, Steven Chen, Uyeong Jang, Varun Chandrasekaran, Sanjit Seshia, Somesh Jha


[Attacking CNN-based anti-spoofing face authentication in the physical domain.](http://arxiv.org/abs/1910.00327)

Bowen Zhang, Benedetta Tondi, Mauro Barni


[An Efficient and Margin-Approaching Zero-Confidence Adversarial Attack.](http://arxiv.org/abs/1910.00511)

Yang Zhang, Shiyu Chang, Mo Yu, Kaizhi Qian


[Cross-Layer Strategic Ensemble Defense Against Adversarial Examples.](http://arxiv.org/abs/1910.01742)

Wenqi Wei, Ling Liu, Margaret Loper, Ka-Ho Chow, Emre Gursoy, Stacey Truex, Yanzhao Wu


[Deep Neural Rejection against Adversarial Examples.](http://arxiv.org/abs/1910.00470)

Angelo Sotgiu, Ambra Demontis, Marco Melis, Battista Biggio, Giorgio Fumera, Xiaoyi Feng, Fabio Roli


## 2019-09-30

[Black-box Adversarial Attacks with Bayesian Optimization.](http://arxiv.org/abs/1909.13857)

Satya Narayan Shukla, Anit Kumar Sahu, Devin Willmott, J. Zico Kolter


[Min-Max Optimization without Gradients: Convergence and Applications to Adversarial ML.](http://arxiv.org/abs/1909.13806)

Sijia Liu, Songtao Lu, Xiangyi Chen, Yao Feng, Kaidi Xu, Abdullah Al-Dujaili, Minyi Hong, Una-May O'Reilly


[Role of Spatial Context in Adversarial Robustness for Object Detection.](http://arxiv.org/abs/1910.00068)

Aniruddha Saha, Akshayvarun Subramanya, Koninika Patil, Hamed Pirsiavash


## 2019-09-29

[Techniques for Adversarial Examples Threatening the Safety of Artificial Intelligence Based Systems.](http://arxiv.org/abs/1910.06907)

Utku Kose


## 2019-09-27

[Maximal adversarial perturbations for obfuscation: Hiding certain attributes while preserving rest.](http://arxiv.org/abs/1909.12734)

Indu Ilanchezian, Praneeth Vepakomma, Abhishek Singh, Otkrist Gupta, G. N. Srinivasa Prasanna, Ramesh Raskar


[Impact of Low-bitwidth Quantization on the Adversarial Robustness for Embedded Neural Networks.](http://arxiv.org/abs/1909.12741)

Rémi Bernhard, Pierre-Alain Moellic, Jean-Max Dutertre


[Training-Free Uncertainty Estimation for Dense Regression: Sensitivity as a Surrogate. (1%)](http://arxiv.org/abs/1910.04858)

Lu Mi, Hao Wang, Yonglong Tian, Hao He, Nir Shavit


## 2019-09-26

[Towards Understanding the Transferability of Deep Representations.](http://arxiv.org/abs/1909.12031)

Hong Liu, Mingsheng Long, Jianmin Wang, Michael I. Jordan


[Adversarial Machine Learning Attack on Modulation Classification.](http://arxiv.org/abs/1909.12167)

Muhammad Usama, Muhammad Asim, Junaid Qadir, Ala Al-Fuqaha, Muhammad Ali Imran


[Adversarial ML Attack on Self Organizing Cellular Networks.](http://arxiv.org/abs/1909.12161)

Salah-ud-din Farooq, Muhammad Usama, Junaid Qadir, Muhammad Ali Imran


[Towards neural networks that provably know when they don't know.](http://arxiv.org/abs/1909.12180)

Alexander Meinke, Matthias Hein


[Lower Bounds on Adversarial Robustness from Optimal Transport.](http://arxiv.org/abs/1909.12272)

Arjun Nitin Bhagoji, Daniel Cullina, Prateek Mittal


## 2019-09-25

[Probabilistic Modeling of Deep Features for Out-of-Distribution and Adversarial Detection.](http://arxiv.org/abs/1909.11786)

Nilesh A. Ahuja, Ibrahima Ndiour, Trushant Kalyanpur, Omesh Tickoo


[Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks.](http://arxiv.org/abs/1909.11515)

Tianyu Pang, Kun Xu, Jun Zhu


[FreeLB: Enhanced Adversarial Training for Natural Language Understanding.](http://arxiv.org/abs/1909.11764)

Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, Jingjing Liu


## 2019-09-24

[A Visual Analytics Framework for Adversarial Text Generation.](http://arxiv.org/abs/1909.11202)

Brandon Laughlin, Christopher Collins, Karthik Sankaranarayanan, Khalil El-Khatib


[Intelligent image synthesis to attack a segmentation CNN using adversarial learning.](http://arxiv.org/abs/1909.11167)

Liang Chen, Paul Bentley, Kensaku Mori, Kazunari Misawa, Michitaka Fujiwara, Daniel Rueckert


[Sign-OPT: A Query-Efficient Hard-label Adversarial Attack.](http://arxiv.org/abs/1909.10773)

Minhao Cheng, Simranjit Singh, Patrick Chen, Pin-Yu Chen, Sijia Liu, Cho-Jui Hsieh


[Matrix Sketching for Secure Collaborative Machine Learning. (1%)](http://arxiv.org/abs/1909.11201)

Mengjiao Zhang, Shusen Wang


## 2019-09-23

[MemGuard: Defending against Black-Box Membership Inference Attacks via Adversarial Examples.](http://arxiv.org/abs/1909.10594)

Jinyuan Jia, Ahmed Salem, Michael Backes, Yang Zhang, Neil Zhenqiang Gong


[Robust Local Features for Improving the Generalization of Adversarial Training.](http://arxiv.org/abs/1909.10147)

Chuanbiao Song, Kun He, Jiadong Lin, Liwei Wang, John E. Hopcroft


[FENCE: Feasible Evasion Attacks on Neural Networks in Constrained Environments.](http://arxiv.org/abs/1909.10480)

Alesia Chernikova, Alina Oprea


## 2019-09-22

[HAWKEYE: Adversarial Example Detector for Deep Neural Networks.](http://arxiv.org/abs/1909.09938)

Jinkyu Koo, Michael Roth, Saurabh Bagchi


[Towards Interpreting Recurrent Neural Networks through Probabilistic Abstraction.](http://arxiv.org/abs/1909.10023)

Guoliang Dong, Jingyi Wang, Jun Sun, Yang Zhang, Xinyu Wang, Ting Dai, Jin Song Dong, Xingen Wang


## 2019-09-20

[Adversarial Learning with Margin-based Triplet Embedding Regularization.](http://arxiv.org/abs/1909.09481)

Yaoyao Zhong, Weihong Deng


[COPYCAT: Practical Adversarial Attacks on Visualization-Based Malware Detection.](http://arxiv.org/abs/1909.09735)

Aminollah Khormali, Ahmed Abusnaina, Songqing Chen, DaeHun Nyang, Aziz Mohaisen


[Defending Against Physically Realizable Attacks on Image Classification.](http://arxiv.org/abs/1909.09552)

Tong Wu, Liang Tong, Yevgeniy Vorobeychik


## 2019-09-19

[Propagated Perturbation of Adversarial Attack for well-known CNNs: Empirical Study and its Explanation.](http://arxiv.org/abs/1909.09263)

Jihyeun Yoon, Kyungyul Kim, Jongseong Jang


[Adversarial Vulnerability Bounds for Gaussian Process Classification.](http://arxiv.org/abs/1909.08864)

Michael Thomas Smith, Kathrin Grosse, Michael Backes, Mauricio A Alvarez


[Absum: Simple Regularization Method for Reducing Structural Sensitivity of Convolutional Neural Networks.](http://arxiv.org/abs/1909.08830)

Sekitoshi Kanai, Yasutoshi Ida, Yasuhiro Fujiwara, Masanori Yamada, Shuichi Adachi


[Toward Robust Image Classification.](http://arxiv.org/abs/1909.12927)

Basemah Alshemali, Alta Graham, Jugal Kalita


[Training Robust Deep Neural Networks via Adversarial Noise Propagation.](http://arxiv.org/abs/1909.09034)

Aishan Liu, Xianglong Liu, Chongzhi Zhang, Hang Yu, Qiang Liu, Dacheng Tao


## 2019-09-17

[Adversarial Attacks and Defenses in Images, Graphs and Text: A Review.](http://arxiv.org/abs/1909.08072)

Han Xu, Yao Ma, Haochen Liu, Debayan Deb, Hui Liu, Jiliang Tang, Anil Jain


[Generating Black-Box Adversarial Examples for Text Classifiers Using a Deep Reinforced Model.](http://arxiv.org/abs/1909.07873)

Prashanth Vijayaraghavan, Deb Roy


[Defending against Machine Learning based Inference Attacks via Adversarial Examples: Opportunities and Challenges.](http://arxiv.org/abs/1909.08526)

Jinyuan Jia, Neil Zhenqiang Gong


## 2019-09-16

[They Might NOT Be Giants: Crafting Black-Box Adversarial Examples with Fewer Queries Using Particle Swarm Optimization.](http://arxiv.org/abs/1909.07490)

Rayan Mosli, Matthew Wright, Bo Yuan, Yin Pan


[HAD-GAN: A Human-perception Auxiliary Defense GAN to Defend Adversarial Examples.](http://arxiv.org/abs/1909.07558)

Wanting Yu, Hongyi Yu, Lingyun Jiang, Mengli Zhang, Kai Qiao, Linyuan Wang, Bin Yan


[Towards Quality Assurance of Software Product Lines with Adversarial Configurations.](http://arxiv.org/abs/1909.07283)

Paul Temple, Mathieu Acher, Gilles Perrouin, Battista Biggio, Jean-marc Jezequel, Fabio Roli


[Interpreting and Improving Adversarial Robustness with Neuron Sensitivity.](http://arxiv.org/abs/1909.06978)

Chongzhi Zhang, Aishan Liu, Xianglong Liu, Yitao Xu, Hang Yu, Yuqing Ma, Tianlin Li


## 2019-09-15

[An Empirical Study towards Characterizing Deep Learning Development and Deployment across Different Frameworks and Platforms.](http://arxiv.org/abs/1909.06727)

Qianyu Guo, Sen Chen, Xiaofei Xie, Lei Ma, Qiang Hu, Hongtao Liu, Yang Liu, Jianjun Zhao, Xiaohong Li


[Detecting Adversarial Samples Using Influence Functions and Nearest Neighbors.](http://arxiv.org/abs/1909.06872)

Gilad Cohen, Guillermo Sapiro, Raja Giryes


## 2019-09-14

[Natural Language Adversarial Attacks and Defenses in Word Level.](http://arxiv.org/abs/1909.06723)

Xiaosen Wang, Hao Jin, Kun He


## 2019-09-13

[Adversarial Attack on Skeleton-based Human Action Recognition.](http://arxiv.org/abs/1909.06500)

Jian Liu, Naveed Akhtar, Ajmal Mian


[Say What I Want: Towards the Dark Side of Neural Dialogue Models.](http://arxiv.org/abs/1909.06044)

Haochen Liu, Tyler Derr, Zitao Liu, Jiliang Tang


[White-Box Adversarial Defense via Self-Supervised Data Estimation.](http://arxiv.org/abs/1909.06271)

Zudi Lin, Hanspeter Pfister, Ziming Zhang


[Defending Against Adversarial Attacks by Suppressing the Largest Eigenvalue of Fisher Information Matrix.](http://arxiv.org/abs/1909.06137)

Chaomin Shen, Yaxin Peng, Guixu Zhang, Jinsong Fan


## 2019-09-12

[Inspecting adversarial examples using the Fisher information.](http://arxiv.org/abs/1909.05527)

Jörg Martin, Clemens Elster


[An Empirical Investigation of Randomized Defenses against Adversarial Attacks.](http://arxiv.org/abs/1909.05580)

Yannik Potdevin, Dirk Nowotka, Vijay Ganesh


[Transferable Adversarial Robustness using Adversarially Trained Autoencoders.](http://arxiv.org/abs/1909.05921)

Pratik Vaishnavi, Kevin Eykholt, Atul Prakash, Amir Rahmati


## 2019-09-11

[Feedback Learning for Improving the Robustness of Neural Networks.](http://arxiv.org/abs/1909.05443)

Chang Song, Zuoguan Wang, Hai Li


[Sparse and Imperceivable Adversarial Attacks.](http://arxiv.org/abs/1909.05040)

Francesco Croce, Matthias Hein


## 2019-09-10

[Localized Adversarial Training for Increased Accuracy and Robustness in Image Classification.](http://arxiv.org/abs/1909.04779)

Eitan Rothberg, Tingting Chen, Luo Jie, Hao Ji


[Identifying and Resisting Adversarial Videos Using Temporal Consistency.](http://arxiv.org/abs/1909.04837)

Xiaojun Jia, Xingxing Wei, Xiaochun Cao


[Effectiveness of Adversarial Examples and Defenses for Malware Classification.](http://arxiv.org/abs/1909.04778)

Robert Podschwadt, Hassan Takabi


[Towards Noise-Robust Neural Networks via Progressive Adversarial Training.](http://arxiv.org/abs/1909.04839)

Hang Yu, Aishan Liu, Xianglong Liu, Jichen Yang, Chongzhi Zhang


[UPC: Learning Universal Physical Camouflage Attacks on Object Detectors.](http://arxiv.org/abs/1909.04326)

Lifeng Huang, Chengying Gao, Yuyin Zhou, Changqing Zou, Cihang Xie, Alan Yuille, Ning Liu


[FDA: Feature Disruptive Attack.](http://arxiv.org/abs/1909.04385)

Aditya Ganeshan, B. S. Vivek, R. Venkatesh Babu


[Learning to Disentangle Robust and Vulnerable Features for Adversarial Detection.](http://arxiv.org/abs/1909.04311)

Byunggill Joe, Sung Ju Hwang, Insik Shin


[Toward Finding The Global Optimal of Adversarial Examples.](http://arxiv.org/abs/1909.04288)

Zhenxin Xiao, Kai-Wei Chang, Cho-Jui Hsieh


## 2019-09-09

[Adversarial Robustness Against the Union of Multiple Perturbation Models.](http://arxiv.org/abs/1909.04068)

Pratyush Maini, Eric Wong, J. Zico Kolter


[DeepObfuscator: Obfuscating Intermediate Representations with Privacy-Preserving Adversarial Learning on Smartphones. (1%)](http://arxiv.org/abs/1909.04126)

Ang Li, Jiayi Guo, Huanrui Yang, Flora D. Salim, Yiran Chen


## 2019-09-08

[STA: Adversarial Attacks on Siamese Trackers.](http://arxiv.org/abs/1909.03413)

Xugang Wu, Xiaoping Wang, Xu Zhou, Songlei Jian


[When Explainability Meets Adversarial Learning: Detecting Adversarial Examples using SHAP Signatures.](http://arxiv.org/abs/1909.03418)

Gil Fidel, Ron Bitton, Asaf Shabtai


## 2019-09-06

[Learning to Discriminate Perturbations for Blocking Adversarial Attacks in Text Classification.](http://arxiv.org/abs/1909.03084)

Yichao Zhou, Jyun-Yu Jiang, Kai-Wei Chang, Wei Wang


[Natural Adversarial Sentence Generation with Gradient-based Perturbation.](http://arxiv.org/abs/1909.04495)

Yu-Lun Hsieh, Minhao Cheng, Da-Cheng Juan, Wei Wei, Wen-Lian Hsu, Cho-Jui Hsieh


[Blackbox Attacks on Reinforcement Learning Agents Using Approximated Temporal Information.](http://arxiv.org/abs/1909.02918)

Yiren Zhao, Ilia Shumailov, Han Cui, Xitong Gao, Robert Mullins, Ross Anderson


## 2019-09-05

[Spatiotemporally Constrained Action Space Attacks on Deep Reinforcement Learning Agents.](http://arxiv.org/abs/1909.02583)

Xian Yeow Lee, Sambit Ghadai, Kai Liang Tan, Chinmay Hegde, Soumik Sarkar


[Adversarial Examples with Difficult Common Words for Paraphrase Identification.](http://arxiv.org/abs/1909.02560)

Zhouxing Shi, Minlie Huang, Ting Yao, Jingfang Xu


## 2019-09-04

[Are Adversarial Robustness and Common Perturbation Robustness Independent Attributes ?](http://arxiv.org/abs/1909.02436)

Alfred Laugros, Alice Caplier, Matthieu Ospici


## 2019-09-03

[Certified Robustness to Adversarial Word Substitutions.](http://arxiv.org/abs/1909.00986)

Robin Jia, Aditi Raghunathan, Kerem Göksel, Percy Liang


[Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation.](http://arxiv.org/abs/1909.01492)

Po-Sen Huang, Robert Stanforth, Johannes Welbl, Chris Dyer, Dani Yogatama, Sven Gowal, Krishnamurthy Dvijotham, Pushmeet Kohli


## 2019-09-02

[Metric Learning for Adversarial Robustness.](http://arxiv.org/abs/1909.00900)

Chengzhi Mao, Ziyuan Zhong, Junfeng Yang, Carl Vondrick, Baishakhi Ray


## 2019-08-29

[Adversarial Training Methods for Network Embedding.](http://arxiv.org/abs/1908.11514)

Quanyu Dai, Xiao Shen, Liang Zhang, Qiang Li, Dan Wang


[Deep Neural Network Ensembles against Deception: Ensemble Diversity, Accuracy and Robustness.](http://arxiv.org/abs/1908.11091)

Ling Liu, Wenqi Wei, Ka-Ho Chow, Margaret Loper, Emre Gursoy, Stacey Truex, Yanzhao Wu


[Defeating Misclassification Attacks Against Transfer Learning.](http://arxiv.org/abs/1908.11230)

Bang Wu, Shuo Wang, Xingliang Yuan, Cong Wang, Carsten Rudolph, Xiangwen Yang


[Universal, transferable and targeted adversarial attacks.](http://arxiv.org/abs/1908.11332)

Junde Wu, Rao Fu


## 2019-08-26

[A Statistical Defense Approach for Detecting Adversarial Examples.](http://arxiv.org/abs/1908.09705)

Alessandro Cennamo, Ido Freeman, Anton Kummert


[Gated Convolutional Networks with Hybrid Connectivity for Image Classification.](http://arxiv.org/abs/1908.09699)

Chuanguang Yang, Zhulin An, Hui Zhu, Xiaolong Hu, Kun Zhang, Kaiqiang Xu, Chao Li, Yongjun Xu


## 2019-08-25

[Adversarial Edit Attacks for Tree Data.](http://arxiv.org/abs/1908.09364)

Benjamin Paaßen


[advPattern: Physical-World Attacks on Deep Person Re-Identification via Adversarially Transformable Patterns.](http://arxiv.org/abs/1908.09327)

Zhibo Wang, Siyan Zheng, Mengkai Song, Qian Wang, Alireza Rahimpour, Hairong Qi


## 2019-08-24

[Targeted Mismatch Adversarial Attack: Query with a Flower to Retrieve the Tower.](http://arxiv.org/abs/1908.09163)

Giorgos Tolias, Filip Radenovic, Ond{ř}ej Chum


## 2019-08-23

[Improving Adversarial Robustness via Attention and Adversarial Logit Pairing.](http://arxiv.org/abs/1908.11435)

Dou Goodman, Xingjian Li, Jun Huan, Tao Wei


[AdvHat: Real-world adversarial attack on ArcFace Face ID system.](http://arxiv.org/abs/1908.08705)

Stepan Komkov, Aleksandr Petiushko


## 2019-08-22

[Saliency Methods for Explaining Adversarial Attacks.](http://arxiv.org/abs/1908.08413)

Jindong Gu, Volker Tresp


## 2019-08-21

[Testing Robustness Against Unforeseen Adversaries.](http://arxiv.org/abs/1908.08016)

Daniel Kang, Yi Sun, Dan Hendrycks, Tom Brown, Jacob Steinhardt


[Evaluating Defensive Distillation For Defending Text Processing Neural Networks Against Adversarial Examples.](http://arxiv.org/abs/1908.07899)

Marcus Soll, Tobias Hinz, Sven Magg, Stefan Wermter


## 2019-08-20

[Denoising and Verification Cross-Layer Ensemble Against Black-box Adversarial Attacks.](http://arxiv.org/abs/1908.07667)

Ka-Ho Chow, Wenqi Wei, Yanzhao Wu, Ling Liu


[Transferring Robustness for Graph Neural Network Against Poisoning Attacks.](http://arxiv.org/abs/1908.07558)

Xianfeng Tang, Yandong Li, Yiwei Sun, Huaxiu Yao, Prasenjit Mitra, Suhang Wang


## 2019-08-19

[Universal Adversarial Triggers for NLP.](http://arxiv.org/abs/1908.07125)

Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, Sameer Singh


[Protecting Neural Networks with Hierarchical Random Switching: Towards Better Robustness-Accuracy Trade-off for Stochastic Defenses.](http://arxiv.org/abs/1908.07116)

Xiao Wang, Siyue Wang, Pin-Yu Chen, Yanzhi Wang, Brian Kulis, Xue Lin, Peter Chin


[Hybrid Batch Attacks: Finding Black-box Adversarial Examples with Limited Queries.](http://arxiv.org/abs/1908.07000)

Fnu Suya, Jianfeng Chi, David Evans, Yuan Tian


## 2019-08-18

[On the Robustness of Human Pose Estimation.](http://arxiv.org/abs/1908.06401)

Sahil Shah, Naman Jain, Abhishek Sharma, Arjun Jain


[Adversarial Defense by Suppressing High-frequency Components.](http://arxiv.org/abs/1908.06566)

Zhendong Zhang, Cheolkon Jung, Xiaolong Liang


## 2019-08-17

[Verification of Neural Network Control Policy Under Persistent Adversarial Perturbation.](http://arxiv.org/abs/1908.06353)

Yuh-Shyang Wang, Tsui-Wei Weng, Luca Daniel


[Nesterov Accelerated Gradient and Scale Invariance for Adversarial Attacks.](http://arxiv.org/abs/1908.06281)

Jiadong Lin, Chuanbiao Song, Kun He, Liwei Wang, John E. Hopcroft


## 2019-08-16

[Adversarial point perturbations on 3D objects.](http://arxiv.org/abs/1908.06062)

Daniel Liu, Ronald Yu, Hao Su


## 2019-08-14

[Once a MAN: Towards Multi-Target Attack via Learning Multi-Target Adversarial Network Once.](http://arxiv.org/abs/1908.05185)

Jiangfan Han, Xiaoyi Dong, Ruimao Zhang, Dongdong Chen, Weiming Zhang, Nenghai Yu, Ping Luo, Xiaogang Wang


[AdvFaces: Adversarial Face Synthesis.](http://arxiv.org/abs/1908.05008)

Debayan Deb, Jianbang Zhang, Anil K. Jain


[DAPAS : Denoising Autoencoder to Prevent Adversarial attack in Semantic Segmentation.](http://arxiv.org/abs/1908.05195)

Seungju Cho, Tae Joon Jun, Byungsoo Oh, Daeyoung Kim


## 2019-08-12

[On Defending Against Label Flipping Attacks on Malware Detection Systems.](http://arxiv.org/abs/1908.04473)

Rahim Taheri, Reza Javidan, Mohammad Shojafar, Zahra Pooranian, Ali Miri, Mauro Conti


[Adversarial Neural Pruning with Latent Vulnerability Suppression.](http://arxiv.org/abs/1908.04355)

Divyam Madaan, Jinwoo Shin, Sung Ju Hwang


## 2019-08-09

[On the Adversarial Robustness of Neural Networks without Weight Transport.](http://arxiv.org/abs/1908.03560)

Mohamed Akrout


## 2019-08-08

[Defending Against Adversarial Iris Examples Using Wavelet Decomposition.](http://arxiv.org/abs/1908.03176)

Sobhan Soleymani, Ali Dabouei, Jeremy Dawson, Nasser M. Nasrabadi


[Universal Adversarial Audio Perturbations.](http://arxiv.org/abs/1908.03173)

Sajjad Abdoli, Luiz G. Hafemann, Jerome Rony, Ismail Ben Ayed, Patrick Cardinal, Alessandro L. Koerich


## 2019-08-07

[Improved Adversarial Robustness by Reducing Open Space Risk via Tent Activations.](http://arxiv.org/abs/1908.02435)

Andras Rozsa, Terrance E. Boult


[Investigating Decision Boundaries of Trained Neural Networks.](http://arxiv.org/abs/1908.02802)

Roozbeh Yousefzadeh, Dianne P O'Leary


## 2019-08-06

[Explaining Deep Neural Networks Using Spectrum-Based Fault Localization.](http://arxiv.org/abs/1908.02374)

Youcheng Sun, Hana Chockler, Xiaowei Huang, Daniel Kroening


[MetaAdvDet: Towards Robust Detection of Evolving Adversarial Attacks.](http://arxiv.org/abs/1908.02199)

Chen Ma, Chenxu Zhao, Hailin Shi, Li Chen, Junhai Yong, Dan Zeng


[BlurNet: Defense by Filtering the Feature Maps.](http://arxiv.org/abs/1908.02256)

Ravi Raju, Mikko Lipasti


## 2019-08-05

[Random Directional Attack for Fooling Deep Neural Networks.](http://arxiv.org/abs/1908.02658)

Wenjian Luo, Chenwang Wu, Nan Zhou, Li Ni


[Adversarial Self-Defense for Cycle-Consistent GANs.](http://arxiv.org/abs/1908.01517)

Dina Bashkirova, Ben Usman, Kate Saenko


[Automated Detection System for Adversarial Examples with High-Frequency Noises Sieve.](http://arxiv.org/abs/1908.01469)

Dang Duy Thang, Toshihiro Matsui


[A principled approach for generating adversarial images under non-smooth dissimilarity metrics.](http://arxiv.org/abs/1908.01667)

Aram-Alexandre Pooladian, Chris Finlay, Tim Hoheisel, Adam Oberman


[Imperio: Robust Over-the-Air Adversarial Examples for Automatic Speech Recognition Systems.](http://arxiv.org/abs/1908.01551)

Lea Schönherr, Thorsten Eisenhofer, Steffen Zeiler, Thorsten Holz, Dorothea Kolossa


## 2019-08-04

[A Restricted Black-box Adversarial Framework Towards Attacking Graph Embedding Models.](http://arxiv.org/abs/1908.01297)

Heng Chang, Yu Rong, Tingyang Xu, Wenbing Huang, Honglei Zhang, Peng Cui, Wenwu Zhu, Junzhou Huang


## 2019-08-03

[Exploring the Robustness of NMT Systems to Nonsensical Inputs.](http://arxiv.org/abs/1908.01165)

Akshay Chaturvedi, Abijith KP, Utpal Garain


## 2019-08-02

[AdvGAN++ : Harnessing latent layers for adversary generation.](http://arxiv.org/abs/1908.00706)

Puneet Mangla, Surgan Jandial, Sakshi Varshney, Vineeth N Balasubramanian


## 2019-08-01

[Black-box Adversarial ML Attack on Modulation Classification.](http://arxiv.org/abs/1908.00635)

Muhammad Usama, Junaid Qadir, Ala Al-Fuqaha


[Robustifying deep networks for image segmentation.](http://arxiv.org/abs/1908.00656)

Zheng Liu, Jinnian Zhang, Varun Jog, Po-Ling Loh, Alan B McMillan


## 2019-07-31

[Adversarial Robustness Curves.](http://arxiv.org/abs/1908.00096)

Christina Göpfert, Jan Philip Göpfert, Barbara Hammer


[Optimal Attacks on Reinforcement Learning Policies.](http://arxiv.org/abs/1907.13548)

Alessio Russo, Alexandre Proutiere


## 2019-07-30

[Impact of Adversarial Examples on Deep Learning Models for Biomedical Image Segmentation.](http://arxiv.org/abs/1907.13124)

Utku Ozbulak, Messem Arnout Van, Neve Wesley De


[Not All Adversarial Examples Require a Complex Defense: Identifying Over-optimized Adversarial Examples with IQR-based Logit Thresholding.](http://arxiv.org/abs/1907.12744)

Utku Ozbulak, Messem Arnout Van, Neve Wesley De


## 2019-07-28

[Are Odds Really Odd? Bypassing Statistical Detection of Adversarial Examples.](http://arxiv.org/abs/1907.12138)

Hossein Hosseini, Sreeram Kannan, Radha Poovendran


## 2019-07-27

[Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment.](http://arxiv.org/abs/1907.11932)

Di Jin, Zhijing Jin, Joey Tianyi Zhou, Peter Szolovits


## 2019-07-26

[Understanding Adversarial Robustness: The Trade-off between Minimum and Average Margin.](http://arxiv.org/abs/1907.11780)

Kaiwen Wu, Yaoliang Yu


[On the Design of Black-box Adversarial Examples by Leveraging Gradient-free Optimization and Operator Splitting Method.](http://arxiv.org/abs/1907.11684)

Pu Zhao, Sijia Liu, Pin-Yu Chen, Nghia Hoang, Kaidi Xu, Bhavya Kailkhura, Xue Lin


## 2019-07-24

[Towards Adversarially Robust Object Detection.](http://arxiv.org/abs/1907.10310)

Haichao Zhang, Jianyu Wang


[Joint Adversarial Training: Incorporating both Spatial and Pixel Attacks.](http://arxiv.org/abs/1907.10737)

Haichao Zhang, Jianyu Wang


[Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training.](http://arxiv.org/abs/1907.10764)

Haichao Zhang, Jianyu Wang


[Weakly Supervised Localization using Min-Max Entropy: an Interpretable Framework.](http://arxiv.org/abs/1907.12934)

Soufiane Belharbi, Jérôme Rony, Jose Dolz, Ismail Ben Ayed, Luke McCaffrey, Eric Granger


[Understanding Adversarial Attacks on Deep Learning Based Medical Image Analysis Systems.](http://arxiv.org/abs/1907.10456)

Xingjun Ma, Yuhao Niu, Lin Gu, Yisen Wang, Yitian Zhao, James Bailey, Feng Lu


## 2019-07-23

[Enhancing Adversarial Example Transferability with an Intermediate Level Attack.](http://arxiv.org/abs/1907.10823)

Qian Huang, Isay Katsman, Horace He, Zeqi Gu, Serge Belongie, Ser-Nam Lim


## 2019-07-21

[Characterizing Attacks on Deep Reinforcement Learning.](http://arxiv.org/abs/1907.09470)

Xinlei Pan, Chaowei Xiao, Warren He, Shuang Yang, Jian Peng, Mingjie Sun, Jinfeng Yi, Zijiang Yang, Mingyan Liu, Bo Li, Dawn Song


## 2019-07-17

[Connecting Lyapunov Control Theory to Adversarial Attacks.](http://arxiv.org/abs/1907.07732)

Arash Rahnama, Andre T. Nguyen, Edward Raff


[Robustness properties of Facebook's ResNeXt WSL models.](http://arxiv.org/abs/1907.07640)

A. Emin Orhan


[Constrained Concealment Attacks against Reconstruction-based Anomaly Detectors in Industrial Control Systems.](http://arxiv.org/abs/1907.07487)

Alessandro Erba, Riccardo Taormina, Stefano Galelli, Marcello Pogliani, Michele Carminati, Stefano Zanero, Nils Ole Tippenhauer


## 2019-07-16

[Adversarial Security Attacks and Perturbations on Machine Learning and Deep Learning Methods.](http://arxiv.org/abs/1907.07291)

Arif Siddiqi


[Latent Adversarial Defence with Boundary-guided Generation.](http://arxiv.org/abs/1907.07001)

Xiaowei Zhou, Ivor W. Tsang, Jie Yin


[Natural Adversarial Examples.](http://arxiv.org/abs/1907.07174)

Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, Dawn Song


[Adversarial Sensor Attack on LiDAR-based Perception in Autonomous Driving.](http://arxiv.org/abs/1907.06826)

Yulong Cao, Chaowei Xiao, Benjamin Cyr, Yimeng Zhou, Won Park, Sara Rampazzi, Qi Alfred Chen, Kevin Fu, Z. Morley Mao


[Explaining Vulnerabilities to Adversarial Machine Learning through Visual Analytics.](http://arxiv.org/abs/1907.07296)

Yuxin Ma, Tiankai Xie, Jundong Li, Ross Maciejewski


## 2019-07-15

[Graph Interpolating Activation Improves Both Natural and Robust Accuracies in Data-Efficient Deep Learning.](http://arxiv.org/abs/1907.06800)

Bao Wang, Stanley J. Osher


[Recovery Guarantees for Compressible Signals with Adversarial Noise.](http://arxiv.org/abs/1907.06565)

Jasjeet Dhaliwal, Kyle Hambrook


## 2019-07-14

[Measuring the Transferability of Adversarial Examples.](http://arxiv.org/abs/1907.06291)

Deyan Petrov, Timothy M. Hospedales


## 2019-07-12

[Unsupervised Adversarial Attacks on Deep Feature-based Retrieval with GAN.](http://arxiv.org/abs/1907.05793)

Guoping Zhao, Mingyu Zhang, Jiajun Liu, Ji-Rong Wen


[Stateful Detection of Black-Box Adversarial Attacks.](http://arxiv.org/abs/1907.05587)

Steven Chen, Nicholas Carlini, David Wagner


[Generative Modeling by Estimating Gradients of the Data Distribution.](http://arxiv.org/abs/1907.05600)

Yang Song, Stefano Ermon


## 2019-07-11

[Why Blocking Targeted Adversarial Perturbations Impairs the Ability to Learn.](http://arxiv.org/abs/1907.05718)

Ziv Katzir, Yuval Elovici


[Adversarial Objects Against LiDAR-Based Autonomous Driving Systems.](http://arxiv.org/abs/1907.05418)

Yulong Cao, Chaowei Xiao, Dawei Yang, Jing Fang, Ruigang Yang, Mingyan Liu, Bo Li


## 2019-07-10

[Metamorphic Detection of Adversarial Examples in Deep Learning Models With Affine Transformations.](http://arxiv.org/abs/1907.04774)

Rohan Reddy Mekala, Gudjon Einar Magnusson, Adam Porter, Mikael Lindvall, Madeline Diep


## 2019-07-09

[PhysGAN: Generating Physical-World-Resilient Adversarial Examples for Autonomous Driving.](http://arxiv.org/abs/1907.04449)

Zelun Kong, Junfeng Guo, Ang Li, Cong Liu


## 2019-07-06

[Affine Disentangled GAN for Interpretable and Robust AV Perception.](http://arxiv.org/abs/1907.05274)

Letao Liu, Martin Saerbeck, Justin Dauwels


## 2019-07-05

[Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions.](http://arxiv.org/abs/1907.02957)

Yao Qin, Nicholas Frosst, Sara Sabour, Colin Raffel, Garrison Cottrell, Geoffrey Hinton


## 2019-07-04

[Adversarial Robustness through Local Linearization.](http://arxiv.org/abs/1907.02610)

Chongli Qin, James Martens, Sven Gowal, Dilip Krishnan, Krishnamurthy Dvijotham, Alhussein Fawzi, Soham De, Robert Stanforth, Pushmeet Kohli


[Adversarial Attacks in Sound Event Classification.](http://arxiv.org/abs/1907.02477)

Vinod Subramanian, Emmanouil Benetos, Ning Xu, SKoT McDonald, Mark Sandler


## 2019-07-03

[Robust Synthesis of Adversarial Visual Examples Using a Deep Image Prior.](http://arxiv.org/abs/1907.01996)

Thomas Gittings, Steve Schneider, John Collomosse


[Minimally distorted Adversarial Examples with a Fast Adaptive Boundary Attack.](http://arxiv.org/abs/1907.02044)

Francesco Croce, Matthias Hein


## 2019-07-02

[Efficient Cyber Attacks Detection in Industrial Control Systems Using Lightweight Neural Networks and PCA.](http://arxiv.org/abs/1907.01216)

Moshe Kravchik, Asaf Shabtai


[Treant: Training Evasion-Aware Decision Trees.](http://arxiv.org/abs/1907.01197)

Stefano Calzavara, Claudio Lucchese, Gabriele Tolomei, Seyum Assefa Abebe, Salvatore Orlando


## 2019-07-01

[Comment on "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network".](http://arxiv.org/abs/1907.00895)

Roland S. Zimmermann


[Diminishing the Effect of Adversarial Perturbations via Refining Feature Representation.](http://arxiv.org/abs/1907.01023)

Nader Asadi, AmirMohammad Sarfi, Sahba Tahsini, Mahdi Eftekhari


[Accurate, reliable and fast robustness evaluation.](http://arxiv.org/abs/1907.01003)

Wieland Brendel, Jonas Rauber, Matthias Kümmerer, Ivan Ustyuzhaninov, Matthias Bethge


## 2019-06-30

[Fooling a Real Car with Adversarial Traffic Signs.](http://arxiv.org/abs/1907.00374)

Nir Morgulis, Alexander Kreines, Shachar Mendelowitz, Yuval Weisglass


## 2019-06-28

[Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty.](http://arxiv.org/abs/1906.12340)

Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, Dawn Song


[Certifiable Robustness and Robust Training for Graph Convolutional Networks.](http://arxiv.org/abs/1906.12269)

Daniel Zügner, Stephan Günnemann


[Learning to Cope with Adversarial Attacks.](http://arxiv.org/abs/1906.12061)

Xian Yeow Lee, Aaron Havens, Girish Chowdhary, Soumik Sarkar


[Robustness Guarantees for Deep Neural Networks on Videos.](http://arxiv.org/abs/1907.00098)

Min Wu, Marta Kwiatkowska


## 2019-06-27

[Using Intuition from Empirical Properties to Simplify Adversarial Training Defense.](http://arxiv.org/abs/1906.11729)

Guanxiong Liu, Issa Khalil, Abdallah Khreishah


[Adversarial Robustness via Label-Smoothing.](http://arxiv.org/abs/1906.11567)

Morgane Goibert, Elvis Dohmatob


[Evolving Robust Neural Architectures to Defend from Adversarial Attacks.](http://arxiv.org/abs/1906.11667)

Shashank Kotyan, Danilo Vasconcellos Vargas


## 2019-06-26

[The Adversarial Robustness of Sampling.](http://arxiv.org/abs/1906.11327)

Omri Ben-Eliezer, Eylon Yogev


[Defending Adversarial Attacks by Correcting logits.](http://arxiv.org/abs/1906.10973)

Yifeng Li, Lingxi Xie, Ya Zhang, Rui Zhang, Yanfeng Wang, Qi Tian


## 2019-06-25

[Quantitative Verification of Neural Networks And its Security Applications.](http://arxiv.org/abs/1906.10395)

Teodora Baluta, Shiqi Shen, Shweta Shinde, Kuldeep S. Meel, Prateek Saxena


[Are Adversarial Perturbations a Showstopper for ML-Based CAD? A Case Study on CNN-Based Lithographic Hotspot Detection.](http://arxiv.org/abs/1906.10773)

Kang Liu, Haoyu Yang, Yuzhe Ma, Benjamin Tan, Bei Yu, Evangeline F. Y. Young, Ramesh Karri, Siddharth Garg


## 2019-06-24

[Deceptive Reinforcement Learning Under Adversarial Manipulations on Cost Signals.](http://arxiv.org/abs/1906.10571)

Yunhan Huang, Quanyan Zhu


## 2019-06-22

[Defending Against Adversarial Examples with K-Nearest Neighbor.](http://arxiv.org/abs/1906.09525)

Chawin Sitawarin, David Wagner


## 2019-06-21

[Hiding Faces in Plain Sight: Disrupting AI Face Synthesis with Adversarial Perturbations.](http://arxiv.org/abs/1906.09288)

Yuezun Li, Xin Yang, Baoyuan Wu, Siwei Lyu


[A Fourier Perspective on Model Robustness in Computer Vision.](http://arxiv.org/abs/1906.08988)

Dong Yin, Raphael Gontijo Lopes, Jonathon Shlens, Ekin D. Cubuk, Justin Gilmer


[Evolution Attack On Neural Networks.](http://arxiv.org/abs/1906.09072)

YiGui Luo, RuiJia Yang, Wei Sha, WeiYi Ding, YouTeng Sun, YiSi Wang


[Adversarial Examples to Fool Iris Recognition Systems.](http://arxiv.org/abs/1906.09300)

Sobhan Soleymani, Ali Dabouei, Jeremy Dawson, Nasser M. Nasrabadi


[A Cyclically-Trained Adversarial Network for Invariant Representation Learning.](http://arxiv.org/abs/1906.09313)

Jiawei Chen, Janusz Konrad, Prakash Ishwar


## 2019-06-20

[On Physical Adversarial Patches for Object Detection.](http://arxiv.org/abs/1906.11897)

Mark Lee, Zico Kolter


## 2019-06-19

[Catfish Effect Between Internal and External Attackers:Being Semi-honest is Helpful.](http://arxiv.org/abs/1907.03720)

Hanqing Liu, Na Ruan, Joseph K. Liu


[Improving the robustness of ImageNet classifiers using elements of human visual cognition.](http://arxiv.org/abs/1906.08416)

A. Emin Orhan, Brenden M. Lake


[A unified view on differential privacy and robustness to adversarial examples.](http://arxiv.org/abs/1906.07982)

Rafael Pinot, Florian Yger, Cédric Gouy-Pailler, Jamal Atif


[Convergence of Adversarial Training in Overparametrized Networks.](http://arxiv.org/abs/1906.07916)

Ruiqi Gao, Tianle Cai, Haochuan Li, Liwei Wang, Cho-Jui Hsieh, Jason D. Lee


[Global Adversarial Attacks for Assessing Deep Learning Robustness.](http://arxiv.org/abs/1906.07920)

Hanbin Hu, Mit Shah, Jianhua Z. Huang, Peng Li


[Cloud-based Image Classification Service Is Not Robust To Simple Transformations: A Forgotten Battlefield.](http://arxiv.org/abs/1906.07997)

Dou Goodman, Tao Wei


[SemanticAdv: Generating Adversarial Examples via Attribute-conditional Image Editing.](http://arxiv.org/abs/1906.07927)

Haonan Qiu, Chaowei Xiao, Lei Yang, Xinchen Yan, Honglak Lee, Bo Li


## 2019-06-17

[Adversarial attacks on Copyright Detection Systems.](http://arxiv.org/abs/1906.07153)

Parsa Saadatpanah, Ali Shafahi, Tom Goldstein


[Improving Black-box Adversarial Attacks with a Transfer-based Prior.](http://arxiv.org/abs/1906.06919)

Shuyu Cheng, Yinpeng Dong, Tianyu Pang, Hang Su, Jun Zhu


[The Attack Generator: A Systematic Approach Towards Constructing Adversarial Attacks.](http://arxiv.org/abs/1906.07077)

Felix Assion, Peter Schlicht, Florens Greßner, Wiebke Günther, Fabian Hüger, Nico Schmidt, Umair Rasheed


## 2019-06-16

[Interpolated Adversarial Training: Achieving Robust Neural Networks without Sacrificing Accuracy.](http://arxiv.org/abs/1906.06784)

Alex Lamb, Vikas Verma, Juho Kannala, Yoshua Bengio


[Defending Against Adversarial Attacks Using Random Forests.](http://arxiv.org/abs/1906.06765)

Yifan Ding, Liqiang Wang, Huan Zhang, Jinfeng Yi, Deliang Fan, Boqing Gong


## 2019-06-15

[Representation Quality Of Neural Networks Links To Adversarial Attacks and Defences.](http://arxiv.org/abs/1906.06627)

Shashank Kotyan, Danilo Vasconcellos Vargas, Moe Matsuki


## 2019-06-14

[Adversarial Training Can Hurt Generalization.](http://arxiv.org/abs/1906.06032)

Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John C. Duchi, Percy Liang


[Towards Compact and Robust Deep Neural Networks.](http://arxiv.org/abs/1906.06110)

Vikash Sehwag, Shiqi Wang, Prateek Mittal, Suman Jana


[Perceptual Based Adversarial Audio Attacks.](http://arxiv.org/abs/1906.06355)

Joseph Szurley, J. Zico Kolter


[Copy and Paste: A Simple But Effective Initialization Method for Black-Box Adversarial Attacks.](http://arxiv.org/abs/1906.06086)

Thomas Brunner, Frederik Diehl, Alois Knoll


[Robust or Private? Adversarial Training Makes Models More Vulnerable to Privacy Attacks.](http://arxiv.org/abs/1906.06449)

Felipe A. Mejia, Paul Gamble, Zigfried Hampel-Arias, Michael Lomnitz, Nina Lopatina, Lucas Tindall, Maria Alejandra Barrios


[Towards Stable and Efficient Training of Verifiably Robust Neural Networks.](http://arxiv.org/abs/1906.06316)

Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Duane Boning, Cho-Jui Hsieh


[Adversarial Robustness Assessment: Why both $L_0$ and $L_\infty$ Attacks Are Necessary.](http://arxiv.org/abs/1906.06026)

Shashank Kotyan, Danilo Vasconcellos Vargas


## 2019-06-13

[A Computationally Efficient Method for Defending Adversarial Deep Learning Attacks.](http://arxiv.org/abs/1906.05599)

Rajeev Sahay, Rehana Mahfuz, Aly El Gamal


[Lower Bounds for Adversarially Robust PAC Learning.](http://arxiv.org/abs/1906.05815)

Dimitrios I. Diochnos, Saeed Mahloujifar, Mohammad Mahmoody


## 2019-06-12

[Tight Certificates of Adversarial Robustness for Randomly Smoothed Classifiers.](http://arxiv.org/abs/1906.04948)

Guang-He Lee, Yang Yuan, Shiyu Chang, Tommi S. Jaakkola


## 2019-06-11

[Subspace Attack: Exploiting Promising Subspaces for Query-Efficient Black-box Attacks.](http://arxiv.org/abs/1906.04392)

Ziang Yan, Yiwen Guo, Changshui Zhang


[Mimic and Fool: A Task Agnostic Adversarial Attack.](http://arxiv.org/abs/1906.04606)

Akshay Chaturvedi, Utpal Garain


[Efficient and Accurate Estimation of Lipschitz Constants for Deep Neural Networks.](http://arxiv.org/abs/1906.04893)

Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, George J. Pappas


## 2019-06-10

[E-LPIPS: Robust Perceptual Image Similarity via Random Transformation Ensembles.](http://arxiv.org/abs/1906.03973)

Markus Kettunen, Erik Härkönen, Jaakko Lehtinen


[Evaluating the Robustness of Nearest Neighbor Classifiers: A Primal-Dual Perspective.](http://arxiv.org/abs/1906.03972)

Lu Wang, Xuanqing Liu, Jinfeng Yi, Zhi-Hua Zhou, Cho-Jui Hsieh


[Robustness Verification of Tree-based Models.](http://arxiv.org/abs/1906.03849)

Hongge Chen, Huan Zhang, Si Si, Yang Li, Duane Boning, Cho-Jui Hsieh


[Topology Attack and Defense for Graph Neural Networks: An Optimization Perspective.](http://arxiv.org/abs/1906.04214)

Kaidi Xu, Hongge Chen, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Mingyi Hong, Xue Lin


## 2019-06-09

[On the Vulnerability of Capsule Networks to Adversarial Attacks.](http://arxiv.org/abs/1906.03612)

Felix Michels, Tobias Uelwer, Eric Upschulte, Stefan Harmeling


[Intriguing properties of adversarial training.](http://arxiv.org/abs/1906.03787)

Cihang Xie, Alan Yuille


[Improved Adversarial Robustness via Logit Regularization Methods.](http://arxiv.org/abs/1906.03749)

Cecilia Summers, Michael J. Dinneen


[Attacking Graph Convolutional Networks via Rewiring.](http://arxiv.org/abs/1906.03750)

Yao Ma, Suhang Wang, Tyler Derr, Lingfei Wu, Jiliang Tang


[Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness.](http://arxiv.org/abs/1906.03563)

Jingkang Wang, Tianyun Zhang, Sijia Liu, Pin-Yu Chen, Jiacen Xu, Makan Fardad, Bo Li


[Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers.](http://arxiv.org/abs/1906.04584)

Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya Razenshteyn, Sebastien Bubeck


## 2019-06-08

[Strategies to architect AI Safety: Defense to guard AI from Adversaries.](http://arxiv.org/abs/1906.03466)

Rajagopal. A, Nirmala. V


[Sensitivity of Deep Convolutional Networks to Gabor Noise.](http://arxiv.org/abs/1906.03455)

Kenneth T. Co, Luis Muñoz-González, Emil C. Lupu


[ML-LOO: Detecting Adversarial Examples with Feature Attribution.](http://arxiv.org/abs/1906.03499)

Puyudi Yang, Jianbo Chen, Cho-Jui Hsieh, Jane-Ling Wang, Michael I. Jordan


[Provably Robust Boosted Decision Stumps and Trees against Adversarial Attacks.](http://arxiv.org/abs/1906.03526)

Maksym Andriushchenko, Matthias Hein


[Making targeted black-box evasion attacks effective and efficient.](http://arxiv.org/abs/1906.03397)

Mika Juuti, Buse Gul Atli, N. Asokan


[Defending Against Universal Attacks Through Selective Feature Regeneration.](http://arxiv.org/abs/1906.03444)

Tejas Borkar, Felix Heide, Lina Karam


## 2019-06-07

[A cryptographic approach to black box adversarial machine learning.](http://arxiv.org/abs/1906.03231)

Kevin Shi, Daniel Hsu, Allison Bishop


[Using learned optimizers to make models robust to input noise.](http://arxiv.org/abs/1906.03367)

Luke Metz, Niru Maheswaranathan, Jonathon Shlens, Jascha Sohl-Dickstein, Ekin D. Cubuk


[Efficient Project Gradient Descent for Ensemble Adversarial Attack.](http://arxiv.org/abs/1906.03333)

Fanyou Wu, Rado Gazo, Eva Haviarova, Bedrich Benes


[Inductive Bias of Gradient Descent based Adversarial Training on Separable Data.](http://arxiv.org/abs/1906.02931)

Yan Li, Ethan X. Fang, Huan Xu, Tuo Zhao


[Adversarial Explanations for Understanding Image Classification Decisions and Improved Neural Network Robustness.](http://arxiv.org/abs/1906.02896)

Walt Woods, Jack Chen, Christof Teuscher


[Robustness for Non-Parametric Classification: A Generic Attack and Defense.](http://arxiv.org/abs/1906.03310)

Yao-Yuan Yang, Cyrus Rashtchian, Yizhen Wang, Kamalika Chaudhuri


## 2019-06-06

[Robust Attacks against Multiple Classifiers.](http://arxiv.org/abs/1906.02816)

Juan C. Perdomo, Yaron Singer


[Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation.](http://arxiv.org/abs/1906.02611)

Raphael Gontijo Lopes, Dong Yin, Ben Poole, Justin Gilmer, Ekin D. Cubuk


[Understanding Adversarial Behavior of DNNs by Disentangling Non-Robust and Robust Components in Performance Metric.](http://arxiv.org/abs/1906.02494)

Yujun Shi, Benben Liao, Guangyong Chen, Yun Liu, Ming-Ming Cheng, Jiashi Feng


[Should Adversarial Attacks Use Pixel p-Norm?](http://arxiv.org/abs/1906.02439)

Ayon Sen, Xiaojin Zhu, Liam Marshall, Robert Nowak


[Image Synthesis with a Single (Robust) Classifier.](http://arxiv.org/abs/1906.09453)

Shibani Santurkar, Dimitris Tsipras, Brandon Tran, Andrew Ilyas, Logan Engstrom, Aleksander Madry


## 2019-06-05

[MNIST-C: A Robustness Benchmark for Computer Vision.](http://arxiv.org/abs/1906.02337)

Norman Mu, Justin Gilmer


[Enhancing Gradient-based Attacks with Symbolic Intervals.](http://arxiv.org/abs/1906.02282)

Shiqi Wang, Yizheng Chen, Ahmed Abdou, Suman Jana


[Query-efficient Meta Attack to Deep Neural Networks.](http://arxiv.org/abs/1906.02398)

Jiawei Du, Hu Zhang, Joey Tianyi Zhou, Yi Yang, Jiashi Feng


[c-Eval: A Unified Metric to Evaluate Feature-based Explanations via Perturbation.](http://arxiv.org/abs/1906.02032)

Minh N. Vu, Truc D. Nguyen, NhatHai Phan, Ralucca Gera, My T. Thai


[Multi-way Encoding for Robustness.](http://arxiv.org/abs/1906.02033)

Donghyun Kim, Sarah Adel Bargal, Jianming Zhang, Stan Sclaroff


## 2019-06-04

[Adversarial Training is a Form of Data-dependent Operator Norm Regularization.](http://arxiv.org/abs/1906.01527)

Kevin Roth, Yannic Kilcher, Thomas Hofmann


## 2019-06-03

[Adversarial Exploitation of Policy Imitation.](http://arxiv.org/abs/1906.01121)

Vahid Behzadan, William Hsu


[RL-Based Method for Benchmarking the Adversarial Resilience and Robustness of Deep Reinforcement Learning Policies.](http://arxiv.org/abs/1906.01110)

Vahid Behzadan, William Hsu


[Adversarial Risk Bounds for Neural Networks through Sparsity based Compression.](http://arxiv.org/abs/1906.00698)

Emilio Rafael Balda, Arash Behboodi, Niklas Koep, Rudolf Mathar


[The Adversarial Machine Learning Conundrum: Can The Insecurity of ML Become The Achilles' Heel of Cognitive Networks?](http://arxiv.org/abs/1906.00679)

Muhammad Usama, Junaid Qadir, Ala Al-Fuqaha, Mounir Hamdi


[Adversarial Robustness as a Prior for Learned Representations.](http://arxiv.org/abs/1906.00945)

Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran, Aleksander Madry


[Achieving Generalizable Robustness of Deep Neural Networks by Stability Training.](http://arxiv.org/abs/1906.00735)

Jan Laermann, Wojciech Samek, Nils Strodthoff


[A Surprising Density of Illusionable Natural Speech.](http://arxiv.org/abs/1906.01040)

Melody Y. Guan, Gregory Valiant


[Fast and Stable Interval Bounds Propagation for Training Verifiably Robust Models.](http://arxiv.org/abs/1906.00628)

Paweł Morawiecki, Przemysław Spurek, Marek Śmieja, Jacek Tabor


[Understanding the Limitations of Conditional Generative Models.](http://arxiv.org/abs/1906.01171)

Ethan Fetaya, Jörn-Henrik Jacobsen, Will Grathwohl, Richard Zemel


## 2019-06-02

[Adversarially Robust Generalization Just Requires More Unlabeled Data.](http://arxiv.org/abs/1906.00555)

Runtian Zhai, Tianle Cai, Di He, Chen Dan, Kun He, John Hopcroft, Liwei Wang


## 2019-06-01

[Adversarial Examples for Edge Detection: They Exist, and They Transfer.](http://arxiv.org/abs/1906.00335)

Christian Cosgrove, Alan L. Yuille


[Perceptual Evaluation of Adversarial Attacks for CNN-based Image Classification.](http://arxiv.org/abs/1906.00204)

Sid Ahmed Fezza, Yassine Bakhti, Wassim Hamidouche, Olivier Déforges


[Enhancing Transformation-based Defenses using a Distribution Classifier.](http://arxiv.org/abs/1906.00258)

Connie Kou, Hwee Kuan Lee, Ee-Chien Chang, Teck Khim Ng


## 2019-05-31

[Unlabeled Data Improves Adversarial Robustness.](http://arxiv.org/abs/1905.13736)

Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang, John C. Duchi


[Reverse KL-Divergence Training of Prior Networks: Improved Uncertainty and Adversarial Robustness.](http://arxiv.org/abs/1905.13472)

Andrey Malinin, Mark Gales


[Are Labels Required for Improving Adversarial Robustness?](http://arxiv.org/abs/1905.13725)

Jonathan Uesato, Jean-Baptiste Alayrac, Po-Sen Huang, Robert Stanforth, Alhussein Fawzi, Pushmeet Kohli


## 2019-05-30

[Real-Time Adversarial Attacks.](http://arxiv.org/abs/1905.13399)

Yuan Gong, Boyang Li, Christian Poellabauer, Yiyu Shi


[Residual Networks as Nonlinear Systems: Stability Analysis using Linearization.](http://arxiv.org/abs/1905.13386)

Kai Rothauge, Zhewei Yao, Zixi Hu, Michael W. Mahoney


[Identifying Classes Susceptible to Adversarial Attacks.](http://arxiv.org/abs/1905.13284)

Rangeet Pan, Md Johirul Islam, Shibbir Ahmed, Hridesh Rajan


[Robust Sparse Regularization: Simultaneously Optimizing Neural Network Robustness and Compactness.](http://arxiv.org/abs/1905.13074)

Adnan Siraj Rakin, Zhezhi He, Li Yang, Yanzhi Wang, Liqiang Wang, Deliang Fan


[Interpretable Adversarial Training for Text.](http://arxiv.org/abs/1905.12864)

Samuel Barham, Soheil Feizi


## 2019-05-29

[Bandlimiting Neural Networks Against Adversarial Attacks.](http://arxiv.org/abs/1905.12797)

Yuping Lin, Kasra Ahmadi K. A., Hui Jiang


[Misleading Authorship Attribution of Source Code using Adversarial Learning.](http://arxiv.org/abs/1905.12386)

Erwin Quiring, Alwin Maier, Konrad Rieck


[Securing Connected & Autonomous Vehicles: Challenges Posed by Adversarial Machine Learning and The Way Forward.](http://arxiv.org/abs/1905.12762)

Adnan Qayyum, Muhammad Usama, Junaid Qadir, Ala Al-Fuqaha


[Functional Adversarial Attacks.](http://arxiv.org/abs/1906.00001)

Cassidy Laidlaw, Soheil Feizi


[CopyCAT: Taking Control of Neural Policies with Constant Attacks.](http://arxiv.org/abs/1905.12282)

Léonard Hussenot, Matthieu Geist, Olivier Pietquin


## 2019-05-28

[ME-Net: Towards Effective Adversarial Robustness with Matrix Estimation.](http://arxiv.org/abs/1905.11971)

Yuzhe Yang, Guo Zhang, Dina Katabi, Zhi Xu


[Adversarial Attacks on Remote User Authentication Using Behavioural Mouse Dynamics.](http://arxiv.org/abs/1905.11831)

Yi Xiang Marcus Tan, Alfonso Iacovazzi, Ivan Homoliak, Yuval Elovici, Alexander Binder


[Improving the Robustness of Deep Neural Networks via Adversarial Training with Triplet Loss.](http://arxiv.org/abs/1905.11713)

Pengcheng Li, Jinfeng Yi, Bowen Zhou, Lijun Zhang


[Snooping Attacks on Deep Reinforcement Learning.](http://arxiv.org/abs/1905.11832)

Matthew Inkawhich, Yiran Chen, Hai Li


[High Frequency Component Helps Explain the Generalization of Convolutional Neural Networks.](http://arxiv.org/abs/1905.13545)

Haohan Wang, Xindi Wu, Zeyi Huang, Eric P. Xing


[Expected Tight Bounds for Robust Training.](http://arxiv.org/abs/1905.12418)

Salman Alsubaihi, Adel Bibi, Modar Alfadly, Abdullah Hamdi, Bernard Ghanem


[Empirically Measuring Concentration: Fundamental Limits on Intrinsic Robustness.](http://arxiv.org/abs/1905.12202)

Saeed Mahloujifar, Xiao Zhang, Mohammad Mahmoody, David Evans


[Cross-Domain Transferability of Adversarial Perturbations.](http://arxiv.org/abs/1905.11736)

Muzammal Naseer, Salman H. Khan, Harris Khan, Fahad Shahbaz Khan, Fatih Porikli


[Certifiably Robust Interpretation in Deep Learning.](http://arxiv.org/abs/1905.12105)

Alexander Levine, Sahil Singla, Soheil Feizi


## 2019-05-27

[Brain-inspired reverse adversarial examples.](http://arxiv.org/abs/1905.12171)

Shaokai Ye, Sia Huat Tan, Kaidi Xu, Yanzhi Wang, Chenglong Bao, Kaisheng Ma


[Label Universal Targeted Attack.](http://arxiv.org/abs/1905.11544)

Naveed Akhtar, Mohammad A. A. K. Jalwana, Mohammed Bennamoun, Ajmal Mian


[Fooling Detection Alone is Not Enough: First Adversarial Attack against Multiple Object Tracking.](http://arxiv.org/abs/1905.11026)

Yunhan Jia, Yantao Lu, Junjie Shen, Qi Alfred Chen, Zhenyu Zhong, Tao Wei


[Provable robustness against all adversarial $l_p$-perturbations for $p\geq 1$.](http://arxiv.org/abs/1905.11213)

Francesco Croce, Matthias Hein


[Scaleable input gradient regularization for adversarial robustness.](http://arxiv.org/abs/1905.11468)

Chris Finlay, Adam M Oberman


[Combating Adversarial Misspellings with Robust Word Recognition.](http://arxiv.org/abs/1905.11268)

Danish Pruthi, Bhuwan Dhingra, Zachary C. Lipton


[Analyzing the Interpretability Robustness of Self-Explaining Models.](http://arxiv.org/abs/1905.12429)

Haizhong Zheng, Earlence Fernandes, Atul Prakash


[Adversarially Robust Learning Could Leverage Computational Hardness.](http://arxiv.org/abs/1905.11564)

Sanjam Garg, Somesh Jha, Saeed Mahloujifar, Mohammad Mahmoody


[Unsupervised Euclidean Distance Attack on Network Embedding.](http://arxiv.org/abs/1905.11015)

Shanqing Yu, Jun Zheng, Jinhuan Wang, Jian Zhang, Lihong Chen, Qi Xuan, Jinyin Chen, Dan Zhang, Qingpeng Zhang


[GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification.](http://arxiv.org/abs/1905.11475)

Xuwang Yin, Soheil Kolouri, Gustavo K. Rohde


## 2019-05-26

[State-Reification Networks: Improving Generalization by Modeling the Distribution of Hidden Representations.](http://arxiv.org/abs/1905.11382)

Alex Lamb, Jonathan Binas, Anirudh Goyal, Sandeep Subramanian, Ioannis Mitliagkas, Denis Kazakov, Yoshua Bengio, Michael C. Mozer


[Non-Determinism in Neural Networks for Adversarial Robustness.](http://arxiv.org/abs/1905.10906)

Daanish Ali Khan, Linhong Li, Ninghao Sha, Zhuoran Liu, Abelino Jimenez, Bhiksha Raj, Rita Singh


[Purifying Adversarial Perturbation with Adversarially Trained Auto-encoders.](http://arxiv.org/abs/1905.10729)

Hebi Li, Qi Xiao, Shixin Tian, Jin Tian


[Rearchitecting Classification Frameworks For Increased Robustness.](http://arxiv.org/abs/1905.10900)

Varun Chandrasekaran, Brian Tang, Nicolas Papernot, Kassem Fawaz, Somesh Jha, Xi Wu


[Robust Classification using Robust Feature Augmentation.](http://arxiv.org/abs/1905.10904)

Kevin Eykholt, Swati Gupta, Atul Prakash, Amir Rahmati, Pratik Vaishnavi, Haizhong Zheng


[Generalizable Adversarial Attacks Using Generative Models.](http://arxiv.org/abs/1905.10864)

Avishek Joey Bose, Andre Cianflone, William L. Hamilton


## 2019-05-25

[Trust but Verify: An Information-Theoretic Explanation for the Adversarial Fragility of Machine Learning Systems, and a General Defense against Adversarial Attacks.](http://arxiv.org/abs/1905.11381)

Jirong Yi, Hui Xie, Leixin Zhou, Xiaodong Wu, Weiyu Xu, Raghuraman Mudumbai


[Adversarial Distillation for Ordered Top-k Attacks.](http://arxiv.org/abs/1905.10695)

Zekun Zhang, Tianfu Wu


[Adversarial Policies: Attacking Deep Reinforcement Learning.](http://arxiv.org/abs/1905.10615)

Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, Stuart Russell


[Rethinking Softmax Cross-Entropy Loss for Adversarial Robustness.](http://arxiv.org/abs/1905.10626)

Tianyu Pang, Kun Xu, Yinpeng Dong, Chao Du, Ning Chen, Jun Zhu


## 2019-05-24

[Robustness to Adversarial Perturbations in Learning from Incomplete Data.](http://arxiv.org/abs/1905.13021)

Amir Najafi, Shin-ichi Maeda, Masanori Koyama, Takeru Miyato


[Enhancing Adversarial Defense by k-Winners-Take-All.](http://arxiv.org/abs/1905.10510)

Chang Xiao, Peilin Zhong, Changxi Zheng


[Power up! Robust Graph Convolutional Network via Graph Powering.](http://arxiv.org/abs/1905.10029)

Ming Jin, Heng Chang, Wenwu Zhu, Somayeh Sojoudi


## 2019-05-23

[A Direct Approach to Robust Deep Learning Using Adversarial Networks.](http://arxiv.org/abs/1905.09591)

Huaxia Wang, Chun-Nam Yu


[PHom-GeM: Persistent Homology for Generative Models.](http://arxiv.org/abs/1905.09894)

Jeremy Charlier, Radu State, Jean Hilger


[Thwarting finite difference adversarial attacks with output randomization.](http://arxiv.org/abs/1905.09871)

Haidar Khan, Daniel Park, Azer Khan, Bülent Yener


[Interpreting Adversarially Trained Convolutional Neural Networks.](http://arxiv.org/abs/1905.09797)

Tianyuan Zhang, Zhanxing Zhu


[Adversarially Robust Distillation.](http://arxiv.org/abs/1905.09747)

Micah Goldblum, Liam Fowl, Soheil Feizi, Tom Goldstein


## 2019-05-22

[Convergence and Margin of Adversarial Training on Separable Data.](http://arxiv.org/abs/1905.09209)

Zachary Charles, Shashank Rajput, Stephen Wright, Dimitris Papailiopoulos


[Detecting Adversarial Examples and Other Misclassifications in Neural Networks by Introspection.](http://arxiv.org/abs/1905.09186)

Jonathan Aigrain, Marcin Detyniecki


## 2019-05-21

[DoPa: A Fast and Comprehensive CNN Defense Methodology against Physical Adversarial Attacks.](http://arxiv.org/abs/1905.08790)

Zirui Xu, Fuxun Yu, Xiang Chen


## 2019-05-20

[Adversarially robust transfer learning.](http://arxiv.org/abs/1905.08232)

Ali Shafahi, Parsa Saadatpanah, Chen Zhu, Amin Ghiasi, Christoph Studer, David Jacobs, Tom Goldstein


## 2019-05-19

[Testing DNN Image Classifiers for Confusion & Bias Errors.](http://arxiv.org/abs/1905.07831)

Yuchi Tian, Ziyuan Zhong, Vicente Ordonez, Gail Kaiser, Baishakhi Ray


## 2019-05-18

[What Do Adversarially Robust Models Look At?](http://arxiv.org/abs/1905.07666)

Takahiro Itazuri, Yoshihiro Fukuhara, Hirokatsu Kataoka, Shigeo Morishima


[Taking Care of The Discretization Problem:A Black-Box Adversarial Image Attack in Discrete Integer Domain.](http://arxiv.org/abs/1905.07672)

Yuchao Duan, Zhe Zhao, Lei Bu, Fu Song


## 2019-05-17

[POPQORN: Quantifying Robustness of Recurrent Neural Networks.](http://arxiv.org/abs/1905.07387)

Ching-Yun Ko, Zhaoyang Lyu, Tsui-Wei Weng, Luca Daniel, Ngai Wong, Dahua Lin


[A critique of the DeepSec Platform for Security Analysis of Deep Learning Models.](http://arxiv.org/abs/1905.07112)

Nicholas Carlini


[Simple Black-box Adversarial Attacks.](http://arxiv.org/abs/1905.07121)

Chuan Guo, Jacob R. Gardner, Yurong You, Andrew Gordon Wilson, Kilian Q. Weinberger


## 2019-05-16

[Parsimonious Black-Box Adversarial Attacks via Efficient Combinatorial Optimization.](http://arxiv.org/abs/1905.06635)

Seungyong Moon, Gaon An, Hyun Oh Song


## 2019-05-15

[On Norm-Agnostic Robustness of Adversarial Training.](http://arxiv.org/abs/1905.06455)

Bai Li, Changyou Chen, Wenlin Wang, Lawrence Carin


[An Efficient Pre-processing Method to Eliminate Adversarial Effects.](http://arxiv.org/abs/1905.08614)

Hua Wang, Jie Wang, Zhaoxia Yin


## 2019-05-14

[Robustification of deep net classifiers by key based diversified aggregation with pre-filtering.](http://arxiv.org/abs/1905.05454)

Olga Taran, Shideh Rezaeifar, Taras Holotyak, Slava Voloshynovskiy


## 2019-05-13

[Adversarial Examples for Electrocardiograms.](http://arxiv.org/abs/1905.05163)

Xintian Han, Yuxuan Hu, Luca Foschini, Larry Chinitz, Lior Jankelson, Rajesh Ranganath


[Analyzing Adversarial Attacks Against Deep Learning for Intrusion Detection in IoT Networks.](http://arxiv.org/abs/1905.05137)

Olakunle Ibitoye, Omair Shafiq, Ashraf Matrawy


[Harnessing the Vulnerability of Latent Layers in Adversarially Trained Models.](http://arxiv.org/abs/1905.05186)

Mayank Singh, Abhishek Sinha, Nupur Kumari, Harshitha Machiraju, Balaji Krishnamurthy, Vineeth N Balasubramanian


## 2019-05-11

[Moving Target Defense for Deep Visual Sensing against Adversarial Examples.](http://arxiv.org/abs/1905.13148)

Qun Song, Zhenyu Yan, Rui Tan


## 2019-05-10

[Interpreting and Evaluating Neural Network Robustness.](http://arxiv.org/abs/1905.04270)

Fuxun Yu, Zhuwei Qin, Chenchen Liu, Liang Zhao, Yanzhi Wang, Xiang Chen


[On the Connection Between Adversarial Robustness and Saliency Map Interpretability.](http://arxiv.org/abs/1905.04172)

Christian Etmann, Sebastian Lunz, Peter Maass, Carola-Bibiane Schönlieb


[Exact Adversarial Attack to Image Captioning via Structured Output Learning with Latent Variables.](http://arxiv.org/abs/1905.04016)

Yan Xu, Baoyuan Wu, Fumin Shen, Yanbo Fan, Yong Zhang, Heng Tao Shen, Wei Liu


## 2019-05-09

[Adversarial Defense Framework for Graph Neural Network.](http://arxiv.org/abs/1905.03679)

Shen Wang, Zhengzhang Chen, Jingchao Ni, Xiao Yu, Zhichun Li, Haifeng Chen, Philip S. Yu


[Mitigating Deep Learning Vulnerabilities from Adversarial Examples Attack in the Cybersecurity Domain.](http://arxiv.org/abs/1905.03517)

Chris Einar San Agustin


[Exploring the Hyperparameter Landscape of Adversarial Robustness.](http://arxiv.org/abs/1905.03837)

Evelyn Duesterwald, Anupama Murthi, Ganesh Venkataraman, Mathieu Sinn, Deepak Vijaykeerthy


[Learning Interpretable Features via Adversarially Robust Optimization.](http://arxiv.org/abs/1905.03767)

Ashkan Khakzar, Shadi Albarqouni, Nassir Navab


[Universal Adversarial Perturbations for Speech Recognition Systems.](http://arxiv.org/abs/1905.03828)

Paarth Neekhara, Shehzeen Hussain, Prakhar Pandey, Shlomo Dubnov, Julian McAuley, Farinaz Koushanfar


## 2019-05-08

[ROSA: Robust Salient Object Detection against Adversarial Attacks.](http://arxiv.org/abs/1905.03434)

Haofeng Li, Guanbin Li, Yizhou Yu


[Enhancing Cross-task Transferability of Adversarial Examples with Dispersion Reduction.](http://arxiv.org/abs/1905.03333)

Yunhan Jia, Yantao Lu, Senem Velipasalar, Zhenyu Zhong, Tao Wei


[Adversarial Image Translation: Unrestricted Adversarial Examples in Face Recognition Systems.](http://arxiv.org/abs/1905.03421)

Kazuya Kakizaki, Kosuke Yoshida


## 2019-05-07

[A Comprehensive Analysis on Adversarial Robustness of Spiking Neural Networks.](http://arxiv.org/abs/1905.02704)

Saima Sharmin, Priyadarshini Panda, Syed Shakib Sarwar, Chankyu Lee, Wachirawit Ponghiran, Kaushik Roy


[Representation of White- and Black-Box Adversarial Examples in Deep Neural Networks and Humans: A Functional Magnetic Resonance Imaging Study.](http://arxiv.org/abs/1905.02422)

Chihye Han, Wonjun Yoon, Gihyun Kwon, Seungkyu Nam, Daeshik Kim


[An Empirical Evaluation of Adversarial Robustness under Transfer Learning.](http://arxiv.org/abs/1905.02675)

Todor Davchev, Timos Korres, Stathi Fotiadis, Nick Antonopoulos, Subramanian Ramamoorthy


[Adaptive Generation of Unrestricted Adversarial Inputs.](http://arxiv.org/abs/1905.02463)

Isaac Dunn, Hadrien Pouget, Tom Melham, Daniel Kroening


## 2019-05-06

[Batch Normalization is a Cause of Adversarial Vulnerability.](http://arxiv.org/abs/1905.02161)

Angus Galloway, Anna Golubeva, Thomas Tanay, Medhat Moussa, Graham W. Taylor


[Adversarial Examples Are Not Bugs, They Are Features.](http://arxiv.org/abs/1905.02175)

Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, Aleksander Madry


## 2019-05-05

[Better the Devil you Know: An Analysis of Evasion Attacks using Out-of-Distribution Adversarial Examples.](http://arxiv.org/abs/1905.01726)

Vikash Sehwag, Arjun Nitin Bhagoji, Liwei Song, Chawin Sitawarin, Daniel Cullina, Mung Chiang, Prateek Mittal


## 2019-05-03

[Transfer of Adversarial Robustness Between Perturbation Types.](http://arxiv.org/abs/1905.01034)

Daniel Kang, Yi Sun, Tom Brown, Dan Hendrycks, Jacob Steinhardt


## 2019-05-02

[Adversarial Training with Voronoi Constraints.](http://arxiv.org/abs/1905.01019)

Marc Khoury, Dylan Hadfield-Menell


[Weight Map Layer for Noise and Adversarial Attack Robustness.](http://arxiv.org/abs/1905.00568)

Mohammed Amer, Tomás Maul


[You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle.](http://arxiv.org/abs/1905.00877)

Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, Bin Dong


## 2019-05-01

[POBA-GA: Perturbation Optimized Black-Box Adversarial Attacks via Genetic Algorithm.](http://arxiv.org/abs/1906.03181)

Jinyin Chen, Mengmeng Su, Shijing Shen, Hui Xiong, Haibin Zheng


[Dropping Pixels for Adversarial Robustness.](http://arxiv.org/abs/1905.00180)

Hossein Hosseini, Sreeram Kannan, Radha Poovendran


[NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks.](http://arxiv.org/abs/1905.00441)

Yandong Li, Lijun Li, Liqiang Wang, Tong Zhang, Boqing Gong


## 2019-04-30

[Test Selection for Deep Learning Systems.](http://arxiv.org/abs/1904.13195)

Wei Ma, Mike Papadakis, Anestis Tsakmalis, Maxime Cordy, Yves Le Traon


[Detecting Adversarial Examples through Nonlinear Dimensionality Reduction.](http://arxiv.org/abs/1904.13094)

Francesco Crecchi, Davide Bacciu, Battista Biggio


## 2019-04-29

[Adversarial Training for Free!](http://arxiv.org/abs/1904.12843)

Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S. Davis, Gavin Taylor, Tom Goldstein


[Adversarial Training and Robustness for Multiple Perturbations.](http://arxiv.org/abs/1904.13000)

Florian Tramèr, Dan Boneh


## 2019-04-27

[Non-Local Context Encoder: Robust Biomedical Image Segmentation against Adversarial Attacks.](http://arxiv.org/abs/1904.12181)

Xiang He, Sibei Yang, Guanbin Li?, Haofeng Li, Huiyou Chang, Yizhou Yu


## 2019-04-26

[Robustness Verification of Support Vector Machines.](http://arxiv.org/abs/1904.11803)

Francesco Ranzato, Marco Zanella


## 2019-04-24

[A Robust Approach for Securing Audio Classification Against Adversarial Attacks.](http://arxiv.org/abs/1904.10990)

Mohammad Esmaeilpour, Patrick Cardinal, Alessandro Lameiras Koerich


[Physical Adversarial Textures that Fool Visual Object Tracking.](http://arxiv.org/abs/1904.11042)

Rey Reza Wiyatno, Anqi Xu


## 2019-04-23

[Minimizing Perceived Image Quality Loss Through Adversarial Attack Scoping.](http://arxiv.org/abs/1904.10390)

Kostiantyn Khabarlak, Larysa Koriashkina


## 2019-04-22

[blessing in disguise: Designing Robust Turing Test by Employing Algorithm Unrobustness.](http://arxiv.org/abs/1904.09804)

Jiaming Zhang, Jitao Sang, Kaiyuan Xu, Shangxi Wu, Yongli Hu, Yanfeng Sun, Jian Yu


[Using Videos to Evaluate Image Model Robustness.](http://arxiv.org/abs/1904.10076)

Keren Gu, Brandon Yang, Jiquan Ngiam, Quoc Le, Jonathon Shlens


## 2019-04-21

[Beyond Explainability: Leveraging Interpretability for Improved Adversarial Learning.](http://arxiv.org/abs/1904.09633)

Devinder Kumar, Ibrahim Ben-Daya, Kanav Vats, Jeffery Feng, Graham Taylor and, Alexander Wong


## 2019-04-20

[Can Machine Learning Model with Static Features be Fooled: an Adversarial Machine Learning Approach.](http://arxiv.org/abs/1904.09433)

Rahim Taheri, Reza Javidan, Mohammad Shojafar, Vinod P, Mauro Conti


## 2019-04-19

[Salient Object Detection in the Deep Learning Era: An In-Depth Survey.](http://arxiv.org/abs/1904.09146)

Wenguan Wang, Qiuxia Lai, Huazhu Fu, Jianbing Shen, Haibin Ling, Ruigang Yang


## 2019-04-18

[Fooling automated surveillance cameras: adversarial patches to attack person detection.](http://arxiv.org/abs/1904.08653)

Simen Thys, Ranst Wiebe Van, Toon Goedemé


## 2019-04-17

[ZK-GanDef: A GAN based Zero Knowledge Adversarial Training Defense for Neural Networks.](http://arxiv.org/abs/1904.08516)

Guanxiong Liu, Issa Khalil, Abdallah Khreishah


[Defensive Quantization: When Efficiency Meets Robustness.](http://arxiv.org/abs/1904.08444)

Ji Lin, Chuang Gan, Song Han


[Interpreting Adversarial Examples with Attributes.](http://arxiv.org/abs/1904.08279)

Sadaf Gulshad, Jan Hendrik Metzen, Arnold Smeulders, Zeynep Akata


[Adversarial Defense Through Network Profiling Based Path Extraction.](http://arxiv.org/abs/1904.08089)

Yuxian Qiu, Jingwen Leng, Cong Guo, Quan Chen, Chao Li, Minyi Guo, Yuhao Zhu


[Gotta Catch 'Em All: Using Concealed Trapdoors to Detect Adversarial Attacks on Neural Networks.](http://arxiv.org/abs/1904.08554)

Shawn Shan, Emily Willson, Bolun Wang, Bo Li, Haitao Zheng, Ben Y. Zhao


[Semantic Adversarial Attacks: Parametric Transformations That Fool Deep Classifiers.](http://arxiv.org/abs/1904.08489)

Ameya Joshi, Amitangshu Mukherjee, Soumik Sarkar, Chinmay Hegde


## 2019-04-16

[Reducing Adversarial Example Transferability Using Gradient Regularization.](http://arxiv.org/abs/1904.07980)

George Adam, Petr Smirnov, Benjamin Haibe-Kains, Anna Goldenberg


[AT-GAN: An Adversarial Generator Model for Non-constrained Adversarial Examples.](http://arxiv.org/abs/1904.07793)

Xiaosen Wang, Kun He, Chuanbiao Song, Liwei Wang, John E. Hopcroft


## 2019-04-15

[Are Self-Driving Cars Secure? Evasion Attacks against Deep Neural Networks for Steering Angle Prediction.](http://arxiv.org/abs/1904.07370)

Alesia Chernikova, Alina Oprea, Cristina Nita-Rotaru, BaekGyu Kim


[Influence of Control Parameters and the Size of Biomedical Image Datasets on the Success of Adversarial Attacks.](http://arxiv.org/abs/1904.06964)

Vassili Kovalev, Dmitry Voynov


## 2019-04-13

[Exploiting Vulnerabilities of Load Forecasting Through Adversarial Attacks.](http://arxiv.org/abs/1904.06606)

Yize Chen, Yushi Tan, Baosen Zhang


## 2019-04-12

[Cycle-Consistent Adversarial GAN: the integration of adversarial attack and defense.](http://arxiv.org/abs/1904.06026)

Lingyun Jiang, Kai Qiao, Ruoxi Qin, Linyuan Wang, Jian Chen, Haibing Bu, Bin Yan


[Generating Minimal Adversarial Perturbations with Integrated Adaptive Gradients.](http://arxiv.org/abs/1904.06186)

Yatie Xiao, Chi-Man Pun


[Evaluating Robustness of Deep Image Super-Resolution against Adversarial Attacks.](http://arxiv.org/abs/1904.06097)

Jun-Ho Choi, Huan Zhang, Jun-Hyuk Kim, Cho-Jui Hsieh, Jong-Seok Lee


[Adversarial Learning in Statistical Classification: A Comprehensive Review of Defenses Against Attacks.](http://arxiv.org/abs/1904.06292)

David J. Miller, Zhen Xiang, George Kesidis


[Unrestricted Adversarial Examples via Semantic Manipulation.](http://arxiv.org/abs/1904.06347)

Anand Bhattad, Min Jin Chong, Kaizhao Liang, Bo Li, D. A. Forsyth


## 2019-04-11

[Black-Box Decision based Adversarial Attack with Symmetric $\alpha$-stable Distribution.](http://arxiv.org/abs/1904.05586)

Vignesh Srinivasan, Ercan E. Kuruoglu, Klaus-Robert Müller, Wojciech Samek, Shinichi Nakajima


## 2019-04-10

[Learning to Generate Synthetic Data via Compositing.](http://arxiv.org/abs/1904.05475)

Shashank Tripathi, Siddhartha Chandra, Amit Agrawal, Ambrish Tyagi, James M. Rehg, Visesh Chari


[Black-box Adversarial Attacks on Video Recognition Models.](http://arxiv.org/abs/1904.05181)

Linxi Jiang, Xingjun Ma, Shaoxiang Chen, James Bailey, Yu-Gang Jiang


## 2019-04-09

[Generation & Evaluation of Adversarial Examples for Malware Obfuscation.](http://arxiv.org/abs/1904.04802)

Daniel Park, Haidar Khan, Bülent Yener


## 2019-04-08

[Efficient Decision-based Black-box Adversarial Attacks on Face Recognition.](http://arxiv.org/abs/1904.04433)

Yinpeng Dong, Hang Su, Baoyuan Wu, Zhifeng Li, Wei Liu, Tong Zhang, Jun Zhu


[A Target-Agnostic Attack on Deep Models: Exploiting Security Vulnerabilities of Transfer Learning.](http://arxiv.org/abs/1904.04334)

Shahbaz Rezaei, Xin Liu


## 2019-04-07

[JumpReLU: A Retrofit Defense Strategy for Adversarial Attacks.](http://arxiv.org/abs/1904.03750)

N. Benjamin Erichson, Zhewei Yao, Michael W. Mahoney


[Malware Evasion Attack and Defense.](http://arxiv.org/abs/1904.05747)

Yonghong Huang, Utkarsh Verma, Celeste Fralick, Gabriel Infante-Lopez, Brajesh Kumarz, Carl Woodward


## 2019-04-06

[On Training Robust PDF Malware Classifiers.](http://arxiv.org/abs/1904.03542)

Yizheng Chen, Shiqi Wang, Dongdong She, Suman Jana


## 2019-04-05

[Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks.](http://arxiv.org/abs/1904.02884)

Yinpeng Dong, Tianyu Pang, Hang Su, Jun Zhu


## 2019-04-04

[White-to-Black: Efficient Distillation of Black-Box Adversarial Attacks.](http://arxiv.org/abs/1904.02405)

Yotam Gil, Yoav Chai, Or Gorodissky, Jonathan Berant


[Minimum Uncertainty Based Detection of Adversaries in Deep Neural Networks.](http://arxiv.org/abs/1904.02841)

Fatemeh Sheikholeslami, Swayambhoo Jain, Georgios B. Giannakis


## 2019-04-03

[Understanding the efficacy, reliability and resiliency of computer vision techniques for malware detection and future research directions.](http://arxiv.org/abs/1904.10504)

Li Chen


[Interpreting Adversarial Examples by Activation Promotion and Suppression.](http://arxiv.org/abs/1904.02057)

Kaidi Xu, Sijia Liu, Gaoyuan Zhang, Mengshu Sun, Pu Zhao, Quanfu Fan, Chuang Gan, Xue Lin


[HopSkipJumpAttack: A Query-Efficient Decision-Based Attack.](http://arxiv.org/abs/1904.02144)

Jianbo Chen, Michael I. Jordan, Martin J. Wainwright


[Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations.](http://arxiv.org/abs/1904.02323)

Fred Hohman, Haekyu Park, Caleb Robinson, Duen Horng Chau


## 2019-04-02

[Adversarial Attacks against Deep Saliency Models.](http://arxiv.org/abs/1904.01231)

Zhaohui Che, Ali Borji, Guangtao Zhai, Suiyi Ling, Guodong Guo, Patrick Le Callet


## 2019-04-01

[Curls & Whey: Boosting Black-Box Adversarial Attacks.](http://arxiv.org/abs/1904.01160)

Yucheng Shi, Siyu Wang, Yahong Han


[Robustness of 3D Deep Learning in an Adversarial Setting.](http://arxiv.org/abs/1904.00923)

Matthew Wicker, Marta Kwiatkowska


[Defending against adversarial attacks by randomized diversification.](http://arxiv.org/abs/1904.00689)

Olga Taran, Shideh Rezaeifar, Taras Holotyak, Slava Voloshynovskiy


[Adversarial Defense by Restricting the Hidden Space of Deep Neural Networks.](http://arxiv.org/abs/1904.00887)

Aamir Mustafa, Salman Khan, Munawar Hayat, Roland Goecke, Jianbing Shen, Ling Shao


[Regional Homogeneity: Towards Learning Transferable Universal Adversarial Perturbations Against Defenses.](http://arxiv.org/abs/1904.00979)

Yingwei Li, Song Bai, Cihang Xie, Zhenyu Liao, Xiaohui Shen, Alan L. Yuille


## 2019-03-31

[On the Vulnerability of CNN Classifiers in EEG-Based BCIs.](http://arxiv.org/abs/1904.01002)

Xiao Zhang, Dongrui Wu


## 2019-03-29

[Adversarial Robustness vs Model Compression, or Both?](http://arxiv.org/abs/1903.12561)

Shaokai Ye, Kaidi Xu, Sijia Liu, Hao Cheng, Jan-Henrik Lambrechts, Huan Zhang, Aojun Zhou, Kaisheng Ma, Yanzhi Wang, Xue Lin


## 2019-03-28

[Benchmarking Neural Network Robustness to Common Corruptions and Perturbations.](http://arxiv.org/abs/1903.12261)

Dan Hendrycks, Thomas Dietterich


[Smooth Adversarial Examples.](http://arxiv.org/abs/1903.11862)

Hanwei Zhang, Yannis Avrithis, Teddy Furon, Laurent Amsaleg


## 2019-03-27

[Bridging Adversarial Robustness and Gradient Interpretability.](http://arxiv.org/abs/1903.11626)

Beomsu Kim, Junghoon Seo, Taegyun Jeon


[Scaling up the randomized gradient-free adversarial attack reveals overestimation of robustness using established attacks.](http://arxiv.org/abs/1903.11359)

Francesco Croce, Jonas Rauber, Matthias Hein


[Rallying Adversarial Techniques against Deep Learning for Network Security.](http://arxiv.org/abs/1903.11688)

Joseph Clements, Yuzhe Yang, Ankur Sharma, Hongxin Hu, Yingjie Lao


[Text Processing Like Humans Do: Visually Attacking and Shielding NLP Systems.](http://arxiv.org/abs/1903.11508)

Steffen Eger, Gözde Gül Şahin, Andreas Rücklé, Ji-Ung Lee, Claudia Schulz, Mohsen Mesgar, Krishnkant Swarnkar, Edwin Simpson, Iryna Gurevych


## 2019-03-26

[On the Adversarial Robustness of Multivariate Robust Estimation.](http://arxiv.org/abs/1903.11220)

Erhan Bayraktar, Lifeng Lai


[A geometry-inspired decision-based attack.](http://arxiv.org/abs/1903.10826)

Yujia Liu, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard


## 2019-03-25

[Defending against Whitebox Adversarial Attacks via Randomized Discretization.](http://arxiv.org/abs/1903.10586)

Yuchen Zhang, Percy Liang


[Exploiting Excessive Invariance caused by Norm-Bounded Adversarial Robustness.](http://arxiv.org/abs/1903.10484)

Jörn-Henrik Jacobsen, Jens Behrmannn, Nicholas Carlini, Florian Tramèr, Nicolas Papernot


[The LogBarrier adversarial attack: making effective use of decision boundary information.](http://arxiv.org/abs/1903.10396)

Chris Finlay, Aram-Alexandre Pooladian, Adam M. Oberman


[Robust Neural Networks using Randomized Adversarial Training.](http://arxiv.org/abs/1903.10219)

Alexandre Araujo, Laurent Meunier, Rafael Pinot, Benjamin Negrevergne


## 2019-03-24

[A Formalization of Robustness for Deep Neural Networks.](http://arxiv.org/abs/1903.10033)

Tommaso Dreossi, Shromona Ghosh, Alberto Sangiovanni-Vincentelli, Sanjit A. Seshia


[Variational Inference with Latent Space Quantization for Adversarial Resilience.](http://arxiv.org/abs/1903.09940)

Vinay Kyatham, Mayank Mishra, Tarun Kumar Yadav, Deepak Mishra, Prathosh AP


## 2019-03-23

[Improving Adversarial Robustness via Guided Complement Entropy.](http://arxiv.org/abs/1903.09799)

Hao-Yun Chen, Jhao-Hong Liang, Shih-Chieh Chang, Jia-Yu Pan, Yu-Ting Chen, Wei Wei, Da-Cheng Juan


## 2019-03-22

[Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition.](http://arxiv.org/abs/1903.10346)

Yao Qin, Nicholas Carlini, Ian Goodfellow, Garrison Cottrell, Colin Raffel


[Fast Bayesian Uncertainty Estimation and Reduction of Batch Normalized Single Image Super-Resolution Network. (45%)](http://arxiv.org/abs/1903.09410)

Aupendu Kar, Prabir Kumar Biswas


## 2019-03-21

[Adversarial camera stickers: A physical camera-based attack on deep learning systems.](http://arxiv.org/abs/1904.00759)

Juncheng Li, Frank R. Schmidt, J. Zico Kolter


## 2019-03-20

[Provable Certificates for Adversarial Examples: Fitting a Ball in the Union of Polytopes.](http://arxiv.org/abs/1903.08778)

Matt Jordan, Justin Lewis, Alexandros G. Dimakis


## 2019-03-19

[On the Robustness of Deep K-Nearest Neighbors.](http://arxiv.org/abs/1903.08333)

Chawin Sitawarin, David Wagner


## 2019-03-18

[Generating Adversarial Examples With Conditional Generative Adversarial Net.](http://arxiv.org/abs/1903.07282)

Ping Yu, Kaitao Song, Jianfeng Lu


[Practical Hidden Voice Attacks against Speech and Speaker Recognition Systems.](http://arxiv.org/abs/1904.05734)

Hadi Abdullah, Washington Garcia, Christian Peeters, Patrick Traynor, Kevin R. B. Butler, Joseph Wilson


## 2019-03-17

[Adversarial Attacks on Deep Neural Networks for Time Series Classification.](http://arxiv.org/abs/1903.07054)

Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, Pierre-Alain Muller


## 2019-03-15

[On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models.](http://arxiv.org/abs/1903.06620)

Paul Michel, Xian Li, Graham Neubig, Juan Miguel Pino


[On Certifying Non-uniform Bound against Adversarial Attacks.](http://arxiv.org/abs/1903.06603)

Chen Liu, Ryota Tomioka, Volkan Cevher


## 2019-03-14

[A Research Agenda: Dynamic Models to Defend Against Correlated Attacks.](http://arxiv.org/abs/1903.06293)

Ian Goodfellow


[Attribution-driven Causal Analysis for Detection of Adversarial Examples.](http://arxiv.org/abs/1903.05821)

Susmit Jha, Sunny Raj, Steven Lawrence Fernandes, Sumit Kumar Jha, Somesh Jha, Gunjan Verma, Brian Jalaian, Ananthram Swami


## 2019-03-13

[Adversarial attacks against Fact Extraction and VERification.](http://arxiv.org/abs/1903.05543)

James Thorne, Andreas Vlachos


## 2019-03-12

[Simple Physical Adversarial Examples against End-to-End Autonomous Driving Models.](http://arxiv.org/abs/1903.05157)

Adith Boloor, Xin He, Christopher Gill, Yevgeniy Vorobeychik, Xuan Zhang


## 2019-03-11

[Can Adversarial Network Attack be Defended?](http://arxiv.org/abs/1903.05994)

Jinyin Chen, Yangyang Wu, Xiang Lin, Qi Xuan


## 2019-03-09

[Manifold Preserving Adversarial Learning.](http://arxiv.org/abs/1903.03905)

Ousmane Amadou Dia, Elnaz Barshan, Reza Babanezhad


## 2019-03-07

[Attack Type Agnostic Perceptual Enhancement of Adversarial Images.](http://arxiv.org/abs/1903.03029)

Bilgin Aksoy, Alptekin Temizel


[Out-domain examples for generative models.](http://arxiv.org/abs/1903.02926)

Dario Pasquini, Marco Mingione, Massimo Bernaschi


## 2019-03-06

[GanDef: A GAN based Adversarial Training Defense for Neural Network Classifier.](http://arxiv.org/abs/1903.02585)

Guanxiong Liu, Issa Khalil, Abdallah Khreishah


## 2019-03-05

[Statistical Guarantees for the Robustness of Bayesian Neural Networks.](http://arxiv.org/abs/1903.01980)

Luca Cardelli, Marta Kwiatkowska, Luca Laurenti, Nicola Paoletti, Andrea Patane, Matthew Wicker


[L 1-norm double backpropagation adversarial defense.](http://arxiv.org/abs/1903.01715)

Ismaïla LIMOS, LITIS Seck, Gaëlle LIMOS Loosli, Stephane LITIS Canu


## 2019-03-04

[Defense Against Adversarial Images using Web-Scale Nearest-Neighbor Search.](http://arxiv.org/abs/1903.01612)

Abhimanyu Dubey, der Maaten Laurens van, Zeki Yalniz, Yixuan Li, Dhruv Mahajan


[The Vulnerabilities of Graph Convolutional Networks: Stronger Attacks and Defensive Techniques.](http://arxiv.org/abs/1903.01610)

Huijun Wu, Chen Wang, Yuriy Tyshetskiy, Andrew Dotcherty, Kai Lu, Liming Zhu


[Complement Objective Training.](http://arxiv.org/abs/1903.01182)

Hao-Yun Chen, Pei-Hsin Wang, Chun-Hao Liu, Shih-Chieh Chang, Jia-Yu Pan, Yu-Ting Chen, Wei Wei, Da-Cheng Juan


[Safety Verification and Robustness Analysis of Neural Networks via Quadratic Constraints and Semidefinite Programming.](http://arxiv.org/abs/1903.01287)

Mahyar Fazlyab, Manfred Morari, George J. Pappas


## 2019-03-03

[A Kernelized Manifold Mapping to Diminish the Effect of Adversarial Perturbations.](http://arxiv.org/abs/1903.01015)

Saeid Asgari Taghanaki, Kumar Abhishek, Shekoofeh Azizi, Ghassan Hamarneh


## 2019-03-01

[Evaluating Adversarial Evasion Attacks in the Context of Wireless Communications.](http://arxiv.org/abs/1903.01563)

Bryse Flowers, R. Michael Buehrer, William C. Headley


[PuVAE: A Variational Autoencoder to Purify Adversarial Examples.](http://arxiv.org/abs/1903.00585)

Uiwon Hwang, Jaewoo Park, Hyemi Jang, Sungroh Yoon, Nam Ik Cho


[Attacking Graph-based Classification via Manipulating the Graph Structure.](http://arxiv.org/abs/1903.00553)

Binghui Wang, Neil Zhenqiang Gong


## 2019-02-28

[On the Effectiveness of Low Frequency Perturbations.](http://arxiv.org/abs/1903.00073)

Yash Sharma, Gavin Weiguang Ding, Marcus Brubaker


[Enhancing the Robustness of Deep Neural Networks by Boundary Conditional GAN.](http://arxiv.org/abs/1902.11029)

Ke Sun, Zhanxing Zhu, Zhouchen Lin


[Towards Understanding Adversarial Examples Systematically: Exploring Data Size, Task and Model Factors.](http://arxiv.org/abs/1902.11019)

Ke Sun, Zhanxing Zhu, Zhouchen Lin


[Adversarial Attack and Defense on Point Sets.](http://arxiv.org/abs/1902.10899)

Qiang Zhang, Jiancheng Yang, Rongyao Fang, Bingbing Ni, Jinxian Liu, Qi Tian


## 2019-02-27

[Adversarial Attacks on Time Series.](http://arxiv.org/abs/1902.10755)

Fazle Karim, Somshubra Majumdar, Houshang Darabi


[Robust Decision Trees Against Adversarial Examples.](http://arxiv.org/abs/1902.10660)

Hongge Chen, Huan Zhang, Duane Boning, Cho-Jui Hsieh


[Tensor Dropout for Robust Learning.](http://arxiv.org/abs/1902.10758)

Arinbjörn Kolbeinsson, Jean Kossaifi, Yannis Panagakis, Adrian Bulat, Anima Anandkumar, Ioanna Tzoulaki, Paul Matthews


[The Best Defense Is a Good Offense: Adversarial Attacks to Avoid Modulation Detection.](http://arxiv.org/abs/1902.10674)

Muhammad Zaid Hameed, Andras Gyorgy, Deniz Gunduz


[A Distributionally Robust Optimization Method for Adversarial Multiple Kernel Learning. (76%)](http://arxiv.org/abs/1902.10365)

Masoud Badiei Khuzani, Hongyi Ren, Md Tauhidul Islam, Lei Xing


[AutoGAN-based Dimension Reduction for Privacy Preservation. (1%)](http://arxiv.org/abs/1902.10799)

Hung Nguyen, Di Zhuang, Pei-Yuan Wu, Morris Chang


## 2019-02-26

[Disentangled Deep Autoencoding Regularization for Robust Image Classification.](http://arxiv.org/abs/1902.11134)

Zhenyu Duan, Martin Renqiang Min, Li Erran Li, Mingbo Cai, Yi Xu, Bingbing Ni


[Analyzing Deep Neural Networks with Symbolic Propagation: Towards Higher Precision and Faster Verification.](http://arxiv.org/abs/1902.09866)

Jianlin Li, Pengfei Yang, Jiangchao Liu, Liqian Chen, Xiaowei Huang, Lijun Zhang


## 2019-02-25

[Verification of Non-Linear Specifications for Neural Networks.](http://arxiv.org/abs/1902.09592)

Chongli Dj Qin, Dj Krishnamurthy, Dvijotham, Brendan O'Donoghue, Rudy Bunel, Robert Stanforth, Sven Gowal, Jonathan Uesato, Grzegorz Swirszcz, Pushmeet Kohli


[Adversarial attacks hidden in plain sight.](http://arxiv.org/abs/1902.09286)

Jan Philip Göpfert, André Artelt, Heiko Wersing, Barbara Hammer


## 2019-02-24

[MaskDGA: A Black-box Evasion Technique Against DGA Classifiers and Adversarial Defenses.](http://arxiv.org/abs/1902.08909)

Lior Sidi, Asaf Nadler, Asaf Shabtai


[Adversarial Reinforcement Learning under Partial Observability in Software-Defined Networking.](http://arxiv.org/abs/1902.09062)

Yi Han, David Hubczenko, Paul Montague, Vel Olivier De, Tamas Abraham, Benjamin I. P. Rubinstein, Christopher Leckie, Tansu Alpcan, Sarah Erfani


## 2019-02-23

[Re-evaluating ADEM: A Deeper Look at Scoring Dialogue Responses.](http://arxiv.org/abs/1902.08832)

Ananya B. Sai, Mithun Das Gupta, Mitesh M. Khapra, Mukundhan Srinivasan


[A Deep, Information-theoretic Framework for Robust Biometric Recognition.](http://arxiv.org/abs/1902.08785)

Renjie Xie, Yanzhi Chen, Yan Wo, Qiao Wang


## 2019-02-22

[Adversarial Attacks on Graph Neural Networks via Meta Learning.](http://arxiv.org/abs/1902.08412)

Daniel Zügner, Stephan Günnemann


[Physical Adversarial Attacks Against End-to-End Autoencoder Communication Systems.](http://arxiv.org/abs/1902.08391)

Meysam Sadeghi, Erik G. Larsson


[A Convex Relaxation Barrier to Tight Robustness Verification of Neural Networks.](http://arxiv.org/abs/1902.08722)

Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, Pengchuan Zhang


## 2019-02-21

[On the Sensitivity of Adversarial Robustness to Input Data Distributions.](http://arxiv.org/abs/1902.08336)

Gavin Weiguang Ding, Kry Yik Chau Lui, Xiaomeng Jin, Luyu Wang, Ruitong Huang


[Quantifying Perceptual Distortion of Adversarial Examples.](http://arxiv.org/abs/1902.08265)

Matt Jordan, Naren Manoj, Surbhi Goel, Alexandros G. Dimakis


[Wasserstein Adversarial Examples via Projected Sinkhorn Iterations.](http://arxiv.org/abs/1902.07906)

Eric Wong, Frank R. Schmidt, J. Zico Kolter


## 2019-02-20

[advertorch v0.1: An Adversarial Robustness Toolbox based on PyTorch.](http://arxiv.org/abs/1902.07623)

Gavin Weiguang Ding, Luyu Wang, Xiaomeng Jin


[Perceptual Quality-preserving Black-Box Attack against Deep Learning Image Classifiers.](http://arxiv.org/abs/1902.07776)

Diego Gragnaniello, Francesco Marra, Giovanni Poggi, Luisa Verdoliva


[Graph Adversarial Training: Dynamically Regularizing Based on Graph Structure.](http://arxiv.org/abs/1902.08226)

Fuli Feng, Xiangnan He, Jie Tang, Tat-Seng Chua


## 2019-02-19

[There are No Bit Parts for Sign Bits in Black-Box Attacks.](http://arxiv.org/abs/1902.06894)

Abdullah Al-Dujaili, Una-May O'Reilly


## 2019-02-18

[On Evaluating Adversarial Robustness.](http://arxiv.org/abs/1902.06705)

Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, Alexey Kurakin


[AuxBlocks: Defense Adversarial Example via Auxiliary Blocks.](http://arxiv.org/abs/1902.06415)

Yueyao Yu, Pengfei Yu, Wenye Li


[Mockingbird: Defending Against Deep-Learning-Based Website Fingerprinting Attacks with Adversarial Traces.](http://arxiv.org/abs/1902.06626)

Mohsen Imani, Mohammad Saidur Rahman, Nate Mathews, Matthew Wright


## 2019-02-16

[Mitigation of Adversarial Examples in RF Deep Classifiers Utilizing AutoEncoder Pre-training.](http://arxiv.org/abs/1902.08034)

Silvija Kokalj-Filipovic, Rob Miller, Nicholas Chang, Chi Leung Lau


[Adversarial Examples in RF Deep Learning: Detection of the Attack and its Physical Robustness.](http://arxiv.org/abs/1902.06044)

Silvija Kokalj-Filipovic, Rob Miller


## 2019-02-15

[DeepFault: Fault Localization for Deep Neural Networks.](http://arxiv.org/abs/1902.05974)

Hasan Ferit Eniser, Simos Gerasimou, Alper Sen


## 2019-02-14

[Can Intelligent Hyperparameter Selection Improve Resistance to Adversarial Examples?](http://arxiv.org/abs/1902.05586)

Cody Burkard, Brent Lagesse


## 2019-02-13

[The Odds are Odd: A Statistical Test for Detecting Adversarial Examples.](http://arxiv.org/abs/1902.04818)

Kevin Roth, Yannic Kilcher, Thomas Hofmann


## 2019-02-12

[Examining Adversarial Learning against Graph-based IoT Malware Detection Systems.](http://arxiv.org/abs/1902.04416)

Ahmed Abusnaina, Aminollah Khormali, Hisham Alasmary, Jeman Park, Afsah Anwar, Ulku Meteriz, Aziz Mohaisen


## 2019-02-11

[Adversarial Samples on Android Malware Detection Systems for IoT Systems.](http://arxiv.org/abs/1902.04238)

Xiaolei Liu, Xiaojiang Du, Xiaosong Zhang, Qingxin Zhu, Mohsen Guizani


[A Survey: Towards a Robust Deep Neural Network in Text Domain.](http://arxiv.org/abs/1902.07285)

Wenqi Wang, Lina Wang, Benxiao Tang, Run Wang, Aoshuang Ye


## 2019-02-09

[Model Compression with Adversarial Robustness: A Unified Optimization Framework.](http://arxiv.org/abs/1902.03538)

Shupeng University of Rochester Gui, Haotao Texas A&M University Wang, Chen University of Rochester Yu, Haichuan University of Rochester Yang, Zhangyang Texas A&M University Wang, Ji Ytech Seattle AI lab, FeDA lab, AI platform, Kwai Inc Liu


[When Causal Intervention Meets Adversarial Examples and Image Masking for Deep Neural Networks.](http://arxiv.org/abs/1902.03380)

Chao-Han Huck Yang, Yi-Chieh Liu, Pin-Yu Chen, Xiaoli Ma, Yi-Chang James Tsai


## 2019-02-08

[Minimal Images in Deep Neural Networks: Fragile Object Recognition in Natural Images.](http://arxiv.org/abs/1902.03227)

Sanjana Srivastava, Guy Ben-Yosef, Xavier Boix


[Understanding the One-Pixel Attack: Propagation Maps and Locality Analysis.](http://arxiv.org/abs/1902.02947)

Danilo Vasconcellos Vargas, Jiawei Su


[Discretization based Solutions for Secure Machine Learning against Adversarial Attacks.](http://arxiv.org/abs/1902.03151)

Priyadarshini Panda, Indranil Chakraborty, Kaushik Roy


## 2019-02-07

[Robustness Of Saak Transform Against Adversarial Attacks.](http://arxiv.org/abs/1902.02826)

Thiyagarajan Ramanathan, Abinaya Manimaran, Suya You, C-C Jay Kuo


[Certified Adversarial Robustness via Randomized Smoothing.](http://arxiv.org/abs/1902.02918)

Jeremy M Cohen, Elan Rosenfeld, J. Zico Kolter


## 2019-02-06

[Fooling Neural Network Interpretations via Adversarial Model Manipulation.](http://arxiv.org/abs/1902.02041)

Juyeon Heo, Sunghwan Joo, Taesup Moon


[Daedalus: Breaking Non-Maximum Suppression in Object Detection via Adversarial Examples.](http://arxiv.org/abs/1902.02067)

Derui Wang, Chaoran Li, Sheng Wen, Xiaojun Chang, Surya Nepal, Yang Xiang


## 2019-02-05

[Fatal Brain Damage.](http://arxiv.org/abs/1902.01686)

El Mahdi El Mhamdi, Rachid Guerraoui, Sergei Volodin


## 2019-02-04

[Theoretical evidence for adversarial robustness through randomization.](http://arxiv.org/abs/1902.01148)

Rafael Pinot, Laurent Meunier, Alexandre Araujo, Hisashi Kashima, Florian Yger, Cédric Gouy-Pailler, Jamal Atif


[Predictive Uncertainty Quantification with Compound Density Networks.](http://arxiv.org/abs/1902.01080)

Agustinus Kristiadi, Sina Däubener, Asja Fischer


[Is Spiking Secure? A Comparative Study on the Security Vulnerabilities of Spiking and Deep Neural Networks.](http://arxiv.org/abs/1902.01147)

Alberto Marchisio, Giorgio Nanfa, Faiq Khalid, Muhammad Abdullah Hanif, Maurizio Martina, Muhammad Shafique


## 2019-02-01

[Robustness Certificates Against Adversarial Examples for ReLU Networks.](http://arxiv.org/abs/1902.01235)

Sahil Singla, Soheil Feizi


[Natural and Adversarial Error Detection using Invariance to Image Transformations.](http://arxiv.org/abs/1902.00236)

Yuval Bahat, Michal Irani, Gregory Shakhnarovich


[Adaptive Gradient for Adversarial Perturbations Generation.](http://arxiv.org/abs/1902.01220)

Yatie Xiao, Chi-Man Pun


[Robustness of Generalized Learning Vector Quantization Models against Adversarial Attacks.](http://arxiv.org/abs/1902.00577)

Sascha Saralajew, Lars Holdijk, Maike Rees, Thomas Villmann


[The Efficacy of SHIELD under Different Threat Models.](http://arxiv.org/abs/1902.00541)

Cory Cornelius, Nilaksh Das, Shang-Tse Chen, Li Chen, Michael E. Kounavis, Duen Horng Chau


## 2019-01-31

[A New Family of Neural Networks Provably Resistant to Adversarial Attacks.](http://arxiv.org/abs/1902.01208)

Rakshit Agrawal, Alfaro Luca de, David Helmbold


[Training Artificial Neural Networks by Generalized Likelihood Ratio Method: Exploring Brain-like Learning to Improve Robustness.](http://arxiv.org/abs/1902.00358)

Li Xiao, Yijie Peng, Jeff Hong, Zewu Ke, Shuhuai Yang


## 2019-01-30

[A Simple Explanation for the Existence of Adversarial Examples with Small Hamming Distance.](http://arxiv.org/abs/1901.10861)

Adi Shamir, Itay Safran, Eyal Ronen, Orr Dunkelman


[Augmenting Model Robustness with Transformation-Invariant Attacks.](http://arxiv.org/abs/1901.11188)

Houpu Yao, Zhe Wang, Guangyu Nie, Yassine Mazboudi, Yezhou Yang, Yi Ren


## 2019-01-29

[Adversarial Examples Are a Natural Consequence of Test Error in Noise.](http://arxiv.org/abs/1901.10513)

Nic Ford, Justin Gilmer, Nicolas Carlini, Dogus Cubuk


[RED-Attack: Resource Efficient Decision based Attack for Machine Learning.](http://arxiv.org/abs/1901.10258)

Faiq Khalid, Hassan Ali, Muhammad Abdullah Hanif, Semeen Rehman, Rehan Ahmed, Muhammad Shafique


[Reliable Smart Road Signs.](http://arxiv.org/abs/1901.10622)

Muhammed O. Sayin, Chung-Wei Lin, Eunsuk Kang, Shinichi Shiraishi, Tamer Basar


[On the Effect of Low-Rank Weights on Adversarial Robustness of Neural Networks.](http://arxiv.org/abs/1901.10371)

Peter Langenberg, Emilio Rafael Balda, Arash Behboodi, Rudolf Mathar


[Adversarial Metric Attack and Defense for Person Re-identification.](http://arxiv.org/abs/1901.10650)

Song Bai, Yingwei Li, Yuyin Zhou, Qizhu Li, Philip H. S. Torr


## 2019-01-28

[Improving Adversarial Robustness of Ensembles with Diversity Training.](http://arxiv.org/abs/1901.09981)

Sanjay Kariyappa, Moinuddin K. Qureshi


[CapsAttacks: Robust and Imperceptible Adversarial Attacks on Capsule Networks.](http://arxiv.org/abs/1901.09878)

Alberto Marchisio, Giorgio Nanfa, Faiq Khalid, Muhammad Abdullah Hanif, Maurizio Martina, Muhammad Shafique


[Defense Methods Against Adversarial Examples for Recurrent Neural Networks.](http://arxiv.org/abs/1901.09963)

Ishai Rosenberg, Asaf Shabtai, Yuval Elovici, Lior Rokach


[Using Pre-Training Can Improve Model Robustness and Uncertainty.](http://arxiv.org/abs/1901.09960)

Dan Hendrycks, Kimin Lee, Mantas Mazeika


[Efficient Multiparty Interactive Coding for Insertions, Deletions and Substitutions. (1%)](http://arxiv.org/abs/1901.09863)

Ran Gelles, Yael T. Kalai, Govind Ramnarayan


## 2019-01-27

[An Information-Theoretic Explanation for the Adversarial Fragility of AI Classifiers.](http://arxiv.org/abs/1901.09413)

Hui Xie, Jirong Yi, Weiyu Xu, Raghu Mudumbai


[Characterizing the Shape of Activation Space in Deep Neural Networks.](http://arxiv.org/abs/1901.09496)

Thomas Gebhart, Paul Schrater, Alan Hylton


[Strong Black-box Adversarial Attacks on Unsupervised Machine Learning Models.](http://arxiv.org/abs/1901.09493)

Anshuman Chhabra, Abhishek Roy, Prasant Mohapatra


## 2019-01-26

[A Black-box Attack on Neural Networks Based on Swarm Evolutionary Algorithm.](http://arxiv.org/abs/1901.09892)

Xiaolei Liu, Yuheng Luo, Xiaosong Zhang, Qingxin Zhu


[Weighted-Sampling Audio Adversarial Example Attack.](http://arxiv.org/abs/1901.10300)

Xiaolei Liu, Xiaosong Zhang, Kun Wan, Qingxin Zhu, Yufei Ding


## 2019-01-25

[Generative Adversarial Networks for Black-Box API Attacks with Limited Training Data.](http://arxiv.org/abs/1901.09113)

Yi Shi, Yalin E. Sagduyu, Kemal Davaslioglu, Jason H. Li


[Improving Adversarial Robustness via Promoting Ensemble Diversity.](http://arxiv.org/abs/1901.08846)

Tianyu Pang, Kun Xu, Chao Du, Ning Chen, Jun Zhu


## 2019-01-24

[Towards Interpretable Deep Neural Networks by Leveraging Adversarial Examples.](http://arxiv.org/abs/1901.09035)

Yinpeng Dong, Fan Bao, Hang Su, Jun Zhu


[Cross-Entropy Loss and Low-Rank Features Have Responsibility for Adversarial Examples.](http://arxiv.org/abs/1901.08360)

Kamil Nar, Orhan Ocal, S. Shankar Sastry, Kannan Ramchandran


[Theoretically Principled Trade-off between Robustness and Accuracy.](http://arxiv.org/abs/1901.08573)

Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, Michael I. Jordan


## 2019-01-23

[SirenAttack: Generating Adversarial Audio for End-to-End Acoustic Systems.](http://arxiv.org/abs/1901.07846)

Tianyu Du, Shouling Ji, Jinfeng Li, Qinchen Gu, Ting Wang, Raheem Beyah


[Sitatapatra: Blocking the Transfer of Adversarial Samples.](http://arxiv.org/abs/1901.08121)

Ilia Shumailov, Xitong Gao, Yiren Zhao, Robert Mullins, Ross Anderson, Cheng-Zhong Xu


## 2019-01-21

[Universal Rules for Fooling Deep Neural Networks based Text Classification.](http://arxiv.org/abs/1901.07132)

Di Li, Danilo Vasconcellos Vargas, Sakurai Kouichi


[Adversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey.](http://arxiv.org/abs/1901.06796)

Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, Chenliang Li


[Sensitivity Analysis of Deep Neural Networks.](http://arxiv.org/abs/1901.07152)

Hai Shu, Hongtu Zhu


[Perception-in-the-Loop Adversarial Examples.](http://arxiv.org/abs/1901.06834)

Mahmoud Salamati, Sadegh Soudjani, Rupak Majumdar


## 2019-01-17

[Easy to Fool? Testing the Anti-evasion Capabilities of PDF Malware Scanners.](http://arxiv.org/abs/1901.05674)

Saeed TU Darmstadt Ehteshamifar, Antonio xorlab Barresi, Thomas R. ETH Zurich Gross, Michael TU Darmstadt Pradel


## 2019-01-15

[The Limitations of Adversarial Training and the Blind-Spot Attack.](http://arxiv.org/abs/1901.04684)

Huan Zhang, Hongge Chen, Zhao Song, Duane Boning, Inderjit S. Dhillon, Cho-Jui Hsieh


## 2019-01-13

[Generating Adversarial Perturbation with Root Mean Square Gradient.](http://arxiv.org/abs/1901.03706)

Yatie Xiao, Chi-Man Pun, Jizhe Zhou


## 2019-01-12

[ECGadv: Generating Adversarial Electrocardiogram to Misguide Arrhythmia Classification System.](http://arxiv.org/abs/1901.03808)

Huangxun Chen, Chenyu Huang, Qianyi Huang, Qian Zhang, Wei Wang


## 2019-01-11

[Explaining Vulnerabilities of Deep Learning to Adversarial Malware Binaries.](http://arxiv.org/abs/1901.03583)

Luca Demetrio, Battista Biggio, Giovanni Lagorio, Fabio Roli, Alessandro Armando


## 2019-01-10

[Characterizing and evaluating adversarial examples for Offline Handwritten Signature Verification.](http://arxiv.org/abs/1901.03398)

Luiz G. Hafemann, Robert Sabourin, Luiz S. Oliveira


[Image Transformation can make Neural Networks more robust against Adversarial Examples.](http://arxiv.org/abs/1901.03037)

Dang Duy Thang, Toshihiro Matsui


## 2019-01-09

[Extending Adversarial Attacks and Defenses to Deep 3D Point Cloud Classifiers.](http://arxiv.org/abs/1901.03006)

Daniel Liu, Ronald Yu, Hao Su


## 2019-01-08

[Interpretable BoW Networks for Adversarial Example Detection.](http://arxiv.org/abs/1901.02229)

Krishna Kanth Nakka, Mathieu Salzmann


## 2019-01-07

[Image Super-Resolution as a Defense Against Adversarial Attacks.](http://arxiv.org/abs/1901.01677)

Aamir Mustafa, Salman H. Khan, Munawar Hayat, Jianbing Shen, Ling Shao


## 2019-01-05

[Fake News Detection via NLP is Vulnerable to Adversarial Attacks.](http://arxiv.org/abs/1901.09657)

Zhixuan Zhou, Huankang Guan, Meghana Moorthy Bhat, Justin Hsu


## 2019-01-04

[Adversarial Examples Versus Cloud-based Detectors: A Black-box Empirical Study.](http://arxiv.org/abs/1901.01223)

Xurong Li, Shouling Ji, Meng Han, Juntao Ji, Zhenyu Ren, Yushan Liu, Chunming Wu


## 2019-01-02

[Multi-Label Adversarial Perturbations.](http://arxiv.org/abs/1901.00546)

Qingquan Song, Haifeng Jin, Xiao Huang, Xia Hu


[Adversarial Robustness May Be at Odds With Simplicity.](http://arxiv.org/abs/1901.00532)

Preetum Nakkiran


## 2019-01-01

[A Noise-Sensitivity-Analysis-Based Test Prioritization Technique for Deep Neural Networks.](http://arxiv.org/abs/1901.00054)

Long Zhang, Xuechao Sun, Yong Li, Zhenyu Zhang


## 2018-12-27

[DeepBillboard: Systematic Physical-World Testing of Autonomous Driving Systems.](http://arxiv.org/abs/1812.10812)

Husheng Zhou, Wei Li, Yuankun Zhu, Yuqun Zhang, Bei Yu, Lingming Zhang, Cong Liu


## 2018-12-26

[Adversarial Attack and Defense on Graph Data: A Survey.](http://arxiv.org/abs/1812.10528)

Lichao Sun, Yingtong Dou, Carl Yang, Ji Wang, Yixin Liu, Philip S. Yu, Lifang He, Bo Li


## 2018-12-25

[Noise Flooding for Detecting Audio Adversarial Examples Against Automatic Speech Recognition.](http://arxiv.org/abs/1812.10061)

Krishan Rajaratnam, Jugal Kalita


[PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning.](http://arxiv.org/abs/1812.10049)

Mehdi Jafarnia-Jahromi, Tasmin Chowdhury, Hsin-Tai Wu, Sayandev Mukherjee


[A Multiversion Programming Inspired Approach to Detecting Audio Adversarial Examples.](http://arxiv.org/abs/1812.10199)

Qiang Zeng, Jianhai Su, Chenglong Fu, Golam Kayas, Lannan Luo


[A Data-driven Adversarial Examples Recognition Framework via Adversarial Feature Genome.](http://arxiv.org/abs/1812.10085)

Li Chen, Qi Li, Weiye Chen, Zeyu Wang, Haifeng Li


[Seeing isn't Believing: Practical Adversarial Attack Against Object Detectors.](http://arxiv.org/abs/1812.10217)

Yue Zhao, Hong Zhu, Ruigang Liang, Qintao Shen, Shengzhi Zhang, Kai Chen


## 2018-12-24

[DUP-Net: Denoiser and Upsampler Network for 3D Adversarial Point Clouds Defense.](http://arxiv.org/abs/1812.11017)

Hang Zhou, Kejiang Chen, Weiming Zhang, Han Fang, Wenbo Zhou, Nenghai Yu


## 2018-12-23

[Markov Game Modeling of Moving Target Defense for Strategic Detection of Threats in Cloud Networks.](http://arxiv.org/abs/1812.09660)

Ankur Chowdhary, Sailik Sengupta, Dijiang Huang, Subbarao Kambhampati


[Guessing Smart: Biased Sampling for Efficient Black-Box Adversarial Attacks.](http://arxiv.org/abs/1812.09803)

Thomas Brunner, Frederik Diehl, Michael Truong Le, Alois Knoll


## 2018-12-22

[Exploiting the Inherent Limitation of L0 Adversarial Examples.](http://arxiv.org/abs/1812.09638)

Fei Zuo, Bokai Yang, Xiaopeng Li, Lannan Luo, Qiang Zeng


## 2018-12-21

[Dissociable neural representations of adversarially perturbed images in convolutional neural networks and the human brain.](http://arxiv.org/abs/1812.09431)

Chi Zhang, Xiaohan Duan, Linyuan Wang, Yongli Li, Bin Yan, Guoen Hu, Ruyuan Zhang, Li Tong


## 2018-12-19

[Enhancing Robustness of Deep Neural Networks Against Adversarial Malware Samples: Principles, Framework, and AICS'2019 Challenge.](http://arxiv.org/abs/1812.08108)

Deqiang Li, Qianmu Li, Yanfang Ye, Shouhuai Xu


## 2018-12-18

[PROVEN: Certifying Robustness of Neural Networks with a Probabilistic Approach.](http://arxiv.org/abs/1812.08329)

Tsui-Wei Weng, Pin-Yu Chen, Lam M. Nguyen, Mark S. Squillante, Ivan Oseledets, Luca Daniel


## 2018-12-17

[Spartan Networks: Self-Feature-Squeezing Neural Networks for increased robustness in adversarial settings.](http://arxiv.org/abs/1812.06815)

François Menet, Paul Berthier, José M. Fernandez, Michel Gagnon


[Designing Adversarially Resilient Classifiers using Resilient Feature Engineering.](http://arxiv.org/abs/1812.06626)

Kevin Eykholt, Atul Prakash


[A Survey of Safety and Trustworthiness of Deep Neural Networks.](http://arxiv.org/abs/1812.08342)

Xiaowei Huang, Daniel Kroening, Wenjie Ruan, James Sharp, Youcheng Sun, Emese Thamo, Min Wu, Xinping Yi


## 2018-12-16

[Defense-VAE: A Fast and Accurate Defense against Adversarial Attacks.](http://arxiv.org/abs/1812.06570)

Xiang Li, Shihao Ji


## 2018-12-15

[Perturbation Analysis of Learning Algorithms: A Unifying Perspective on Generation of Adversarial Examples.](http://arxiv.org/abs/1812.07385)

Emilio Rafael Balda, Arash Behboodi, Rudolf Mathar


[Trust Region Based Adversarial Attack on Neural Networks.](http://arxiv.org/abs/1812.06371)

Zhewei Yao, Amir Gholami, Peng Xu, Kurt Keutzer, Michael Mahoney


## 2018-12-14

[Adversarial Sample Detection for Deep Neural Network through Model Mutation Testing.](http://arxiv.org/abs/1812.05793)

Jingyi Wang, Guoliang Dong, Jun Sun, Xinyu Wang, Peixin Zhang


## 2018-12-13

[TextBugger: Generating Adversarial Text Against Real-world Applications.](http://arxiv.org/abs/1812.05271)

Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, Ting Wang


[Why ReLU networks yield high-confidence predictions far away from the training data and how to mitigate the problem.](http://arxiv.org/abs/1812.05720)

Matthias Hein, Maksym Andriushchenko, Julian Bitterwolf


[Generating Hard Examples for Pixel-wise Classification. (4%)](http://arxiv.org/abs/1812.05447)

Hyungtae Lee, Heesung Kwon, Wonkook Kim


## 2018-12-12

[Thwarting Adversarial Examples: An $L_0$-RobustSparse Fourier Transform.](http://arxiv.org/abs/1812.05013)

Mitali Bafna, Jack Murtagh, Nikhil Vyas


## 2018-12-11

[On the Security of Randomized Defenses Against Adversarial Samples.](http://arxiv.org/abs/1812.04293)

Kumar Sharad, Giorgia Azzurra Marson, Hien Thi Thu Truong, Ghassan Karame


[Adversarial Framing for Image and Video Classification.](http://arxiv.org/abs/1812.04599)

Konrad Zolna, Michal Zajac, Negar Rostamzadeh, Pedro O. Pinheiro


## 2018-12-10

[Defending Against Universal Perturbations With Shared Adversarial Training.](http://arxiv.org/abs/1812.03705)

Chaithanya Kumar Mummadi, Thomas Brox, Jan Hendrik Metzen


## 2018-12-08

[Feature Denoising for Improving Adversarial Robustness.](http://arxiv.org/abs/1812.03411)

Cihang Xie, Yuxin Wu, der Maaten Laurens van, Alan Yuille, Kaiming He


[AutoGAN: Robust Classifier Against Adversarial Attacks.](http://arxiv.org/abs/1812.03405)

Blerta Lindqvist, Shridatt Sugrim, Rauf Izmailov


[Detecting Adversarial Examples in Convolutional Neural Networks.](http://arxiv.org/abs/1812.03303)

Stefanos Pertigkiozoglou, Petros Maragos


[Learning Transferable Adversarial Examples via Ghost Networks.](http://arxiv.org/abs/1812.03413)

Yingwei Li, Song Bai, Yuyin Zhou, Cihang Xie, Zhishuai Zhang, Alan Yuille


## 2018-12-07

[Deep-RBF Networks Revisited: Robust Classification with Rejection.](http://arxiv.org/abs/1812.03190)

Pourya Habib Zadeh, Reshad Hosseini, Suvrit Sra


[Combatting Adversarial Attacks through Denoising and Dimensionality Reduction: A Cascaded Autoencoder Approach.](http://arxiv.org/abs/1812.03087)

Rajeev Sahay, Rehana Mahfuz, Aly El Gamal


## 2018-12-06

[Adversarial Defense of Image Classification Using a Variational Auto-Encoder.](http://arxiv.org/abs/1812.02891)

Yi Luo, Henry Pfister


[Adversarial Attacks, Regression, and Numerical Stability Regularization.](http://arxiv.org/abs/1812.02885)

Andre T. Nguyen, Edward Raff


[Prior Networks for Detection of Adversarial Attacks.](http://arxiv.org/abs/1812.02575)

Andrey Malinin, Mark Gales


[Towards Leveraging the Information of Gradients in Optimization-based Adversarial Attack.](http://arxiv.org/abs/1812.02524)

Jingyang Zhang, Hsin-Pai Cheng, Chunpeng Wu, Hai Li, Yiran Chen


[Fooling Network Interpretation in Image Classification.](http://arxiv.org/abs/1812.02843)

Akshayvarun Subramanya, Vipin Pillai, Hamed Pirsiavash


[The Limitations of Model Uncertainty in Adversarial Settings.](http://arxiv.org/abs/1812.02606)

Kathrin Grosse, David Pfaff, Michael Thomas Smith, Michael Backes


[MMA Training: Direct Input Space Margin Maximization through Adversarial Training.](http://arxiv.org/abs/1812.02637)

Gavin Weiguang Ding, Yash Sharma, Kry Yik Chau Lui, Ruitong Huang


## 2018-12-05

[On Configurable Defense against Adversarial Example Attacks.](http://arxiv.org/abs/1812.02737)

Bo Luo, Min Li, Yu Li, Qiang Xu


[Regularized Ensembles and Transferability in Adversarial Learning.](http://arxiv.org/abs/1812.01821)

Yifan Chen, Yevgeniy Vorobeychik


[SADA: Semantic Adversarial Diagnostic Attacks for Autonomous Applications.](http://arxiv.org/abs/1812.02132)

Abdullah Hamdi, Matthias Müller, Bernard Ghanem


## 2018-12-04

[Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures.](http://arxiv.org/abs/1812.01647)

Jonathan Dj Uesato, Ananya Dj Kumar, Csaba Dj Szepesvari, Tom Dj Erez, Avraham Dj Ruderman, Keith Dj Anderson, Dj Krishmamurthy, Dvijotham, Nicolas Heess, Pushmeet Kohli


[Random Spiking and Systematic Evaluation of Defenses Against Adversarial Examples.](http://arxiv.org/abs/1812.01804)

Huangyi Ge, Sze Yiu Chau, Bruno Ribeiro, Ninghui Li


## 2018-12-03

[Disentangling Adversarial Robustness and Generalization.](http://arxiv.org/abs/1812.00740)

David Stutz, Matthias Hein, Bernt Schiele


[Interpretable Deep Learning under Fire.](http://arxiv.org/abs/1812.00891)

Xinyang Zhang, Ningfei Wang, Hua Shen, Shouling Ji, Xiapu Luo, Ting Wang


[Adversarial Example Decomposition.](http://arxiv.org/abs/1812.01198)

Horace He, Aaron Lou, Qingxuan Jiang, Isay Katsman, Serge Belongie, Ser-Nam Lim


## 2018-12-02

[Model-Reuse Attacks on Deep Learning Systems.](http://arxiv.org/abs/1812.00483)

Yujie Ji, Xinyang Zhang, Shouling Ji, Xiapu Luo, Ting Wang


[Universal Perturbation Attack Against Image Retrieval.](http://arxiv.org/abs/1812.00552)

Jie Li, Rongrong Ji, Hong Liu, Xiaopeng Hong, Yue Gao, Qi Tian


## 2018-12-01

[FineFool: Fine Object Contour Attack via Attention.](http://arxiv.org/abs/1812.01713)

Jinyin Chen, Haibin Zheng, Hui Xiong, Mengmeng Su


[Building robust classifiers through generation of confident out of distribution examples.](http://arxiv.org/abs/1812.00239)

Kumar Sricharan, Ashok Srivastava


[Discrete Adversarial Attacks and Submodular Optimization with Applications to Text Classification.](http://arxiv.org/abs/1812.00151)

Qi Lei, Lingfei Wu, Pin-Yu Chen, Alexandros G. Dimakis, Inderjit S. Dhillon, Michael Witbrock


[Effects of Loss Functions And Target Representations on Adversarial Robustness.](http://arxiv.org/abs/1812.00181)

Sean Saito, Sujoy Roy


[SentiNet: Detecting Localized Universal Attacks Against Deep Learning Systems.](http://arxiv.org/abs/1812.00292)

Edward Chou, Florian Tramèr, Giancarlo Pellegrino


## 2018-11-30

[Transferable Adversarial Attacks for Image and Video Object Detection.](http://arxiv.org/abs/1811.12641)

Xingxing Wei, Siyuan Liang, Xiaochun Cao, Jun Zhu


[ComDefend: An Efficient Image Compression Model to Defend Adversarial Examples.](http://arxiv.org/abs/1811.12673)

Xiaojun Jia, Xingxing Wei, Xiaochun Cao, Hassan Foroosh


[Adversarial Defense by Stratified Convolutional Sparse Coding.](http://arxiv.org/abs/1812.00037)

Bo Sun, Nian-hsuan Tsai, Fangchen Liu, Ronald Yu, Hao Su


## 2018-11-29

[CNN-Cert: An Efficient Framework for Certifying Robustness of Convolutional Neural Networks.](http://arxiv.org/abs/1811.12395)

Akhilan Boopathy, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, Luca Daniel


[Bayesian Adversarial Spheres: Bayesian Inference and Adversarial Examples in a Noiseless Setting.](http://arxiv.org/abs/1811.12335)

Artur Bekasov, Iain Murray


[Adversarial Examples as an Input-Fault Tolerance Problem.](http://arxiv.org/abs/1811.12601)

Angus Galloway, Anna Golubeva, Graham W. Taylor


[Analyzing Federated Learning through an Adversarial Lens.](http://arxiv.org/abs/1811.12470)

Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, Seraphin Calo


## 2018-11-28

[Adversarial Attacks for Optical Flow-Based Action Recognition Classifiers.](http://arxiv.org/abs/1811.11875)

Nathan Inkawhich, Matthew Inkawhich, Yiran Chen, Hai Li


[Strike (with) a Pose: Neural Networks Are Easily Fooled by Strange Poses of Familiar Objects.](http://arxiv.org/abs/1811.11553)

Michael A. Alcorn, Qi Li, Zhitao Gong, Chengfei Wang, Long Mai, Wei-Shinn Ku, Anh Nguyen


[A randomized gradient-free attack on ReLU networks.](http://arxiv.org/abs/1811.11493)

Francesco Croce, Matthias Hein


[Adversarial Machine Learning And Speech Emotion Recognition: Utilizing Generative Adversarial Networks For Robustness.](http://arxiv.org/abs/1811.11402)

Siddique Latif, Rajib Rana, Junaid Qadir


## 2018-11-27

[Robust Classification of Financial Risk.](http://arxiv.org/abs/1811.11079)

Suproteem K. Sarkar, Kojin Oshiba, Daniel Giebisch, Yaron Singer


[Universal Adversarial Training.](http://arxiv.org/abs/1811.11304)

Ali Shafahi, Mahyar Najibi, Zheng Xu, John Dickerson, Larry S. Davis, Tom Goldstein


[Using Attribution to Decode Dataset Bias in Neural Network Models for Chemistry.](http://arxiv.org/abs/1811.11310)

Kevin McCloskey, Ankur Taly, Federico Monti, Michael P. Brenner, Lucy Colwell


[A Frank-Wolfe Framework for Efficient and Effective Adversarial Attacks.](http://arxiv.org/abs/1811.10828)

Jinghui Chen, Dongruo Zhou, Jinfeng Yi, Quanquan Gu


## 2018-11-26

[ResNets Ensemble via the Feynman-Kac Formalism to Improve Natural and Robust Accuracies.](http://arxiv.org/abs/1811.10745)

Bao Wang, Binjie Yuan, Zuoqiang Shi, Stanley J. Osher


[Bilateral Adversarial Training: Towards Fast Training of More Robust Models Against Adversarial Attacks.](http://arxiv.org/abs/1811.10716)

Jianyu Wang, Haichao Zhang


## 2018-11-25

[Is Data Clustering in Adversarial Settings Secure?](http://arxiv.org/abs/1811.09982)

Battista Biggio, Ignazio Pillai, Samuel Rota Bulò, Davide Ariu, Marcello Pelillo, Fabio Roli


## 2018-11-24

[Attention, Please! Adversarial Defense via Activation Rectification and Preservation.](http://arxiv.org/abs/1811.09831)

Shangxi Wu, Jitao Sang, Kaiyuan Xu, Jiaming Zhang, Yanfeng Sun, Liping Jing, Jian Yu


## 2018-11-23

[Robustness via curvature regularization, and vice versa.](http://arxiv.org/abs/1811.09716)

Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, Pascal Frossard


[Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses.](http://arxiv.org/abs/1811.09600)

Jérôme Rony, Luiz G. Hafemann, Luiz S. Oliveira, Ismail Ben Ayed, Robert Sabourin, Eric Granger


## 2018-11-22

[Parametric Noise Injection: Trainable Randomness to Improve Deep Neural Network Robustness against Adversarial Attack.](http://arxiv.org/abs/1811.09310)

Adnan Siraj Rakin, Zhezhi He, Deliang Fan


[Strength in Numbers: Trading-off Robustness and Computation via Adversarially-Trained Ensembles.](http://arxiv.org/abs/1811.09300)

Edward Grefenstette, Robert Stanforth, Brendan O'Donoghue, Jonathan Uesato, Grzegorz Swirszcz, Pushmeet Kohli


[Detecting Adversarial Perturbations Through Spatial Behavior in Activation Spaces.](http://arxiv.org/abs/1811.09043)

Ziv Katzir, Yuval Elovici


## 2018-11-21

[Task-generalizable Adversarial Attack based on Perceptual Metric.](http://arxiv.org/abs/1811.09020)

Muzammal Naseer, Salman H. Khan, Shafin Rahman, Fatih Porikli


[Towards Robust Neural Networks with Lipschitz Continuity.](http://arxiv.org/abs/1811.09008)

Muhammad Usama, Dong Eui Chang


## 2018-11-20

[How the Softmax Output is Misleading for Evaluating the Strength of Adversarial Examples.](http://arxiv.org/abs/1811.08577)

Utku Ozbulak, Neve Wesley De, Messem Arnout Van


[MimicGAN: Corruption-Mimicking for Blind Image Recovery & Adversarial Defense.](http://arxiv.org/abs/1811.08484)

Rushil Anirudh, Jayaraman J. Thiagarajan, Bhavya Kailkhura, Timo Bremer


[Intermediate Level Adversarial Attack for Enhanced Transferability.](http://arxiv.org/abs/1811.08458)

Qian Huang, Zeqi Gu, Isay Katsman, Horace He, Pian Pawakapan, Zhiqiu Lin, Serge Belongie, Ser-Nam Lim


[Lightweight Lipschitz Margin Training for Certified Defense against Adversarial Examples.](http://arxiv.org/abs/1811.08080)

Hajime Ono, Tsubasa Takahashi, Kazuya Kakizaki


[Convolutional Neural Networks with Transformed Input based on Robust Tensor Network Decomposition.](http://arxiv.org/abs/1812.02622)

Jenn-Bing Ong, Wee-Keong Ng, C. -C. Jay Kuo


## 2018-11-19

[Optimal Transport Classifier: Defending Against Adversarial Attacks by Regularized Deep Embedding.](http://arxiv.org/abs/1811.07950)

Yao Li, Martin Renqiang Min, Wenchao Yu, Cho-Jui Hsieh, Thomas C. M. Lee, Erik Kruus


## 2018-11-18

[Generalizable Adversarial Training via Spectral Normalization.](http://arxiv.org/abs/1811.07457)

Farzan Farnia, Jesse M. Zhang, David Tse


[Regularized adversarial examples for model interpretability.](http://arxiv.org/abs/1811.07311)

Yoel Shoshan, Vadim Ratner


[The Taboo Trap: Behavioural Detection of Adversarial Samples.](http://arxiv.org/abs/1811.07375)

Ilia Shumailov, Yiren Zhao, Robert Mullins, Ross Anderson


## 2018-11-17

[DeepConsensus: using the consensus of features from multiple layers to attain robust image classification.](http://arxiv.org/abs/1811.07266)

Yuchen Li, Safwan Hossain, Kiarash Jamali, Frank Rudzicz


[Classifiers Based on Deep Sparse Coding Architectures are Robust to Deep Learning Transferable Examples.](http://arxiv.org/abs/1811.07211)

Jacob M. Springer, Charles S. Strauss, Austin M. Thresher, Edward Kim, Garrett T. Kenyon


[Boosting the Robustness Verification of DNN by Identifying the Achilles's Heel.](http://arxiv.org/abs/1811.07108)

Chengdong Feng, Zhenbang Chen, Weijiang Hong, Hengbiao Yu, Wei Dong, Ji Wang


## 2018-11-16

[Protecting Voice Controlled Systems Using Sound Source Identification Based on Acoustic Cues.](http://arxiv.org/abs/1811.07018)

Yuan Gong, Christian Poellabauer


[DARCCC: Detecting Adversaries by Reconstruction from Class Conditional Capsules.](http://arxiv.org/abs/1811.06969)

Nicholas Frosst, Sara Sabour, Geoffrey Hinton


## 2018-11-15

[A Spectral View of Adversarially Robust Features.](http://arxiv.org/abs/1811.06609)

Shivam Garg, Vatsal Sharan, Brian Hu Zhang, Gregory Valiant


[A note on hyperparameters in black-box adversarial examples.](http://arxiv.org/abs/1811.06539)

Jamie Hayes


[Mathematical Analysis of Adversarial Attacks.](http://arxiv.org/abs/1811.06492)

Zehao Dou, Stanley J. Osher, Bao Wang


[Adversarial Examples from Cryptographic Pseudo-Random Generators.](http://arxiv.org/abs/1811.06418)

Sébastien Bubeck, Yin Tat Lee, Eric Price, Ilya Razenshteyn


## 2018-11-14

[Verification of Recurrent Neural Networks Through Rule Extraction.](http://arxiv.org/abs/1811.06029)

Qinglong Wang, Kaixuan Zhang, Xue Liu, C. Lee Giles


[Robustness of spectral methods for community detection.](http://arxiv.org/abs/1811.05808)

Ludovic Stephan, Laurent Massoulié


## 2018-11-13

[Deep Q learning for fooling neural networks.](http://arxiv.org/abs/1811.05521)

Mandar Kulkarni


## 2018-11-08

[Universal Decision-Based Black-Box Perturbations: Breaking Security-Through-Obscurity Defenses.](http://arxiv.org/abs/1811.03733)

Thomas A. Hogan, Bhavya Kailkhura


[New CleverHans Feature: Better Adversarial Robustness Evaluations with Attack Bundling.](http://arxiv.org/abs/1811.03685)

Ian Goodfellow


[A Geometric Perspective on the Transferability of Adversarial Directions.](http://arxiv.org/abs/1811.03531)

Zachary Charles, Harrison Rosenberg, Dimitris Papailiopoulos


## 2018-11-07

[CAAD 2018: Iterative Ensemble Adversarial Attack.](http://arxiv.org/abs/1811.03456)

Jiayang Liu, Weiming Zhang, Nenghai Yu


[AdVersarial: Perceptual Ad Blocking meets Adversarial Machine Learning.](http://arxiv.org/abs/1811.03194)

Florian Tramèr, Pascal Dupré, Gili Rusak, Giancarlo Pellegrino, Dan Boneh


## 2018-11-06

[MixTrain: Scalable Training of Verifiably Robust Neural Networks.](http://arxiv.org/abs/1811.02625)

Shiqi Wang, Yizheng Chen, Ahmed Abdou, Suman Jana


[SparseFool: a few pixels make a big difference.](http://arxiv.org/abs/1811.02248)

Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard


## 2018-11-05

[Active Deep Learning Attacks under Strict Rate Limitations for Online API Calls.](http://arxiv.org/abs/1811.01811)

Yi Shi, Yalin E. Sagduyu, Kemal Davaslioglu, Jason H. Li


[FUNN: Flexible Unsupervised Neural Network.](http://arxiv.org/abs/1811.01749)

David Vigouroux, Sylvain Picard


[On the Transferability of Adversarial Examples Against CNN-Based Image Forensics.](http://arxiv.org/abs/1811.01629)

Mauro Barni, Kassem Kallas, Ehsan Nowroozi, Benedetta Tondi


## 2018-11-04

[FAdeML: Understanding the Impact of Pre-Processing Noise Filtering on Adversarial Machine Learning.](http://arxiv.org/abs/1811.01444)

Faiq Khalid, Muhammmad Abdullah Hanif, Semeen Rehman, Junaid Qadir, Muhammad Shafique


[QuSecNets: Quantization-based Defense Mechanism for Securing Deep Neural Network against Adversarial Attacks.](http://arxiv.org/abs/1811.01437)

Faiq Khalid, Hassan Ali, Hammad Tariq, Muhammad Abdullah Hanif, Semeen Rehman, Rehan Ahmed, Muhammad Shafique


[SSCNets: Robustifying DNNs using Secure Selective Convolutional Filters.](http://arxiv.org/abs/1811.01443)

Hassan Ali, Faiq Khalid, Hammad Tariq, Muhammad Abdullah Hanif, Semeen Rehman, Rehan Ahmed, Muhammad Shafique


## 2018-11-03

[Adversarial Gain.](http://arxiv.org/abs/1811.01302)

Peter Henderson, Koustuv Sinha, Rosemary Nan Ke, Joelle Pineau


[CAAD 2018: Powerful None-Access Black-Box Attack Based on Adversarial Transformation Network.](http://arxiv.org/abs/1811.01225)

Xiaoyi Dong, Weiming Zhang, Nenghai Yu


[Adversarial Black-Box Attacks on Automatic Speech Recognition Systems using Multi-Objective Evolutionary Optimization.](http://arxiv.org/abs/1811.01312)

Shreya Khare, Rahul Aralikatte, Senthil Mani


[Learning to Defense by Learning to Attack.](http://arxiv.org/abs/1811.01213)

Haoming Jiang, Zhehui Chen, Yuyang Shi, Bo Dai, Tuo Zhao


## 2018-11-02

[A Marauder's Map of Security and Privacy in Machine Learning.](http://arxiv.org/abs/1811.01134)

Nicolas Papernot


[Semidefinite relaxations for certifying robustness to adversarial examples.](http://arxiv.org/abs/1811.01057)

Aditi Raghunathan, Jacob Steinhardt, Percy Liang


[Efficient Neural Network Robustness Certification with General Activation Functions.](http://arxiv.org/abs/1811.00866)

Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, Luca Daniel


[Towards Adversarial Malware Detection: Lessons Learned from PDF-based Attacks.](http://arxiv.org/abs/1811.00830)

Davide Maiorca, Battista Biggio, Giorgio Giacinto


[TrISec: Training Data-Unaware Imperceptible Security Attacks on Deep Neural Networks.](http://arxiv.org/abs/1811.01031)

Faiq Khalid, Muhammad Abdullah Hanif, Semeen Rehman, Rehan Ahmed, Muhammad Shafique


## 2018-11-01

[Improving Adversarial Robustness by Encouraging Discriminative Features.](http://arxiv.org/abs/1811.00621)

Chirag Agarwal, Anh Nguyen, Dan Schonfeld


[On the Geometry of Adversarial Examples.](http://arxiv.org/abs/1811.00525)

Marc Khoury, Dylan Hadfield-Menell


[Excessive Invariance Causes Adversarial Vulnerability.](http://arxiv.org/abs/1811.00401)

Jörn-Henrik Jacobsen, Jens Behrmann, Richard Zemel, Matthias Bethge


## 2018-10-31

[When Not to Classify: Detection of Reverse Engineering Attacks on DNN Image Classifiers.](http://arxiv.org/abs/1811.02658)

Yujia Wang, David J. Miller, George Kesidis


[Unauthorized AI cannot Recognize Me: Reversible Adversarial Example.](http://arxiv.org/abs/1811.00189)

Jiayang Liu, Weiming Zhang, Kazuto Fukuchi, Youhei Akimoto, Jun Sakuma


## 2018-10-30

[Improved Network Robustness with Adversary Critic.](http://arxiv.org/abs/1810.12576)

Alexander Matyasko, Lap-Pui Chau


[On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models.](http://arxiv.org/abs/1810.12715)

Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Relja Arandjelovic, Timothy Mann, Pushmeet Kohli


## 2018-10-29

[Adversarial Risk and Robustness: General Definitions and Implications for the Uniform Distribution.](http://arxiv.org/abs/1810.12272)

Dimitrios I. Diochnos, Saeed Mahloujifar, Mohammad Mahmoody


[Logit Pairing Methods Can Fool Gradient-Based Attacks.](http://arxiv.org/abs/1810.12042)

Marius Mosbach, Maksym Andriushchenko, Thomas Trost, Matthias Hein, Dietrich Klakow


## 2018-10-28

[RecurJac: An Efficient Recursive Algorithm for Bounding Jacobian Matrix of Neural Networks and Its Applications.](http://arxiv.org/abs/1810.11783)

Huan Zhang, Pengchuan Zhang, Cho-Jui Hsieh


[Rademacher Complexity for Adversarially Robust Generalization.](http://arxiv.org/abs/1810.11914)

Dong Yin, Kannan Ramchandran, Peter Bartlett


[Robust Audio Adversarial Example for a Physical Attack.](http://arxiv.org/abs/1810.11793)

Hiromu Yakura, Jun Sakuma


## 2018-10-27

[Towards Robust Deep Neural Networks.](http://arxiv.org/abs/1810.11726)

Timothy E. Wang, Yiming Gu, Dhagash Mehta, Xiaojun Zhao, Edgar A. Bernal


[Regularization Effect of Fast Gradient Sign Method and its Generalization.](http://arxiv.org/abs/1810.11711)

Chandler Zuo


## 2018-10-26

[Attacks Meet Interpretability: Attribute-steered Detection of Adversarial Samples.](http://arxiv.org/abs/1810.11580)

Guanhong Tao, Shiqing Ma, Yingqi Liu, Xiangyu Zhang


## 2018-10-25

[Law and Adversarial Machine Learning.](http://arxiv.org/abs/1810.10731)

Ram Shankar Siva Kumar, David R. O'Brien, Kendra Albert, Salome Vilojen


[Attack Graph Convolutional Networks by Adding Fake Nodes.](http://arxiv.org/abs/1810.10751)

Xiaoyun Wang, Minhao Cheng, Joe Eaton, Cho-Jui Hsieh, Felix Wu


[Evading classifiers in discrete domains with provable optimality guarantees.](http://arxiv.org/abs/1810.10939)

Bogdan Kulynych, Jamie Hayes, Nikita Samarin, Carmela Troncoso


## 2018-10-24

[Robust Adversarial Learning via Sparsifying Front Ends.](http://arxiv.org/abs/1810.10625)

Soorya Gopalakrishnan, Zhinus Marzi, Metehan Cekic, Upamanyu Madhow, Ramtin Pedarsani


## 2018-10-23

[Stochastic Substitute Training: A Gray-box Approach to Craft Adversarial Examples Against Gradient Obfuscation Defenses.](http://arxiv.org/abs/1810.10031)

Mohammad Hashemi, Greg Cusack, Eric Keller


[One Bit Matters: Understanding Adversarial Examples as the Abuse of Redundancy.](http://arxiv.org/abs/1810.09650)

Jingkang Wang, Ruoxi Jia, Gerald Friedland, Bo Li, Costas Spanos


[Et Tu Alexa? When Commodity WiFi Devices Turn into Adversarial Motion Sensors.](http://arxiv.org/abs/1810.10109)

Yanzi Zhu, Zhujun Xiao, Yuxin Chen, Zhijing Li, Max Liu, Ben Y. Zhao, Haitao Zheng


## 2018-10-22

[Adversarial Risk Bounds via Function Transformation.](http://arxiv.org/abs/1810.09519)

Justin Khim, Po-Ling Loh


[Cost-Sensitive Robustness against Adversarial Examples.](http://arxiv.org/abs/1810.09225)

Xiao Zhang, David Evans


[Sparse DNNs with Improved Adversarial Robustness.](http://arxiv.org/abs/1810.09619)

Yiwen Guo, Chao Zhang, Changshui Zhang, Yurong Chen


## 2018-10-19

[On Extensions of CLEVER: A Neural Network Robustness Evaluation Algorithm.](http://arxiv.org/abs/1810.08640)

Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Aurelie Lozano, Cho-Jui Hsieh, Luca Daniel


## 2018-10-18

[Exploring Adversarial Examples in Malware Detection.](http://arxiv.org/abs/1810.08280)

Octavian Suciu, Scott E. Coull, Jeffrey Johns


[A Training-based Identification Approach to VIN Adversarial Examples.](http://arxiv.org/abs/1810.08070)

Yingdi Wang, Wenjia Niu, Tong Chen, Yingxiao Xiang, Jingjing Liu, Gang Li, Jiqiang Liu


## 2018-10-17

[Provable Robustness of ReLU networks via Maximization of Linear Regions.](http://arxiv.org/abs/1810.07481)

Francesco University of Tübingen Croce, Maksym Saarland University Andriushchenko, Matthias University of Tübingen Hein


## 2018-10-16

[Projecting Trouble: Light Based Adversarial Attacks on Deep Learning Classifiers.](http://arxiv.org/abs/1810.10337)

Nicole Nichols, Robert Jasper


[Security Matters: A Survey on Adversarial Machine Learning.](http://arxiv.org/abs/1810.07339)

Guofu Li, Pengjia Zhu, Jin Li, Zhemin Yang, Ning Cao, Zhiyi Chen


## 2018-10-15

[Concise Explanations of Neural Networks using Adversarial Training.](http://arxiv.org/abs/1810.06583)

Prasad Chalasani, Jiefeng Chen, Amrita Roy Chowdhury, Somesh Jha, Xi Wu


## 2018-10-11

[Characterizing Adversarial Examples Based on Spatial Consistency Information for Semantic Segmentation.](http://arxiv.org/abs/1810.05162)

Chaowei Xiao, Ruizhi Deng, Bo Li, Fisher Yu, Mingyan Liu, Dawn Song


[MeshAdv: Adversarial Meshes for Visual Recognition.](http://arxiv.org/abs/1810.05206)

Chaowei Xiao, Dawei Yang, Bo Li, Jia Deng, Mingyan Liu


## 2018-10-09

[Is PGD-Adversarial Training Necessary? Alternative Training via a Soft-Quantization Network with Noisy-Natural Samples Only.](http://arxiv.org/abs/1810.05665)

Tianhang Zheng, Changyou Chen, Kui Ren


[Analyzing the Noise Robustness of Deep Neural Networks.](http://arxiv.org/abs/1810.03913)

Mengchen Liu, Shixia Liu, Hang Su, Kelei Cao, Jun Zhu


[The Adversarial Attack and Detection under the Fisher Information Metric.](http://arxiv.org/abs/1810.03806)

Chenxiao Zhao, P. Thomas Fletcher, Mixue Yu, Yaxin Peng, Guixu Zhang, Chaomin Shen


## 2018-10-08

[Limitations of adversarial robustness: strong No Free Lunch Theorem.](http://arxiv.org/abs/1810.04065)

Elvis Dohmatob


[Efficient Two-Step Adversarial Defense for Deep Neural Networks.](http://arxiv.org/abs/1810.03739)

Ting-Jui Chang, Yukun He, Peng Li


[Combinatorial Attacks on Binarized Neural Networks.](http://arxiv.org/abs/1810.03538)

Elias B. Khalil, Amrita Gupta, Bistra Dilkina


[Average Margin Regularization for Classifiers.](http://arxiv.org/abs/1810.03773)

Matt Olfat, Anil Aswani


## 2018-10-04

[Feature Prioritization and Regularization Improve Standard Accuracy and Adversarial Robustness.](http://arxiv.org/abs/1810.02424)

Chihuang Liu, Joseph JaJa


[Improved Generalization Bounds for Robust Learning.](http://arxiv.org/abs/1810.02180)

Idan Attias, Aryeh Kontorovich, Yishay Mansour


## 2018-10-02

[Can Adversarially Robust Learning Leverage Computational Hardness?](http://arxiv.org/abs/1810.01407)

Saeed Mahloujifar, Mohammad Mahmoody


[Adversarial Examples - A Complete Characterisation of the Phenomenon.](http://arxiv.org/abs/1810.01185)

Alexandru Constantin Serban, Erik Poll, Joost Visser


[Link Prediction Adversarial Attack.](http://arxiv.org/abs/1810.01110)

Jinyin Chen, Ziqiang Shi, Yangyang Wu, Xuanheng Xu, Haibin Zheng


## 2018-10-01

[Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network.](http://arxiv.org/abs/1810.01279)

Xuanqing Liu, Yao Li, Chongruo Wu, Cho-Jui Hsieh


[Improving the Generalization of Adversarial Training with Domain Adaptation.](http://arxiv.org/abs/1810.00740)

Chuanbiao Song, Kun He, Liwei Wang, John E. Hopcroft


[Large batch size training of neural networks with adversarial training and second-order information.](http://arxiv.org/abs/1810.01021)

Zhewei Yao, Amir Gholami, Daiyaan Arfeen, Richard Liaw, Joseph Gonzalez, Kurt Keutzer, Michael Mahoney


[Improved robustness to adversarial examples using Lipschitz regularization of the loss.](http://arxiv.org/abs/1810.00953)

Chris Finlay, Adam Oberman, Bilal Abbasi


## 2018-09-30

[Procedural Noise Adversarial Examples for Black-Box Attacks on Deep Convolutional Networks.](http://arxiv.org/abs/1810.00470)

Kenneth T. Co, Luis Muñoz-González, Maupeou Sixte de, Emil C. Lupu


## 2018-09-29

[CAAD 2018: Generating Transferable Adversarial Examples.](http://arxiv.org/abs/1810.01268)

Yash Sharma, Tien-Dung Le, Moustafa Alzantot


[Interpreting Adversarial Robustness: A View from Decision Surface in Input Space.](http://arxiv.org/abs/1810.00144)

Fuxun Yu, Chenchen Liu, Yanzhi Wang, Liang Zhao, Xiang Chen


[To compress or not to compress: Understanding the Interactions between Adversarial Attacks and Neural Network Compression.](http://arxiv.org/abs/1810.00208)

Yiren Zhao, Ilia Shumailov, Robert Mullins, Ross Anderson


## 2018-09-28

[Characterizing Audio Adversarial Examples Using Temporal Dependency.](http://arxiv.org/abs/1809.10875)

Zhuolin Yang, Bo Li, Pin-Yu Chen, Dawn Song


[Adversarial Attacks and Defences: A Survey.](http://arxiv.org/abs/1810.00069)

Anirban Chakraborty, Manaar Alam, Vishal Dey, Anupam Chattopadhyay, Debdeep Mukhopadhyay


[Explainable Black-Box Attacks Against Model-based Authentication.](http://arxiv.org/abs/1810.00024)

Washington Garcia, Joseph I. Choi, Suman K. Adari, Somesh Jha, Kevin R. B. Butler


## 2018-09-26

[Adversarial Attacks on Cognitive Self-Organizing Networks: The Challenge and the Way Forward.](http://arxiv.org/abs/1810.07242)

Muhammad Usama, Junaid Qadir, Ala Al-Fuqaha


## 2018-09-24

[Neural Networks with Structural Resistance to Adversarial Attacks.](http://arxiv.org/abs/1809.09262)

Alfaro Luca de


[Fast Geometrically-Perturbed Adversarial Faces.](http://arxiv.org/abs/1809.08999)

Ali Dabouei, Sobhan Soleymani, Jeremy Dawson, Nasser M. Nasrabadi


[On The Utility of Conditional Generation Based Mutual Information for Characterizing Adversarial Subspaces.](http://arxiv.org/abs/1809.08986)

Chia-Yi Hsu, Pei-Hsuan Lu, Pin-Yu Chen, Chia-Mu Yu


[Low Frequency Adversarial Perturbation.](http://arxiv.org/abs/1809.08758)

Chuan Guo, Jared S. Frank, Kilian Q. Weinberger


## 2018-09-23

[Is Ordered Weighted $\ell_1$ Regularized Regression Robust to Adversarial Perturbation? A Case Study on OSCAR.](http://arxiv.org/abs/1809.08706)

Pin-Yu Chen, Bhanukiran Vinzamuri, Sijia Liu


## 2018-09-22

[Adversarial Defense via Data Dependent Activation Function and Total Variation Minimization.](http://arxiv.org/abs/1809.08516)

Bao Wang, Alex T. Lin, Wei Zhu, Penghang Yin, Andrea L. Bertozzi, Stanley J. Osher


## 2018-09-21

[Unrestricted Adversarial Examples.](http://arxiv.org/abs/1809.08352)

Tom B. Brown, Nicholas Carlini, Chiyuan Zhang, Catherine Olsson, Paul Christiano, Ian Goodfellow


[Adversarial Binaries for Authorship Identification.](http://arxiv.org/abs/1809.08316)

Xiaozhu Meng, Barton P. Miller, Somesh Jha


## 2018-09-20

[Playing the Game of Universal Adversarial Perturbations.](http://arxiv.org/abs/1809.07802)

Julien Perolat, Mateusz Malinowski, Bilal Piot, Olivier Pietquin


## 2018-09-19

[Efficient Formal Safety Analysis of Neural Networks.](http://arxiv.org/abs/1809.08098)

Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, Suman Jana


[Adversarial Training Towards Robust Multimedia Recommender System.](http://arxiv.org/abs/1809.07062)

Jinhui Tang, Xiaoyu Du, Xiangnan He, Fajie Yuan, Qi Tian, Tat-Seng Chua


[Generating 3D Adversarial Point Clouds.](http://arxiv.org/abs/1809.07016)

Chong Xiang, Charles R. Qi, Bo Li


## 2018-09-17

[HashTran-DNN: A Framework for Enhancing Robustness of Deep Neural Networks against Adversarial Malware Samples.](http://arxiv.org/abs/1809.06498)

Deqiang Li, Ramesh Baral, Tao Li, Han Wang, Qianmu Li, Shouhuai Xu


[Robustness Guarantees for Bayesian Inference with Gaussian Processes.](http://arxiv.org/abs/1809.06452)

Luca Cardelli, Marta Kwiatkowska, Luca Laurenti, Andrea Patane


## 2018-09-16

[Exploring the Vulnerability of Single Shot Module in Object Detectors via Imperceptible Background Patches.](http://arxiv.org/abs/1809.05966)

Yuezun Li, Xiao Bian, Ming-ching Chang, Siwei Lyu


[Robust Adversarial Perturbation on Deep Proposal-based Models.](http://arxiv.org/abs/1809.05962)

Yuezun Li, Daniel Tian, Ming-Ching Chang, Xiao Bian, Siwei Lyu


## 2018-09-13

[Defensive Dropout for Hardening Deep Neural Networks under Adversarial Attacks.](http://arxiv.org/abs/1809.05165)

Siyue Wang, Xiao Wang, Pu Zhao, Wujie Wen, David Kaeli, Peter Chin, Xue Lin


[Query-Efficient Black-Box Attack by Active Learning.](http://arxiv.org/abs/1809.04913)

Pengcheng Li, Jinfeng Yi, Lijun Zhang


[Adversarial Examples: Opportunities and Challenges.](http://arxiv.org/abs/1809.04790)

Jiliang Zhang, Chen Li


## 2018-09-11

[On the Structural Sensitivity of Deep Convolutional Networks to the Directions of Fourier Basis Functions.](http://arxiv.org/abs/1809.04098)

Yusuke Tsuzuku, Issei Sato


[Isolated and Ensemble Audio Preprocessing Methods for Detecting Adversarial Examples against Automatic Speech Recognition.](http://arxiv.org/abs/1809.04397)

Krishan Rajaratnam, Kunal Shah, Jugal Kalita


[Humans can decipher adversarial images.](http://arxiv.org/abs/1809.04120)

Zhenglong Zhou, Chaz Firestone


[Does it care what you asked? Understanding Importance of Verbs in Deep Learning QA System. (22%)](http://arxiv.org/abs/1809.03740)

Barbara Rychalska, Dominika Basaj, Przemyslaw Biecek, Anna Wroblewska


## 2018-09-09

[The Curse of Concentration in Robust Learning: Evasion and Poisoning Attacks from Concentration of Measure.](http://arxiv.org/abs/1809.03063)

Saeed Mahloujifar, Dimitrios I. Diochnos, Mohammad Mahmoody


[Training for Faster Adversarial Robustness Verification via Inducing ReLU Stability.](http://arxiv.org/abs/1809.03008)

Kai Y. Xiao, Vincent Tjeng, Nur Muhammad Shafiullah, Aleksander Madry


[Certified Adversarial Robustness with Additive Noise.](http://arxiv.org/abs/1809.03113)

Bai Li, Changyou Chen, Wenlin Wang, Lawrence Carin


## 2018-09-08

[Towards Query Efficient Black-box Attacks: An Input-free Perspective.](http://arxiv.org/abs/1809.02918)

Yali Du, Meng Fang, Jinfeng Yi, Jun Cheng, Dacheng Tao


[Fast Gradient Attack on Network Embedding.](http://arxiv.org/abs/1809.02797)

Jinyin Chen, Yangyang Wu, Xuanheng Xu, Yixian Chen, Haibin Zheng, Qi Xuan


[Structure-Preserving Transformation: Generating Diverse and Transferable Adversarial Examples.](http://arxiv.org/abs/1809.02786)

Dan Peng, Zizhan Zheng, Xiaofeng Zhang


[Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks.](http://arxiv.org/abs/1809.02861)

Ambra Demontis, Marco Melis, Maura Pintor, Matthew Jagielski, Battista Biggio, Alina Oprea, Cristina Nita-Rotaru, Fabio Roli


## 2018-09-07

[A Deeper Look at 3D Shape Classifiers.](http://arxiv.org/abs/1809.02560)

Jong-Chyi Su, Matheus Gadelha, Rui Wang, Subhransu Maji


[Metamorphic Relation Based Adversarial Attacks on Differentiable Neural Computer.](http://arxiv.org/abs/1809.02444)

Alvin Chan, Lei Ma, Felix Juefei-Xu, Xiaofei Xie, Yang Liu, Yew Soon Ong


[Trick Me If You Can: Human-in-the-loop Generation of Adversarial Examples for Question Answering.](http://arxiv.org/abs/1809.02701)

Eric Wallace, Pedro Rodriguez, Shi Feng, Ikuya Yamada, Jordan Boyd-Graber


[Query Attack via Opposite-Direction Feature:Towards Robust Image Retrieval.](http://arxiv.org/abs/1809.02681)

Zhedong Zheng, Liang Zheng, Yi Yang, Fei Wu


## 2018-09-06

[Adversarial Over-Sensitivity and Over-Stability Strategies for Dialogue Models.](http://arxiv.org/abs/1809.02079)

Tong Niu, Mohit Bansal


[Are adversarial examples inevitable?](http://arxiv.org/abs/1809.02104)

Ali Shafahi, W. Ronny Huang, Christoph Studer, Soheil Feizi, Tom Goldstein


[IDSGAN: Generative Adversarial Networks for Attack Generation against Intrusion Detection.](http://arxiv.org/abs/1809.02077)

Zilong Lin, Yong Shi, Zhi Xue


[Adversarial Reprogramming of Text Classification Neural Networks.](http://arxiv.org/abs/1809.01829)

Paarth Neekhara, Shehzeen Hussain, Shlomo Dubnov, Farinaz Koushanfar


## 2018-09-05

[Bridging machine learning and cryptography in defence against adversarial attacks.](http://arxiv.org/abs/1809.01715)

Olga Taran, Shideh Rezaeifar, Slava Voloshynovskiy


## 2018-09-04

[Adversarial Attacks on Node Embeddings.](http://arxiv.org/abs/1809.01093)

Aleksandar Bojchevski, Stephan Günnemann


## 2018-09-03

[HASP: A High-Performance Adaptive Mobile Security Enhancement Against Malicious Speech Recognition.](http://arxiv.org/abs/1809.01697)

Zirui Xu, Fuxun Yu, Chenchen Liu, Xiang Chen


[Adversarial Attack Type I: Cheat Classifiers by Significant Changes.](http://arxiv.org/abs/1809.00594)

Sanli Tang, Xiaolin Huang, Mingjian Chen, Chengjin Sun, Jie Yang


## 2018-08-31

[MULDEF: Multi-model-based Defense Against Adversarial Examples for Neural Networks.](http://arxiv.org/abs/1809.00065)

Siwakorn Srisakaokul, Yuhao Zhang, Zexuan Zhong, Wei Yang, Tao Xie, Bo Li


## 2018-08-28

[DLFuzz: Differential Fuzzing Testing of Deep Learning Systems.](http://arxiv.org/abs/1808.09413)

Jianmin Guo, Yu Jiang, Yue Zhao, Quan Chen, Jiaguang Sun


[All You Need is "Love": Evading Hate-speech Detection.](http://arxiv.org/abs/1808.09115)

Tommi Gröndahl, Luca Pajola, Mika Juuti, Mauro Conti, N. Asokan


[Lipschitz regularized Deep Neural Networks generalize and are adversarially robust.](http://arxiv.org/abs/1808.09540)

Chris Finlay, Jeff Calder, Bilal Abbasi, Adam Oberman


## 2018-08-27

[Targeted Nonlinear Adversarial Perturbations in Images and Videos.](http://arxiv.org/abs/1809.00958)

Roberto Rey-de-Castro, Herschel Rabitz


[Generalisation in humans and deep neural networks.](http://arxiv.org/abs/1808.08750)

Robert Geirhos, Carlos R. Medina Temme, Jonas Rauber, Heiko H. Schütt, Matthias Bethge, Felix A. Wichmann


## 2018-08-26

[Adversarially Regularising Neural NLI Models to Integrate Logical Background Knowledge.](http://arxiv.org/abs/1808.08609)

Pasquale Minervini, Sebastian Riedel


## 2018-08-25

[Analysis of adversarial attacks against CNN-based image forgery detectors.](http://arxiv.org/abs/1808.08426)

Diego Gragnaniello, Francesco Marra, Giovanni Poggi, Luisa Verdoliva


[Guiding Deep Learning System Testing using Surprise Adequacy.](http://arxiv.org/abs/1808.08444)

Jinhan Kim, Robert Feldt, Shin Yoo


## 2018-08-24

[Is Machine Learning in Power Systems Vulnerable?](http://arxiv.org/abs/1808.08197)

Yize Chen, Yushi Tan, Deepjyoti Deka


## 2018-08-23

[Maximal Jacobian-based Saliency Map Attack.](http://arxiv.org/abs/1808.07945)

Rey Wiyatno, Anqi Xu


[Adversarial Attacks on Deep-Learning Based Radio Signal Classification.](http://arxiv.org/abs/1808.07713)

Meysam Sadeghi, Erik G. Larsson


## 2018-08-20

[Controlling Over-generalization and its Effect on Adversarial Examples Generation and Detection.](http://arxiv.org/abs/1808.08282)

Mahdieh Abbasi, Arezoo Rajabi, Azadeh Sadat Mozafari, Rakesh B. Bobba, Christian Gagne


[Stochastic Combinatorial Ensembles for Defending Against Adversarial Examples.](http://arxiv.org/abs/1808.06645)

George A. Adam, Petr Smirnov, David Duvenaud, Benjamin Haibe-Kains, Anna Goldenberg


## 2018-08-17

[Reinforcement Learning for Autonomous Defence in Software-Defined Networking.](http://arxiv.org/abs/1808.05770)

Yi Han, Benjamin I. P. Rubinstein, Tamas Abraham, Tansu Alpcan, Vel Olivier De, Sarah Erfani, David Hubczenko, Christopher Leckie, Paul Montague


## 2018-08-16

[Mitigation of Adversarial Attacks through Embedded Feature Selection.](http://arxiv.org/abs/1808.05705)

Ziyi Bao, Luis Muñoz-González, Emil C. Lupu


[Adversarial Attacks Against Automatic Speech Recognition Systems via Psychoacoustic Hiding.](http://arxiv.org/abs/1808.05665)

Lea Schönherr, Katharina Kohls, Steffen Zeiler, Thorsten Holz, Dorothea Kolossa


[Distributionally Adversarial Attack.](http://arxiv.org/abs/1808.05537)

Tianhang Zheng, Changyou Chen, Kui Ren


## 2018-08-10

[Using Randomness to Improve Robustness of Machine-Learning Models Against Evasion Attacks.](http://arxiv.org/abs/1808.03601)

Fan Yang, Zhiyuan Chen


[Android HIV: A Study of Repackaging Malware for Evading Machine-Learning Detection.](http://arxiv.org/abs/1808.04218)

Xiao Chen, Chaoran Li, Derui Wang, Sheng Wen, Jun Zhang, Surya Nepal, Yang Xiang, Kui Ren


## 2018-08-08

[Beyond Pixel Norm-Balls: Parametric Adversaries using an Analytically Differentiable Renderer.](http://arxiv.org/abs/1808.02651)

Hsueh-Ti Derek Liu, Michael Tao, Chun-Liang Li, Derek Nowrouzezahrai, Alec Jacobson


## 2018-08-07

[Data augmentation using synthetic data for time series classification with deep residual networks.](http://arxiv.org/abs/1808.02455)

Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, Pierre-Alain Muller


## 2018-08-06

[Adversarial Vision Challenge.](http://arxiv.org/abs/1808.01976)

Wieland Brendel, Jonas Rauber, Alexey Kurakin, Nicolas Papernot, Behar Veliqi, Marcel Salathé, Sharada P. Mohanty, Matthias Bethge


[Defense Against Adversarial Attacks with Saak Transform.](http://arxiv.org/abs/1808.01785)

Sibo Song, Yueru Chen, Ngai-Man Cheung, C. -C. Jay Kuo


[Gray-box Adversarial Training.](http://arxiv.org/abs/1808.01753)

Vivek B. S., Konda Reddy Mopuri, R. Venkatesh Babu


## 2018-08-05

[Is Robustness the Cost of Accuracy? -- A Comprehensive Study on the Robustness of 18 Deep Image Classification Models.](http://arxiv.org/abs/1808.01688)

Dong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu Chen, Yupeng Gao


[Structured Adversarial Attack: Towards General Implementation and Better Interpretability.](http://arxiv.org/abs/1808.01664)

Kaidi Xu, Sijia Liu, Pu Zhao, Pin-Yu Chen, Huan Zhang, Quanfu Fan, Deniz Erdogmus, Yanzhi Wang, Xue Lin


## 2018-08-04

[Traits & Transferability of Adversarial Examples against Instance Segmentation & Object Detection.](http://arxiv.org/abs/1808.01452)

Raghav Gurbaxani, Shivank Mishra


[ATMPA: Attacking Machine Learning-based Malware Visualization Detection Methods via Adversarial Examples.](http://arxiv.org/abs/1808.01546)

Xinbo Liu, Jiliang Zhang, Yaping Lin, He Li


## 2018-08-03

[Ask, Acquire, and Attack: Data-free UAP Generation using Class Impressions.](http://arxiv.org/abs/1808.01153)

Konda Reddy Mopuri, Phani Krishna Uppala, R. Venkatesh Babu


[DeepCloak: Adversarial Crafting As a Defensive Measure to Cloak Processes.](http://arxiv.org/abs/1808.01352)

Mehmet Sinan Inci, Thomas Eisenbarth, Berk Sunar


## 2018-07-31

[EagleEye: Attack-Agnostic Defense against Adversarial Inputs (Technical Report).](http://arxiv.org/abs/1808.00123)

Yujie Ji, Xinyang Zhang, Ting Wang


## 2018-07-27

[Rob-GAN: Generator, Discriminator, and Adversarial Attacker.](http://arxiv.org/abs/1807.10454)

Xuanqing Liu, Cho-Jui Hsieh


## 2018-07-26

[A general metric for identifying adversarial images.](http://arxiv.org/abs/1807.10335)

Siddharth Krishna Kumar


[Evaluating and Understanding the Robustness of Adversarial Logit Pairing.](http://arxiv.org/abs/1807.10272)

Logan Engstrom, Andrew Ilyas, Anish Athalye


## 2018-07-25

[HiDDeN: Hiding Data With Deep Networks.](http://arxiv.org/abs/1807.09937)

Jiren Zhu, Russell Kaplan, Justin Johnson, Li Fei-Fei


[Limitations of the Lipschitz constant as a defense against adversarial examples.](http://arxiv.org/abs/1807.09705)

Todd Huster, Cho-Yu Jason Chiang, Ritu Chadha


[Unbounded Output Networks for Classification.](http://arxiv.org/abs/1807.09443)

Stefan Elfwing, Eiji Uchibe, Kenji Doya


## 2018-07-24

[Contrastive Video Representation Learning via Adversarial Perturbations.](http://arxiv.org/abs/1807.09380)

Jue Wang, Anoop Cherian


## 2018-07-21

[Simultaneous Adversarial Training - Learn from Others Mistakes.](http://arxiv.org/abs/1807.08108)

Zukang Liao


## 2018-07-20

[Prior Convictions: Black-Box Adversarial Attacks with Bandits and Priors.](http://arxiv.org/abs/1807.07978)

Andrew Ilyas, Logan Engstrom, Aleksander Madry


[Physical Adversarial Examples for Object Detectors.](http://arxiv.org/abs/1807.07769)

Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Florian Tramer, Atul Prakash, Tadayoshi Kohno, Dawn Song


## 2018-07-18

[Harmonic Adversarial Attack Method.](http://arxiv.org/abs/1807.10590)

Wen Heng, Shuchang Zhou, Tingting Jiang


## 2018-07-17

[Gradient Band-based Adversarial Training for Generalized Attack Immunity of A3C Path Finding.](http://arxiv.org/abs/1807.06752)

Tong Chen, Wenjia Niu, Yingxiao Xiang, Xiaoxuan Bai, Jiqiang Liu, Zhen Han, Gang Li


[Motivating the Rules of the Game for Adversarial Example Research.](http://arxiv.org/abs/1807.06732)

Justin Gilmer, Ryan P. Adams, Ian Goodfellow, David Andersen, George E. Dahl


[Defend Deep Neural Networks Against Adversarial Examples via Fixed and Dynamic Quantized Activation Functions.](http://arxiv.org/abs/1807.06714)

Adnan Siraj Rakin, Jinfeng Yi, Boqing Gong, Deliang Fan


## 2018-07-16

[Online Robust Policy Learning in the Presence of Unknown Adversaries.](http://arxiv.org/abs/1807.06064)

Aaron J. Havens, Zhanhong Jiang, Soumik Sarkar


[Manifold Adversarial Learning.](http://arxiv.org/abs/1807.05832)

Shufei Zhang, Kaizhu Huang, Jianke Zhu, Yang Liu


## 2018-07-12

[Query-Efficient Hard-label Black-box Attack:An Optimization-based Approach.](http://arxiv.org/abs/1807.04457)

Minhao Cheng, Thong Le, Pin-Yu Chen, Jinfeng Yi, Huan Zhang, Cho-Jui Hsieh


## 2018-07-11

[With Friends Like These, Who Needs Adversaries?](http://arxiv.org/abs/1807.04200)

Saumya Jetley, Nicholas A. Lord, Philip H. S. Torr


## 2018-07-10

[A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks.](http://arxiv.org/abs/1807.03888)

Kimin Lee, Kibok Lee, Honglak Lee, Jinwoo Shin


[A Game-Based Approximate Verification of Deep Neural Networks with Provable Guarantees.](http://arxiv.org/abs/1807.03571)

Min Wu, Matthew Wicker, Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska


[Attack and defence in cellular decision-making: lessons from machine learning.](http://arxiv.org/abs/1807.04270)

Thomas J. Rademaker, Emmanuel Bengio, Paul François


## 2018-07-09

[Adaptive Adversarial Attack on Scene Text Recognition.](http://arxiv.org/abs/1807.03326)

Xiaoyong Yuan, Pan He, Xiaolin Andy Li, Dapeng Oliver Wu


## 2018-07-08

[Vulnerability Analysis of Chest X-Ray Image Classification Against Adversarial Attacks.](http://arxiv.org/abs/1807.02905)

Saeid Asgari Taghanaki, Arkadeep Das, Ghassan Hamarneh


## 2018-07-05

[Implicit Generative Modeling of Random Noise during Training for Adversarial Robustness.](http://arxiv.org/abs/1807.02188)

Priyadarshini Panda, Kaushik Roy


## 2018-07-04

[Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations.](http://arxiv.org/abs/1807.01697)

Dan Hendrycks, Thomas G. Dietterich


## 2018-07-03

[Local Gradients Smoothing: Defense against localized adversarial attacks.](http://arxiv.org/abs/1807.01216)

Muzammal Naseer, Salman H. Khan, Fatih Porikli


[Adversarial Robustness Toolbox v1.0.0.](http://arxiv.org/abs/1807.01069)

Maria-Irina Nicolae, Mathieu Sinn, Minh Ngoc Tran, Beat Buesser, Ambrish Rawat, Martin Wistuba, Valentina Zantedeschi, Nathalie Baracaldo, Bryant Chen, Heiko Ludwig, Ian M. Molloy, Ben Edwards


## 2018-07-02

[Adversarial Perturbations Against Real-Time Video Classification Systems.](http://arxiv.org/abs/1807.00458)

Shasha Li, Ajaya Neupane, Sujoy Paul, Chengyu Song, Srikanth V. Krishnamurthy, Amit K. Roy Chowdhury, Ananthram Swami


## 2018-07-01

[Towards Adversarial Training with Moderate Performance Improvement for Neural Network Classification.](http://arxiv.org/abs/1807.00340)

Xinhan Di, Pengqian Yu, Meng Tian


## 2018-06-29

[Adversarial Examples in Deep Learning: Characterization and Divergence.](http://arxiv.org/abs/1807.00051)

Wenqi Wei, Ling Liu, Margaret Loper, Stacey Truex, Lei Yu, Mehmet Emre Gursoy, Yanzhao Wu


## 2018-06-28

[Adversarial Reprogramming of Neural Networks.](http://arxiv.org/abs/1806.11146)

Gamaleldin F. Elsayed, Ian Goodfellow, Jascha Sohl-Dickstein


## 2018-06-27

[Gradient Similarity: An Explainable Approach to Detect Adversarial Attacks against Deep Learning.](http://arxiv.org/abs/1806.10707)

Jasjeet Dhaliwal, Saurabh Shintre


[Customizing an Adversarial Example Generator with Class-Conditional GANs.](http://arxiv.org/abs/1806.10496)

Shih-hong Tsai


## 2018-06-25

[Exploring Adversarial Examples: Patterns of One-Pixel Attacks.](http://arxiv.org/abs/1806.09410)

David Kügler, Alexander Distergoft, Arjan Kuijper, Anirban Mukhopadhyay


## 2018-06-23

[Defending Malware Classification Networks Against Adversarial Perturbations with Non-Negative Weight Restrictions.](http://arxiv.org/abs/1806.09035)

Alex Kouzemtchenko


[On Adversarial Examples for Character-Level Neural Machine Translation.](http://arxiv.org/abs/1806.09030)

Javid Ebrahimi, Daniel Lowd, Dejing Dou


[Evaluation of Momentum Diverse Input Iterative Fast Gradient Sign Method (M-DI2-FGSM) Based Attack Method on MCS 2018 Adversarial Attacks on Black Box Face Recognition System.](http://arxiv.org/abs/1806.08970)

Md Ashraful Alam Milton


## 2018-06-21

[Detection based Defense against Adversarial Examples from the Steganalysis Point of View.](http://arxiv.org/abs/1806.09186)

Jiayang Liu, Weiming Zhang, Yiwei Zhang, Dongdong Hou, Yujia Liu, Hongyue Zha, Nenghai Yu


## 2018-06-20

[Gradient Adversarial Training of Neural Networks.](http://arxiv.org/abs/1806.08028)

Ayan Sinha, Zhao Chen, Vijay Badrinarayanan, Andrew Rabinovich


[Combinatorial Testing for Deep Learning Systems.](http://arxiv.org/abs/1806.07723)

Lei Ma, Fuyuan Zhang, Minhui Xue, Bo Li, Yang Liu, Jianjun Zhao, Yadong Wang


## 2018-06-19

[On the Learning of Deep Local Features for Robust Face Spoofing Detection.](http://arxiv.org/abs/1806.07492)

Souza Gustavo Botelho de, João Paulo Papa, Aparecido Nilceu Marana


[Built-in Vulnerabilities to Imperceptible Adversarial Perturbations.](http://arxiv.org/abs/1806.07409)

Thomas Tanay, Jerone T. A. Andrews, Lewis D. Griffin


## 2018-06-15

[Non-Negative Networks Against Adversarial Attacks.](http://arxiv.org/abs/1806.06108)

William Fleshman, Edward Raff, Jared Sylvester, Steven Forsyth, Mark McLean


## 2018-06-14

[Copycat CNN: Stealing Knowledge by Persuading Confession with Random Non-Labeled Data.](http://arxiv.org/abs/1806.05476)

Jacson Rodrigues Correia-Silva, Rodrigo F. Berriel, Claudine Badue, Souza Alberto F. de, Thiago Oliveira-Santos


## 2018-06-13

[Hierarchical interpretations for neural network predictions.](http://arxiv.org/abs/1806.05337)

Chandan Singh, W. James Murdoch, Bin Yu


[Manifold Mixup: Better Representations by Interpolating Hidden States.](http://arxiv.org/abs/1806.05236)

Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, Aaron Courville, David Lopez-Paz, Yoshua Bengio


## 2018-06-12

[Adversarial Attacks on Variational Autoencoders.](http://arxiv.org/abs/1806.04646)

George Gondim-Ribeiro, Pedro Tabacof, Eduardo Valle


[Ranking Robustness Under Adversarial Document Manipulations.](http://arxiv.org/abs/1806.04425)

Gregory Goren, Oren Kurland, Moshe Tennenholtz, Fiana Raiber


## 2018-06-11

[Defense Against the Dark Arts: An overview of adversarial example security research and future research directions.](http://arxiv.org/abs/1806.04169)

Ian Goodfellow


## 2018-06-08

[Monge blunts Bayes: Hardness Results for Adversarial Training.](http://arxiv.org/abs/1806.02977)

Zac Cranko, Aditya Krishna Menon, Richard Nock, Cheng Soon Ong, Zhan Shi, Christian Walder


## 2018-06-07

[Revisiting Adversarial Risk.](http://arxiv.org/abs/1806.02924)

Arun Sai Suggala, Adarsh Prasad, Vaishnavh Nagarajan, Pradeep Ravikumar


[Training Augmentation with Adversarial Examples for Robust Speech Recognition.](http://arxiv.org/abs/1806.02782)

Sining Sun, Ching-Feng Yeh, Mari Ostendorf, Mei-Yuh Hwang, Lei Xie


## 2018-06-06

[Adversarial Attack on Graph Structured Data.](http://arxiv.org/abs/1806.02371)

Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, Le Song


[Adversarial Regression with Multiple Learners.](http://arxiv.org/abs/1806.02256)

Liang Tong, Sixie Yu, Scott Alfeld, Yevgeniy Vorobeychik


[Killing four birds with one Gaussian process: the relation between different test-time attacks.](http://arxiv.org/abs/1806.02032)

Kathrin Grosse, Michael T. Smith, Michael Backes


## 2018-06-05

[DPatch: An Adversarial Patch Attack on Object Detectors.](http://arxiv.org/abs/1806.02299)

Xin Liu, Huanrui Yang, Ziwei Liu, Linghao Song, Hai Li, Yiran Chen


## 2018-06-04

[Mitigation of Policy Manipulation Attacks on Deep Q-Networks with Parameter-Space Noise.](http://arxiv.org/abs/1806.02190)

Vahid Behzadan, Arslan Munir


[An Explainable Adversarial Robustness Metric for Deep Learning Neural Networks.](http://arxiv.org/abs/1806.01477)

Chirag Agarwal, Bo Dong, Dan Schonfeld, Anthony Hoogs


[PAC-learning in the presence of evasion adversaries.](http://arxiv.org/abs/1806.01471)

Daniel Cullina, Arjun Nitin Bhagoji, Prateek Mittal


## 2018-06-02

[Sufficient Conditions for Idealised Models to Have No Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks.](http://arxiv.org/abs/1806.00667)

Yarin Gal, Lewis Smith


[Detecting Adversarial Examples via Key-based Network.](http://arxiv.org/abs/1806.00580)

Pinlong Zhao, Zhouyu Fu, Ou wu, Qinghua Hu, Jun Wang


## 2018-05-31

[PeerNets: Exploiting Peer Wisdom Against Adversarial Attacks.](http://arxiv.org/abs/1806.00088)

Jan Svoboda, Jonathan Masci, Federico Monti, Michael M. Bronstein, Leonidas Guibas


[Resisting Adversarial Attacks using Gaussian Mixture Variational Autoencoders.](http://arxiv.org/abs/1806.00081)

Partha Ghosh, Arpan Losalka, Michael J Black


[Scaling provable adversarial defenses.](http://arxiv.org/abs/1805.12514)

Eric Wong, Frank R. Schmidt, Jan Hendrik Metzen, J. Zico Kolter


[Sequential Attacks on Agents for Long-Term Adversarial Goals.](http://arxiv.org/abs/1805.12487)

Edgar Tretschk, Seong Joon Oh, Mario Fritz


[Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data.](http://arxiv.org/abs/1805.12316)

Puyudi Yang, Jianbo Chen, Cho-Jui Hsieh, Jane-Ling Wang, Michael I. Jordan


## 2018-05-30

[Adversarial Attacks on Face Detectors using Neural Net based Constrained Optimization.](http://arxiv.org/abs/1805.12302)

Avishek Joey Bose, Parham Aarabi


[ADAGIO: Interactive Experimentation with Adversarial Attack and Defense for Audio.](http://arxiv.org/abs/1805.11852)

Nilaksh Das, Madhuri Shanbhogue, Shang-Tse Chen, Li Chen, Michael E. Kounavis, Duen Horng Chau


[Robustifying Models Against Adversarial Attacks by Langevin Dynamics.](http://arxiv.org/abs/1805.12017)

Vignesh Srinivasan, Arturo Marban, Klaus-Robert Müller, Wojciech Samek, Shinichi Nakajima


[Robustness May Be at Odds with Accuracy.](http://arxiv.org/abs/1805.12152)

Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, Aleksander Madry


## 2018-05-29

[AutoZOOM: Autoencoder-based Zeroth Order Optimization Method for Attacking Black-box Neural Networks.](http://arxiv.org/abs/1805.11770)

Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, Shin-Ming Cheng


[Adversarial Noise Attacks of Deep Learning Architectures -- Stability Analysis via Sparse Modeled Signals.](http://arxiv.org/abs/1805.11596)

Yaniv Romano, Aviad Aberdam, Jeremias Sulam, Michael Elad


[Why Botnets Work: Distributed Brute-Force Attacks Need No Synchronization.](http://arxiv.org/abs/1805.11666)

Salman Salamatian, Wasim Huleihel, Ahmad Beirami, Asaf Cohen, Muriel Médard


## 2018-05-28

[Adversarial Examples in Remote Sensing.](http://arxiv.org/abs/1805.10997)

Wojciech Czaja, Neil Fendley, Michael Pekala, Christopher Ratto, I-Jeng Wang


[GenAttack: Practical Black-box Attacks with Gradient-Free Optimization.](http://arxiv.org/abs/1805.11090)

Moustafa Alzantot, Yash Sharma, Supriyo Chakraborty, Huan Zhang, Cho-Jui Hsieh, Mani Srivastava


## 2018-05-27

[Defending Against Adversarial Attacks by Leveraging an Entire GAN.](http://arxiv.org/abs/1805.10652)

Gokula Krishnan Santhanam, Paulina Grnarova


## 2018-05-25

[Training verified learners with learned verifiers.](http://arxiv.org/abs/1805.10265)

Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, Brendan O'Donoghue, Jonathan Uesato, Pushmeet Kohli


[Adversarial examples from computational constraints.](http://arxiv.org/abs/1805.10204)

Sébastien Bubeck, Eric Price, Ilya Razenshteyn


## 2018-05-24

[Laplacian Networks: Bounding Indicator Function Smoothness for Neural Network Robustness.](http://arxiv.org/abs/1805.10133)

Carlos Eduardo Rosar Kos Lassance, Vincent Gripon, Antonio Ortega


## 2018-05-23

[Anonymizing k-Facial Attributes via Adversarial Perturbations.](http://arxiv.org/abs/1805.09380)

Saheb Chhabra, Richa Singh, Mayank Vatsa, Gaurav Gupta


[Towards Robust Training of Neural Networks by Regularizing Adversarial Gradients.](http://arxiv.org/abs/1805.09370)

Fuxun Yu, Zirui Xu, Yanzhi Wang, Chenchen Liu, Xiang Chen


[Towards the first adversarially robust neural network model on MNIST.](http://arxiv.org/abs/1805.09190)

Lukas Schott, Jonas Rauber, Matthias Bethge, Wieland Brendel


## 2018-05-22

[Adversarially Robust Training through Structured Gradient Regularization.](http://arxiv.org/abs/1805.08736)

Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, Thomas Hofmann


## 2018-05-21

[Adversarial Noise Layer: Regularize Neural Network By Adding Noise.](http://arxiv.org/abs/1805.08000)

Zhonghui You, Jinmian Ye, Kunming Li, Zenglin Xu, Ping Wang


[Constructing Unrestricted Adversarial Examples with Generative Models.](http://arxiv.org/abs/1805.07894)

Yang Song, Rui Shu, Nate Kushman, Stefano Ermon


[Bidirectional Learning for Robust Neural Networks.](http://arxiv.org/abs/1805.08006)

Sidney Pontes-Filho, Marcus Liwicki


[Adversarial Attacks on Neural Networks for Graph Data.](http://arxiv.org/abs/1805.07984)

Daniel Zügner, Amir Akbarnejad, Stephan Günnemann


## 2018-05-20

[Featurized Bidirectional GAN: Adversarial Defense via Adversarially Learned Semantic Inference.](http://arxiv.org/abs/1805.07862)

Ruying Bao, Sihang Liang, Qingcan Wang


[Towards Understanding Limitations of Pixel Discretization Against Adversarial Attacks.](http://arxiv.org/abs/1805.07816)

Jiefeng Chen, Xi Wu, Vaibhav Rastogi, Yingyu Liang, Somesh Jha


[Targeted Adversarial Examples for Black Box Audio Systems.](http://arxiv.org/abs/1805.07820)

Rohan Taori, Amog Kamsetty, Brenton Chu, Nikita Vemuri


## 2018-05-17

[Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models.](http://arxiv.org/abs/1805.06605)

Pouya Samangouei, Maya Kabkab, Rama Chellappa


## 2018-05-16

[Towards Robust Neural Machine Translation.](http://arxiv.org/abs/1805.06130)

Yong Cheng, Zhaopeng Tu, Fandong Meng, Junjie Zhai, Yang Liu


## 2018-05-14

[Detecting Adversarial Samples for Deep Neural Networks through Mutation Testing.](http://arxiv.org/abs/1805.05010)

Jingyi Wang, Jun Sun, Peixin Zhang, Xinyu Wang


## 2018-05-12

[Curriculum Adversarial Training.](http://arxiv.org/abs/1805.04807)

Qi-Zhi Cai, Min Du, Chang Liu, Dawn Song


[AttriGuard: A Practical Defense Against Attribute Inference Attacks via Adversarial Machine Learning.](http://arxiv.org/abs/1805.04810)

Jinyuan Jia, Neil Zhenqiang Gong


## 2018-05-11

[Breaking Transferability of Adversarial Samples with Randomness.](http://arxiv.org/abs/1805.04613)

Yan Zhou, Murat Kantarcioglu, Bowei Xi


## 2018-05-09

[On Visual Hallmarks of Robustness to Adversarial Malware.](http://arxiv.org/abs/1805.03553)

Alex Huang, Abdullah Al-Dujaili, Erik Hemberg, Una-May O'Reilly


[Robust Classification with Convolutional Prototype Learning.](http://arxiv.org/abs/1805.03438)

Hong-Ming Yang, Xu-Yao Zhang, Fei Yin, Cheng-Lin Liu


## 2018-05-08

[Interpretable Adversarial Perturbation in Input Embedding Space for Text.](http://arxiv.org/abs/1805.02917)

Motoki Sato, Jun Suzuki, Hiroyuki Shindo, Yuji Matsumoto


## 2018-05-05

[A Counter-Forensic Method for CNN-Based Camera Model Identification.](http://arxiv.org/abs/1805.02131)

David Güera, Yu Wang, Luca Bondi, Paolo Bestagini, Stefano Tubaro, Edward J. Delp


## 2018-05-03

[Siamese networks for generating adversarial examples.](http://arxiv.org/abs/1805.01431)

Mandar Kulkarni, Aria Abubakar


## 2018-04-30

[Concolic Testing for Deep Neural Networks.](http://arxiv.org/abs/1805.00089)

Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska, Daniel Kroening


[How Robust are Deep Neural Networks?](http://arxiv.org/abs/1804.11313)

Biswa Sengupta, Karl J. Friston


[Adversarially Robust Generalization Requires More Data.](http://arxiv.org/abs/1804.11285)

Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, Aleksander Mądry


## 2018-04-29

[Adversarial Regression for Detecting Attacks in Cyber-Physical Systems.](http://arxiv.org/abs/1804.11022)

Amin Ghafouri, Yevgeniy Vorobeychik, Xenofon Koutsoukos


## 2018-04-28

[Formal Security Analysis of Neural Networks using Symbolic Intervals.](http://arxiv.org/abs/1804.10829)

Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, Suman Jana


## 2018-04-25

[Towards Fast Computation of Certified Robustness for ReLU Networks.](http://arxiv.org/abs/1804.09699)

Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Duane Boning, Inderjit S. Dhillon, Luca Daniel


## 2018-04-23

[Towards Dependable Deep Convolutional Neural Networks (CNNs) with Out-distribution Learning.](http://arxiv.org/abs/1804.08794)

Mahdieh Abbasi, Arezoo Rajabi, Christian Gagné, Rakesh B. Bobba


[Siamese Generative Adversarial Privatizer for Biometric Data.](http://arxiv.org/abs/1804.08757)

Witold Oleszkiewicz, Peter Kairouz, Karol Piczak, Ram Rajagopal, Tomasz Trzcinski


[Black-box Adversarial Attacks with Limited Queries and Information.](http://arxiv.org/abs/1804.08598)

Andrew Ilyas, Logan Engstrom, Anish Athalye, Jessy Lin


[VectorDefense: Vectorization as a Defense to Adversarial Examples.](http://arxiv.org/abs/1804.08529)

Vishaal Munusamy Kabilan, Brandon Morris, Anh Nguyen


[Query-Efficient Black-Box Attack Against Sequence-Based Malware Classifiers.](http://arxiv.org/abs/1804.08778)

Ishai Rosenberg, Asaf Shabtai, Yuval Elovici, Lior Rokach


## 2018-04-21

[Generating Natural Language Adversarial Examples.](http://arxiv.org/abs/1804.07998)

Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, Kai-Wei Chang


## 2018-04-20

[Gradient Masking Causes CLEVER to Overestimate Adversarial Perturbation Size.](http://arxiv.org/abs/1804.07870)

Ian Goodfellow


[Learning More Robust Features with Adversarial Training.](http://arxiv.org/abs/1804.07757)

Shuangtao Li, Yuanke Chen, Yanlin Peng, Lin Bai


[ADef: an Iterative Algorithm to Construct Adversarial Deformations.](http://arxiv.org/abs/1804.07729)

Rima Alaifari, Giovanni S. Alberti, Tandri Gauksson


## 2018-04-19

[Attacking Convolutional Neural Network using Differential Evolution.](http://arxiv.org/abs/1804.07062)

Jiawei Su, Danilo Vasconcellos Vargas, Kouichi Sakurai


[Semantic Adversarial Deep Learning.](http://arxiv.org/abs/1804.07045)

Tommaso Dreossi, Somesh Jha, Sanjit A. Seshia


## 2018-04-18

[Simulation-based Adversarial Test Generation for Autonomous Vehicles with Machine Learning Components.](http://arxiv.org/abs/1804.06760)

Cumhur Erkan Tuncali, Georgios Fainekos, Hisahiro Ito, James Kapinski


[Neural Automated Essay Scoring and Coherence Modeling for Adversarially Crafted Input.](http://arxiv.org/abs/1804.06898)

Youmna Farag, Helen Yannakoudakis, Ted Briscoe


## 2018-04-17

[Robust Machine Comprehension Models via Adversarial Training.](http://arxiv.org/abs/1804.06473)

Yicheng Wang, Mohit Bansal


[Adversarial Example Generation with Syntactically Controlled Paraphrase Networks.](http://arxiv.org/abs/1804.06059)

Mohit Iyyer, John Wieting, Kevin Gimpel, Luke Zettlemoyer


## 2018-04-16

[Global Robustness Evaluation of Deep Neural Networks with Provable Guarantees for the $L_0$ Norm.](http://arxiv.org/abs/1804.05805)

Wenjie Ruan, Min Wu, Youcheng Sun, Xiaowei Huang, Daniel Kroening, Marta Kwiatkowska


[ShapeShifter: Robust Physical Adversarial Attack on Faster R-CNN Object Detector.](http://arxiv.org/abs/1804.05810)

Shang-Tse Chen, Cory Cornelius, Jason Martin, Duen Horng Chau


## 2018-04-14

[On the Limitation of MagNet Defense against $L_1$-based Adversarial Examples.](http://arxiv.org/abs/1805.00310)

Pei-Hsuan Lu, Pin-Yu Chen, Kang-Cheng Chen, Chia-Mu Yu


[Adversarial Attacks Against Medical Deep Learning Systems.](http://arxiv.org/abs/1804.05296)

Samuel G. Finlayson, Hyung Won Chung, Isaac S. Kohane, Andrew L. Beam


## 2018-04-11

[Detecting Malicious PowerShell Commands using Deep Neural Networks.](http://arxiv.org/abs/1804.04177)

Danny Hendler, Shay Kels, Amir Rubin


## 2018-04-10

[On the Robustness of the CVPR 2018 White-Box Adversarial Example Defenses.](http://arxiv.org/abs/1804.03286)

Anish Athalye, Nicholas Carlini


## 2018-04-09

[Adversarial Training Versus Weight Decay.](http://arxiv.org/abs/1804.03308)

Angus Galloway, Thomas Tanay, Graham W. Taylor


[An ADMM-Based Universal Framework for Adversarial Attacks on Deep Neural Networks.](http://arxiv.org/abs/1804.03193)

Pu Zhao, Sijia Liu, Yanzhi Wang, Xue Lin


## 2018-04-08

[Adaptive Spatial Steganography Based on Probability-Controlled Adversarial Examples.](http://arxiv.org/abs/1804.02691)

Sai Ma, Qingxiao Guan, Xianfeng Zhao, Yaqi Liu


## 2018-04-06

[Fortified Networks: Improving the Robustness of Deep Networks by Modeling the Manifold of Hidden Representations.](http://arxiv.org/abs/1804.02485)

Alex Lamb, Jonathan Binas, Anirudh Goyal, Dmitriy Serdyuk, Sandeep Subramanian, Ioannis Mitliagkas, Yoshua Bengio


## 2018-04-04

[Unifying Bilateral Filtering and Adversarial Training for Robust Neural Networks.](http://arxiv.org/abs/1804.01635)

Neale Ratzlaff, Li Fuxin


## 2018-03-30

[Adversarial Attacks and Defences Competition.](http://arxiv.org/abs/1804.00097)

Alexey Kurakin, Ian Goodfellow, Samy Bengio, Yinpeng Dong, Fangzhou Liao, Ming Liang, Tianyu Pang, Jun Zhu, Xiaolin Hu, Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, Alan Yuille, Sangxia Huang, Yao Zhao, Yuzhe Zhao, Zhonglin Han, Junjiajia Long, Yerkebulan Berdibekov, Takuya Akiba, Seiya Tokui, Motoki Abe


## 2018-03-29

[Security Consideration For Deep Learning-Based Image Forensics.](http://arxiv.org/abs/1803.11157)

Wei Zhao, Pengpeng Yang, Rongrong Ni, Yao Zhao, Haorui Wu


## 2018-03-28

[Defending against Adversarial Images using Basis Functions Transformations.](http://arxiv.org/abs/1803.10840)

Uri Shaham, James Garritano, Yutaro Yamada, Ethan Weinberger, Alex Cloninger, Xiuyuan Cheng, Kelly Stanton, Yuval Kluger


[The Effects of JPEG and JPEG2000 Compression on Attacks using Adversarial Examples.](http://arxiv.org/abs/1803.10418)

Ayse Elvan Aydemir, Alptekin Temizel, Tugba Taskaya Temizel


## 2018-03-26

[Bypassing Feature Squeezing by Increasing Adversary Strength.](http://arxiv.org/abs/1803.09868)

Yash Sharma, Pin-Yu Chen


[On the Limitation of Local Intrinsic Dimensionality for Characterizing the Subspaces of Adversarial Examples.](http://arxiv.org/abs/1803.09638)

Pei-Hsuan Lu, Pin-Yu Chen, Chia-Mu Yu


[Clipping free attacks against artificial neural networks.](http://arxiv.org/abs/1803.09468)

Boussad Addad, Jerome Kodjabachian, Christophe Meyer


## 2018-03-24

[Security Theater: On the Vulnerability of Classifiers to Exploratory Attacks.](http://arxiv.org/abs/1803.09163)

Tegjyot Singh Sethi, Mehmed Kantardzic, Joung Woo Ryu


[A Dynamic-Adversarial Mining Approach to the Security of Machine Learning.](http://arxiv.org/abs/1803.09162)

Tegjyot Singh Sethi, Mehmed Kantardzic, Lingyu Lyua, Jiashun Chen


[An Overview of Vulnerabilities of Voice Controlled Systems.](http://arxiv.org/abs/1803.09156)

Yuan Gong, Christian Poellabauer


## 2018-03-23

[Generalizability vs. Robustness: Adversarial Examples for Medical Imaging.](http://arxiv.org/abs/1804.00504)

Magdalini Paschali, Sailesh Conjeti, Fernando Navarro, Nassir Navab


[CNN Based Adversarial Embedding with Minimum Alteration for Image Steganography.](http://arxiv.org/abs/1803.09043)

Weixuan Tang, Bin Li, Shunquan Tan, Mauro Barni, Jiwu Huang


[Detecting Adversarial Perturbations with Saliency.](http://arxiv.org/abs/1803.08773)

Chiliang Zhang, Zhimou Yang, Zuochang Ye


[Improving DNN Robustness to Adversarial Attacks using Jacobian Regularization.](http://arxiv.org/abs/1803.08680)

Daniel Jakubovitz, Raja Giryes


## 2018-03-22

[Understanding Measures of Uncertainty for Adversarial Example Detection.](http://arxiv.org/abs/1803.08533)

Lewis Smith, Yarin Gal


## 2018-03-21

[Adversarial Defense based on Structure-to-Signal Autoencoders.](http://arxiv.org/abs/1803.07994)

Joachim Folz, Sebastian Palacio, Joern Hees, Damian Borth, Andreas Dengel


[Task dependent Deep LDA pruning of neural networks.](http://arxiv.org/abs/1803.08134)

Qing Tian, Tal Arbel, James J. Clark


## 2018-03-20

[DeepGauge: Multi-Granularity Testing Criteria for Deep Learning Systems.](http://arxiv.org/abs/1803.07519)

Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chunyang Chen, Ting Su, Li Li, Yang Liu, Jianjun Zhao, Yadong Wang


## 2018-03-19

[Technical Report: When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks.](http://arxiv.org/abs/1803.06975)

Octavian Suciu, Radu Mărginean, Yiğitcan Kaya, Hal III Daumé, Tudor Dumitraş


[Improving Transferability of Adversarial Examples with Input Diversity.](http://arxiv.org/abs/1803.06978)

Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, Alan Yuille


## 2018-03-17

[A Dual Approach to Scalable Verification of Deep Networks.](http://arxiv.org/abs/1803.06567)

Dj Krishnamurthy, Dvijotham, Robert Stanforth, Sven Gowal, Timothy Mann, Pushmeet Kohli


## 2018-03-16

[Adversarial Logit Pairing.](http://arxiv.org/abs/1803.06373)

Harini Kannan, Alexey Kurakin, Ian Goodfellow


[Semantic Adversarial Examples.](http://arxiv.org/abs/1804.00499)

Hossein Hosseini, Radha Poovendran


## 2018-03-15

[Large Margin Deep Networks for Classification.](http://arxiv.org/abs/1803.05598)

Gamaleldin F. Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, Samy Bengio


## 2018-03-13

[Feature Distillation: DNN-Oriented JPEG Compression Against Adversarial Examples.](http://arxiv.org/abs/1803.05787)

Zihao Liu, Qi Liu, Tao Liu, Nuo Xu, Xue Lin, Yanzhi Wang, Wujie Wen


[Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning.](http://arxiv.org/abs/1803.04765)

Nicolas Papernot, Patrick McDaniel


[Invisible Mask: Practical Attacks on Face Recognition with Infrared.](http://arxiv.org/abs/1803.04683)

Zhe Zhou, Di Tang, Xiaofeng Wang, Weili Han, Xiangyu Liu, Kehuan Zhang


[Defending against Adversarial Attack towards Deep Neural Networks via Collaborative Multi-task Training.](http://arxiv.org/abs/1803.05123)

Derek Wang, Chaoran Li, Sheng Wen, Surya Nepal, Yang Xiang


## 2018-03-12

[Adversarial Malware Binaries: Evading Deep Learning for Malware Detection in Executables.](http://arxiv.org/abs/1803.04173)

Bojan Kolosnjaji, Ambra Demontis, Battista Biggio, Davide Maiorca, Giorgio Giacinto, Claudia Eckert, Fabio Roli


## 2018-03-10

[Combating Adversarial Attacks Using Sparse Representations.](http://arxiv.org/abs/1803.03880)

Soorya Gopalakrishnan, Zhinus Marzi, Upamanyu Madhow, Ramtin Pedarsani


[Detecting Adversarial Examples via Neural Fingerprinting.](http://arxiv.org/abs/1803.03870)

Sumanth Dathathri, Stephan Zheng, Tianwei Yin, Richard M. Murray, Yisong Yue


## 2018-03-09

[Detecting Adversarial Examples - A Lesson from Multimedia Forensics.](http://arxiv.org/abs/1803.03613)

Pascal Schöttle, Alexander Schlögl, Cecilia Pasquini, Rainer Böhme


[On Generation of Adversarial Examples using Convex Programming.](http://arxiv.org/abs/1803.03607)

Emilio Rafael Balda, Arash Behboodi, Rudolf Mathar


[Explaining Black-box Android Malware Detection.](http://arxiv.org/abs/1803.03544)

Marco Melis, Davide Maiorca, Battista Biggio, Giorgio Giacinto, Fabio Roli


## 2018-03-08

[Rethinking Feature Distribution for Loss Functions in Image Classification.](http://arxiv.org/abs/1803.02988)

Weitao Wan, Yuanyi Zhong, Tianpeng Li, Jiansheng Chen


## 2018-03-07

[Sparse Adversarial Perturbations for Videos.](http://arxiv.org/abs/1803.02536)

Xingxing Wei, Jun Zhu, Hang Su


## 2018-03-04

[Stochastic Activation Pruning for Robust Adversarial Defense.](http://arxiv.org/abs/1803.01442)

Guneet S. Dhillon, Kamyar Azizzadenesheli, Zachary C. Lipton, Jeremy Bernstein, Jean Kossaifi, Aran Khanna, Anima Anandkumar


## 2018-03-03

[Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples.](http://arxiv.org/abs/1803.01128)

Minhao Cheng, Jinfeng Yi, Pin-Yu Chen, Huan Zhang, Cho-Jui Hsieh


## 2018-03-02

[Protecting JPEG Images Against Adversarial Attacks.](http://arxiv.org/abs/1803.00940)

Aaditya Prakash, Nick Moran, Solomon Garber, Antonella DiLillo, James Storer


## 2018-02-26

[Understanding and Enhancing the Transferability of Adversarial Examples.](http://arxiv.org/abs/1802.09707)

Lei Wu, Zhanxing Zhu, Cheng Tai, Weinan E


[On the Suitability of $L_p$-norms for Creating and Preventing Adversarial Examples.](http://arxiv.org/abs/1802.09653)

Mahmood Sharif, Lujo Bauer, Michael K. Reiter


[Retrieval-Augmented Convolutional Neural Networks for Improved Robustness against Adversarial Examples.](http://arxiv.org/abs/1802.09502)

Jake Zhao, Kyunghyun Cho


[Max-Mahalanobis Linear Discriminant Analysis Networks.](http://arxiv.org/abs/1802.09308)

Tianyu Pang, Chao Du, Jun Zhu


## 2018-02-23

[Deep Defense: Training DNNs with Improved Adversarial Robustness.](http://arxiv.org/abs/1803.00404)

Ziang Yan, Yiwen Guo, Changshui Zhang


[Sensitivity and Generalization in Neural Networks: an Empirical Study.](http://arxiv.org/abs/1802.08760)

Roman Novak, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington, Jascha Sohl-Dickstein


[Adversarial vulnerability for any classifier.](http://arxiv.org/abs/1802.08686)

Alhussein Fawzi, Hamza Fawzi, Omar Fawzi


[Verifying Controllers Against Adversarial Examples with Bayesian Optimization.](http://arxiv.org/abs/1802.08678)

Shromona Ghosh, Felix Berkenkamp, Gireeja Ranade, Shaz Qadeer, Ashish Kapoor


## 2018-02-22

[Unravelling Robustness of Deep Learning based Face Recognition Against Adversarial Attacks.](http://arxiv.org/abs/1803.00401)

Gaurav Goswami, Nalini Ratha, Akshay Agarwal, Richa Singh, Mayank Vatsa


[Hessian-based Analysis of Large Batch Training and Robustness to Adversaries.](http://arxiv.org/abs/1802.08241)

Zhewei Yao, Amir Gholami, Qi Lei, Kurt Keutzer, Michael W. Mahoney


[Adversarial Examples that Fool both Computer Vision and Time-Limited Humans.](http://arxiv.org/abs/1802.08195)

Gamaleldin F. Elsayed, Shreya Shankar, Brian Cheung, Nicolas Papernot, Alex Kurakin, Ian Goodfellow, Jascha Sohl-Dickstein


## 2018-02-21

[Adversarial Training for Probabilistic Spiking Neural Networks.](http://arxiv.org/abs/1802.08567)

Alireza Bagheri, Osvaldo Simeone, Bipin Rajendran


[L2-Nonexpansive Neural Networks.](http://arxiv.org/abs/1802.07896)

Haifeng Qian, Mark N. Wegman


[Generalizable Adversarial Examples Detection Based on Bi-model Decision Mismatch.](http://arxiv.org/abs/1802.07770)

João Monteiro, Isabela Albuquerque, Zahid Akhtar, Tiago H. Falk


## 2018-02-20

[Attack Strength vs. Detectability Dilemma in Adversarial Machine Learning.](http://arxiv.org/abs/1802.07295)

Christopher Frederickson, Michael Moore, Glenn Dawson, Robi Polikar


[Out-distribution training confers robustness to deep neural networks.](http://arxiv.org/abs/1802.07124)

Mahdieh Abbasi, Christian Gagné


## 2018-02-19

[On Lyapunov exponents and adversarial perturbation.](http://arxiv.org/abs/1802.06927)

Vinay Uday Prabhu, Nishant Desai, John Whaley


[Shield: Fast, Practical Defense and Vaccination for Deep Learning using JPEG Compression.](http://arxiv.org/abs/1802.06816)

Nilaksh Das, Madhuri Shanbhogue, Shang-Tse Chen, Fred Hohman, Siwei Li, Li Chen, Michael E. Kounavis, Duen Horng Chau


[Divide, Denoise, and Defend against Adversarial Attacks.](http://arxiv.org/abs/1802.06806)

Seyed-Mohsen Moosavi-Dezfooli, Ashish Shrivastava, Oncel Tuzel


[Robustness of Rotation-Equivariant Networks to Adversarial Perturbations.](http://arxiv.org/abs/1802.06627)

Beranger Dumont, Simona Maggio, Pablo Montalvo


[Are Generative Classifiers More Robust to Adversarial Attacks?](http://arxiv.org/abs/1802.06552)

Yingzhen Li, John Bradshaw, Yash Sharma


## 2018-02-18

[DARTS: Deceiving Autonomous Cars with Toxic Signs.](http://arxiv.org/abs/1802.06430)

Chawin Sitawarin, Arjun Nitin Bhagoji, Arsalan Mosenia, Mung Chiang, Prateek Mittal


## 2018-02-15

[ASP:A Fast Adversarial Attack Example Generation Framework based on Adversarial Saliency Prediction.](http://arxiv.org/abs/1802.05763)

Fuxun Yu, Qide Dong, Xiang Chen


[Adversarial Risk and the Dangers of Evaluating Against Weak Attacks.](http://arxiv.org/abs/1802.05666)

Jonathan Uesato, Brendan O'Donoghue, Aaron van den Oord, Pushmeet Kohli


## 2018-02-14

[Fooling OCR Systems with Adversarial Text Images.](http://arxiv.org/abs/1802.05385)

Congzheng Song, Vitaly Shmatikov


[Security Analysis and Enhancement of Model Compressed Deep Learning Systems under Adversarial Attacks.](http://arxiv.org/abs/1802.05193)

Qi Liu, Tao Liu, Zihao Liu, Yanzhi Wang, Yier Jin, Wujie Wen


## 2018-02-13

[Query-Free Attacks on Industry-Grade Face Recognition Systems under Resource Constraints.](http://arxiv.org/abs/1802.09900)

Di Tang, XiaoFeng Wang, Kehuan Zhang


[Identify Susceptible Locations in Medical Records via Adversarial Attacks on Deep Predictive Models.](http://arxiv.org/abs/1802.04822)

Mengying Sun, Fengyi Tang, Jinfeng Yi, Fei Wang, Jiayu Zhou


[Deceiving End-to-End Deep Learning Malware Detectors using Adversarial Examples.](http://arxiv.org/abs/1802.04528)

Felix Kreuk, Assi Barak, Shir Aviv-Reuven, Moran Baruch, Benny Pinkas, Joseph Keshet


## 2018-02-12

[Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks.](http://arxiv.org/abs/1802.04034)

Yusuke Tsuzuku, Issei Sato, Masashi Sugiyama


[Predicting Adversarial Examples with High Confidence.](http://arxiv.org/abs/1802.04457)

Angus Galloway, Graham W. Taylor, Medhat Moussa


## 2018-02-09

[Certified Robustness to Adversarial Examples with Differential Privacy.](http://arxiv.org/abs/1802.03471)

Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, Suman Jana


## 2018-02-08

[Detection of Adversarial Training Examples in Poisoning Attacks through Anomaly Detection.](http://arxiv.org/abs/1802.03041)

Andrea Paudice, Luis Muñoz-González, Andras Gyorgy, Emil C. Lupu


## 2018-02-05

[Blind Pre-Processing: A Robust Defense Method Against Adversarial Examples.](http://arxiv.org/abs/1802.01549)

Adnan Siraj Rakin, Zhezhi He, Boqing Gong, Deliang Fan


[First-order Adversarial Vulnerability of Neural Networks and Input Dimension.](http://arxiv.org/abs/1802.01421)

Carl-Johann Simon-Gabriel, Yann Ollivier, Léon Bottou, Bernhard Schölkopf, David Lopez-Paz


## 2018-02-02

[Secure Detection of Image Manipulation by means of Random Feature Selection.](http://arxiv.org/abs/1802.00573)

Zhipeng Chen, Benedetta Tondi, Xiaolong Li, Rongrong Ni, Yao Zhao, Mauro Barni


[Hardening Deep Neural Networks via Adversarial Model Cascades.](http://arxiv.org/abs/1802.01448)

Deepak Vijaykeerthy, Anshuman Suri, Sameep Mehta, Ponnurangam Kumaraguru


## 2018-02-01

[Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples.](http://arxiv.org/abs/1802.00420)

Anish Athalye, Nicholas Carlini, David Wagner


## 2018-01-31

[Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach.](http://arxiv.org/abs/1801.10578)

Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, Luca Daniel


## 2018-01-29

[Robustness of classification ability of spiking neural networks.](http://arxiv.org/abs/1801.09827)

Jie Yang, Pingping Zhang, Yan Liu


## 2018-01-28

[Certified Defenses against Adversarial Examples.](http://arxiv.org/abs/1801.09344)

Aditi Raghunathan, Jacob Steinhardt, Percy Liang


## 2018-01-27

[Towards an Understanding of Neural Networks in Natural-Image Spaces.](http://arxiv.org/abs/1801.09097)

Yifei Fan, Anthony Yezzi


## 2018-01-26

[Deflecting Adversarial Attacks with Pixel Deflection.](http://arxiv.org/abs/1801.08926)

Aaditya Prakash, Nick Moran, Solomon Garber, Antonella DiLillo, James Storer


[Learning to Evade Static PE Machine Learning Malware Models via Reinforcement Learning.](http://arxiv.org/abs/1801.08917)

Hyrum S. Anderson, Anant Kharkar, Bobby Filar, David Evans, Phil Roth


## 2018-01-24

[CommanderSong: A Systematic Approach for Practical Adversarial Voice Recognition.](http://arxiv.org/abs/1801.08535)

Xuejing Yuan, Yuxuan Chen, Yue Zhao, Yunhui Long, Xiaokang Liu, Kai Chen, Shengzhi Zhang, Heqing Huang, Xiaofeng Wang, Carl A. Gunter


[Generalizable Data-free Objective for Crafting Universal Adversarial Perturbations.](http://arxiv.org/abs/1801.08092)

Konda Reddy Mopuri, Aditya Ganeshan, R. Venkatesh Babu


## 2018-01-22

[Adversarial Texts with Gradient Methods.](http://arxiv.org/abs/1801.07175)

Zhitao Gong, Wenlu Wang, Bo Li, Dawn Song, Wei-Shinn Ku


## 2018-01-15

[A Comparative Study of Rule Extraction for Recurrent Neural Networks.](http://arxiv.org/abs/1801.05420)

Qinglong Wang, Kaixuan Zhang, Alexander G. II Ororbia, Xinyu Xing, Xue Liu, C. Lee Giles


[Sparsity-based Defense against Adversarial Attacks on Linear Classifiers.](http://arxiv.org/abs/1801.04695)

Zhinus Marzi, Soorya Gopalakrishnan, Upamanyu Madhow, Ramtin Pedarsani


[Towards Imperceptible and Robust Adversarial Example Attacks against Neural Networks.](http://arxiv.org/abs/1801.04693)

Bo Luo, Yannan Liu, Lingxiao Wei, Qiang Xu


## 2018-01-12

[Black-box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers.](http://arxiv.org/abs/1801.04354)

Ji Gao, Jack Lanchantin, Mary Lou Soffa, Yanjun Qi


## 2018-01-11

[A3T: Adversarially Augmented Adversarial Training.](http://arxiv.org/abs/1801.04055)

Akram Erraqabi, Aristide Baratin, Yoshua Bengio, Simon Lacoste-Julien


## 2018-01-10

[Fooling End-to-end Speaker Verification by Adversarial Examples.](http://arxiv.org/abs/1801.03339)

Felix Kreuk, Yossi Adi, Moustapha Cisse, Joseph Keshet


## 2018-01-09

[Adversarial Deep Learning for Robust Detection of Binary Encoded Malware.](http://arxiv.org/abs/1801.02950)

Abdullah Al-Dujaili, Alex Huang, Erik Hemberg, Una-May O'Reilly


[Less is More: Culling the Training Set to Improve Robustness of Deep Neural Networks.](http://arxiv.org/abs/1801.02850)

Yongshuai Liu, Jiyu Chen, Hao Chen


## 2018-01-08

[Rogue Signs: Deceiving Traffic Sign Recognition with Malicious Ads and Logos.](http://arxiv.org/abs/1801.02780)

Chawin Sitawarin, Arjun Nitin Bhagoji, Arsalan Mosenia, Prateek Mittal, Mung Chiang


[Adversarial Spheres.](http://arxiv.org/abs/1801.02774)

Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S. Schoenholz, Maithra Raghu, Martin Wattenberg, Ian Goodfellow


[Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality.](http://arxiv.org/abs/1801.02613)

Xingjun Ma, Bo Li, Yisen Wang, Sarah M. Erfani, Sudanthi Wijewickrema, Grant Schoenebeck, Dawn Song, Michael E. Houle, James Bailey


[Spatially Transformed Adversarial Examples.](http://arxiv.org/abs/1801.02612)

Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, Dawn Song


[Generating Adversarial Examples with Adversarial Networks.](http://arxiv.org/abs/1801.02610)

Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, Dawn Song


[LaVAN: Localized and Visible Adversarial Noise.](http://arxiv.org/abs/1801.02608)

Danny Karmon, Daniel Zoran, Yoav Goldberg


[Attacking Speaker Recognition With Deep Generative Models.](http://arxiv.org/abs/1801.02384)

Wilson Cai, Anish Doshi, Rafael Valle


[HeNet: A Deep Learning Approach on Intel$^\circledR$ Processor Trace for Effective Exploit Detection.](http://arxiv.org/abs/1801.02318)

Li Chen, Salmin Sultana, Ravi Sahita


## 2018-01-07

[Denoising Dictionary Learning Against Adversarial Perturbations.](http://arxiv.org/abs/1801.02257)

John Mitro, Derek Bridge, Steven Prestwich


## 2018-01-05

[Adversarial Perturbation Intensity Achieving Chosen Intra-Technique Transferability Level for Logistic Regression.](http://arxiv.org/abs/1801.01953)

Martin Gubri


[Audio Adversarial Examples: Targeted Attacks on Speech-to-Text.](http://arxiv.org/abs/1801.01944)

Nicholas Carlini, David Wagner


[Shielding Google's language toxicity model against adversarial attacks.](http://arxiv.org/abs/1801.01828)

Nestor Rodriguez, Sergio Rojas-Galeano


## 2018-01-03

[Facial Attributes: Accuracy and Adversarial Robustness.](http://arxiv.org/abs/1801.02480)

Andras Rozsa, Manuel Günther, Ethan M. Rudd, Terrance E. Boult


[Neural Networks in Adversarial Setting and Ill-Conditioned Weight Space.](http://arxiv.org/abs/1801.00905)

Mayank Singh, Abhishek Sinha, Balaji Krishnamurthy


## 2018-01-02

[High Dimensional Spaces, Deep Learning and Adversarial Examples.](http://arxiv.org/abs/1801.00634)

Simant Dube


[Did you hear that? Adversarial Examples Against Automatic Speech Recognition.](http://arxiv.org/abs/1801.00554)

Moustafa Alzantot, Bharathan Balaji, Mani Srivastava


[Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey.](http://arxiv.org/abs/1801.00553)

Naveed Akhtar, Ajmal Mian


## 2017-12-31

[A General Framework for Adversarial Examples with Objectives.](http://arxiv.org/abs/1801.00349)

Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, Michael K. Reiter


## 2017-12-28

[Gradient Regularization Improves Accuracy of Discriminative Models.](http://arxiv.org/abs/1712.09936)

Dániel Varga, Adrián Csiszárik, Zsolt Zombori


## 2017-12-27

[Adversarial Patch.](http://arxiv.org/abs/1712.09665)

Tom B. Brown, Dandelion Mané, Aurko Roy, Martín Abadi, Justin Gilmer


## 2017-12-26

[Exploring the Space of Black-box Attacks on Deep Neural Networks.](http://arxiv.org/abs/1712.09491)

Arjun Nitin Bhagoji, Warren He, Bo Li, Dawn Song


[Building Robust Deep Neural Networks for Road Sign Detection.](http://arxiv.org/abs/1712.09327)

Arkar Min Aung, Yousef Fadila, Radian Gondokaryono, Luis Gonzalez


[The Robust Manifold Defense: Adversarial Training using Generative Models.](http://arxiv.org/abs/1712.09196)

Ajil Jalal, Andrew Ilyas, Constantinos Daskalakis, Alexandros G. Dimakis


## 2017-12-24

[Android Malware Detection using Deep Learning on API Method Sequences.](http://arxiv.org/abs/1712.08996)

ElMouatez Billah Karbab, Mourad Debbabi, Abdelouahid Derhab, Djedjiga Mouheb


## 2017-12-23

[Whatever Does Not Kill Deep Reinforcement Learning, Makes It Stronger.](http://arxiv.org/abs/1712.09344)

Vahid Behzadan, Arslan Munir


## 2017-12-22

[Query-limited Black-box Attacks to Classifiers.](http://arxiv.org/abs/1712.08713)

Fnu Suya, Yuan Tian, David Evans, Paolo Papotti


## 2017-12-21

[Using LIP to Gloss Over Faces in Single-Stage Face Detection Networks.](http://arxiv.org/abs/1712.08263)

Siqi Yang, Arnold Wiliem, Shaokang Chen, Brian C. Lovell


[ReabsNet: Detecting and Revising Adversarial Examples.](http://arxiv.org/abs/1712.08250)

Jiefeng Chen, Zihang Meng, Changtian Sun, Wei Tang, Yinglun Zhu


[Note on Attacking Object Detectors with Adversarial Stickers.](http://arxiv.org/abs/1712.08062)

Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Dawn Song, Tadayoshi Kohno, Amir Rahmati, Atul Prakash, Florian Tramer


[Wolf in Sheep's Clothing - The Downscaling Attack Against Deep Learning Applications.](http://arxiv.org/abs/1712.07805)

Qixue Xiao, Kang Li, Deyue Zhang, Yier Jin


## 2017-12-19

[Query-Efficient Black-box Adversarial Examples (superceded).](http://arxiv.org/abs/1712.07113)

Andrew Ilyas, Logan Engstrom, Anish Athalye, Jessy Lin


[Adversarial Examples: Attacks and Defenses for Deep Learning.](http://arxiv.org/abs/1712.07107)

Xiaoyong Yuan, Pan He, Qile Zhu, Xiaolin Li


## 2017-12-18

[HotFlip: White-Box Adversarial Examples for Text Classification.](http://arxiv.org/abs/1712.06751)

Javid Ebrahimi, Anyi Rao, Daniel Lowd, Dejing Dou


[When Not to Classify: Anomaly Detection of Attacks (ADA) on DNN Classifiers at Test Time.](http://arxiv.org/abs/1712.06646)

David J. Miller, Yulia Wang, George Kesidis


## 2017-12-17

[Deep Neural Networks as 0-1 Mixed Integer Linear Programs: A Feasibility Study.](http://arxiv.org/abs/1712.06174)

Matteo Fischetti, Jason Jo


[Super-sparse Learning in Similarity Spaces.](http://arxiv.org/abs/1712.06131)

Ambra Demontis, Marco Melis, Battista Biggio, Giorgio Fumera, Fabio Roli


## 2017-12-16

[Attack and Defense of Dynamic Analysis-Based, Adversarial Neural Malware Classification Models.](http://arxiv.org/abs/1712.05919)

Jack W. Stokes, De Wang, Mady Marinescu, Marc Marino, Brian Bussone


## 2017-12-14

[DANCin SEQ2SEQ: Fooling Text Classifiers with Adversarial Text Example Generation.](http://arxiv.org/abs/1712.05419)

Catherine Wong


## 2017-12-12

[Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models.](http://arxiv.org/abs/1712.04248)

Wieland Brendel, Jonas Rauber, Matthias Bethge


## 2017-12-11

[Training Ensembles to Detect Adversarial Examples.](http://arxiv.org/abs/1712.04006)

Alexander Bagnall, Razvan Bunescu, Gordon Stewart


## 2017-12-10

[Robust Deep Reinforcement Learning with Adversarial Attacks.](http://arxiv.org/abs/1712.03632)

Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, Girish Chowdhary


## 2017-12-09

[NAG: Network for Adversary Generation.](http://arxiv.org/abs/1712.03390)

Konda Reddy Mopuri, Utkarsh Ojha, Utsav Garg, R. Venkatesh Babu


## 2017-12-08

[Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning.](http://arxiv.org/abs/1712.03141)

Battista Biggio, Fabio Roli


[Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser.](http://arxiv.org/abs/1712.02976)

Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Xiaolin Hu, Jun Zhu


## 2017-12-07

[Adversarial Examples that Fool Detectors.](http://arxiv.org/abs/1712.02494)

Jiajun Lu, Hussein Sibai, Evan Fabry


[Exploring the Landscape of Spatial Robustness.](http://arxiv.org/abs/1712.02779)

Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, Aleksander Madry


## 2017-12-06

[Generative Adversarial Perturbations.](http://arxiv.org/abs/1712.02328)

Omid Poursaeed, Isay Katsman, Bicheng Gao, Serge Belongie


[Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning.](http://arxiv.org/abs/1712.02051)

Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Cho-Jui Hsieh


## 2017-12-05

[Towards Practical Verification of Machine Learning: The Case of Computer Vision Systems.](http://arxiv.org/abs/1712.01785)

Kexin Pei, Linjie Zhu, Yinzhi Cao, Junfeng Yang, Carl Vondrick, Suman Jana


## 2017-12-02

[Improving Network Robustness against Adversarial Attacks with Compact Convolution.](http://arxiv.org/abs/1712.00699)

Rajeev Ranjan, Swami Sankaranarayanan, Carlos D. Castillo, Rama Chellappa


[Towards Robust Neural Networks via Random Self-ensemble.](http://arxiv.org/abs/1712.00673)

Xuanqing Liu, Minhao Cheng, Huan Zhang, Cho-Jui Hsieh


[Where Classification Fails, Interpretation Rises.](http://arxiv.org/abs/1712.00558)

Chanh Nguyen, Georgi Georgiev, Yujie Ji, Ting Wang


## 2017-11-30

[Measuring the tendency of CNNs to Learn Surface Statistical Regularities.](http://arxiv.org/abs/1711.11561)

Jason Jo, Yoshua Bengio


## 2017-11-27

[Adversary Detection in Neural Networks via Persistent Homology.](http://arxiv.org/abs/1711.10056)

Thomas Gebhart, Paul Schrater


[On the Robustness of Semantic Segmentation Models to Adversarial Attacks.](http://arxiv.org/abs/1711.09856)

Anurag Arnab, Ondrej Miksik, Philip H. S. Torr


[Butterfly Effect: Bidirectional Control of Classification Performance by Small Additive Perturbation.](http://arxiv.org/abs/1711.09681)

YoungJoon Yoo, Seonguk Park, Junyoung Choi, Sangdoo Yun, Nojun Kwak


## 2017-11-26

[Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients.](http://arxiv.org/abs/1711.09404)

Andrew Slavin Ross, Finale Doshi-Velez


## 2017-11-24

[Geometric robustness of deep networks: analysis and improvement.](http://arxiv.org/abs/1711.09115)

Can Kanbak, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard


## 2017-11-22

[Safer Classification by Synthesis.](http://arxiv.org/abs/1711.08534)

William Wang, Angelina Wang, Aviv Tamar, Xi Chen, Pieter Abbeel


[MagNet and "Efficient Defenses Against Adversarial Attacks" are Not Robust to Adversarial Examples.](http://arxiv.org/abs/1711.08478)

Nicholas Carlini, David Wagner


[Adversarial Phenomenon in the Eyes of Bayesian Deep Learning.](http://arxiv.org/abs/1711.08244)

Ambrish Rawat, Martin Wistuba, Maria-Irina Nicolae


## 2017-11-21

[Reinforcing Adversarial Robustness using Model Confidence Induced by Adversarial Training.](http://arxiv.org/abs/1711.08001)

Xi Wu, Uyeong Jang, Jiefeng Chen, Lingjiao Chen, Somesh Jha


## 2017-11-20

[Evaluating Robustness of Neural Networks with Mixed Integer Programming.](http://arxiv.org/abs/1711.07356)

Vincent Tjeng, Kai Xiao, Russ Tedrake


[Adversarial Attacks Beyond the Image Space.](http://arxiv.org/abs/1711.07183)

Xiaohui Zeng, Chenxi Liu, Yu-Siang Wang, Weichao Qiu, Lingxi Xie, Yu-Wing Tai, Chi Keung Tang, Alan L. Yuille


## 2017-11-17

[How Wrong Am I? - Studying Adversarial Examples and their Impact on Uncertainty in Gaussian Process Machine Learning Models.](http://arxiv.org/abs/1711.06598)

Kathrin Grosse, David Pfaff, Michael Thomas Smith, Michael Backes


## 2017-11-16

[Enhanced Attacks on Defensively Distilled Deep Neural Networks.](http://arxiv.org/abs/1711.05934)

Yujia Liu, Weiming Zhang, Shaohua Li, Nenghai Yu


[Defense against Universal Adversarial Perturbations.](http://arxiv.org/abs/1711.05929)

Naveed Akhtar, Jian Liu, Ajmal Mian


## 2017-11-15

[The best defense is a good offense: Countering black box attacks by predicting slightly wrong labels.](http://arxiv.org/abs/1711.05475)

Yannic Kilcher, Thomas Hofmann


## 2017-11-12

[Machine vs Machine: Minimax-Optimal Defense Against Adversarial Examples.](http://arxiv.org/abs/1711.04368)

Jihun Hamm, Akshay Mehra


## 2017-11-09

[Crafting Adversarial Examples For Speech Paralinguistics Applications.](http://arxiv.org/abs/1711.03280)

Yuan Gong, Christian Poellabauer


## 2017-11-08

[Intriguing Properties of Adversarial Examples.](http://arxiv.org/abs/1711.02846)

Ekin D. Cubuk, Barret Zoph, Samuel S. Schoenholz, Quoc V. Le


## 2017-11-06

[Mitigating Adversarial Effects Through Randomization.](http://arxiv.org/abs/1711.01991)

Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, Alan Yuille


[HyperNetworks with statistical filtering for defending adversarial examples.](http://arxiv.org/abs/1711.01791)

Zhun Sun, Mete Ozay, Takayuki Okatani


[Towards Reverse-Engineering Black-Box Neural Networks.](http://arxiv.org/abs/1711.01768)

Seong Joon Oh, Max Augustin, Bernt Schiele, Mario Fritz


## 2017-11-02

[The (Un)reliability of saliency methods.](http://arxiv.org/abs/1711.00867)

Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T. Schütt, Sven Dähne, Dumitru Erhan, Been Kim


[Provable defenses against adversarial examples via the convex outer adversarial polytope.](http://arxiv.org/abs/1711.00851)

Eric Wong, J. Zico Kolter


## 2017-11-01

[Attacking Binarized Neural Networks.](http://arxiv.org/abs/1711.00449)

Angus Galloway, Graham W. Taylor, Medhat Moussa


## 2017-10-31

[Countering Adversarial Images using Input Transformations.](http://arxiv.org/abs/1711.00117)

Chuan Guo, Mayank Rana, Moustapha Cisse, der Maaten Laurens van


[Conditional Variance Penalties and Domain Shift Robustness.](http://arxiv.org/abs/1710.11469)

Christina Heinze-Deml, Nicolai Meinshausen


[Generating Natural Adversarial Examples.](http://arxiv.org/abs/1710.11342)

Zhengli Zhao, Dheeru Dua, Sameer Singh


## 2017-10-30

[PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples.](http://arxiv.org/abs/1710.10766)

Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, Nate Kushman


## 2017-10-29

[Attacking the Madry Defense Model with $L_1$-based Adversarial Examples.](http://arxiv.org/abs/1710.10733)

Yash Sharma, Pin-Yu Chen


[Certifying Some Distributional Robustness with Principled Adversarial Training.](http://arxiv.org/abs/1710.10571)

Aman Sinha, Hongseok Namkoong, Riccardo Volpi, John Duchi


## 2017-10-28

[Interpretation of Neural Networks is Fragile.](http://arxiv.org/abs/1710.10547)

Amirata Ghorbani, Abubakar Abid, James Zou


## 2017-10-27

[Adversarial Detection of Flash Malware: Limitations and Open Issues.](http://arxiv.org/abs/1710.10225)

Davide Maiorca, Ambra Demontis, Battista Biggio, Fabio Roli, Giorgio Giacinto


## 2017-10-25

[mixup: Beyond Empirical Risk Minimization.](http://arxiv.org/abs/1710.09412)

Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz


## 2017-10-24

[One pixel attack for fooling deep neural networks.](http://arxiv.org/abs/1710.08864)

Jiawei Su, Danilo Vasconcellos Vargas, Sakurai Kouichi


## 2017-10-21

[Feature-Guided Black-Box Safety Testing of Deep Neural Networks.](http://arxiv.org/abs/1710.07859)

Matthew Wicker, Xiaowei Huang, Marta Kwiatkowska


## 2017-10-17

[Boosting Adversarial Attacks with Momentum.](http://arxiv.org/abs/1710.06081)

Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, Jianguo Li


## 2017-10-12

[Game-Theoretic Design of Secure and Resilient Distributed Support Vector Machines with Adversaries.](http://arxiv.org/abs/1710.04677)

Rui Zhang, Quanyan Zhu


## 2017-10-09

[Standard detectors aren't (currently) fooled by physical adversarial stop signs.](http://arxiv.org/abs/1710.03337)

Jiajun Lu, Hussein Sibai, Evan Fabry, David Forsyth


[Verification of Binarized Neural Networks via Inter-Neuron Factoring.](http://arxiv.org/abs/1710.03107)

Chih-Hong Cheng, Georg Nührenberg, Chung-Hao Huang, Harald Ruess


## 2017-10-02

[Detecting Adversarial Attacks on Neural Network Policies with Visual Foresight.](http://arxiv.org/abs/1710.00814)

Yen-Chen Lin, Ming-Yu Liu, Min Sun, Jia-Bin Huang


[DeepSafe: A Data-driven Approach for Checking Adversarial Robustness in Neural Networks.](http://arxiv.org/abs/1710.00486)

Divya Gopinath, Guy Katz, Corina S. Pasareanu, Clark Barrett


## 2017-09-28

[Provably Minimally-Distorted Adversarial Examples.](http://arxiv.org/abs/1709.10207)

Nicholas Carlini, Guy Katz, Clark Barrett, David L. Dill


[DR.SGX: Hardening SGX Enclaves against Cache Attacks with Data Location Randomization.](http://arxiv.org/abs/1709.09917)

Ferdinand Technische Universität Darmstadt, Germany Brasser, Srdjan ETH Zurich, Switzerland Capkun, Alexandra University of Würzburg Dmitrienko, Tommaso Technische Universität Darmstadt, Germany Frassetto, Kari ETH Zurich, Switzerland Kostiainen, Ahmad-Reza Technische Universität Darmstadt, Germany Sadeghi


## 2017-09-26

[Output Range Analysis for Deep Neural Networks.](http://arxiv.org/abs/1709.09130)

Souradeep Dutta, Susmit Jha, Sriram Sanakaranarayanan, Ashish Tiwari


## 2017-09-25

[Fooling Vision and Language Models Despite Localization and Attention Mechanism.](http://arxiv.org/abs/1709.08693)

Xiaojun Xu, Xinyun Chen, Chang Liu, Anna Rohrbach, Trevor Darrell, Dawn Song


## 2017-09-19

[Verifying Properties of Binarized Deep Neural Networks.](http://arxiv.org/abs/1709.06662)

Nina Narodytska, Shiva Prasad Kasiviswanathan, Leonid Ryzhyk, Mooly Sagiv, Toby Walsh


## 2017-09-16

[Mitigating Evasion Attacks to Deep Neural Networks via Region-based Classification.](http://arxiv.org/abs/1709.05583)

Xiaoyu Cao, Neil Zhenqiang Gong


## 2017-09-13

[A Learning and Masking Approach to Secure Learning.](http://arxiv.org/abs/1709.04447)

Linh Nguyen, Sky Wang, Arunesh Sinha


[Models and Framework for Adversarial Attacks on Complex Adaptive Systems.](http://arxiv.org/abs/1709.04137)

Vahid Behzadan, Arslan Munir


## 2017-09-12

[EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial Examples.](http://arxiv.org/abs/1709.04114)

Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh


## 2017-09-11

[Art of singular vectors and universal adversarial perturbations.](http://arxiv.org/abs/1709.03582)

Valentin Khrulkov, Ivan Oseledets


[Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks.](http://arxiv.org/abs/1709.03423)

Thilo Strauss, Markus Hanselmann, Andrej Junginger, Holger Ulmer


## 2017-09-08

[Towards Proving the Adversarial Robustness of Deep Neural Networks.](http://arxiv.org/abs/1709.02802)

Guy Stanford University Katz, Clark Stanford University Barrett, David L. Stanford University Dill, Kyle Stanford University Julian, Mykel J. Stanford University Kochenderfer


[DeepFense: Online Accelerated Defense Against Adversarial Deep Learning.](http://arxiv.org/abs/1709.02538)

Bita Darvish Rouhani, Mohammad Samragh, Mojan Javaheripi, Tara Javidi, Farinaz Koushanfar


## 2017-09-02

[Security Evaluation of Pattern Classifiers under Attack.](http://arxiv.org/abs/1709.00609)

Battista Biggio, Giorgio Fumera, Fabio Roli


## 2017-08-31

[On Security and Sparsity of Linear Classifiers for Adversarial Settings.](http://arxiv.org/abs/1709.00045)

Ambra Demontis, Paolo Russu, Battista Biggio, Giorgio Fumera, Fabio Roli


[Be Selfish and Avoid Dilemmas: Fork After Withholding (FAW) Attacks on Bitcoin.](http://arxiv.org/abs/1708.09790)

Yujin Kwon, Dohyun Kim, Yunmok Son, Eugene Vasserman, Yongdae Kim


## 2017-08-29

[Practical Attacks Against Graph-based Clustering.](http://arxiv.org/abs/1708.09056)

Yizheng Chen, Yacin Nadji, Athanasios Kountouras, Fabian Monrose, Roberto Perdisci, Manos Antonakakis, Nikolaos Vasiloglou


## 2017-08-28

[DeepTest: Automated Testing of Deep-Neural-Network-driven Autonomous Cars.](http://arxiv.org/abs/1708.08559)

Yuchi Tian, Kexin Pei, Suman Jana, Baishakhi Ray


[Improving Robustness of ML Classifiers against Realizable Evasion Attacks Using Conserved Features.](http://arxiv.org/abs/1708.08327)

Liang Tong, Bo Li, Chen Hajaj, Chaowei Xiao, Ning Zhang, Yevgeniy Vorobeychik


## 2017-08-23

[Is Deep Learning Safe for Robot Vision? Adversarial Examples against the iCub Humanoid.](http://arxiv.org/abs/1708.06939)

Marco Melis, Ambra Demontis, Battista Biggio, Gavin Brown, Giorgio Fumera, Fabio Roli


## 2017-08-22

[CNN Fixations: An unraveling approach to visualize the discriminative image regions.](http://arxiv.org/abs/1708.06670)

Konda Reddy Mopuri, Utsav Garg, R. Venkatesh Babu


## 2017-08-21

[Evasion Attacks against Machine Learning at Test Time.](http://arxiv.org/abs/1708.06131)

Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov, Giorgio Giacinto, Fabio Roli


## 2017-08-17

[Towards Interpretable Deep Neural Networks by Leveraging Adversarial Examples.](http://arxiv.org/abs/1708.05493)

Yinpeng Dong, Hang Su, Jun Zhu, Fan Bao


[Learning Universal Adversarial Perturbations with Generative Models.](http://arxiv.org/abs/1708.05207)

Jamie Hayes, George Danezis


## 2017-08-14

[Attacking Automatic Video Analysis Algorithms: A Case Study of Google Cloud Video Intelligence API.](http://arxiv.org/abs/1708.04301)

Hossein Hosseini, Baicen Xiao, Andrew Clark, Radha Poovendran


## 2017-08-13

[ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models.](http://arxiv.org/abs/1708.03999)

Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, Cho-Jui Hsieh


## 2017-08-08

[Cascade Adversarial Machine Learning Regularized with a Unified Embedding.](http://arxiv.org/abs/1708.02582)

Taesik Na, Jong Hwan Ko, Saibal Mukhopadhyay


## 2017-08-04

[Adversarial Robustness: Softmax versus Openmax.](http://arxiv.org/abs/1708.01697)

Andras Rozsa, Manuel Günther, Terrance E. Boult


## 2017-08-01

[Adversarial-Playground: A Visualization Suite Showing How Adversarial Examples Fool Deep Learning.](http://arxiv.org/abs/1708.00807)

Andrew P. Norton, Yanjun Qi


## 2017-07-27

[Robust Physical-World Attacks on Deep Learning Models.](http://arxiv.org/abs/1707.08945)

Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, Dawn Song


## 2017-07-24

[Synthesizing Robust Adversarial Examples.](http://arxiv.org/abs/1707.07397)

Anish Athalye, Logan Engstrom, Andrew Ilyas, Kevin Kwok


## 2017-07-23

[Adversarial Examples for Evaluating Reading Comprehension Systems.](http://arxiv.org/abs/1707.07328)

Robin Jia, Percy Liang


## 2017-07-21

[Confidence estimation in Deep Neural networks via density modelling.](http://arxiv.org/abs/1707.07013)

Akshayvarun Subramanya, Suraj Srinivas, R. Venkatesh Babu


## 2017-07-20

[Efficient Defenses Against Adversarial Attacks.](http://arxiv.org/abs/1707.06728)

Valentina Zantedeschi, Maria-Irina Nicolae, Ambrish Rawat


## 2017-07-19

[Generic Black-Box End-to-End Attack Against State of the Art API Call Based Malware Classifiers.](http://arxiv.org/abs/1707.05970)

Ishai Rosenberg, Asaf Shabtai, Lior Rokach, Yuval Elovici


## 2017-07-18

[Fast Feature Fool: A data independent approach to universal adversarial perturbations.](http://arxiv.org/abs/1707.05572)

Konda Reddy Mopuri, Utsav Garg, R. Venkatesh Babu


[APE-GAN: Adversarial Perturbation Elimination with GAN.](http://arxiv.org/abs/1707.05474)

Shiwei Shen, Guoqing Jin, Ke Gao, Yongdong Zhang


## 2017-07-17

[Houdini: Fooling Deep Structured Prediction Models.](http://arxiv.org/abs/1707.05373)

Moustapha Cisse, Yossi Adi, Natalia Neverova, Joseph Keshet


## 2017-07-13

[Foolbox: A Python toolbox to benchmark the robustness of machine learning models.](http://arxiv.org/abs/1707.04131)

Jonas Rauber, Wieland Brendel, Matthias Bethge


## 2017-07-11

[NO Need to Worry about Adversarial Examples in Object Detection in Autonomous Vehicles.](http://arxiv.org/abs/1707.03501)

Jiajun Lu, Hussein Sibai, Evan Fabry, David Forsyth


[A Survey on Resilient Machine Learning.](http://arxiv.org/abs/1707.03184)

Atul Kumar, Sameep Mehta


## 2017-07-10

[Towards Crafting Text Adversarial Samples.](http://arxiv.org/abs/1707.02812)

Suranjana Samanta, Sameep Mehta


## 2017-07-04

[UPSET and ANGRI : Breaking High Performance Image Classifiers.](http://arxiv.org/abs/1707.01159)

Sayantan Sarkar, Ankan Bansal, Upal Mahbub, Rama Chellappa


## 2017-06-21

[Comparing deep neural networks against humans: object recognition when the signal gets weaker.](http://arxiv.org/abs/1706.06969)

Robert Geirhos, David H. J. Janssen, Heiko H. Schütt, Jonas Rauber, Matthias Bethge, Felix A. Wichmann


## 2017-06-19

[Towards Deep Learning Models Resistant to Adversarial Attacks.](http://arxiv.org/abs/1706.06083)

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu


## 2017-06-14

[Adversarial Example Defenses: Ensembles of Weak Defenses are not Strong.](http://arxiv.org/abs/1706.04701)

Warren He, James Wei, Xinyun Chen, Nicholas Carlini, Dawn Song


## 2017-06-13

[Analyzing the Robustness of Nearest Neighbors to Adversarial Examples.](http://arxiv.org/abs/1706.03922)

Yizhen Wang, Somesh Jha, Kamalika Chaudhuri


## 2017-06-06

[Adversarial-Playground: A Visualization Suite for Adversarial Sample Generation.](http://arxiv.org/abs/1706.01763)

Andrew Norton, Yanjun Qi


## 2017-06-02

[Towards Robust Detection of Adversarial Examples.](http://arxiv.org/abs/1706.00633)

Tianyu Pang, Chao Du, Yinpeng Dong, Jun Zhu


## 2017-05-30

[Feature Squeezing Mitigates and Detects Carlini/Wagner Adversarial Examples.](http://arxiv.org/abs/1705.10686)

Weilin Xu, David Evans, Yanjun Qi


## 2017-05-27

[MAT: A Multi-strength Adversarial Training Method to Mitigate Adversarial Attacks.](http://arxiv.org/abs/1705.09764)

Chang Song, Hsin-Pai Cheng, Huanrui Yang, Sicheng Li, Chunpeng Wu, Qing Wu, Hai Li, Yiran Chen


## 2017-05-26

[Classification regions of deep neural networks.](http://arxiv.org/abs/1705.09552)

Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard, Stefano Soatto


[Robustness of classifiers to universal perturbations: a geometric perspective.](http://arxiv.org/abs/1705.09554)

Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal Frossard, Stefano Soatto


## 2017-05-25

[MagNet: a Two-Pronged Defense against Adversarial Examples.](http://arxiv.org/abs/1705.09064)

Dongyu Meng, Hao Chen


## 2017-05-23

[Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation.](http://arxiv.org/abs/1705.08475)

Matthias Hein, Maksym Andriushchenko


[Detecting Adversarial Image Examples in Deep Networks with Adaptive Noise Reduction.](http://arxiv.org/abs/1705.08378)

Bin Liang, Hongcheng Li, Miaoqiang Su, Xirong Li, Wenchang Shi, Xiaofeng Wang


[Black-Box Attacks against RNN based Malware Detection Algorithms.](http://arxiv.org/abs/1705.08131)

Weiwei Hu, Ying Tan


## 2017-05-22

[Regularizing deep networks using efficient layerwise adversarial training.](http://arxiv.org/abs/1705.07819)

Swami Sankaranarayanan, Arpit Jain, Rama Chellappa, Ser Nam Lim


## 2017-05-21

[Evading Classifiers by Morphing in the Dark.](http://arxiv.org/abs/1705.07535)

Hung Dang, Yue Huang, Ee-Chien Chang


## 2017-05-20

[Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods.](http://arxiv.org/abs/1705.07263)

Nicholas Carlini, David Wagner


## 2017-05-19

[Ensemble Adversarial Training: Attacks and Defenses.](http://arxiv.org/abs/1705.07204)

Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, Patrick McDaniel


[MTDeep: Boosting the Security of Deep Neural Nets Against Adversarial Attacks with Moving Target Defense.](http://arxiv.org/abs/1705.07213)

Sailik Sengupta, Tathagata Chakraborti, Subbarao Kambhampati


## 2017-05-18

[DeepXplore: Automated Whitebox Testing of Deep Learning Systems.](http://arxiv.org/abs/1705.06640)

Kexin Pei, Yinzhi Cao, Junfeng Yang, Suman Jana


[Delving into adversarial attacks on deep policies.](http://arxiv.org/abs/1705.06452)

Jernej Kos, Dawn Song


## 2017-05-15

[Extending Defensive Distillation.](http://arxiv.org/abs/1705.05264)

Nicolas Papernot, Patrick McDaniel


## 2017-05-09

[Generative Adversarial Trainer: Defense to Adversarial Perturbations with GAN.](http://arxiv.org/abs/1705.03387)

Hyeungill Lee, Sungyeob Han, Jungwoo Lee


## 2017-05-08

[Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning with JPEG Compression.](http://arxiv.org/abs/1705.02900)

Nilaksh Das, Madhuri Shanbhogue, Shang-Tse Chen, Fred Hohman, Li Chen, Michael E. Kounavis, Duen Horng Chau


## 2017-05-05

[Detecting Adversarial Samples Using Density Ratio Estimates.](http://arxiv.org/abs/1705.02224)

Lovedeep Gondara


## 2017-04-28

[Yes, Machine Learning Can Be More Secure! A Case Study on Android Malware Detection.](http://arxiv.org/abs/1704.08996)

Ambra Demontis, Marco Melis, Battista Biggio, Davide Maiorca, Daniel Arp, Konrad Rieck, Igino Corona, Giorgio Giacinto, Fabio Roli


[Parseval Networks: Improving Robustness to Adversarial Examples.](http://arxiv.org/abs/1704.08847)

Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, Nicolas Usunier


## 2017-04-26

[Deep Text Classification Can be Fooled.](http://arxiv.org/abs/1704.08006)

Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li, Wenchang Shi


## 2017-04-19

[Universal Adversarial Perturbations Against Semantic Image Segmentation.](http://arxiv.org/abs/1704.05712)

Jan Hendrik Metzen, Mummadi Chaithanya Kumar, Thomas Brox, Volker Fischer


## 2017-04-17

[Adversarial and Clean Data Are Not Twins.](http://arxiv.org/abs/1704.04960)

Zhitao Gong, Wenlu Wang, Wei-Shinn Ku


## 2017-04-16

[Google's Cloud Vision API Is Not Robust To Noise.](http://arxiv.org/abs/1704.05051)

Hossein Hosseini, Baicen Xiao, Radha Poovendran


## 2017-04-11

[The Space of Transferable Adversarial Examples.](http://arxiv.org/abs/1704.03453)

Florian Tramèr, Nicolas Papernot, Ian Goodfellow, Dan Boneh, Patrick McDaniel


[Interpretable Explanations of Black Boxes by Meaningful Perturbation. (1%)](http://arxiv.org/abs/1704.03296)

Ruth Fong, Andrea Vedaldi


## 2017-04-09

[Enhancing Robustness of Machine Learning Systems via Data Transformations.](http://arxiv.org/abs/1704.02654)

Arjun Nitin Bhagoji, Daniel Cullina, Chawin Sitawarin, Prateek Mittal


## 2017-04-06

[Adequacy of the Gradient-Descent Method for Classifier Evasion Attacks.](http://arxiv.org/abs/1704.01704)

Yi Han, Benjamin I. P. Rubinstein


## 2017-04-05

[Comment on "Biologically inspired protection of deep networks from adversarial attacks".](http://arxiv.org/abs/1704.01547)

Wieland Brendel, Matthias Bethge


## 2017-04-04

[Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks.](http://arxiv.org/abs/1704.01155)

Weilin Xu, David Evans, Yanjun Qi


## 2017-03-31

[SafetyNet: Detecting and Rejecting Adversarial Examples Robustly.](http://arxiv.org/abs/1704.00103)

Jiajun Lu, Theerasit Issaranon, David Forsyth


## 2017-03-27

[Adversarial Transformation Networks: Learning to Generate Adversarial Examples.](http://arxiv.org/abs/1703.09387)

Shumeet Baluja, Ian Fischer


[Biologically inspired protection of deep networks from adversarial attacks.](http://arxiv.org/abs/1703.09202)

Aran Nayebi, Surya Ganguli


## 2017-03-26

[Deceiving Google's Cloud Video Intelligence API Built for Summarizing Videos.](http://arxiv.org/abs/1703.09793)

Hossein Hosseini, Baicen Xiao, Radha Poovendran


## 2017-03-24

[Adversarial Examples for Semantic Segmentation and Object Detection.](http://arxiv.org/abs/1703.08603)

Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, Alan Yuille


## 2017-03-23

[Self corrective Perturbations for Semantic Segmentation and Classification.](http://arxiv.org/abs/1703.07928)

Swami Sankaranarayanan, Arpit Jain, Ser Nam Lim


## 2017-03-22

[Data Driven Exploratory Attacks on Black Box Classifiers in Adversarial Domains.](http://arxiv.org/abs/1703.07909)

Tegjyot Singh Sethi, Mehmed Kantardzic


## 2017-03-20

[On the Limitation of Convolutional Neural Networks in Recognizing Negative Images.](http://arxiv.org/abs/1703.06857)

Hossein Hosseini, Baicen Xiao, Mayoore Jaiswal, Radha Poovendran


## 2017-03-16

[Fraternal Twins: Unifying Attacks on Machine Learning and Digital Watermarking.](http://arxiv.org/abs/1703.05561)

Erwin Quiring, Daniel Arp, Konrad Rieck


## 2017-03-13

[Blocking Transferability of Adversarial Examples in Black-Box Learning Systems.](http://arxiv.org/abs/1703.04318)

Hossein Hosseini, Yize Chen, Sreeram Kannan, Baosen Zhang, Radha Poovendran


## 2017-03-07

[Tactics of Adversarial Attack on Deep Reinforcement Learning Agents.](http://arxiv.org/abs/1703.06748)

Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, Min Sun


## 2017-03-03

[Adversarial Examples for Semantic Image Segmentation.](http://arxiv.org/abs/1703.01101)

Volker Fischer, Mummadi Chaithanya Kumar, Jan Hendrik Metzen, Thomas Brox


## 2017-03-02

[Compositional Falsification of Cyber-Physical Systems with Machine Learning Components.](http://arxiv.org/abs/1703.00978)

Tommaso Dreossi, Alexandre Donzé, Sanjit A. Seshia


## 2017-03-01

[Detecting Adversarial Samples from Artifacts.](http://arxiv.org/abs/1703.00410)

Reuben Feinman, Ryan R. Curtin, Saurabh Shintre, Andrew B. Gardner


## 2017-02-26

[Deceiving Google's Perspective API Built for Detecting Toxic Comments.](http://arxiv.org/abs/1702.08138)

Hossein Hosseini, Sreeram Kannan, Baosen Zhang, Radha Poovendran


## 2017-02-22

[Robustness to Adversarial Examples through an Ensemble of Specialists.](http://arxiv.org/abs/1702.06856)

Mahdieh Abbasi, Christian Gagné


[Adversarial examples for generative models.](http://arxiv.org/abs/1702.06832)

Jernej Kos, Ian Fischer, Dawn Song


[DeepCloak: Masking Deep Neural Network Models for Robustness Against Adversarial Samples.](http://arxiv.org/abs/1702.06763)

Ji Gao, Beilun Wang, Zeming Lin, Weilin Xu, Yanjun Qi


## 2017-02-21

[On the (Statistical) Detection of Adversarial Examples.](http://arxiv.org/abs/1702.06280)

Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, Patrick McDaniel


## 2017-02-20

[Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN.](http://arxiv.org/abs/1702.05983)

Weiwei Hu, Ying Tan


## 2017-02-14

[On Detecting Adversarial Perturbations.](http://arxiv.org/abs/1702.04267)

Jan Hendrik Metzen, Tim Genewein, Volker Fischer, Bastian Bischoff


## 2017-02-07

[Adversarial Attacks on Neural Network Policies.](http://arxiv.org/abs/1702.02284)

Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, Pieter Abbeel


## 2017-02-03

[Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks.](http://arxiv.org/abs/1702.01135)

Guy Katz, Clark Barrett, David Dill, Kyle Julian, Mykel Kochenderfer


## 2017-01-15

[Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks.](http://arxiv.org/abs/1701.04143)

Vahid Behzadan, Arslan Munir


## 2017-01-04

[Dense Associative Memory is Robust to Adversarial Inputs.](http://arxiv.org/abs/1701.00939)

Dmitry Krotov, John J Hopfield


## 2016-12-22

[Adversarial Examples Detection in Deep Networks with Convolutional Filter Statistics.](http://arxiv.org/abs/1612.07767)

Xin Li, Fuxin Li


## 2016-12-19

[Simple Black-Box Adversarial Perturbations for Deep Networks.](http://arxiv.org/abs/1612.06299)

Nina Narodytska, Shiva Prasad Kasiviswanathan


## 2016-12-05

[Learning Adversary-Resistant Deep Neural Networks.](http://arxiv.org/abs/1612.01401)

Qinglong Wang, Wenbo Guo, Kaixuan Zhang, Alexander G. II Ororbia, Xinyu Xing, Xue Liu, C. Lee Giles


## 2016-12-01

[A Theoretical Framework for Robustness of (Deep) Classifiers against Adversarial Examples.](http://arxiv.org/abs/1612.00334)

Beilun Wang, Ji Gao, Yanjun Qi


[Adversarial Images for Variational Autoencoders.](http://arxiv.org/abs/1612.00155)

Pedro Tabacof, Julia Tavares, Eduardo Valle


[Deep Variational Information Bottleneck.](http://arxiv.org/abs/1612.00410)

Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, Kevin Murphy


## 2016-11-30

[Towards Robust Deep Neural Networks with BANG.](http://arxiv.org/abs/1612.00138)

Andras Rozsa, Manuel Gunther, Terrance E. Boult


## 2016-11-18

[LOTS about Attacking Deep Features.](http://arxiv.org/abs/1611.06179)

Andras Rozsa, Manuel Günther, Terrance E. Boult


## 2016-11-15

[AdversariaLib: An Open-source Library for the Security Evaluation of Machine Learning Algorithms Under Attack.](http://arxiv.org/abs/1611.04786)

Igino Corona, Battista Biggio, Davide Maiorca


## 2016-11-11

[Towards the Science of Security and Privacy in Machine Learning.](http://arxiv.org/abs/1611.03814)

Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, Michael Wellman


## 2016-11-08

[Delving into Transferable Adversarial Examples and Black-box Attacks.](http://arxiv.org/abs/1611.02770)

Yanpei Liu, Xinyun Chen, Chang Liu, Dawn Song


## 2016-11-03

[Adversarial Machine Learning at Scale.](http://arxiv.org/abs/1611.01236)

Alexey Kurakin, Ian Goodfellow, Samy Bengio


## 2016-10-26

[Universal adversarial perturbations.](http://arxiv.org/abs/1610.08401)

Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal Frossard


## 2016-10-21

[Safety Verification of Deep Neural Networks.](http://arxiv.org/abs/1610.06940)

Xiaowei Huang, Marta Kwiatkowska, Sen Wang, Min Wu


## 2016-10-14

[Are Accuracy and Robustness Correlated?](http://arxiv.org/abs/1610.04563)

Andras Rozsa, Manuel Günther, Terrance E. Boult


## 2016-10-13

[Assessing Threat of Adversarial Examples on Deep Neural Networks.](http://arxiv.org/abs/1610.04256)

Abigail Graese, Andras Rozsa, Terrance E. Boult


## 2016-10-06

[Using Non-invertible Data Transformations to Build Adversarial-Robust Neural Networks.](http://arxiv.org/abs/1610.01934)

Qinglong Wang, Wenbo Guo, Alexander G. II Ororbia, Xinyu Xing, Lin Lin, C. Lee Giles, Xue Liu, Peng Liu, Gang Xiong


## 2016-10-04

[Adversary Resistant Deep Neural Networks with an Application to Malware Detection.](http://arxiv.org/abs/1610.01239)

Qinglong Wang, Wenbo Guo, Kaixuan Zhang, Alexander G. II Ororbia, Xinyu Xing, C. Lee Giles, Xue Liu


## 2016-10-03

[Technical Report on the CleverHans v2.1.0 Adversarial Examples Library.](http://arxiv.org/abs/1610.00768)

Nicolas Papernot, Fartash Faghri, Nicholas Carlini, Ian Goodfellow, Reuben Feinman, Alexey Kurakin, Cihang Xie, Yash Sharma, Tom Brown, Aurko Roy, Alexander Matyasko, Vahid Behzadan, Karen Hambardzumyan, Zhishuai Zhang, Yi-Lin Juang, Zhi Li, Ryan Sheatsley, Abhibhav Garg, Jonathan Uesato, Willi Gierke, Yinpeng Dong, David Berthelot, Paul Hendricks, Jonas Rauber, Rujun Long, Patrick McDaniel


## 2016-09-06

[Statistical Meta-Analysis of Presentation Attacks for Secure Multibiometric Systems.](http://arxiv.org/abs/1609.01461)

Battista Biggio, Giorgio Fumera, Gian Luca Marcialis, Fabio Roli


## 2016-09-03

[Randomized Prediction Games for Adversarial Machine Learning.](http://arxiv.org/abs/1609.00804)

Samuel Rota Bulò, Battista Biggio, Ignazio Pillai, Marcello Pelillo, Fabio Roli


## 2016-08-31

[Robustness of classifiers: from adversarial to random noise.](http://arxiv.org/abs/1608.08967)

Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard


## 2016-08-27

[A Boundary Tilting Persepective on the Phenomenon of Adversarial Examples.](http://arxiv.org/abs/1608.07690)

Thomas Tanay, Lewis Griffin


## 2016-08-16

[Towards Evaluating the Robustness of Neural Networks.](http://arxiv.org/abs/1608.04644)

Nicholas Carlini, David Wagner


## 2016-08-02

[A study of the effect of JPG compression on adversarial images.](http://arxiv.org/abs/1608.00853)

Gintare Karolina Dziugaite, Zoubin Ghahramani, Daniel M. Roy


## 2016-08-01

[Early Methods for Detecting Adversarial Images.](http://arxiv.org/abs/1608.00530)

Dan Hendrycks, Kevin Gimpel


## 2016-07-18

[On the Effectiveness of Defensive Distillation.](http://arxiv.org/abs/1607.05113)

Nicolas Papernot, Patrick McDaniel


## 2016-07-14

[Defensive Distillation is Not Robust to Adversarial Examples.](http://arxiv.org/abs/1607.04311)

Nicholas Carlini, David Wagner


## 2016-07-08

[Adversarial examples in the physical world.](http://arxiv.org/abs/1607.02533)

Alexey Kurakin, Ian Goodfellow, Samy Bengio


## 2016-06-14

[Adversarial Perturbations Against Deep Neural Networks for Malware Classification.](http://arxiv.org/abs/1606.04435)

Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, Patrick McDaniel


## 2016-05-23

[Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples.](http://arxiv.org/abs/1605.07277)

Nicolas Papernot, Patrick McDaniel, Ian Goodfellow


[Measuring Neural Net Robustness with Constraints.](http://arxiv.org/abs/1605.07262)

Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya Nori, Antonio Criminisi


## 2016-05-17

[Are Facial Attributes Adversarially Robust?](http://arxiv.org/abs/1605.05411)

Andras Rozsa, Manuel Günther, Ethan M. Rudd, Terrance E. Boult


## 2016-05-05

[Adversarial Diversity and Hard Positive Generation.](http://arxiv.org/abs/1605.01775)

Andras Rozsa, Ethan M. Rudd, Terrance E. Boult


## 2016-04-27

[Crafting Adversarial Input Sequences for Recurrent Neural Networks.](http://arxiv.org/abs/1604.08275)

Nicolas Papernot, Patrick McDaniel, Ananthram Swami, Richard Harang


## 2016-04-14

[Improving the Robustness of Deep Neural Networks via Stability Training.](http://arxiv.org/abs/1604.04326)

Stephan Zheng, Yang Song, Thomas Leung, Ian Goodfellow


## 2016-04-09

[A General Retraining Framework for Scalable Adversarial Classification.](http://arxiv.org/abs/1604.02606)

Bo Li, Yevgeniy Vorobeychik, Xinyun Chen


## 2016-03-16

[Suppressing the Unusual: towards Robust CNNs using Symmetric Activation Functions.](http://arxiv.org/abs/1603.05145)

Qiyang Zhao, Lewis D Griffin


## 2016-02-18

[Breaking Symmetric Cryptosystems using Quantum Period Finding. (1%)](http://arxiv.org/abs/1602.05973)

Marc Kaplan, Gaëtan Leurent, Anthony Leverrier, María Naya-Plasencia


## 2016-02-08

[Practical Black-Box Attacks against Machine Learning.](http://arxiv.org/abs/1602.02697)

Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, Ananthram Swami


## 2016-02-07

[Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms.](http://arxiv.org/abs/1602.02389)

Tom Zahavy, Bingyi Kang, Alex Sivak, Jiashi Feng, Huan Xu, Shie Mannor


## 2016-01-26

[Unifying Adversarial Training Algorithms with Flexible Deep Data Gradient Regularization.](http://arxiv.org/abs/1601.07213)

Alexander G. II Ororbia, C. Lee Giles, Daniel Kifer


## 2015-11-23

[The Limitations of Deep Learning in Adversarial Settings.](http://arxiv.org/abs/1511.07528)

Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, Ananthram Swami


## 2015-11-19

[A Unified Gradient Regularization Family for Adversarial Examples.](http://arxiv.org/abs/1511.06385)

Chunchuan Lyu, Kaizhu Huang, Hai-Ning Liang


[Manifold Regularized Deep Neural Networks using Adversarial Examples.](http://arxiv.org/abs/1511.06381)

Taehoon Lee, Minsuk Choi, Sungroh Yoon


[Robust Convolutional Neural Networks under Adversarial Noise.](http://arxiv.org/abs/1511.06306)

Jonghoon Jin, Aysegul Dundar, Eugenio Culurciello


[Foveation-based Mechanisms Alleviate Adversarial Examples.](http://arxiv.org/abs/1511.06292)

Yan Luo, Xavier Boix, Gemma Roig, Tomaso Poggio, Qi Zhao


[Towards Open Set Deep Networks.](http://arxiv.org/abs/1511.06233)

Abhijit Bendale, Terrance Boult


## 2015-11-17

[Understanding Adversarial Training: Increasing Local Stability of Neural Nets through Robust Optimization.](http://arxiv.org/abs/1511.05432)

Uri Shaham, Yutaro Yamada, Sahand Negahban


## 2015-11-16

[Adversarial Manipulation of Deep Representations.](http://arxiv.org/abs/1511.05122)

Sara Sabour, Yanshuai Cao, Fartash Faghri, David J. Fleet


## 2015-11-14

[DeepFool: a simple and accurate method to fool deep neural networks.](http://arxiv.org/abs/1511.04599)

Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Pascal Frossard


## 2015-11-13

[Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks.](http://arxiv.org/abs/1511.04508)

Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, Ananthram Swami


## 2015-11-10

[Learning with a Strong Adversary.](http://arxiv.org/abs/1511.03034)

Ruitong Huang, Bing Xu, Dale Schuurmans, Csaba Szepesvari


## 2015-10-18

[Exploring the Space of Adversarial Images.](http://arxiv.org/abs/1510.05328)

Pedro Tabacof, Eduardo Valle


## 2015-10-14

[Improving Back-Propagation by Adding an Adversarial Gradient.](http://arxiv.org/abs/1510.04189)

Arild Nøkland


## 2015-07-16

[Deep Learning and Music Adversaries.](http://arxiv.org/abs/1507.04761)

Corey Kereliuk, Bob L. Sturm, Jan Larsen


## 2015-02-09

[Analysis of classifiers' robustness to adversarial perturbations.](http://arxiv.org/abs/1502.02590)

Alhussein Fawzi, Omar Fawzi, Pascal Frossard


## 2014-12-19

[Explaining and Harnessing Adversarial Examples.](http://arxiv.org/abs/1412.6572)

Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy


## 2014-12-11

[Towards Deep Neural Network Architectures Robust to Adversarial Examples.](http://arxiv.org/abs/1412.5068)

Shixiang Gu, Luca Rigazio


## 2014-12-05

[Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images.](http://arxiv.org/abs/1412.1897)

Anh Nguyen, Jason Yosinski, Jeff Clune


## 2014-01-29

[Security Evaluation of Support Vector Machines in Adversarial Environments.](http://arxiv.org/abs/1401.7727)

Battista Biggio, Igino Corona, Blaine Nelson, Benjamin I. P. Rubinstein, Davide Maiorca, Giorgio Fumera, Giorgio Giacinto, and Fabio Roli


## 2013-12-20

[Intriguing properties of neural networks.](http://arxiv.org/abs/1312.6199)

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus


