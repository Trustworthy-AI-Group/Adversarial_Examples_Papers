
# A complete list of papers about adversarial examples

It appears that the [List of All Adversarial Example Papers](https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html) has been experiencing crashes over the past few days. In the absence of this valuable resource, staying up-to-date with the latest research papers in this field has become challenging. Consequently, I created a repository aimed at aggregating and maintaining the most current papers in this domain. While this repository may not encompass every paper, I did try. If you find any papers we have missed, just drop me an [email](mailto:xswanghuster@gmail.com). We have included the [data](./nicholas.md) from [List of All Adversarial Example Papers](https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html) till 2023-09-01. We also provide a list of papers about transfer-based attacks [here](https://xiaosenwang.com/transfer_based_attack_papers.html).
# 2025-01-06
+ [From Models to Network Topologies: A Topology Inference Attack in Decentralized Federated Learning](https://arxiv.org//abs/2501.03119)

	Chao Feng, Yuanzhe Gao, Alberto Huertas Celdran, Gerome Bovet, Burkhard Stiller


+ [MBTSAD: Mitigating Backdoors in Language Models Based on Token Splitting and Attention Distillation](https://arxiv.org//abs/2501.02754)

	Yidong Ding, Jiafei Niu, Ping Yi


+ [Persistence of Backdoor-based Watermarks for Neural Networks: A Comprehensive Evaluation](https://arxiv.org//abs/2501.02704)

	Anh Tu Ngo, Chuan Song Heng, Nandish Chattopadhyay, Anupam Chattopadhyay


# 2025-01-05
+ [Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for Jailbreak Attack Defense](https://arxiv.org//abs/2501.02629)

	Yang Ouyang, Hengrui Gu, Shuhang Lin, Wenyue Hua, Jie Peng, Bhavya Kailkhura, Tianlong Chen, Kaixiong Zhou


+ [Tougher Text, Smarter Models: Raising the Bar for Adversarial Defence Benchmarks](https://arxiv.org//abs/2501.02654)

	Yang Wang, Chenghua Lin


+ [GCP: Guarded Collaborative Perception with Spatial-Temporal Aware Malicious Agent Detection](https://arxiv.org//abs/2501.02450)

	Yihang Tao, Senkang Hu, Yue Hu, Haonan An, Hangcheng Cao, Yuguang Fang


# 2025-01-04
+ [AdaMixup: A Dynamic Defense Framework for Membership Inference Attack Mitigation](https://arxiv.org//abs/2501.02182)

	Ying Chen, Jiajing Chen, Yijie Weng, ChiaHua Chang, Dezhi Yu, Guanbiao Lin


+ [Distillation-Enhanced Physical Adversarial Attacks](https://arxiv.org//abs/2501.02232)

	Wei Liu, Yonglin Wu, Chaoqun Li, Zhuodong Liu, Huanqian Yan


+ [BADTV: Unveiling Backdoor Threats in Third-Party Task Vectors](https://arxiv.org//abs/2501.02373)

	Chia-Yi Hsu, Yu-Lin Tsai, Yu Zhe, Yan-Lun Chen, Chih-Hsun Lin, Chia-Mu Yu, Yang Zhang, Chun-Ying Huang, Jun Sakuma


+ [Exploring Secure Machine Learning Through Payload Injection and FGSM Attacks on ResNet-50](https://arxiv.org//abs/2501.02147)

	Umesh Yadav, Suman Niraula, Gaurav Kumar Gupta, Bicky Yadav



# 2025-01-03
+ [BLAST: A Stealthy Backdoor Leverage Attack against Cooperative Multi-Agent Deep Reinforcement Learning based Systems](https://arxiv.org//abs/2501.01593)

	Yinbo Yu, Saihao Yan, Xueyu Yin, Jing Fang, Jiajia Liu


+ [Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models](https://arxiv.org//abs/2501.01830)

	Yanjiang Liu, Shuhen Zhou, Yaojie Lu, Huijia Zhu, Weiqiang Wang, Hongyu Lin, Ben He, Xianpei Han, Le Sun


+ [Mingling with the Good to Backdoor Federated Learning](https://arxiv.org//abs/2501.01913)

	Nuno Neves


+ [Turning Logic Against Itself : Probing Model Defenses Through Contrastive Questions](https://arxiv.org//abs/2501.01872)

	Rachneet Sachdeva, Rima Hazra, Iryna Gurevych


+ [Detecting and Mitigating Adversarial Attacks on Deep Learning-Based MRI Reconstruction Without Any Retraining](https://arxiv.org//abs/2501.01908)

	Mahdi Saberi, Chi Zhang, Mehmet Akcakaya


+ [Adaptive Meta-learning-based Adversarial Training for Robust Automatic Modulation Classification](https://arxiv.org//abs/2501.01620)

	Amirmohammad Bamdad, Ali Owfi, Fatemeh Afghah


+ [Spot Risks Before Speaking! Unraveling Safety Attention Heads in Large Vision-Language Models](https://arxiv.org//abs/2501.02029)

	Ziwei Zheng, Junyao Zhao, Le Yang, Lijun He, Fan Li


+ [AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs](https://arxiv.org//abs/2501.02135)

	Sanjoy Chowdhury, Sayan Nag, Subhrajyoti Dasgupta, Yaoting Wang, Mohamed Elhoseiny, Ruohan Gao, Dinesh Manocha


+ [Towards Robust and Accurate Stability Estimation of Local Surrogate Models in Text-based Explainable AI](https://arxiv.org//abs/2501.02042)

	Christopher Burger, Charles Walter, Thai Le, Lingwei Chen



# 2025-01-02
+ [Towards Adversarially Robust Deep Metric Learning](https://arxiv.org//abs/2501.01025)

	Xiaopeng Ke


+ [Stealthy Backdoor Attack to Real-world Models in Android Apps](https://arxiv.org//abs/2501.01263)

	Jiali Wei, Ming Fan, Xicheng Zhang, Wenjing Jiao, Haijun Wang, Ting Liu


+ [Boosting Adversarial Transferability with Spatial Adversarial Alignment](https://arxiv.org//abs/2501.01015)

	Zhaoyu Chen, Haijing Guo, Kaixun Jiang, Jiyuan Fu, Xinyu Zhou, Dingkang Yang, Hao Tang, Bo Li, Wenqiang Zhang


+ [Image-based Multimodal Models as Intruders: Transferable Multimodal Attacks on Video-based MLLMs](https://arxiv.org//abs/2501.01042)

	Linhao Huang, Xue Jiang, Zhiqiang Wang, Wentao Mo, Xi Xiao, Bo Han, Yongjie Yin, Feng Zheng


+ [AIM: Additional Image Guided Generation of Transferable Adversarial Attacks](https://arxiv.org//abs/2501.01106)

	Teng Li, Xingjun Ma, Yu-Gang Jiang


+ [HoneypotNet: Backdoor Attacks Against Model Extraction](https://arxiv.org//abs/2501.01090)

	Yixu Wang, Tianle Gu, Yan Teng, Yingchun Wang, Xingjun Ma


+ [Best Transition Matrix Esitimation or Best Label Noise Robustness Classifier? Two Possible Methods to Enhance the Performance of T-revision](https://arxiv.org//abs/2501.01402)

	Haixu Liu, Zerui Tao, Naihui Zhang, Sixing Liu


+ [A Game Between the Defender and the Attacker for Trigger-based Black-box Model Watermarking](https://arxiv.org//abs/2501.01194)

	Chaoyue Huang, Hanzhou Wu


+ [Improving Robustness Estimates in Natural Language Explainable AI though Synonymity Weighted Similarity Measures](https://arxiv.org//abs/2501.01516)

	Christopher Burger


+ [SAFER: Sharpness Aware layer-selective Finetuning for Enhanced Robustness in vision transformers](https://arxiv.org//abs/2501.01529)

	Bhavna Gopal, Huanrui Yang, Mark Horton, Yiran Chen


+ [Safeguarding Large Language Models in Real-time with Tunable Safety-Performance Trade-offs](https://arxiv.org//abs/2501.02018)

	Joao Fonseca, Andrew Bell, Julia Stoyanovich


# 2025-01-01
+ [Everywhere Attack: Attacking Locally and Globally to Boost Targeted Transferability](https://arxiv.org//abs/2501.00707)

	Hui Zeng, Sanshuai Cui, Biwei Chen, Anjie Peng


+ [Dynamics of Adversarial Attacks on Large Language Model-Based Search Engines](https://arxiv.org//abs/2501.00745)

	Xiyang Hu


+ [Make Shuffling Great Again: A Side-Channel Resistant Fisher-Yates Algorithm for Protecting Neural Networks](https://arxiv.org//abs/2501.00798)

	Leonard Puškáč, Marek Benovič, Jakub Breier, Xiaolu Hou


+ [TrustRAG: Enhancing Robustness and Trustworthiness in RAG](https://arxiv.org//abs/2501.00879)

	Huichi Zhou, Kin-Hei Lee, Zhonghao Zhan, Yue Chen, Zhenhao Li


+ [Information Sifting Funnel: Privacy-preserving Collaborative Inference Against Model Inversion Attacks](https://arxiv.org//abs/2501.00824)

	Rongke Liu


+ [A Survey of Secure Semantic Communications](https://arxiv.org//abs/2501.00842)

	Rui Meng, Song Gao, Dayu Fan, Haixiao Gao, Yining Wang, Xiaodong Xu, Bizhu Wang, Suyu Lv, Zhidi Zhang, Mengying Sun, Shujun Han, Chen Dong, Xiaofeng Tao, Ping Zhang


# 2024-12-31
+ [Extending XReason: Formal Explanations for Adversarial Detection](https://arxiv.org//abs/2501.00537)

	Amira Jemaa, Adnan Rashid, Sofiene Tahar


+ [Generalizing Trust: Weak-to-Strong Trustworthiness in Language Models](https://arxiv.org//abs/2501.00418)

	Martin Pawelczyk, Lillian Sun, Zhenting Qi, Aounon Kumar, Himabindu Lakkaraju


+ [A Method for Enhancing the Safety of Large Model Generation Based on Multi-dimensional Attack and Defense](https://arxiv.org//abs/2501.00517)

	Keke Zhai


# 2024-12-29
+ [On Adversarial Robustness of Language Models in Transfer Learning](https://arxiv.org//abs/2501.00066)

	Bohdan Turbal, Anastasiia Mazur, Jiaxu Zhao, Mykola Pechenizkiy


+ [Adversarial Negotiation Dynamics in Generative Language Models](https://arxiv.org//abs/2501.00069)

	Arinbjörn Kolbeinsson, Benedikt Kolbeinsson


# 2024-12-28
+ [AdvAnchor: Enhancing Diffusion Model Unlearning with Adversarial Anchors](https://arxiv.org//abs/2501.00054)

	Mengnan Zhao, Lihe Zhang, Xingyi Yang, Tianhang Zheng, Baocai Yin


+ [LLM-Virus: Evolutionary Jailbreak Attack on Large Language Models](https://arxiv.org//abs/2501.00055)

	Miao Yu, Junfeng Fang, Yingjie Zhou, Xing Fan, Kun Wang, Shirui Pan, Qingsong Wen


+ [Learning in Multiple Spaces: Few-Shot Network Attack Detection with Metric-Fused Prototypical Networks](https://arxiv.org//abs/2501.00050)

	Fernando Martinez-Lopez, Lesther Santana, Mohamed Rahouti



# 2024-12-24
+ [Pirates of the RAG: Adaptively Attacking LLMs to Leak Knowledge Bases](https://arxiv.org//abs/2412.18295)

	Christian Di Maio, Cristian Cosci, Marco Maggini, Valentina Poggioni, Stefano Melacci


+ [Hypergraph Attacks via Injecting Homogeneous Nodes into Elite Hyperedges](https://arxiv.org//abs/2412.18365)

	Meixia He, Peican Zhu, Keke Tang, Yangming Guo


+ [Unveiling the Threat of Fraud Gangs to Graph Neural Networks: Multi-Target Graph Injection Attacks against GNN-Based Fraud Detectors](https://arxiv.org//abs/2412.18370)

	Jinhyeok Choi, Heehyeon Kim, Joyce Jiyoung Whang


+ [Robustness-aware Automatic Prompt Optimization](https://arxiv.org//abs/2412.18196)

	Zeru Shi, Zhenting Wang, Yongye Su, Weidi Luo, Fan Yang, Yongfeng Zhang


+ [AEIOU: A Unified Defense Framework against NSFW Prompts in Text-to-Image Models](https://arxiv.org//abs/2412.18123)

	Yiming Wang, Jiahao Chen, Qingming Li, Xing Yang, Shouling Ji


+ [FameBias: Embedding Manipulation Bias Attack in Text-to-Image Models](https://arxiv.org//abs/2412.18302)

	Jaechul Roh, Andrew Yuan, Jinsong Mao


+ [On the Effectiveness of Adversarial Training on Malware Classifiers](https://arxiv.org//abs/2412.18218)

	Hamid Bostani, Jacopo Cortellazzi, Daniel Arp, Fabio Pierazzi, Veelasha Moonsamy, Lorenzo Cavallaro


+ [An Empirical Analysis of Federated Learning Models Subject to Label-Flipping Adversarial Attack](https://arxiv.org//abs/2412.18507)

	Kunal Bhatnagar, Sagana Chattanathan, Angela Dang, Bhargav Eranki, Ronnit Rana, Charan Sridhar, Siddharth Vedam, Angie Yao, Mark Stamp


+ [Token Highlighter: Inspecting and Mitigating Jailbreak Prompts for Large Language Models](https://arxiv.org//abs/2412.18171)

	Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho


# 2024-12-23
+ [Retention Score: Quantifying Jailbreak Risks for Vision Language Models](https://arxiv.org//abs/2412.17544)

	Zaitang Li, Pin-Yu Chen, Tsung-Yi Ho


+ [Large Language Model Safety: A Holistic Survey](https://arxiv.org//abs/2412.17686)

	Dan Shi, Tianhao Shen, Yufei Huang, Zhigen Li, Yongqi Leng, Renren Jin, Chuang Liu, Xinwei Wu, Zishan Guo, Linhao Yu, Ling Shi, Bojian Jiang, Deyi Xiong


+ [Double Landmines: Invisible Textual Backdoor Attacks based on Dual-Trigger](https://arxiv.org//abs/2412.17531)

	Yang Hou, Qiuling Yue, Lujia Chai, Guozhao Liao, Wenbao Han, Wei Ou


+ [Emerging Security Challenges of Large Language Models](https://arxiv.org//abs/2412.17614)

	Herve Debar, Sven Dietrich, Pavel Laskov, Emil C. Lupu, Eirini Ntoutsi


+ [Learning from Mistakes: Self-correct Adversarial Training for Chinese Unnatural Text Correction](https://arxiv.org//abs/2412.17279)

	Xuan Feng, Tianlong Gu, Xiaoli Liu, Liang Chang


+ [DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM Jailbreak](https://arxiv.org//abs/2412.17522)

	Hao Wang, Hao Li, Junda Zhu, Xinyuan Wang, Chengwei Pan, MinLie Huang, Lei Sha


+ [Sensitivity Curve Maximization: Attacking Robust Aggregators in Distributed Learning](https://arxiv.org//abs/2412.17740)

	Christian A. Schroth, Stefan Vlaski, Abdelhak M. Zoubir


+ [Attack by Yourself: Effective and Unnoticeable Multi-Category Graph Backdoor Attacks with Subgraph Triggers Pool](https://arxiv.org//abs/2412.17213)

	Jiangtong Li, Dungy Liu, Dawei Cheng, Changchun Jiang


+ [EM-MIAs: Enhancing Membership Inference Attacks in Large Language Models through Ensemble Modeling](https://arxiv.org//abs/2412.17249)

	Zichen Song, Sitan Huang, Zhongfeng Kang


+ [Trading Devil RL: Backdoor attack via Stock market, Bayesian Optimization and Reinforcement Learning](https://arxiv.org//abs/2412.17908)

	Orson Mengara


# 2024-12-22
+ [Adversarial Diffusion Model for Unsupervised Domain-Adaptive Semantic Segmentation](https://arxiv.org//abs/2412.16859)

	Jongmin Yu, Zhongtian Sun, Shan Luo


+ [Preventing Non-intrusive Load Monitoring Privacy Invasion: A Precise Adversarial Attack Scheme for Networked Smart Meters](https://arxiv.org//abs/2412.16893)

	Jialing He, Jiacheng Wang, Ning Wang, Shangwei Guo, Liehuang Zhu, Dusit Niyato, Tao Xiang


+ [A Backdoor Attack Scheme with Invisible Triggers Based on Model Architecture Modification](https://arxiv.org//abs/2412.16905)

	Yuan Ma, Xu Ma, Jiankang Wei, Jinmeng Tang, Xiaoyu Zhang, Yilun Lyu, Kehao Chen, Jingtong Huang


+ [ErasableMask: A Robust and Erasable Privacy Protection Scheme against Black-box Face Recognition Models](https://arxiv.org//abs/2412.17038)

	Sipeng Shen, Yunming Zhang, Dengpan Ye, Xiuwen Shi, Long Tang, Haoran Duan, Ziyi Liu


+ [DR-Encoder: Encode Low-rank Gradients with Random Prior for Large Language Models Differentially Privately](https://arxiv.org//abs/2412.17053)

	Huiwen Wu, Deyi Zhang, Xiaohan Li, Xiaogang Xu, Jiafei Wu, Zhe Liu


+ [Robustness of Large Language Models Against Adversarial Attacks](https://arxiv.org//abs/2412.17011)

	Yiyi Tao, Yixian Shen, Hang Zhang, Yanxin Shen, Lun Wang, Chuanqi Shi, Shaoshuai Du


+ [Shaping the Safety Boundaries: Understanding and Defending Against Jailbreaks in Large Language Models](https://arxiv.org//abs/2412.17034)

	Lang Gao, Xiangliang Zhang, Preslav Nakov, Xiuying Chen


+ [NumbOD: A Spatial-Frequency Fusion Attack Against Object Detectors](https://arxiv.org//abs/2412.16955)

	Ziqi Zhou, Bowen Li, Yufei Song, Zhifei Yu, Shengshan Hu, Wei Wan, Leo Yu Zhang, Dezhong Yao, Hai Jin


+ [Breaking Barriers in Physical-World Adversarial Examples: Improving Robustness and Transferability via Robust Feature](https://arxiv.org//abs/2412.16958)

	Yichen Wang, Yuxuan Chou, Ziqi Zhou, Hangtao Zhang, Wei Wan, Shengshan Hu, Minghui Li


# 2024-12-21
+ [Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and Future Directions](https://arxiv.org//abs/2412.16504)

	Hao Du, Shang Liu, Lele Zheng, Yang Cao, Atsuyoshi Nakamura, Lei Chen


+ [TrojFlow: Flow Models are Natural Targets for Trojan Attacks](https://arxiv.org//abs/2412.16512)

	Zhengyang Qi, Xiaohua Xu


+ [POEX: Policy Executable Embodied AI Jailbreak Attacks](https://arxiv.org//abs/2412.16633)

	Xuancun Lu, Zhengxian Huang, Xinfeng Li, Xiaoyu ji, Wenyuan Xu


+ [PB-UAP: Hybrid Universal Adversarial Attack For Image Segmentation](https://arxiv.org//abs/2412.16651)

	Yufei Song, Ziqi Zhou, Minghui Li, Xianlong Wang, Menghao Deng, Wei Wan, Shengshan Hu, Leo Yu Zhang


+ [Adversarial Attack Against Images Classification based on Generative Adversarial Networks](https://arxiv.org//abs/2412.16662)

	Yahe Yang


+ [The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents](https://arxiv.org//abs/2412.16682)

	Feiran Jia, Tong Wu, Xin Qin, Anna Squicciarini


+ [Divide and Conquer: A Hybrid Strategy Defeats Multimodal Large Language Models](https://arxiv.org//abs/2412.16555)

	Yanxu Mao, Peipei Liu, Tiehan Cui, Congying Liu, Datao You


+ [Forget Vectors at Play: Universal Input Perturbations Driving Machine Unlearning in Image Classification](https://arxiv.org//abs/2412.16780)

	Changchang Sun, Ren Wang, Yihua Zhang, Jinghan Jia, Jiancheng Liu, Gaowen Liu, Sijia Liu, Yan Yan


# 2024-12-20
+ [JailPO: A Novel Black-box Jailbreak Framework via Preference Optimization against Aligned LLMs](https://arxiv.org//abs/2412.15623)

	Hongyi Li, Jiawei Ye, Jie Wu, Tianjie Yan, Chu Wang, Zhixin Li


+ [Watertox: The Art of Simplicity in Universal Attacks A Cross-Model Framework for Robust Adversarial Generation](https://arxiv.org//abs/2412.15924)

	Zhenghao Gao, Shengjie Xu, Meixi Chen, Fangyao Zhao


+ [Technical Report for ICML 2024 TiFA Workshop MLLM Attack Challenge: Suffix Injection and Projected Gradient Descent Can Easily Fool An MLLM](https://arxiv.org//abs/2412.15614)

	Yangyang Guo, Ziwei Xu, Xilie Xu, YongKang Wong, Liqiang Nie, Mohan Kankanhalli


+ [Prompt-based Unifying Inference Attack on Graph Neural Networks](https://arxiv.org//abs/2412.15735)

	Yuecen Wei, Xingcheng Fu, Lingyun Liu, Qingyun Sun, Hao Peng, Chunming Hu


+ [Meme Trojan: Backdoor Attacks Against Hateful Meme Detection via Cross-Modal Triggers](https://arxiv.org//abs/2412.15503)

	Ruofei Wang, Hongzhan Lin, Ziyuan Luo, Ka Chun Cheung, Simon See, Jing Ma, Renjie Wan


+ [PoisonCatcher: Revealing and Identifying LDP Poisoning Attacks in IIoT](https://arxiv.org//abs/2412.15704)

	Lisha Shuai, Shaofeng Tan, Nan Zhang, Jiamin Zhang, Min Zhang, Xiaolong Yang


+ [Adversarial Robustness through Dynamic Ensemble Learning](https://arxiv.org//abs/2412.16254)

	Hetvi Waghela, Jaydip Sen, Sneha Rakshit


+ [Texture- and Shape-based Adversarial Attacks for Vehicle Detection in Synthetic Overhead Imagery](https://arxiv.org//abs/2412.16358)

	Mikael Yeghiazaryan, Sai Abhishek Siddhartha Namburu, Emily Kim, Stanislav Panev, Celso de Melo, Brent Lance, Fernando De la Torre, Jessica K. Hodgins


# 2024-12-19
+ [FRIDAY: Mitigating Unintentional Facial Identity in Deepfake Detectors Guided by Facial Recognizers](https://arxiv.org//abs/2412.14623)

	Younhun Kim, Myung-Joon Kwon, Wonjun Lee, Changick Kim


+ [AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models for Autonomous Driving](https://arxiv.org//abs/2412.15206)

	Shuo Xing, Hongyuan Hua, Xiangbo Gao, Shenzhe Zhu, Renjie Li, Kexin Tian, Xiaopeng Li, Heng Huang, Tianbao Yang, Zhangyang Wang, Yang Zhou, Huaxiu Yao, Zhengzhong Tu


+ [Holistic Adversarially Robust Pruning](https://arxiv.org//abs/2412.14714)

	Qi Zhao, Christian Wressnegger


+ [Boosting GNN Performance via Training Sample Selection Based on Adversarial Robustness Evaluation](https://arxiv.org//abs/2412.14738)

	Yongyu Wang


+ [SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage](https://arxiv.org//abs/2412.15289)

	Xiaoning Dong, Wenbo Hu, Wei Xu, Tianxing He



# 2024-12-18
+ [Safeguarding System Prompts for LLMs](https://arxiv.org//abs/2412.13426)

	Zhifeng Jiang, Zhihua Jin, Guoliang He


+ [A Statistical and Multi-Perspective Revisiting of the Membership Inference Attack in Large Language Models](https://arxiv.org//abs/2412.13475)

	Bowen Chen, Namgi Han, Yusuke Miyao


+ [Mitigating Adversarial Attacks in LLMs through Defensive Suffix Generation](https://arxiv.org//abs/2412.13705)

	Minkyoung Kim, Yunha Kim, Hyeram Seo, Heejung Choi, Jiye Han, Gaeun Kee, Soyoung Ko, HyoJe Jung, Byeolhee Kim, Young-Hak Kim, Sanghyun Park, Tae Joon Jun


+ [Physics-Based Adversarial Attack on Near-Infrared Human Detector for Nighttime Surveillance Camera Systems](https://arxiv.org//abs/2412.13709)

	Muyao Niu, Zhuoxiao Li, Yifan Zhan, Huy H. Nguyen, Isao Echizen, Yinqiang Zheng


+ [A Black-Box Evaluation Framework for Semantic Robustness in Bird's Eye View Detection](https://arxiv.org//abs/2412.13913)

	Fu Wang, Yanghao Zhang, Xiangyu Yin, Guangliang Cheng, Zeyu Fu, Xiaowei Huang, Wenjie Ruan


+ [Cultivating Archipelago of Forests: Evolving Robust Decision Trees through Island Coevolution](https://arxiv.org//abs/2412.13762)

	Adam Żychowski, Andrew Perrault, Jacek Mańdziuk


+ [On the Robustness of Distributed Machine Learning against Transfer Attacks](https://arxiv.org//abs/2412.14080)

	Sébastien Andreina, Pascal Zimmer, Ghassan Karame


+ [A Review of the Duality of Adversarial Learning in Network Intrusion: Attacks and Countermeasures](https://arxiv.org//abs/2412.13880)

	Shalini Saini, Anitha Chennamaneni, Babatunde Sawyerr


+ [Adversarial Hubness in Multi-Modal Retrieval](https://arxiv.org//abs/2412.14113)

	Tingwei Zhang, Fnu Suya, Rishi Jha, Collin Zhang, Vitaly Shmatikov


+ [Exploring Query Efficient Data Generation towards Data-free Model Stealing in Hard Label Setting](https://arxiv.org//abs/2412.15276)

	Gaozheng Pei, Shaojie lyu, Ke Ma, Pinci Yang, Qianqian Xu, Yingfei Sun


# 2024-12-17
+ [Solid-SQL: Enhanced Schema-linking based In-context Learning for Robust Text-to-SQL](https://arxiv.org//abs/2412.12522)

	Geling Liu, Yunzhi Tan, Ruichao Zhong, Yuanzhen Xie, Lingchen Zhao, Qian Wang, Bo Hu, Zang Li


+ [Defending LVLMs Against Vision Attacks through Partial-Perception Supervision](https://arxiv.org//abs/2412.12722)

	Qi Zhou, Tianlin Li, Qing Guo, Dongxia Wang, Yun Lin, Yang Liu, Jin Song Dong


+ [Boosting Fine-Grained Visual Anomaly Detection with Coarse-Knowledge-Aware Adversarial Learning](https://arxiv.org//abs/2412.12850)

	Qingqing Fang, Qinliang Su, Wenxi Lv, Wenchao Xu, Jianxing Yu


+ [Human-in-the-Loop Generation of Adversarial Texts: A Case Study on Tibetan Script](https://arxiv.org//abs/2412.12478)

	Xi Cao, Yuan Sun, Jiajun Li, Quzong Gesang, Nuo Qun, Tashi Nyima


+ [NLSR: Neuron-Level Safety Realignment of Large Language Models Against Harmful Fine-Tuning](https://arxiv.org//abs/2412.12497)

	Xin Yi, Shunfan Zheng, Linlin Wang, Gerard de Melo, Xiaoling Wang, Liang He


+ [Jailbreaking? One Step Is Enough!](https://arxiv.org//abs/2412.12621)

	Weixiong Zheng, Peijian Zeng, Yiwei Li, Hongyan Wu, Nankai Lin, Junhao Chen, Aimin Yang, Yongmei Zhou


+ [Truthful Text Sanitization Guided by Inference Attacks](https://arxiv.org//abs/2412.12928)

	Ildikó Pilán, Benet Manzanares-Salor, David Sánchez, Pierre Lison


+ [Invisible Watermarks: Attacks and Robustness](https://arxiv.org//abs/2412.12511)

	Dongjun Hwang, Sungwon Woo, Tom Gao, Raymond Luo, Sunghwan Baek


+ [Improving the Transferability of 3D Point Cloud Attack via Spectral-aware Admix and Optimization Designs](https://arxiv.org//abs/2412.12626)

	Shiyu Hu, Daizong Liu, Wei Hu


+ [A New Adversarial Perspective for LiDAR-based 3D Object Detection](https://arxiv.org//abs/2412.13017)

	Shijun Zheng, Weiquan Liu, Yu Guo, Yu Zang, Siqi Shen, Cheng Wang


+ [Building Gradient Bridges: Label Leakage from Restricted Gradient Sharing in Federated Learning](https://arxiv.org//abs/2412.12640)

	Rui Zhang, Ka-Ho Chow, Ping Li


+ [Deep Learning for Resilient Adversarial Decision Fusion in Byzantine Networks](https://arxiv.org//abs/2412.12739)

	Kassem Kallas


+ [Scrutinizing the Vulnerability of Decentralized Learning to Membership Inference Attacks](https://arxiv.org//abs/2412.12837)

	Ousmane Touat, Jezekael Brunon, Yacine Belal, Julien Nicolas, Mohamed Maouche, César Sabater, Sonia Ben Mokhtar


+ [Adversarially robust generalization theory via Jacobian regularization for deep neural networks](https://arxiv.org//abs/2412.12449)

	Dongya Wu, Xin Li


+ [Practicable Black-box Evasion Attacks on Link Prediction in Dynamic Graphs -- A Graph Sequential Embedding Method](https://arxiv.org//abs/2412.13134)

	Jiate Li, Meng Pang, Binghui Wang


+ [RemoteRAG: A Privacy-Preserving LLM Cloud RAG Service](https://arxiv.org//abs/2412.12775)

	Yihang Cheng, Lan Zhang, Junyang Wang, Mu Yuan, Yunhao Yao


+ [BadSAD: Clean-Label Backdoor Attacks against Deep Semi-Supervised Anomaly Detection](https://arxiv.org//abs/2412.13324)

	He Cheng, Depeng Xu, Shuhan Yuan


+ [Targeted View-Invariant Adversarial Perturbations for 3D Object Recognition](https://arxiv.org//abs/2412.13376)

	Christian Green, Mehmet Ergezer, Abdurrahman Zeybey



# 2024-12-16
+ [Stepwise Reasoning Error Disruption Attack of LLMs](https://arxiv.org//abs/2412.11934)

	Jingyu Peng, Maolin Wang, Xiangyu Zhao, Kai Zhang, Wanyu Wang, Pengyue Jia, Qidong Liu, Ruocheng Guo, Qi Liu


+ [Red Pill and Blue Pill: Controllable Website Fingerprinting Defense via Dynamic Backdoor Learning](https://arxiv.org//abs/2412.11471)

	Siyuan Liang, Jiajun Gong, Tianmeng Fang, Aishan Liu, Tao Wang, Xianglong Liu, Xiaochun Cao, Dacheng Tao, Chang Ee-Chien


+ [Transferable Adversarial Face Attack with Text Controlled Attribute](https://arxiv.org//abs/2412.11735)

	Wenyun Li, Zheng Zhang, Xiangyuan Lan, Dongmei Jiang


+ [The Impact of Generalization Techniques on the Interplay Among Privacy, Utility, and Fairness in Image Classification](https://arxiv.org//abs/2412.11951)

	Ahmad Hassanpour, Amir Zarei, Khawla Mallat, Anderson Santana de Oliveira, Bian Yang


+ [How Private are Language Models in Abstractive Summarization?](https://arxiv.org//abs/2412.12040)

	Anthony Hughes, Nikolaos Aletras, Ning Ma


+ [Relation-Guided Adversarial Learning for Data-free Knowledge Transfer](https://arxiv.org//abs/2412.11380)

	Yingping Liang, Ying Fu


+ [Towards Adversarial Robustness of Model-Level Mixture-of-Experts Architectures for Semantic Segmentation](https://arxiv.org//abs/2412.11608)

	Svetlana Pavlitska, Enrico Eisen, J. Marius Zöllner


+ [IDProtector: An Adversarial Noise Encoder to Protect Against ID-Preserving Image Generation](https://arxiv.org//abs/2412.11638)

	Yiren Song, Pei Yang, Hai Ci, Mike Zheng Shou


+ [Vertical Federated Unlearning via Backdoor Certification](https://arxiv.org//abs/2412.11476)

	Mengde Han, Tianqing Zhu, Lefeng Zhang, Huan Huo, Wanlei Zhou


+ [Just a Simple Transformation is Enough for Data Protection in Vertical Federated Learning](https://arxiv.org//abs/2412.11689)

	Andrei Semenov, Philip Zmushko, Alexander Pichugin, Aleksandr Beznosikov


+ [Efficiently Achieving Secure Model Training and Secure Aggregation to Ensure Bidirectional Privacy-Preservation in Federated Learning](https://arxiv.org//abs/2412.11737)

	Xue Yang, Depan Peng, Yan Feng, Xiaohu Tang, Weijun Fang, Jun Shao


+ [Accurate, Robust and Privacy-Preserving Brain-Computer Interface Decoding](https://arxiv.org//abs/2412.11390)

	Xiaoqing Chen, Tianwang Jia, Dongrui Wu


+ [UIBDiffusion: Universal Imperceptible Backdoor Attack for Diffusion Models](https://arxiv.org//abs/2412.11441)

	Yuning Han, Bingyin Zhao, Rui Chu, Feng Luo, Biplab Sikdar, Yingjie Lao


+ [A Comprehensive Review of Adversarial Attacks on Machine Learning](https://arxiv.org//abs/2412.11384)

	Syed Quiser Ahmed, Bharathi Vokkaliga Ganesh, Sathyanarayana Sampath Kumar, Prakhar Mishra, Ravi Anand, Bhanuteja Akurathi


+ [Comprehensive Survey on Adversarial Examples in Cybersecurity: Impacts, Challenges, and Mitigation Strategies](https://arxiv.org//abs/2412.12217)

	Li Li


+ [Quantum Adversarial Machine Learning and Defense Strategies: Challenges and Opportunities](https://arxiv.org//abs/2412.12373)

	Eric Yocam, Anthony Rizi, Mahesh Kamepalli, Varghese Vaidyan, Yong Wang, Gurcan Comert


# 2024-12-15
+ [Impact of Adversarial Attacks on Deep Learning Model Explainability](https://arxiv.org//abs/2412.11119)

	Gazi Nazia Nur, Mohammad Ahnaf Sadat


+ [Unpacking the Resilience of SNLI Contradiction Examples to Attacks](https://arxiv.org//abs/2412.11172)

	Chetan Verma, Archit Agarwal


+ [Sequence-Level Analysis of Leakage Risk of Training Data in Large Language Models](https://arxiv.org//abs/2412.11302)

	Trishita Tiwari, G. Edward Suh


+ [Learning Robust and Privacy-Preserving Representations via Information Theory](https://arxiv.org//abs/2412.11066)

	Binghui Zhang, Sayedeh Leila Noorbakhsh, Yun Dong, Yuan Hong, Binghui Wang


+ [PGD-Imp: Rethinking and Unleashing Potential of Classic PGD with Dual Strategies for Imperceptible Adversarial Attacks](https://arxiv.org//abs/2412.11168)

	Jin Li, Zitong Yu, Ziqiang He, Z. Jane Wang, Xiangui Kang


+ [Finding a Wolf in Sheep's Clothing: Combating Adversarial Text-To-Image Prompts with Text Summarization](https://arxiv.org//abs/2412.12212)

	Portia Cooper, Harshita Narnoli, Mihai Surdeanu


# 2024-12-14
+ [Are Language Models Agnostic to Linguistically Grounded Perturbations? A Case Study of Indic Languages](https://arxiv.org//abs/2412.10805)

	Poulami Ghosh, Raj Dabre, Pushpak Bhattacharyya


+ [One Pixel is All I Need](https://arxiv.org//abs/2412.10681)

	Deng Siqin, Zhou Xiaoyi


+ [Centaur: Bridging the Impossible Trinity of Privacy, Efficiency, and Performance in Privacy-Preserving Transformer Inference](https://arxiv.org//abs/2412.10652)

	Jinglong Luo, Guanzhong Chen, Yehong Zhang, Shiyu Liu, Hui Wang, Yue Yu, Xun Zhou, Yuan Qi, Zenglin Xu


+ [Improving Graph Neural Networks via Adversarial Robustness Evaluation](https://arxiv.org//abs/2412.10850)

	Yongyu Wang


+ [Towards Action Hijacking of Large Language Model-based Agent](https://arxiv.org//abs/2412.10807)

	Yuyang Zhang, Kangjie Chen, Xudong Jiang, Yuxiang Sun, Run Wang, Lina Wang


+ [TrendSim: Simulating Trending Topics in Social Media Under Poisoning Attacks with LLM-based Multi-agent System](https://arxiv.org//abs/2412.12196)

	Zeyu Zhang, Jianxun Lian, Chen Ma, Yaning Qu, Ye Luo, Lei Wang, Rui Li, Xu Chen, Yankai Lin, Le Wu, Xing Xie, Ji-Rong Wen


+ [BlockDoor: Blocking Backdoor Based Watermarks in Deep Neural Networks](https://arxiv.org//abs/2412.12194)

	Yi Hao Puah, Anh Tu Ngo, Nandish Chattopadhyay, Anupam Chattopadhyay



# 2024-12-13
+ [BiCert: A Bilinear Mixed Integer Programming Formulation for Precise Certified Bounds Against Data Poisoning Attacks](https://arxiv.org//abs/2412.10186)

	Tobias Lorenz, Marta Kwiatkowska, Mario Fritz


+ [From Allies to Adversaries: Manipulating LLM Tool-Calling through Adversarial Injection](https://arxiv.org//abs/2412.10198)

	Haowei Wang, Rupeng Zhang, Junjie Wang, Mingyang Li, Yuekai Huang, Dandan Wang, Qing Wang


+ [Targeted Angular Reversal of Weights (TARS) for Knowledge Removal in Large Language Models](https://arxiv.org//abs/2412.10257)

	Harry J. Davies, Giorgos Iacovides, Danilo P. Mandic


+ [AdvPrefix: An Objective for Nuanced LLM Jailbreaks](https://arxiv.org//abs/2412.10321)

	Sicheng Zhu, Brandon Amos, Yuandong Tian, Chuan Guo, Ivan Evtimov


+ [Real-time Identity Defenses against Malicious Personalization of Diffusion Models](https://arxiv.org//abs/2412.09844)

	Hanzhong Guo, Shen Nie, Chao Du, Tianyu Pang, Hao Sun, Chongxuan Li


+ [Prompt2Perturb (P2P): Text-Guided Diffusion-Based Adversarial Attacks on Breast Ultrasound Images](https://arxiv.org//abs/2412.09910)

	Yasamin Medghalchi, Moein Heidari, Clayton Allard, Leonid Sigal, Ilker Hacihaliloglu


+ [FaceShield: Defending Facial Image against Deepfake Threats](https://arxiv.org//abs/2412.09921)

	Jaehwan Jeong, Sumin In, Sieun Kim, Hannie Shin, Jongheon Jeong, Sang Ho Yoon, Jaewook Chung, Sangpil Kim


+ [$\textrm{A}^{\textrm{2}}$RNet: Adversarial Attack Resilient Network for Robust Infrared and Visible Image Fusion](https://arxiv.org//abs/2412.09954)

	Jiawei Li, Hongwei Yu, Jiansheng Chen, Xinlong Ding, Jinlong Wang, Jinyuan Liu, Bochao Zou, Huimin Ma


+ [Robust image classification with multi-modal large language models](https://arxiv.org//abs/2412.10353)

	Francesco Villani, Igor Maljkovic, Dario Lazzaro, Angelo Sotgiu, Antonio Emanuele Cinà, Fabio Roli


+ [Adversarial Robustness of Bottleneck Injected Deep Neural Networks for Task-Oriented Communication](https://arxiv.org//abs/2412.10265)

	Alireza Furutanpey, Pantelis A. Frangoudis, Patrik Szabo, Schahram Dustdar


+ [On Adversarial Robustness and Out-of-Distribution Robustness of Large Language Models](https://arxiv.org//abs/2412.10535)

	April Yang, Jordan Tab, Parth Shah, Paul Kotchavong


+ [Too Big to Fool: Resisting Deception in Language Models](https://arxiv.org//abs/2412.10558)

	Mohammad Reza Samsami, Mats Leon Richter, Juan Rodriguez, Megh Thakkar, Sarath Chandar, Maxime Gasse


+ [Client-Side Patching against Backdoor Attacks in Federated Learning](https://arxiv.org//abs/2412.10605)

	Borja Molina Coronado


+ [BinarySelect to Improve Accessibility of Black-Box Attack Research](https://arxiv.org//abs/2412.10617)

	Shatarupa Ghosh, Jonathan Rusert


+ [Err on the Side of Texture: Texture Bias on Real Data](https://arxiv.org//abs/2412.10597)

	Blaine Hoak, Ryan Sheatsley, Patrick McDaniel


+ [No Free Lunch for Defending Against Prefilling Attack by In-Context Learning](https://arxiv.org//abs/2412.12192)

	Zhiyu Xue, Guangliang Liu, Bocheng Chen, Kristen Marie Johnson, Ramtin Pedarsani



# 2024-12-12
+ [SVasP: Self-Versatility Adversarial Style Perturbation for Cross-Domain Few-Shot Learning](https://arxiv.org//abs/2412.09073)

	Wenqian Li, Pengfei Fang, Hui Xue


+ [Evaluating Adversarial Attacks on Traffic Sign Classifiers beyond Standard Baselines](https://arxiv.org//abs/2412.09150)

	Svetlana Pavlitska, Leopold Müller, J. Marius Zöllner


+ [Obfuscated Activations Bypass LLM Latent-Space Defenses](https://arxiv.org//abs/2412.09565)

	Luke Bailey, Alex Serrano, Abhay Sheshadri, Mikhail Seleznyov, Jordan Taylor, Erik Jenner, Jacob Hilton, Stephen Casper, Carlos Guestrin, Scott Emmons


+ [Deep Learning Model Security: Threats and Defenses](https://arxiv.org//abs/2412.08969)

	Tianyang Wang, Ziqian Bi, Yichao Zhang, Ming Liu, Weiche Hsieh, Pohsun Feng, Lawrence K.Q. Yan, Yizhu Wen, Benji Peng, Junyu Liu, Keyu Chen, Sen Zhang, Ming Li, Chuanqi Jiang, Xinyuan Song, Junjie Yang, Bowen Jing, Jintao Ren, Junhao Song, Hong-Ming Tseng, Silin Chen, Yunze Wang, Chia Xin Liang, Jiawei Xu, Xuanhe Pan, Jinlang Wang, Qian Niu


+ [On the Generation and Removal of Speaker Adversarial Perturbation for Voice-Privacy Protection](https://arxiv.org//abs/2412.09195)

	Chenyang Guo, Liping Chen, Zhuhai Li, Kong Aik Lee, Zhen-Hua Ling, Wu Guo


+ [A Semi Black-Box Adversarial Bit-Flip Attack with Limited DNN Model Information](https://arxiv.org//abs/2412.09450)

	Behnam Ghavami, Mani Sadati, Mohammad Shahidzadeh, Lesley Shannon, Steve Wilton


+ [AI Red-Teaming is a Sociotechnical System. Now What?](https://arxiv.org//abs/2412.09751)

	Tarleton Gillespie, Ryland Shaw, Mary L. Gray, Jina Suh


+ [TOAP: Towards Better Robustness in Universal Transferable Anti-Facial Retrieval](https://arxiv.org//abs/2412.09692)

	Yunna Lv, Long Tang, Dengpan Ye, Caiyun Xie, Jiacheng Deng, Yiheng He


# 2024-12-11
+ [MAGIC: Mastering Physical Adversarial Generation in Context through Collaborative LLM Agents](https://arxiv.org//abs/2412.08014)

	Yun Xing, Nhat Chung, Jie Zhang, Yue Cao, Ivor Tsang, Yang Liu, Lei Ma, Qing Guo


+ [DynamicPAE: Generating Scene-Aware Physical Adversarial Examples in Real-Time](https://arxiv.org//abs/2412.08053)

	Jin Hu, Xianglong Liu, Jiakai Wang, Junkai Zhang, Xianqi Yang, Haotong Qin, Yuqing Ma, Ke Xu


+ [Adversarial Vulnerabilities in Large Language Models for Time Series Forecasting](https://arxiv.org//abs/2412.08099)

	Fuqiang Liu, Sicong Jiang, Luis Miranda-Moreno, Seongjin Choi, Lijun Sun


+ [Antelope: Potent and Concealed Jailbreak Attack Strategy](https://arxiv.org//abs/2412.08156)

	Xin Zhao, Xiaojun Chen, Haoyu Gao


+ [How Does the Smoothness Approximation Method Facilitate Generalization for Federated Adversarial Learning?](https://arxiv.org//abs/2412.08282)

	Wenjun Ding, Ying An, Lixing Chen, Shichao Kan, Fan Wu, Zhe Qu


+ [AdvWave: Stealthy Adversarial Jailbreak Attack against Large Audio-Language Models](https://arxiv.org//abs/2412.08608)

	Mintong Kang, Chejian Xu, Bo Li


+ [Exploiting the Index Gradients for Optimization-Based Jailbreaking on Large Language Models](https://arxiv.org//abs/2412.08615)

	Jiahui Li, Yongchang Hao, Haoyu Xu, Xing Wang, Yu Hong


+ [Doubly-Universal Adversarial Perturbations: Deceiving Vision-Language Models Across Both Images and Text with a Single Perturbation](https://arxiv.org//abs/2412.08108)

	Hee-Seon Kim, Minbeom Kim, Changick Kim


+ [Enhancing Remote Adversarial Patch Attacks on Face Detectors with Tiling and Scaling](https://arxiv.org//abs/2412.07996)

	Masora Okano, Koichi Ito, Masakatsu Nishigaki, Tetsushi Ohki


+ [Local Features Meet Stochastic Anonymization: Revolutionizing Privacy-Preserving Face Recognition for Black-Box Models](https://arxiv.org//abs/2412.08276)

	Yuanwei Liu, Chengyu Jia, Ruqi Xiao, Xuemai Jia, Hui Wei, Kui Jiang, Zheng Wang


+ [Backdoor attacks on DNN and GBDT -- A Case Study from the insurance domain](https://arxiv.org//abs/2412.08366)

	Robin Kühlem, Daniel Otten, Daniel Ludwig, Anselm Hudde, Alexander Rosenbaum, Andreas Mauthe


+ [Adversarial Purification by Consistency-aware Latent Space Optimization on Data Manifolds](https://arxiv.org//abs/2412.08394)

	Shuhai Zhang, Jiahao Yang, Hui Luo, Jie Chen, Li Wang, Feng Liu, Bo Han, Mingkui Tan


+ [Training Data Reconstruction: Privacy due to Uncertainty?](https://arxiv.org//abs/2412.08544)

	Christina Runkel, Kanchana Vaishnavi Gandikota, Jonas Geiping, Carola-Bibiane Schönlieb, Michael Moeller


+ [Grimm: A Plug-and-Play Perturbation Rectifier for Graph Neural Networks Defending against Poisoning Attacks](https://arxiv.org//abs/2412.08555)

	Ao Liu, Wenshan Li, Beibei Li, Wengang Ma, Tao Li, Pan Zhou


+ [Underestimated Privacy Risks for Minority Populations in Large Language Model Unlearning](https://arxiv.org//abs/2412.08559)

	Rongzhe Wei, Mufei Li, Mohsen Ghassemi, Eleonora Kreačić, Yifan Li, Xiang Yue, Bo Li, Vamsi K. Potluru, Pan Li, Eli Chien


+ [Model-Editing-Based Jailbreak against Safety-aligned Large Language Models](https://arxiv.org//abs/2412.08201)

	Yuxi Li, Zhibo Zhang, Kailong Wang, Ling Shi, Haoyu Wang


+ [Proactive Adversarial Defense: Harnessing Prompt Tuning in Vision-Language Models to Detect Unseen Backdoored Images](https://arxiv.org//abs/2412.08755)

	Kyle Stein, Andrew Arash Mahyari, Guillermo Francia, Eman El-Sheikh



# 2024-12-10
+ [Defensive Dual Masking for Robust Adversarial Defense](https://arxiv.org//abs/2412.07078)

	Wangli Yang, Jie Yang, Yi Guo, Johan Barthelemy


+ [On Evaluating the Durability of Safeguards for Open-Weight LLMs](https://arxiv.org//abs/2412.07097)

	Xiangyu Qi, Boyi Wei, Nicholas Carlini, Yangsibo Huang, Tinghao Xie, Luxi He, Matthew Jagielski, Milad Nasr, Prateek Mittal, Peter Henderson


+ [Buster: Incorporating Backdoor Attacks into Text Encoder to Mitigate NSFW Content Generation](https://arxiv.org//abs/2412.07249)

	Xin Zhao, Xiaojun Chen, Yuexin Xuan, Zhendong Zhao


+ [Tazza: Shuffling Neural Network Parameters for Secure and Private Federated Learning](https://arxiv.org//abs/2412.07454)

	Kichang Lee, Jaeho Jin, JaeYeon Park, JeongGil Ko


+ [FlexLLM: Exploring LLM Customization for Moving Target Defense on Black-Box LLMs Against Jailbreak Attacks](https://arxiv.org//abs/2412.07672)

	Bocheng Chen, Hanqing Guo, Qiben Yan


+ [A Parametric Approach to Adversarial Augmentation for Cross-Domain Iris Presentation Attack Detection](https://arxiv.org//abs/2412.07199)

	Debasmita Pal, Redwan Sony, Arun Ross


+ [CapGen:An Environment-Adaptive Generator of Adversarial Patches](https://arxiv.org//abs/2412.07253)

	Chaoqun Li, Zhuodong Liu, Huanqian Yan, Hang Su


+ [Backdoor Attacks against No-Reference Image Quality Assessment Models via A Scalable Trigger](https://arxiv.org//abs/2412.07277)

	Yi Yu, Song Xia, Xun Lin, Wenhan Yang, Shijian Lu, Yap-peng Tan, Alex Kot


+ [Stealthy and Robust Backdoor Attack against 3D Point Clouds through Additional Point Features](https://arxiv.org//abs/2412.07511)

	Xiaoyang Ning, Qing Xie, Jinyu Xu, Wenbo Jiang, Jiachen Li, Yanchun Ma


+ [A New Federated Learning Framework Against Gradient Inversion Attacks](https://arxiv.org//abs/2412.07187)

	Pengxin Guo, Shuang Zeng, Wenhao Chen, Xiaodan Zhang, Weihong Ren, Yuyin Zhou, Liangqiong Qu


+ [Addressing Key Challenges of Adversarial Attacks and Defenses in the Tabular Domain: A Methodological Framework for Coherence and Consistency](https://arxiv.org//abs/2412.07326)

	Yael Itzhakev, Amit Giloni, Yuval Elovici, Asaf Shabtai


+ [AHSG: Adversarial Attacks on High-level Semantics in Graph Neural Networks](https://arxiv.org//abs/2412.07468)

	Kai Yuan, Xiaobing Pei, Haoran Yang


+ [Adaptive Epsilon Adversarial Training for Robust Gravitational Wave Parameter Estimation Using Normalizing Flows](https://arxiv.org//abs/2412.07559)

	Yiqian Yang, Xihua Zhu, Fan Zhang


+ [PrisonBreak: Jailbreaking Large Language Models with Fewer Than Twenty-Five Targeted Bit-flips](https://arxiv.org//abs/2412.07192)

	Zachary Coalson, Jeonghyun Woo, Shiyang Chen, Yu Sun, Lishan Yang, Prashant Nair, Bo Fang, Sanghyun Hong


+ [Adversarial Filtering Based Evasion and Backdoor Attacks to EEG-Based Brain-Computer Interfaces](https://arxiv.org//abs/2412.07231)

	Lubin Meng, Xue Jiang, Xiaoqing Chen, Wenzhong Liu, Hanbin Luo, Dongrui Wu


+ [Defending Against Neural Network Model Inversion Attacks via Data Poisoning](https://arxiv.org//abs/2412.07575)

	Shuai Zhou, Dayong Ye, Tianqing Zhu, Wanlei Zhou


+ [Adversarial Autoencoders in Operator Learning](https://arxiv.org//abs/2412.07811)

	Dustin Enyeart, Guang Lin



# 2024-12-09
+ [Enhancing Adversarial Resistance in LLMs with Recursion](https://arxiv.org//abs/2412.06181)

	Bryan Li, Sounak Bagchi, Zizhan Wang


+ [A Real-Time Defense Against Object Vanishing Adversarial Patch Attacks for Object Detection in Autonomous Vehicles](https://arxiv.org//abs/2412.06215)

	Jaden Mu


+ [Data Free Backdoor Attacks](https://arxiv.org//abs/2412.06219)

	Bochuan Cao, Jinyuan Jia, Chuxuan Hu, Wenbo Guo, Zhen Xiang, Jinghui Chen, Bo Li, Dawn Song


+ [An Effective and Resilient Backdoor Attack Framework against Deep Neural Networks and Vision Transformers](https://arxiv.org//abs/2412.06149)

	Xueluan Gong, Bowei Tian, Meng Xue, Yuan Wu, Yanjiao Chen, Qian Wang


+ [Take Fake as Real: Realistic-like Robust Black-box Adversarial Attack to Evade AIGC Detection](https://arxiv.org//abs/2412.06727)

	Caiyun Xie, Dengpan Ye, Yunming Zhang, Long Tang, Yunna Lv, Jiacheng Deng, Jiawei Song


+ [Membership Inference Attacks and Defenses in Federated Learning: A Survey](https://arxiv.org//abs/2412.06157)

	Li Bai, Haibo Hu, Qingqing Ye, Haoyang Li, Leixia Wang, Jianliang Xu


# 2024-12-08
+ [Large Language Models Merging for Enhancing the Link Stealing Attack on Graph Neural Networks](https://arxiv.org//abs/2412.05830)

	Faqian Guan, Tianqing Zhu, Wenhan Chang, Wei Ren, Wanlei Zhou


+ [BAMBA: A Bimodal Adversarial Multi-Round Black-Box Jailbreak Attacker for LVLMs](https://arxiv.org//abs/2412.05892)

	Ruoxi Cheng, Yizhong Ding, Shuirong Cao, Shaowei Yuan, Zhiqiang Wang, Xiaojun Jia


+ [Heuristic-Induced Multimodal Risk Distribution Jailbreak Attack for Multimodal Large Language Models](https://arxiv.org//abs/2412.05934)

	Ma Teng, Jia Xiaojun, Duan Ranjie, Li Xinfeng, Huang Yihao, Chu Zhixuan, Liu Yang, Ren Wenqi


+ [Trust No AI: Prompt Injection Along The CIA Security Triad](https://arxiv.org//abs/2412.06090)

	Johann Rehberger (Independent Researcher, Embrace The Red)


+ [Adversarial Transferability in Deep Denoising Models: Theoretical Insights and Robustness Enhancement via Out-of-Distribution Typical Set Sampling](https://arxiv.org//abs/2412.05943)

	Jie Ning, Jiebao Sun, Shengzhu Shi, Zhichang Guo, Yao Li, Hongwei Li, Boying Wu


+ [Anti-Reference: Universal and Immediate Defense Against Reference-Based Generation](https://arxiv.org//abs/2412.05980)

	Yiren Song, Shengtao Lou, Xiaokang Liu, Hai Ci, Pei Yang, Jiaming Liu, Mike Zheng Shou


+ [DeMem: Privacy-Enhanced Robust Adversarial Learning via De-Memorization](https://arxiv.org//abs/2412.05767)

	Xiaoyu Luo, Qiongxiu Li


+ [Understanding the Impact of Graph Reduction on Adversarial Robustness in Graph Neural Networks](https://arxiv.org//abs/2412.05883)

	Kerui Wu, Ka-Ho Chow, Wenqi Wei, Lei Yu


+ [Perceptual Hash Inversion Attacks on Image-Based Sexual Abuse Removal Tools](https://arxiv.org//abs/2412.06056)

	Sophie Hawkes, Christian Weinert, Teresa Almeida, Maryam Mehrnezhad


# 2024-12-07
+ [PrivAgent: Agentic-based Red-teaming for LLM Privacy Leakage](https://arxiv.org//abs/2412.05734)

	Yuzhou Nie, Zhun Wang, Ye Yu, Xian Wu, Xuandong Zhao, Wenbo Guo, Dawn Song


+ [Uncovering Vision Modality Threats in Image-to-Image Tasks](https://arxiv.org//abs/2412.05538)

	Hao Cheng, Erjia Xiao, Jiayan Yang, Jiahang Cao, Qiang Zhang, Jize Zhang, Kaidi Xu, Jindong Gu, Renjing Xu


+ [Nearly Solved? Robust Deepfake Detection Requires More than Visual Forensics](https://arxiv.org//abs/2412.05676)

	Guy Levy, Nathan Liebmann



# 2024-12-06
+ [Backdooring Outlier Detection Methods: A Novel Attack Approach](https://arxiv.org//abs/2412.05010)

	ZeinabSadat Taghavi, Hossein Mirzaei


+ [A Practical Examination of AI-Generated Text Detectors for Large Language Models](https://arxiv.org//abs/2412.05139)

	Brian Tufts, Xuandong Zhao, Lei Li


+ [LIAR: Leveraging Alignment (Best-of-N) to Jailbreak LLMs in Seconds](https://arxiv.org//abs/2412.05232)

	James Beetham, Souradip Chakraborty, Mengdi Wang, Furong Huang, Amrit Singh Bedi, Mubarak Shah


+ [Megatron: Evasive Clean-Label Backdoor Attacks against Vision Transformer](https://arxiv.org//abs/2412.04776)

	Xueluan Gong, Bowei Tian, Meng Xue, Shuike Li, Yanjiao Chen, Qian Wang


+ [SleeperMark: Towards Robust Watermark against Fine-Tuning Text-to-image Diffusion Models](https://arxiv.org//abs/2412.04852)

	Zilan Wang, Junfeng Guo, Jiacheng Zhu, Yiming Li, Heng Huang, Muhao Chen, Zhengzhong Tu


+ [Privacy Drift: Evolving Privacy Concerns in Incremental Learning](https://arxiv.org//abs/2412.05183)

	Sayyed Farid Ahamed, Soumya Banerjee, Sandip Roy, Aayush Kapoor, Marc Vucovich, Kevin Choi, Abdul Rahman, Edward Bowen, Sachin Shetty


+ [A Differentially Private Kaplan-Meier Estimator for Privacy-Preserving Survival Analysis](https://arxiv.org//abs/2412.05164)

	Narasimha Raghavan Veeraragavan, Sai Praneeth Karimireddy, Jan Franz Nygård


+ [Towards Predicting the Success of Transfer-based Attacks by Quantifying Shared Feature Representations](https://arxiv.org//abs/2412.05351)

	Ashley S. Dale, Mei Qiu, Foo Bin Che, Thomas Bsaibes, Lauren Christopher, Paul Salama


+ [BadGPT-4o: stripping safety finetuning from GPT models](https://arxiv.org//abs/2412.05346)

	Ekaterina Krupkina, Dmitrii Volkov



# 2024-12-04
+ [Less is More: A Stealthy and Efficient Adversarial Attack Method for DRL-based Autonomous Driving Policies](https://arxiv.org//abs/2412.03051)

	Junchao Fan, Xuyang Lei, Xiaolin Chang, Jelena Mišić, Vojislav B. Mišić


+ [Does Safety Training of LLMs Generalize to Semantically Related Natural Prompts?](https://arxiv.org//abs/2412.03235)

	Sravanti Addepalli, Yerram Varun, Arun Suggala, Karthikeyan Shanmugam, Prateek Jain


+ [Black-Box Forgery Attacks on Semantic Watermarks for Diffusion Models](https://arxiv.org//abs/2412.03283)

	Andreas Müller, Denis Lukovnikov, Jonas Thietke, Asja Fischer, Erwin Quiring


+ [PBP: Post-training Backdoor Purification for Malware Classifiers](https://arxiv.org//abs/2412.03441)

	Dung Thuy Nguyen, Ngoc N. Tran, Taylor T. Johnson, Kevin Leach


+ [NODE-AdvGAN: Improving the transferability and perceptual similarity of adversarial examples by dynamic-system-driven adversarial generative model](https://arxiv.org//abs/2412.03539)

	Xinheng Xie, Yue Wu, Cuiyu He


+ [Best-of-N Jailbreaking](https://arxiv.org//abs/2412.03556)

	John Hughes, Sara Price, Aengus Lynch, Rylan Schaeffer, Fazl Barez, Sanmi Koyejo, Henry Sleight, Erik Jones, Ethan Perez, Mrinank Sharma


+ [Pre-trained Multiple Latent Variable Generative Models are good defenders against Adversarial Attacks](https://arxiv.org//abs/2412.03453)

	Dario Serez, Marco Cristani, Alessio Del Bue, Vittorio Murino, Pietro Morerio



# 2024-12-03
+ [Trust & Safety of LLMs and LLMs in Trust & Safety](https://arxiv.org//abs/2412.02113)

	Doohee You, Dan Chon


+ [Jailbreak Defense in a Narrow Domain: Limitations of Existing Methods and a New Transcript-Classifier Approach](https://arxiv.org//abs/2412.02159)

	Tony T. Wang, John Hughes, Henry Sleight, Rylan Schaeffer, Rajashree Agrawal, Fazl Barez, Mrinank Sharma, Jesse Mu, Nir Shavit, Ethan Perez


+ [Sustainable Self-evolution Adversarial Training](https://arxiv.org//abs/2412.02270)

	Wenxuan Wang, Chenglei Wang, Huihui Qi, Menghao Ye, Xuelin Qian, Peng Wang, Yanning Zhang


+ [Gracefully Filtering Backdoor Samples for Generative Large Language Models without Retraining](https://arxiv.org//abs/2412.02454)

	Zongru Wu, Pengzhou Cheng, Lingyong Fang, Zhuosheng Zhang, Gongshen Liu


+ [Pay Attention to the Robustness of Chinese Minority Language Models! Syllable-level Textual Adversarial Attack on Tibetan Script](https://arxiv.org//abs/2412.02323)

	Xi Cao, Dolma Dawa, Nuo Qun, Trashi Nyima


+ [Multi-Granularity Tibetan Textual Adversarial Attack Method Based on Masked Language Model](https://arxiv.org//abs/2412.02343)

	Xi Cao, Nuo Qun, Quzong Gesang, Yulei Zhu, Trashi Nyima


+ [TSCheater: Generating High-Quality Tibetan Adversarial Texts via Visual Similarity](https://arxiv.org//abs/2412.02371)

	Xi Cao, Quzong Gesang, Yuan Sun, Nuo Qun, Tashi Nyima


+ [Underload: Defending against Latency Attacks for Object Detectors on Edge Devices](https://arxiv.org//abs/2412.02171)

	Tianyi Wang, Zichen Wang, Cong Wang, Yuanchao Shu, Ruilong Deng, Peng Cheng, Jiming Chen (Zhejiang University, Hangzhou, China)


+ [Defending Against Diverse Attacks in Federated Learning Through Consensus-Based Bi-Level Optimization](https://arxiv.org//abs/2412.02535)

	Nicolás García Trillos, Aditya Kumar Akash, Sixu Li, Konstantin Riedl, Yuhua Zhu


+ [The Efficacy of Transfer-based No-box Attacks on Image Watermarking: A Pragmatic Analysis](https://arxiv.org//abs/2412.02576)

	Qilong Wu, Varun Chandrasekaran


+ [Gaussian Splatting Under Attack: Investigating Adversarial Noise in 3D Objects](https://arxiv.org//abs/2412.02803)

	Abdurrahman Zeybey, Mehmet Ergezer, Tommy Nguyen


+ [Hijacking Vision-and-Language Navigation Agents with Adversarial Environmental Attacks](https://arxiv.org//abs/2412.02795)

	Zijiao Yang, Xiangxi Shi, Eric Slyman, Stefan Lee



# 2024-12-02
+ [CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models](https://arxiv.org//abs/2412.01528)

	Zhixiang Guo, Siyuan Liang, Aishan Liu, Dacheng Tao


+ [R.I.P.: A Simple Black-box Attack on Continual Test-time Adaptation](https://arxiv.org//abs/2412.01154)

	Trung-Hieu Hoang, Duc Minh Vo, Minh N. Do


+ [Negative Token Merging: Image-based Adversarial Feature Guidance](https://arxiv.org//abs/2412.01339)

	Jaskirat Singh, Lindsey Li, Weijia Shi, Ranjay Krishna, Yejin Choi, Pang Wei Koh, Michael F. Cohen, Stephen Gould, Liang Zheng, Luke Zettlemoyer


+ [Behavior Backdoor for Deep Learning Models](https://arxiv.org//abs/2412.01369)

	Jiakai Wang, Pengfei Zhang, Renshuai Tao, Jian Yang, Hao Liu, Xianglong Liu, Yunchao Wei, Yao Zhao


+ [Adversarial Attacks on Hyperbolic Networks](https://arxiv.org//abs/2412.01495)

	Max van Spengler, Jan Zahálka, Pascal Mettes


+ [Improved Large Language Model Jailbreak Detection via Pretrained Embeddings](https://arxiv.org//abs/2412.01547)

	Erick Galinkin, Martin Sablotny


+ [Privacy-Preserving Federated Learning via Homomorphic Adversarial Networks](https://arxiv.org//abs/2412.01650)

	Wenhan Dong, Chao Lin, Xinlei He, Xinyi Huang, Shengmin Xu


+ [Adversarial Sample-Based Approach for Tighter Privacy Auditing in Final Model-Only Scenarios](https://arxiv.org//abs/2412.01756)

	Sangyeon Yoon, Wonje Jeung, Albert No


+ [NitroFusion: High-Fidelity Single-Step Diffusion through Dynamic Adversarial Training](https://arxiv.org//abs/2412.02030)

	Dar-Yen Chen, Hmrishav Bandyopadhyay, Kai Zou, Yi-Zhe Song


# 2024-12-01
+ [Exposing LLM Vulnerabilities: Adversarial Scam Detection and Performance](https://arxiv.org//abs/2412.00621)

	Chen-Wei Chang, Shailik Sarkar, Shutonu Mitra, Qi Zhang, Hossein Salemi, Hemant Purohit, Fengxiu Zhang, Michin Hong, Jin-Hee Cho, Chang-Tien Lu


+ [SelfPrompt: Autonomously Evaluating LLM Robustness via Domain-Constrained Knowledge Guidelines and Refined Adversarial Prompts](https://arxiv.org//abs/2412.00765)

	Aihua Pei, Zehua Yang, Shunan Zhu, Ruoxi Cheng, Ju Jia


+ [Online Poisoning Attack Against Reinforcement Learning under Black-box Environments](https://arxiv.org//abs/2412.00797)

	Jianhui Li, Bokang Zhang, Junfeng Wu


# 2024-11-30
+ [Fusing Physics-Driven Strategies and Cross-Modal Adversarial Learning: Toward Multi-Domain Applications](https://arxiv.org//abs/2412.00341)

	Hana Satou, Alan Mitkiy


+ [Hard-Label Black-Box Attacks on 3D Point Clouds](https://arxiv.org//abs/2412.00404)

	Daizong Liu, Yunbo Tao, Pan Zhou, Wei Hu


+ [Jailbreak Large Visual Language Models Through Multi-Modal Linkage](https://arxiv.org//abs/2412.00473)

	Yu Wang, Xiaofei Zhou, Yichen Wang, Geyuan Zhang, Tianxing He


+ [Exact Certification of (Graph) Neural Networks Against Label Poisoning](https://arxiv.org//abs/2412.00537)

	Mahalakshmi Sabanayagam, Lukas Gosch, Stephan Günnemann, Debarghya Ghoshdastidar


# 2024-11-29
+ [Gradient Inversion Attack on Graph Neural Networks](https://arxiv.org//abs/2411.19440)

	Divya Anand Sinha, Yezi Liu, Ruijie Du, Yanning Shen


+ [FLARE: Towards Universal Dataset Purification against Backdoor Attacks](https://arxiv.org//abs/2411.19479)

	Linshan Hou, Wei Luo, Zhongyun Hua, Songhua Chen, Leo Yu Zhang, Yiming Li


+ [Deepfake Media Generation and Detection in the Generative AI Era: A Survey and Outlook](https://arxiv.org//abs/2411.19537)

	Florinel-Alin Croitoru, Andrei-Iulian Hiji, Vlad Hondru, Nicolae Catalin Ristea, Paul Irofti, Marius Popescu, Cristian Rusu, Radu Tudor Ionescu, Fahad Shahbaz Khan, Mubarak Shah


+ [LUMIA: Linear probing for Unimodal and MultiModal Membership Inference A!acks leveraging internal LLM states](https://arxiv.org//abs/2411.19876)

	Luis Ibanez-Lissen, Lorena Gonzalez-Manzano, Jose Maria de Fuentes, Nicolas Anciaux, Joaquin Garcia-Alfaro


+ [Towards Class-wise Robustness Analysis](https://arxiv.org//abs/2411.19853)

	Tejaswini Medi, Julia Grabinski, Margret Keuper


+ [On the Adversarial Robustness of Instruction-Tuned Large Language Models for Code](https://arxiv.org//abs/2411.19508)

	Md Imran Hossen, Xiali Hei


# 2024-11-28
+ [Knowledge Database or Poison Base? Detecting RAG Poisoning Attack through LLM Activations](https://arxiv.org//abs/2411.18948)

	Xue Tan, Hao Luan, Mingyu Luo, Xiaoyan Sun, Ping Chen, Jun Dai


+ [Random Sampling for Diffusion-based Adversarial Purification](https://arxiv.org//abs/2411.18956)

	Jiancheng Zhang, Peiran Dong, Yongyong Chen, Yin-Ping Zhao, Song Guo


+ [LADDER: Multi-objective Backdoor Attack via Evolutionary Algorithm](https://arxiv.org//abs/2411.19075)

	Dazhuang Liu, Yanqi Qiao, Rui Wang, Kaitai Liang, Georgios Smaragdakis


+ [PEFT-as-an-Attack! Jailbreaking Language Models during Federated Parameter-Efficient Fine-Tuning](https://arxiv.org//abs/2411.19335)

	Shenghui Li, Edith C.-H. Ngai, Fanghua Ye, Thiemo Voigt


+ [Swarm Intelligence-Driven Client Selection for Federated Learning in Cybersecurity applications](https://arxiv.org//abs/2411.18877)

	Koffka Khan, Wayne Goodridge


+ [SceneTAP: Scene-Coherent Typographic Adversarial Planner against Vision-Language Models in Real-World Environments](https://arxiv.org//abs/2412.00114)

	Yue Cao, Yun Xing, Jie Zhang, Di Lin, Tianwei Zhang, Ivor Tsang, Yang Liu, Qing Guo


# 2024-11-27
+ [Hidden Data Privacy Breaches in Federated Learning](https://arxiv.org//abs/2411.18269)

	Xueluan Gong, Yuji Wang, Shuaike Li, Mengyuan Sun, Songze Li, Qian Wang, Kwok-Yan Lam, Chen Chen


+ [Neutralizing Backdoors through Information Conflicts for Large Language Models](https://arxiv.org//abs/2411.18280)

	Chen Chen, Yuchen Sun, Xueluan Gong, Jiaxin Gao, Kwok-Yan Lam


+ [Exploring Visual Vulnerabilities via Multi-Loss Adversarial Search for Jailbreaking Vision-Language Models](https://arxiv.org//abs/2411.18000)

	Shuyang Hao, Bryan Hooi, Jun Liu, Kai-Wei Chang, Zi Huang, Yujun Cai


+ [Visual Adversarial Attack on Vision-Language Models for Autonomous Driving](https://arxiv.org//abs/2411.18275)

	Tianyuan Zhang, Lu Wang, Xinwei Zhang, Yitong Zhang, Boyi Jia, Siyuan Liang, Shengshan Hu, Qiang Fu, Aishan Liu, Xianglong Liu


+ [Adversarial Training in Low-Label Regimes with Margin-Based Interpolation](https://arxiv.org//abs/2411.17959)

	Tian Ye, Rajgopal Kannan, Viktor Prasanna


+ [InputSnatch: Stealing Input in LLM Services via Timing Side-Channel Attacks](https://arxiv.org//abs/2411.18191)

	Xinyao Zheng, Husheng Han, Shangyi Shi, Qiyan Fang, Zidong Du, Qi Guo, Xing Hu


+ [Evaluating and Improving the Robustness of Security Attack Detectors Generated by LLMs](https://arxiv.org//abs/2411.18216)

	Samuele Pasini, Jinhan Kim, Tommaso Aiello, Rocio Cabrera Lozoya, Antonino Sabetta, Paolo Tonella


+ [PRSI: Privacy-Preserving Recommendation Model Based on Vector Splitting and Interactive Protocols](https://arxiv.org//abs/2411.18653)

	Xiaokai Cao, Wenjin Mo, Zhenyu He, Changdong Wang


+ [Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via Inference-Time Alignment](https://arxiv.org//abs/2411.18688)

	Soumya Suvra Ghosal, Souradip Chakraborty, Vaibhav Singh, Tianrui Guan, Mengdi Wang, Ahmad Beirami, Furong Huang, Alvaro Velasquez, Dinesh Manocha, Amrit Singh Bedi


+ [An indicator for effectiveness of text-to-image guardrails utilizing the Single-Turn Crescendo Attack (STCA)](https://arxiv.org//abs/2411.18699)

	Ted Kwartler, Nataliia Bagan, Ivan Banny, Alan Aqrawi, Arian Abbasi


+ [Fall Leaf Adversarial Attack on Traffic Sign Classification](https://arxiv.org//abs/2411.18776)

	Anthony Etim, Jakub Szefer


# 2024-11-26
+ [RED: Robust Environmental Design](https://arxiv.org//abs/2411.17026)

	Jinghan Yan


+ [LampMark: Proactive Deepfake Detection via Training-Free Landmark Perceptual Watermarks](https://arxiv.org//abs/2411.17209)

	Tianyi Wang, Mengxiao Huang, Harry Cheng, Xiao Zhang, Zhiqi Shen


+ [BadScan: An Architectural Backdoor Attack on Visual State Space Models](https://arxiv.org//abs/2411.17283)

	Om Suhas Deshmukh, Sankalp Nagaonkar, Achyut Mani Tripathi, Ashish Mishra


+ [Adversarial Bounding Boxes Generation (ABBG) Attack against Visual Object Trackers](https://arxiv.org//abs/2411.17468)

	Fatemeh Nourilenjan Nokabadi, Jean-Francois Lalonde, Christian Gagné


+ [CRASH: Challenging Reinforcement-Learning Based Adversarial Scenarios For Safety Hardening](https://arxiv.org//abs/2411.16996)

	Amar Kulkarni, Shangtong Zhang, Madhur Behl


+ [PEFTGuard: Detecting Backdoor Attacks Against Parameter-Efficient Fine-Tuning](https://arxiv.org//abs/2411.17453)

	Zhen Sun, Tianshuo Cong, Yule Liu, Chenhao Lin, Xinlei He, Rongmao Chen, Xingshuo Han, Xinyi Huang


+ [RTL-Breaker: Assessing the Security of LLMs against Backdoor Attacks on HDL Code Generation](https://arxiv.org//abs/2411.17569)

	Lakshmi Likhitha Mankali, Jitendra Bhandari, Manaar Alam, Ramesh Karri, Michail Maniatakos, Ozgur Sinanoglu, Johann Knechtel


+ [Passive Deepfake Detection Across Multi-modalities: A Comprehensive Survey](https://arxiv.org//abs/2411.17911)

	Hong-Hanh Nguyen-Le, Van-Tuan Tran, Dinh-Thuc Nguyen, Nhien-An Le-Khac


+ [Stealthy Multi-Task Adversarial Attacks](https://arxiv.org//abs/2411.17936)

	Jiacheng Guo, Tianyun Zhang, Lei Li, Haochen Yang, Hongkai Yu, Minghai Qin


+ [MADE: Graph Backdoor Defense with Masked Unlearning](https://arxiv.org//abs/2411.18648)

	Xiao Lin amd Mingjie Li, Yisen Wang


# 2024-11-25
+ [Imperceptible Adversarial Examples in the Physical World](https://arxiv.org//abs/2411.16622)

	Weilin Xu, Sebastian Szyller, Cory Cornelius, Luis Murillo Rojas, Marius Arvinte, Alvaro Velasquez, Jason Martin, Nageen Himayat


+ [Preventing Jailbreak Prompts as Malicious Tools for Cybercriminals: A Cyber Defense Perspective](https://arxiv.org//abs/2411.16642)

	Jean Marie Tshimula, Xavier Ndona, D'Jeff K. Nkashama, Pierre-Martin Tardif, Froduald Kabanza, Marc Frappier, Shengrui Wang


+ [Sparse patches adversarial attacks via extrapolating point-wise information](https://arxiv.org//abs/2411.16162)

	Yaniv Nemcovsky, Avi Mendelson, Chaim Baskin


+ [Privacy Protection in Personalized Diffusion Models via Targeted Cross-Attention Adversarial Attack](https://arxiv.org//abs/2411.16437)

	Xide Xu, Muhammad Atif Butt, Sandesh Kamath, Bogdan Raducanu


+ [Guarding the Gate: ConceptGuard Battles Concept-Level Backdoors in Concept Bottleneck Models](https://arxiv.org//abs/2411.16512)

	Songning Lai, Yu Huang, Jiayu Yang, Gaoxiang Huang, Wenshuo Chen, Yutao Yue


+ [Unlocking The Potential of Adaptive Attacks on Diffusion-Based Purification](https://arxiv.org//abs/2411.16598)

	Andre Kassis, Urs Hengartner, Yaoliang Yu


+ [DeDe: Detecting Backdoor Samples for SSL Encoders via Decoders](https://arxiv.org//abs/2411.16154)

	Sizai Hou, Songze Li, Duanyi Yao


+ [BadSFL: Backdoor Attack against Scaffold Federated Learning](https://arxiv.org//abs/2411.16167)

	Xingshuo Han, Xiang Lan, Haozhao Wang, Shengmin Xu, Shen Ren, Jason Zeng, Ming Wu, Michael Heinrich, Tianwei Zhang


+ [Adversarial Attacks for Drift Detection](https://arxiv.org//abs/2411.16591)

	Fabian Hinder, Valerie Vaquet, Barbara Hammer


+ [Edit Away and My Face Will not Stay: Personal Biometric Defense against Malicious Generative Editing](https://arxiv.org//abs/2411.16832)

	Hanhui Wang, Yihua Zhang, Ruizheng Bai, Yue Zhao, Sijia Liu, Zhengzhong Tu


+ [In-Context Experience Replay Facilitates Safety Red-Teaming of Text-to-Image Diffusion Models](https://arxiv.org//abs/2411.16769)

	Zhi-Yi Chin, Kuan-Chen Mu, Mario Fritz, Pin-Yu Chen, Wei-Chen Chiu


+ [Scaling Laws for Black box Adversarial Attacks](https://arxiv.org//abs/2411.16782)

	Chuan Liu, Huanran Chen, Yichi Zhang, Yinpeng Dong, Jun Zhu



# 2024-11-24
+ [Data Lineage Inference: Uncovering Privacy Vulnerabilities of Dataset Pruning](https://arxiv.org//abs/2411.15796)

	Qi Li, Cheng-Long Wang, Yinzhi Cao, Di Wang


+ [Chain of Attack: On the Robustness of Vision-Language Models Against Transfer-Based Adversarial Attacks](https://arxiv.org//abs/2411.15720)

	Peng Xie, Yequan Bie, Jianda Mao, Yangqiu Song, Yang Wang, Hao Chen, Kani Chen


+ [A Tunable Despeckling Neural Network Stabilized via Diffusion Equation](https://arxiv.org//abs/2411.15921)

	Yi Ran, Zhichang Guo, Jia Li, Yao Li, Martin Burger, Boying Wu


+ [ExAL: An Exploration Enhanced Adversarial Learning Algorithm](https://arxiv.org//abs/2411.15878)

	A Vinil, Aneesh Sreevallabh Chivukula, Pranav Chintareddy


+ [Hide in Plain Sight: Clean-Label Backdoor for Auditing Membership Inference](https://arxiv.org//abs/2411.16763)

	Depeng Chen, Hao Chen, Hulin Jin, Jie Cui, Hong Zhong


# 2024-11-23
+ [Twin Trigger Generative Networks for Backdoor Attacks against Object Detection](https://arxiv.org//abs/2411.15439)

	Zhiying Li, Zhi Liu, Guanggang Geng, Shreyank N Gowda, Shuyuan Lin, Jian Weng, Xiaobo Jin


+ [Improving Transferable Targeted Attacks with Feature Tuning Mixup](https://arxiv.org//abs/2411.15553)

	Kaisheng Liang, Xuelong Dai, Yanjie Li, Dong Wang, Bin Xiao


+ [Enhancing the Transferability of Adversarial Attacks on Face Recognition with Diverse Parameters Augmentation](https://arxiv.org//abs/2411.15555)

	Fengfan Zhou, Bangjie Yin, Hefei Ling, Qianyu Zhou, Wenxuan Wang


+ [Semantic Shield: Defending Vision-Language Models Against Backdooring and Poisoning via Fine-grained Knowledge Alignment](https://arxiv.org//abs/2411.15673)

	Alvi Md Ishmam, Christopher Thomas


+ [Unveiling the Achilles' Heel: Backdoor Watermarking Forgery Attack in Public Dataset Protection](https://arxiv.org//abs/2411.15450)

	Zhiying Li, Zhi Liu, Dongjie Liu, Shengda Zhuo, Guanggang Geng, Jian Weng, Shanxiang Lyu, Xiaobo Jin


+ [Steering Away from Harm: An Adaptive Approach to Defending Vision Language Model Against Jailbreaks](https://arxiv.org//abs/2411.16721)

	Han Wang, Gang Wang, Huan Zhang


+ ["Moralized" Multi-Step Jailbreak Prompts: Black-Box Testing of Guardrails in Large Language Models for Verbal Attacks](https://arxiv.org//abs/2411.16730)

	Libo Wang


+ [LoBAM: LoRA-Based Backdoor Attack on Model Merging](https://arxiv.org//abs/2411.16746)

	Ming Yin, Jingyang Zhang, Jingwei Sun, Minghong Fang, Hai Li, Yiran Chen


+ [Devils in Middle Layers of Large Vision-Language Models: Interpreting, Detecting and Mitigating Object Hallucinations via Attention Lens](https://arxiv.org//abs/2411.16724)

	Zhangqi Jiang, Junkai Chen, Beier Zhu, Tingjin Luo, Yankun Shen, Xu Yang



# 2024-11-22
+ [Universal and Context-Independent Triggers for Precise Control of LLM Outputs](https://arxiv.org//abs/2411.14738)

	Jiashuo Liang, Guancheng Li, Yang Yu


+ [Who Can Withstand Chat-Audio Attacks? An Evaluation Benchmark for Large Language Models](https://arxiv.org//abs/2411.14842)

	Wanqi Yang, Yanda Li, Meng Fang, Yunchao Wei, Tianyi Zhou, Ling Chen


+ [Geminio: Language-Guided Gradient Inversion Attacks in Federated Learning](https://arxiv.org//abs/2411.14937)

	Junjie Shan, Ziqi Zhao, Jialin Lu, Rui Zhang, Siu Ming Yiu, Ka-Ho Chow


+ [TrojanEdit: Backdooring Text-Based Image Editing Models](https://arxiv.org//abs/2411.14681)

	Ji Guo, Peihong Chen, Wenbo Jiang, Guoming Lu


+ [GraphTheft: Quantifying Privacy Risks in Graph Prompt Learning](https://arxiv.org//abs/2411.14718)

	Jiani Zhu, Xi Lin, Yuxin Qi, Qinghua Mao


+ [SecONN: An Optical Neural Network Framework with Concurrent Detection of Thermal Fault Injection Attacks](https://arxiv.org//abs/2411.14741)

	Kota Nishida, Yoshihiro Midoh, Noriyuki Miura, Satoshi Kawakami, Jun Shiomi


+ [Adversarial Prompt Distillation for Vision-Language Models](https://arxiv.org//abs/2411.15244)

	Lin Luo, Xin Wang, Bojia Zi, Shihao Zhao, Xingjun Ma


+ [Heavy-tailed Contamination is Easier than Adversarial Contamination](https://arxiv.org//abs/2411.15306)

	Yeshwanth Cherapanamjeri, Daniel Lee


+ [Exploring the Robustness and Transferability of Patch-Based Adversarial Attacks in Quantized Neural Networks](https://arxiv.org//abs/2411.15246)

	Amira Guesmi, Bassem Ouni, Muhammad Shafique


# 2024-11-21
+ [AttentionBreaker: Adaptive Evolutionary Optimization for Unmasking Vulnerabilities in LLMs through Bit-Flip Attacks](https://arxiv.org//abs/2411.13757)

	Sanjay Das, Swastik Bhattacharya, Souvik Kundu, Shamik Kundu, Anand Menon, Arnab Raha, Kanad Basu


+ [A Survey on Adversarial Robustness of LiDAR-based Machine Learning Perception in Autonomous Vehicles](https://arxiv.org//abs/2411.13778)

	Junae Kim, Amardeep Kaur


+ [GASP: Efficient Black-Box Generation of Adversarial Suffixes for Jailbreaking LLMs](https://arxiv.org//abs/2411.14133)

	Advik Raj Basani, Xiao Zhang


+ [AnywhereDoor: Multi-Target Backdoor Attacks on Object Detection](https://arxiv.org//abs/2411.14243)

	Jialin Lu, Junjie Shan, Ziqi Zhao, Ka-Ho Chow


+ [Generating Realistic Adversarial Examples for Business Processes using Variational Autoencoders](https://arxiv.org//abs/2411.14263)

	Alexander Stevens, Jari Peeperkorn, Johannes De Smedt, Jochen De Weerdt


+ [Adversarial Poisoning Attack on Quantum Machine Learning Models](https://arxiv.org//abs/2411.14412)

	Satwik Kundu, Swaroop Ghosh


+ [Learning Fair Robustness via Domain Mixup](https://arxiv.org//abs/2411.14424)

	Meiyu Zhong, Ravi Tandon


+ [RAG-Thief: Scalable Extraction of Private Data from Retrieval-Augmented Generation Applications with Agent-based Attacks](https://arxiv.org//abs/2411.14110)

	Changyue Jiang, Xudong Pan, Geng Hong, Chenfu Bao, Min Yang


+ [Memory Backdoor Attacks on Neural Networks](https://arxiv.org//abs/2411.14516)

	Eden Luzon, Guy Amit, Roy Weiss, Yisroel Mirsky


+ [Rethinking the Intermediate Features in Adversarial Attacks: Misleading Robotic Models via Adversarial Distillation](https://arxiv.org//abs/2411.15222)

	Ke Zhao, Huayang Huang, Miao Li, Yu Wu



# 2024-11-20
+ [Provably Efficient Action-Manipulation Attack Against Continuous Reinforcement Learning](https://arxiv.org//abs/2411.13116)

	Zhi Luo, Xiyuan Yang, Pan Zhou, Di Wang


+ [SoK: A Systems Perspective on Compound AI Threats and Countermeasures](https://arxiv.org//abs/2411.13459)

	Sarbartha Banerjee, Prateek Sahu, Mulong Luo, Anjo Vahldiek-Oberwagner, Neeraja J. Yadwadkar, Mohit Tiwari


+ [WaterPark: A Robustness Assessment of Language Model Watermarking](https://arxiv.org//abs/2411.13425)

	Jiacheng Liang, Zian Wang, Lauren Hong, Shouling Ji, Ting Wang


+ [TAPT: Test-Time Adversarial Prompt Tuning for Robust Inference in Vision-Language Models](https://arxiv.org//abs/2411.13136)

	Xin Wang, Kai Chen, Jiaming Zhang, Jingjing Chen, Xingjun Ma


+ [Bounding-box Watermarking: Defense against Model Extraction Attacks on Object Detectors](https://arxiv.org//abs/2411.13047)

	Satoru Koda, Ikuya Morikawa


+ [Towards Million-Scale Adversarial Robustness Evaluation With Stronger Individual Attacks](https://arxiv.org//abs/2411.15210)

	Yong Xie, Weijie Zheng, Hanxun Huang, Guangnan Ye, Xingjun Ma



# 2024-11-19
+ [DeTrigger: A Gradient-Centric Approach to Backdoor Attack Mitigation in Federated Learning](https://arxiv.org//abs/2411.12220)

	Kichang Lee, Yujin Shin, Jonghyuk Yun, Jun Han, JeongGil Ko


+ [Attribute Inference Attacks for Federated Regression Tasks](https://arxiv.org//abs/2411.12697)

	Francesco Diana, Othmane Marfoq, Chuan Xu, Giovanni Neglia, Frédéric Giroire, Eoin Thomas


+ [Combinational Backdoor Attack against Customized Text-to-Image Models](https://arxiv.org//abs/2411.12389)

	Wenbo Jiang, Jiaming He, Hongwei Li, Guowen Xu, Rui Zhang, Hanxiao Chen, Meng Hao, Haomiao Yang


+ [When Backdoors Speak: Understanding LLM Backdoor Attacks Through Model-Generated Explanations](https://arxiv.org//abs/2411.12701)

	Huaizhi Ge, Yiming Li, Qifan Wang, Yongfeng Zhang, Ruixiang Tang


+ [NMT-Obfuscator Attack: Ignore a sentence in translation with only one word](https://arxiv.org//abs/2411.12473)

	Sahar Sadrizadeh, César Descalzo, Ljiljana Dolamic, Pascal Frossard


+ [Stochastic BIQA: Median Randomized Smoothing for Certified Blind Image Quality Assessment](https://arxiv.org//abs/2411.12575)

	Ekaterina Shumitskaya, Mikhail Pautov, Dmitriy Vatolin, Anastasia Antsiferova


+ [CDI: Copyrighted Data Identification in Diffusion Models](https://arxiv.org//abs/2411.12858)

	Jan Dubiński, Antoni Kowalczuk, Franziska Boenisch, Adam Dziedzic


+ [Trojan Cleansing with Neural Collapse](https://arxiv.org//abs/2411.12914)

	Xihe Gu, Greg Fields, Yaman Jandali, Tara Javidi, Farinaz Koushanfar



# 2024-11-18
+ [Enhancing Vision-Language Model Safety through Progressive Concept-Bottleneck-Driven Alignment](https://arxiv.org//abs/2411.11543)

	Zhendong Liu, Yuanbi Nie, Yingshui Tan, Xiangyu Yue, Qiushi Cui, Chongjun Wang, Xiaoyong Zhu, Bo Zheng


+ [TrojanRobot: Backdoor Attacks Against Robotic Manipulation in the Physical World](https://arxiv.org//abs/2411.11683)

	Xianlong Wang, Hewen Pan, Hangtao Zhang, Minghui Li, Shengshan Hu, Ziqi Zhou, Lulu Xue, Peijin Guo, Yichen Wang, Wei Wan, Aishan Liu, Leo Yu Zhang


+ [Exploring adversarial robustness of JPEG AI: methodology, comparison and new methods](https://arxiv.org//abs/2411.11795)

	Egor Kovalev, Georgii Bychkov, Khaled Abud, Aleksandr Gushchin, Anna Chistyakova, Sergey Lavrushkin, Dmitriy Vatolin, Anastasia Antsiferova


+ [Membership Inference Attack against Long-Context Large Language Models](https://arxiv.org//abs/2411.11424)

	Zixiong Wang, Gaoyang Liu, Yang Yang, Chen Wang


+ [Safe + Safe = Unsafe? Exploring How Safe Images Can Be Exploited to Jailbreak Large Vision-Language Models](https://arxiv.org//abs/2411.11496)

	Chenhang Cui, Gelei Deng, An Zhang, Jingnan Zheng, Yicong Li, Lianli Gao, Tianwei Zhang, Tat-Seng Chua


+ [Reliable Poisoned Sample Detection against Backdoor Attacks Enhanced by Sharpness Aware Minimization](https://arxiv.org//abs/2411.11525)

	Mingda Zhang, Mingli Zhu, Zihao Zhu, Baoyuan Wu


+ [Theoretical Corrections and the Leveraging of Reinforcement Learning to Enhance Triangle Attack](https://arxiv.org//abs/2411.12071)

	Nicole Meng, Caleb Manicke, David Chen, Yingjie Lao, Caiwen Ding, Pengyu Hong, Kaleel Mahmood


+ [Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics](https://arxiv.org//abs/2411.13587)

	Taowen Wang, Dongfang Liu, James Chenhao Liang, Wenhao Yang, Qifan Wang, Cheng Han, Jiebo Luo, Ruixiang Tang


# 2024-11-17
+ [BackdoorMBTI: A Backdoor Learning Multimodal Benchmark Tool Kit for Backdoor Defense Evaluation](https://arxiv.org//abs/2411.11006)

	Haiyang Yu, Tian Xie, Jiaping Gui, Pengyang Wang, Ping Yi, Yue Wu


+ [Time Step Generating: A Universal Synthesized Deepfake Image Detector](https://arxiv.org//abs/2411.11016)

	Ziyue Zeng, Haoyuan Liu, Dingjie Peng, Luoxu Jing, Hiroshi Watanabe


+ [CLMIA: Membership Inference Attacks via Unsupervised Contrastive Learning](https://arxiv.org//abs/2411.11144)

	Depeng Chen, Xiao Liu, Jie Cui, Hong Zhong (School of Computer Science and Technology, Anhui University)


# 2024-11-16
+ [Playing Language Game with LLMs Leads to Jailbreaking](https://arxiv.org//abs/2411.12762)

	Yu Peng, Zewen Long, Fangming Dong, Congyi Li, Shu Wu, Kai Chen



# 2024-11-15
+ [TEESlice: Protecting Sensitive Neural Network Models in Trusted Execution Environments When Attackers have Pre-Trained Models](https://arxiv.org//abs/2411.09945)

	Ding Li, Ziqi Zhang, Mengyu Yao, Yifeng Cai, Yao Guo, Xiangqun Chen


+ [A Hard-Label Cryptanalytic Extraction of Non-Fully Connected Deep Neural Networks using Side-Channel Attacks](https://arxiv.org//abs/2411.10174)

	Benoit Coqueret, Mathieu Carbone, Olivier Sentieys, Gabriel Zaid


+ [Safe Text-to-Image Generation: Simply Sanitize the Prompt Embedding](https://arxiv.org//abs/2411.10329)

	Huming Qiu, Guanxu Chen, Mi Zhang, Min Yang


+ [Continual Adversarial Reinforcement Learning (CARL) of False Data Injection detection: forgetting and explainability](https://arxiv.org//abs/2411.10367)

	Pooja Aslami, Kejun Chen, Timothy M. Hansen, Malik Hassanaly


+ [Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding Conversations](https://arxiv.org//abs/2411.10414)

	Jianfeng Chi, Ujjwal Karn, Hongyuan Zhan, Eric Smith, Javier Rando, Yiming Zhang, Kate Plawiak, Zacharie Delpierre Coudert, Kartikeya Upasani, Mahesh Pasupuleti


+ [Toward Robust and Accurate Adversarial Camouflage Generation against Vehicle Detectors](https://arxiv.org//abs/2411.10029)

	Jiawei Zhou, Linye Lyu, Daojing He, Yu Li


+ [Model Inversion Attacks: A Survey of Approaches and Countermeasures](https://arxiv.org//abs/2411.10023)

	Zhanke Zhou, Jianing Zhu, Fengfei Yu, Xuan Li, Xiong Peng, Tongliang Liu, Bo Han


+ [EveGuard: Defeating Vibration-based Side-Channel Eavesdropping with Audio Adversarial Perturbations](https://arxiv.org//abs/2411.10034)

	Jung-Woo Chang, Ke Sun, David Xia, Xinyu Zhang, Farinaz Koushanfar


+ [Edge-Only Universal Adversarial Attacks in Distributed Learning](https://arxiv.org//abs/2411.10500)

	Giulio Rossolini, Tommaso Baldi, Alessandro Biondi, Giorgio Buttazzo


+ [Prompt-Guided Environmentally Consistent Adversarial Patch](https://arxiv.org//abs/2411.10498)

	Chaoqun Li, Huanqian Yan, Lifeng Zhou, Tairan Chen, Zhuodong Liu, Hang Su



# 2024-11-14
+ [DROJ: A Prompt-Driven Attack against Large Language Models](https://arxiv.org//abs/2411.09125)

	Leyang Hu, Boran Wang


+ [Transferable Adversarial Attacks against ASR](https://arxiv.org//abs/2411.09220)

	Xiaoxue Gao, Zexin Li, Yiming Chen, Cong Liu, Haizhou Li


+ [How Good is ChatGPT at Audiovisual Deepfake Detection: A Comparative Study of ChatGPT, AI Models and Human Perception](https://arxiv.org//abs/2411.09266)

	Sahibzada Adil Shahzad, Ammarah Hashmi, Yan-Tsung Peng, Yu Tsao, Hsin-Min Wang


+ [Prompting the Unseen: Detecting Hidden Backdoors in Black-Box Models](https://arxiv.org//abs/2411.09540)

	Zi-Xuan Huang, Jia-Wei Chen, Zhi-Peng Zhang, Chia-Mu Yu


+ [Adversarial Vessel-Unveiling Semi-Supervised Segmentation for Retinopathy of Prematurity Diagnosis](https://arxiv.org//abs/2411.09140)

	Gozde Merve Demirci, Jiachen Yao, Ming-Chih Ho, Xiaoling Hu, Wei-Chi Wu, Chao Chen, Chia-Ling Tsai


+ [BEARD: Benchmarking the Adversarial Robustness for Dataset Distillation](https://arxiv.org//abs/2411.09265)

	Zheng Zhou, Wenquan Feng, Shuchang Lyu, Guangliang Cheng, Xiaowei Huang, Qi Zhao


+ [Injection Attacks Against End-to-End Encrypted Applications](https://arxiv.org//abs/2411.09228)

	Andrés Fábrega, Carolina Ortega Pérez, Armin Namavari, Ben Nassi, Rachit Agarwal, Thomas Ristenpart


+ [Backdoor Mitigation by Distance-Driven Detoxification](https://arxiv.org//abs/2411.09585)

	Shaokui Wei, Jiayin Liu, Hongyuan Zha


+ [Adversarial Attacks Using Differentiable Rendering: A Survey](https://arxiv.org//abs/2411.09749)

	Matthew Hull, Chao Zhang, Zsolt Kira, Duen Horng Chau


+ [Combining Machine Learning Defenses without Conflicts](https://arxiv.org//abs/2411.09776)

	Vasisht Duddu, Rui Zhang, N. Asokan



# 2024-11-12
+ [New Emerged Security and Privacy of Pre-trained Model: a Survey and Outlook](https://arxiv.org//abs/2411.07691)

	Meng Yang, Tianqing Zhu, Chi Liu, WanLei Zhou, Shui Yu, Philip S. Yu


+ [Can adversarial attacks by large language models be attributed?](https://arxiv.org//abs/2411.08003)

	Manuel Cebrian, Jan Arne Telle


+ [SecEncoder: Logs are All You Need in Security](https://arxiv.org//abs/2411.07528)

	Muhammed Fatih Bulut, Yingqi Liu, Naveed Ahmad, Maximilian Turner, Sami Ait Ouahmane, Cameron Andrews, Lloyd Greenwald


+ [Model Stealing for Any Low-Rank Language Model](https://arxiv.org//abs/2411.07536)

	Allen Liu, Ankur Moitra


+ [Zer0-Jack: A Memory-efficient Gradient-based Jailbreaking Method for Black-box Multi-modal Large Language Models](https://arxiv.org//abs/2411.07559)

	Tiejin Chen, Kaishen Wang, Hua Wei


+ [DecoPrompt : Decoding Prompts Reduces Hallucinations when Large Language Models Meet False Premises](https://arxiv.org//abs/2411.07457)

	Nan Xu, Xuezhe Ma


+ [Rapid Response: Mitigating LLM Jailbreaks with a Few Examples](https://arxiv.org//abs/2411.07494)

	Alwin Peng, Julian Michael, Henry Sleight, Ethan Perez, Mrinank Sharma


+ [Semi-Truths: A Large-Scale Dataset of AI-Augmented Images for Evaluating Robustness of AI-Generated Image detectors](https://arxiv.org//abs/2411.07472)

	Anisha Pal, Julia Kruk, Mansi Phute, Manognya Bhattaram, Diyi Yang, Duen Horng Chau, Judy Hoffman


+ [A Survey on Adversarial Machine Learning for Code Data: Realistic Threats, Countermeasures, and Interpretations](https://arxiv.org//abs/2411.07597)

	Yulong Yang, Haoran Fan, Chenhao Lin, Qian Li, Zhengyu Zhao, Chao Shen, Xiaohong Guan



# 2024-11-11
+ [Adversarial Detection with a Dynamically Stable System](https://arxiv.org//abs/2411.06666)

	Xiaowei Long, Jie Lin, Xiangyuan Yang


+ [Computable Model-Independent Bounds for Adversarial Quantum Machine Learning](https://arxiv.org//abs/2411.06863)

	Bacui Li, Tansu Alpcan, Chandra Thapa, Udaya Parampalli


+ [LongSafetyBench: Long-Context LLMs Struggle with Safety Issues](https://arxiv.org//abs/2411.06899)

	Mianqiu Huang, Xiaoran Liu, Shaojun Zhou, Mozhi Zhang, Chenkun Tan, Pengyu Wang, Qipeng Guo, Zhe Xu, Linyang Li, Zhikai Lei, Linlin Li, Qun Liu, Yaqian Zhou, Xipeng Qiu, Xuanjing Huang


+ [TinyML Security: Exploring Vulnerabilities in Resource-Constrained Machine Learning Systems](https://arxiv.org//abs/2411.07114)

	Jacob Huckelberry, Yuke Zhang, Allison Sansone, James Mickens, Peter A. Beerel, Vijay Janapa Reddi


+ [ProP: Efficient Backdoor Detection via Propagation Perturbation for Overparametrized Models](https://arxiv.org//abs/2411.07036)

	Tao Ren, Qiongxiu Li


+ [The Inherent Adversarial Robustness of Analog In-Memory Computing](https://arxiv.org//abs/2411.07023)

	Corey Lammie, Julian Büchel, Athanasios Vasilopoulos, Manuel Le Gallo, Abu Sebastian


+ [SetLexSem Challenge: Using Set Operations to Evaluate the Lexical and Semantic Robustness of Language Models](https://arxiv.org//abs/2411.07336)

	Bardiya Akhbari, Manish Gawali, Nicholas A. Dronen



# 2024-11-10
+ [SequentialBreak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains](https://arxiv.org//abs/2411.06426)

	Bijoy Ahmed Saiem, MD Sadik Hossain Shanto, Rakib Ahsan, Md Rafi ur Rashid


+ [Prompt-Efficient Fine-Tuning for GPT-like Deep Models to Reduce Hallucination and to Improve Reproducibility in Scientific Text Generation Using Stochastic Optimisation Techniques](https://arxiv.org//abs/2411.06445)

	Daniil Sulimov


+ [vTune: Verifiable Fine-Tuning for LLMs Through Backdooring](https://arxiv.org//abs/2411.06611)

	Eva Zhang, Arka Pal, Akilesh Potti, Micah Goldblum


+ [SamRobNODDI: Q-Space Sampling-Augmented Continuous Representation Learning for Robust and Generalized NODDI](https://arxiv.org//abs/2411.06444)

	Taohui Xiao, Jian Cheng, Wenxin Fan, Enqing Dong, Hairong Zheng, Shanshan Wang


+ [Protection against Source Inference Attacks in Federated Learning using Unary Encoding and Shuffling](https://arxiv.org//abs/2411.06458)

	Andreas Athanasiou, Kangsoo Jung, Catuscia Palamidessi


+ [InvisMark: Invisible and Robust Watermarking for AI-generated Image Provenance](https://arxiv.org//abs/2411.07795)

	Rui Xu, Mengya (Mia)Hu, Deren Lei, Yaxi Li, David Lowe, Alex Gorevski, Mingyu Wang, Emily Ching, Alex Deng


+ [Deferred Backdoor Functionality Attacks on Deep Learning Models](https://arxiv.org//abs/2411.14449)

	Jeongjin Shin, Sangdon Park


# 2024-11-08
+ [Reasoning Robustness of LLMs to Adversarial Typographical Errors](https://arxiv.org//abs/2411.05345)

	Esther Gan, Yiran Zhao, Liying Cheng, Yancan Mao, Anirudh Goyal, Kenji Kawaguchi, Min-Yen Kan, Michael Shieh


+ [A Retrospective on the Robot Air Hockey Challenge: Benchmarking Robust, Reliable, and Safe Learning Techniques for Real-world Robotics](https://arxiv.org//abs/2411.05718)

	Puze Liu, Jonas Günster, Niklas Funk, Simon Gröger, Dong Chen, Haitham Bou-Ammar, Julius Jankowski, Ante Marić, Sylvain Calinon, Andrej Orsula, Miguel Olivares-Mendez, Hongyi Zhou, Rudolf Lioutikov, Gerhard Neumann, Amarildo Likmeta Amirhossein Zhalehmehrabi, Thomas Bonenfant, Marcello Restelli, Davide Tateo, Ziyuan Liu, Jan Peters


+ [A Quality-Centric Framework for Generic Deepfake Detection](https://arxiv.org//abs/2411.05335)

	Wentang Song, Zhiyuan Yan, Yuzhen Lin, Taiping Yao, Changsheng Chen, Shen Chen, Yandan Zhao, Shouhong Ding, Bin Li


+ [Post-Hoc Robustness Enhancement in Graph Neural Networks with Conditional Random Fields](https://arxiv.org//abs/2411.05399)

	Yassine Abbahaddou, Sofiane Ennadir, Johannes F. Lutzeyer, Fragkiskos D. Malliaros, Michalis Vazirgiannis


+ [Towards a Re-evaluation of Data Forging Attacks in Practice](https://arxiv.org//abs/2411.05658)

	Mohamed Suliman, Anisa Halimi, Swanand Kadhe, Nathalie Baracaldo, Douglas Leith


+ [Revisiting the Robustness of Watermarking to Paraphrasing Attacks](https://arxiv.org//abs/2411.05277)

	Saksham Rastogi, Danish Pruthi


+ [Sample and Computationally Efficient Robust Learning of Gaussian Single-Index Models](https://arxiv.org//abs/2411.05708)

	Puqian Wang, Nikos Zarifis, Ilias Diakonikolas, Jelena Diakonikolas


+ [Joint-Optimized Unsupervised Adversarial Domain Adaptation in Remote Sensing Segmentation with Prompted Foundation Model](https://arxiv.org//abs/2411.05878)

	Shuchang Lyu, Qi Zhaoa, Guangliang Cheng, Yiwei He, Zheng Zhou, Guangbiao Wang, Zhenwei Shi



# 2024-11-07
+ [Adversarial Robustness of In-Context Learning in Transformers for Linear Regression](https://arxiv.org//abs/2411.05189)

	Usman Anwar, Johannes Von Oswald, Louis Kirsch, David Krueger, Spencer Frei


# 2024-11-06
+ [Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination](https://arxiv.org//abs/2411.03823)

	Dingjie Song, Sicheng Lai, Shunian Chen, Lichao Sun, Benyou Wang


+ [Generalize or Detect? Towards Robust Semantic Segmentation Under Multiple Distribution Shifts](https://arxiv.org//abs/2411.03829)

	Zhitong Gao, Bingnan Li, Mathieu Salzmann, Xuming He


+ [Act in Collusion: A Persistent Distributed Multi-Target Backdoor in Federated Learning](https://arxiv.org//abs/2411.03926)

	Tao Liu, Wu Yang, Chen Xu, Jiguang Lv, Huanran Wang, Yuhang Zhang, Shuchun Xu, Dapeng Man


+ [Optimal Defenses Against Gradient Reconstruction Attacks](https://arxiv.org//abs/2411.03746)

	Yuxiao Chen, Gamze Gürsoy, Qi Lei


+ [Mitigating Privacy Risks in LLM Embeddings from Embedding Inversion](https://arxiv.org//abs/2411.05034)

	Tiantian Liu, Hongwei Yao, Tong Wu, Zhan Qin, Feng Lin, Kui Ren, Chun Chen



# 2024-11-05
+ [Membership Inference Attacks against Large Vision-Language Models](https://arxiv.org//abs/2411.02902)

	Zhan Li, Yongtao Wu, Yihang Chen, Francesco Tonin, Elias Abad Rocamora, Volkan Cevher


+ [DM4Steal: Diffusion Model For Link Stealing Attack On Graph Neural Networks](https://arxiv.org//abs/2411.03364)

	Jinyin Chen, Haonan Ma, Haibin Zheng



# 2024-11-03
+ [Undermining Image and Text Classification Algorithms Using Adversarial Attacks](https://arxiv.org//abs/2411.03348)

	Langalibalele Lunga, Suhas Sreehari



# 2024-11-02
+ [What Features in Prompts Jailbreak LLMs? Investigating the Mechanisms Behind Attacks](https://arxiv.org//abs/2411.03343)

	Nathalie Maria Kirch, Severin Field, Stephen Casper



# 2024-11-01
+ [Cityscape-Adverse: Benchmarking Robustness of Semantic Segmentation with Realistic Scene Modifications via Diffusion-Based Image Editing](https://arxiv.org//abs/2411.00425)

	Naufal Suryanto, Andro Aprila Adiputra, Ahmada Yusril Kadiptya, Thi-Thu-Huong Le, Derry Pratama, Yongsu Kim, Howon Kim


+ [ROSS:RObust decentralized Stochastic learning based on Shapley values](https://arxiv.org//abs/2411.00365)

	Lina Wang, Yunsheng Yuan, Feng Li, Lingjie Duan


+ [Defense Against Prompt Injection Attack by Leveraging Attack Techniques](https://arxiv.org//abs/2411.00459)

	Yulin Chen, Haoran Li, Zihao Zheng, Yangqiu Song, Dekai Wu, Bryan Hooi



# 2024-10-31
+ [Transferable Ensemble Black-box Jailbreak Attacks on Large Language Models](https://arxiv.org//abs/2410.23558)

	Yiqi Yang, Hongye Fu


+ [Pseudo-Conversation Injection for LLM Goal Hijacking](https://arxiv.org//abs/2410.23678)

	Zheng Chen, Buhui Yao


+ [Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models](https://arxiv.org//abs/2410.23861)

	Hao Yang, Lizhen Qu, Ehsan Shareghi, Gholamreza Haffari


+ [DIP: Diffusion Learning of Inconsistency Pattern for General DeepFake Detection](https://arxiv.org//abs/2410.23663)

	Fan Nie, Jiangqun Ni, Jian Zhang, Bin Zhang, Weizhe Zhang


+ [Adversarial Attacks of Vision Tasks in the Past 10 Years: A Survey](https://arxiv.org//abs/2410.23687)

	Chiyu Zhang, Xiaogang Xu, Jiafei Wu, Zhe Liu, Lu Zhou


+ [DiffPAD: Denoising Diffusion-based Adversarial Patch Decontamination](https://arxiv.org//abs/2410.24006)

	Jia Fu, Xiao Zhang, Sepideh Pashami, Fatemeh Rahimian, Anders Holst


+ [Wide Two-Layer Networks can Learn from Adversarial Perturbations](https://arxiv.org//abs/2410.23677)

	Soichiro Kumano, Hiroshi Kera, Toshihiko Yamasaki


+ [Noise as a Double-Edged Sword: Reinforcement Learning Exploits Randomized Defenses in Neural Networks](https://arxiv.org//abs/2410.23870)

	Steve Bakos, Pooria Madani, Heidar Davoudi


+ [I Can Hear You: Selective Robust Training for Deepfake Audio Detection](https://arxiv.org//abs/2411.00121)

	Zirui Zhang, Wei Hao, Aroon Sankoh, William Lin, Emanuel Mendiola-Ortiz, Junfeng Yang, Chengzhi Mao


+ [Protecting Feed-Forward Networks from Adversarial Attacks Using Predictive Coding](https://arxiv.org//abs/2411.00222)

	Ehsan Ganjidoost, Jeff Orchard


+ [Optical Lens Attack on Monocular Depth Estimation for Autonomous Driving](https://arxiv.org//abs/2411.00192)

	Ce Zhou, Qiben Yan, Daniel Kent, Guangjing Wang, Weikang Ding, Ziqi Zhang, Hayder Radha


+ [Attention Tracker: Detecting Prompt Injection Attacks in LLMs](https://arxiv.org//abs/2411.00348)

	Kuo-Han Hung, Ching-Yun Ko, Ambrish Rawat, I-Hsin Chung, Winston H. Hsu, Pin-Yu Chen



# 2024-10-30
+ [Backdoor Attack Against Vision Transformers via Attention Gradient-Based Image Erosion](https://arxiv.org//abs/2410.22678)

	Ji Guo, Hongwei Li, Wenbo Jiang, Guoming Lu


+ [InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection Guardrail Models](https://arxiv.org//abs/2410.22770)

	Hao Li, Xiaogeng Liu, Chaowei Xiao


+ [Contrastive Learning and Adversarial Disentanglement for Privacy-Preserving Task-Oriented Semantic Communications](https://arxiv.org//abs/2410.22784)

	Omar Erak, Omar Alhussein, Wen Tong


+ [HijackRAG: Hijacking Attacks against Retrieval-Augmented Large Language Models](https://arxiv.org//abs/2410.22832)

	Yucheng Zhang, Qinfeng Li, Tianyu Du, Xuhong Zhang, Xinkui Zhao, Zhengwen Feng, Jianwei Yin


+ [Stealing User Prompts from Mixture of Experts](https://arxiv.org//abs/2410.22884)

	Itay Yona, Ilia Shumailov, Jamie Hayes, Nicholas Carlini


+ [Teaching a Language Model to Distinguish Between Similar Details using a Small Adversarial Training Set](https://arxiv.org//abs/2410.23118)

	Chris Achard


+ [ProTransformer: Robustify Transformers via Plug-and-Play Paradigm](https://arxiv.org//abs/2410.23182)

	Zhichao Hou, Weizhi Gao, Yuchen Shen, Feiyi Wang, Xiaorui Liu


+ [One Prompt to Verify Your Models: Black-Box Text-to-Image Models Verification via Non-Transferable Adversarial Attacks](https://arxiv.org//abs/2410.22725)

	Ji Guo, Wenbo Jiang, Rui Zhang, Guoming Lu, Hongwei Li, Weiren Wu


+ [CausalDiff: Causality-Inspired Disentanglement via Diffusion Model for Adversarial Defense](https://arxiv.org//abs/2410.23091)

	Mingkun Zhang, Keping Bi, Wei Chen, Quanrun Chen, Jiafeng Guo, Xueqi Cheng


+ [FAIR-TAT: Improving Model Fairness Using Targeted Adversarial Training](https://arxiv.org//abs/2410.23142)

	Tejaswini Medi, Steffen Jung, Margret Keuper


+ [Byzantine-Robust Federated Learning: An Overview With Focus on Developing Sybil-based Attacks to Backdoor Augmented Secure Aggregation Protocols](https://arxiv.org//abs/2410.22680)

	Atharv Deshmukh


+ [Crosstalk Attack Resilient RNS Quantum Addition](https://arxiv.org//abs/2410.23217)

	Bhaskar Gaur, Himanshu Thapliyal


+ [Keep on Swimming: Real Attackers Only Need Partial Knowledge of a Multi-Model System](https://arxiv.org//abs/2410.23483)

	Julian Collado, Kevin Stangl


+ [Causality-Driven Audits of Model Robustness](https://arxiv.org//abs/2410.23494)

	Nathan Drenkow, Chris Ribaudo, Mathias Unberath



# 2024-10-29
+ [Text-Guided Attention is All You Need for Zero-Shot Robustness in Vision-Language Models](https://arxiv.org//abs/2410.21802)

	Lu Yu, Haiyang Zhang, Changsheng Xu


+ [Benchmarking OpenAI o1 in Cyber Security](https://arxiv.org//abs/2410.21939)

	Dan Ristea, Vasilios Mavroudis, Chris Hicks


+ [CFSafety: Comprehensive Fine-grained Safety Assessment for LLMs](https://arxiv.org//abs/2410.21695)

	Zhihao Liu, Chenhui Hu


+ [Enhancing Adversarial Attacks through Chain of Thought](https://arxiv.org//abs/2410.21791)

	Jingbo Su


+ [Distinguishing Ignorance from Error in LLM Hallucinations](https://arxiv.org//abs/2410.22071)

	Adi Simhi, Jonathan Herzig, Idan Szpektor, Yonatan Belinkov


+ [AmpleGCG-Plus: A Strong Generative Model of Adversarial Suffixes to Jailbreak LLMs with Higher Success Rates in Fewer Attempts](https://arxiv.org//abs/2410.22143)

	Vishal Kumar, Zeyi Liao, Jaylen Jones, Huan Sun


+ [Benchmarking LLM Guardrails in Handling Multilingual Toxicity](https://arxiv.org//abs/2410.22153)

	Yahan Yang, Soham Dan, Dan Roth, Insup Lee


+ [Fingerprints of Super Resolution Networks](https://arxiv.org//abs/2410.21653)

	Jeremy Vonderfecht, Feng Liu


+ [FakeFormer: Efficient Vulnerability-Driven Transformers for Generalisable Deepfake Detection](https://arxiv.org//abs/2410.21964)

	Dat Nguyen, Marcella Astrid, Enjie Ghorbel, Djamila Aouada


+ [Embedding-based classifiers can detect prompt injection attacks](https://arxiv.org//abs/2410.22284)

	Md. Ahsan Ayub, Subhabrata Majumdar


+ [Robust training of implicit generative models for multivariate and heavy-tailed distributions with an invariant statistical loss](https://arxiv.org//abs/2410.22381)

	José Manuel de Frutos, Manuel A. Vázquez, Pablo Olmos, Joaquín Míguez


+ [Power side-channel leakage localization through adversarial training of deep neural networks](https://arxiv.org//abs/2410.22425)

	Jimmy Gammell, Anand Raghunathan, Kaushik Roy



# 2024-10-28
+ [Hacking Back the AI-Hacker: Prompt Injection as a Defense Against LLM-driven Cyberattacks](https://arxiv.org//abs/2410.20911)

	Dario Pasquini, Evgenios M. Kornaropoulos, Giuseppe Ateniese


+ [BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against Jailbreak Attacks](https://arxiv.org//abs/2410.20971)

	Yunhan Zhao, Xiang Zheng, Lin Luo, Yige Li, Xingjun Ma, Yu-Gang Jiang


+ [Stealthy Jailbreak Attacks on Large Language Models via Benign Data Mirroring](https://arxiv.org//abs/2410.21083)

	Honglin Mu, Han He, Yuxin Zhou, Yunlong Feng, Yang Xu, Libo Qin, Xiaoming Shi, Zeming Liu, Xudong Han, Qi Shi, Qingfu Zhu, Wanxiang Che


+ [SeriesGAN: Time Series Generation via Adversarial and Autoregressive Learning](https://arxiv.org//abs/2410.21203)

	MohammadReza EskandariNasab, Shah Muhammad Hamdi, Soukaina Filali Boubrahimi


+ [Attacking Misinformation Detection Using Adversarial Examples Generated by Language Models](https://arxiv.org//abs/2410.20940)

	Piotr Przybyła


+ [Evaluating the Robustness of LiDAR Point Cloud Tracking Against Adversarial Attack](https://arxiv.org//abs/2410.20893)

	Shengjing Tian, Yinan Han, Xiantong Zhao, Bin Liu, Xiuping Liu


+ [Shallow Diffuse: Robust and Invisible Watermarking through Low-Dimensional Subspaces in Diffusion Models](https://arxiv.org//abs/2410.21088)

	Wenda Li, Huijie Zhang, Qing Qu


+ [Robustness and Generalization in Quantum Reinforcement Learning via Lipschitz Regularization](https://arxiv.org//abs/2410.21117)

	Nico Meyer, Julian Berberich, Christopher Mutschler, Daniel D. Scherer


+ [Fine-tuned Large Language Models (LLMs): Improved Prompt Injection Attacks Detection](https://arxiv.org/abs/2410.21337)

	Md Abdur Rahman, Fan Wu, Alfredo Cuzzocrea, Sheikh Iqbal Ahamed


+ [TACO: Adversarial Camouflage Optimization on Trucks to Fool Object Detectors](https://arxiv.org//abs/2410.21443)

	Adonisz Dimitriu, Tamás Michaletzky, Viktor Remeli


+ [AdvI2I: Adversarial Image Attack on Image-to-Image Diffusion models](https://arxiv.org//abs/2410.21471)

	Yaopei Zeng, Yuanpu Cao, Bochuan Cao, Yurui Chang, Jinghui Chen, Lu Lin


+ [Trustworthiness of Stochastic Gradient Descent in Distributed Learning](https://arxiv.org//abs/2410.21491)

	Hongyang Li, Caesar Wu, Mohammed Chadli, Said Mammar, Pascal Bouvry


+ [FATH: Authentication-based Test-time Defense against Indirect Prompt Injection Attacks](https://arxiv.org//abs/2410.21492)

	Jiongxiao Wang, Fangzhou Wu, Wendi Li, Jinsheng Pan, Edward Suh, Z. Morley Mao, Muhao Chen, Chaowei Xiao


+ [Inverting Gradient Attacks Naturally Makes Data Poisons: An Availability Attack on Neural Networks](https://arxiv.org//abs/2410.21453)

	Wassim Bouaziz, El-Mahdi El-Mhamdi, Nicolas Usunier



# 2024-10-27
+ [Maintaining Informative Coherence: Migrating Hallucinations in Large Language Models via Absorbing Markov Chains](https://arxiv.org//abs/2410.20340)

	Jiemin Wu, Songning Lai, Ruiqiang Xiao, Tianlang Xue, Jiayu Yang, Yutao Yue


+ [Integrating uncertainty quantification into randomized smoothing based robustness guarantees](https://arxiv.org//abs/2410.20432)

	Sina Däubener, Kira Maag, David Krueger, Asja Fischer


+ [LLM Robustness Against Misinformation in Biomedical Question Answering](https://arxiv.org//abs/2410.21330)

	Alexander Bondarenko, Adrian Viehweger


+ [Beyond Interpretability: The Gains of Feature Monosemanticity on Model Robustness](https://arxiv.org//abs/2410.21331)

	Qi Zhang, Yifei Wang, Jingyi Cui, Xiang Pan, Qi Lei, Stefanie Jegelka, Yisen Wang



# 2024-10-26
+ [Beyond Fine-Tuning: Effective Strategies for Mitigating Hallucinations in Large Language Models for Data Analytics](https://arxiv.org//abs/2410.20024)

	Mikhail Rumiantsau, Aliaksei Vertsel, Ilya Hrytsuk, Isaiah Ballah


+ [Vulnerability of LLMs to Vertically Aligned Text Manipulations](https://arxiv.org//abs/2410.20016)

	Zhecheng Li, Yiwei Wang, Bryan Hooi, Yujun Cai, Zhen Xiong, Nanyun Peng, Kai-wei Chang


+ [Generative Adversarial Patches for Physical Attacks on Cross-Modal Pedestrian Re-Identification](https://arxiv.org//abs/2410.20097)

	Yue Su, Hao Li, Maoguo Gong


+ [Prompt Diffusion Robustifies Any-Modality Prompt Learning](https://arxiv.org//abs/2410.20164)

	Yingjun Du, Gaowen Liu, Yuzhang Shang, Yuguang Yao, Ramana Kompella, Cees G. M. Snoek


+ [Transferable Adversarial Attacks on SAM and Its Downstream Models](https://arxiv.org//abs/2410.20197)

	Song Xia, Wenhan Yang, Yi Yu, Xun Lin, Henghui Ding, Lingyu Duan, Xudong Jiang


+ [Proactive Fraud Defense: Machine Learning's Evolving Role in Protecting Against Online Fraud](https://arxiv.org//abs/2410.20281)

	Md Kamrul Hasan Chy


+ [CodePurify: Defend Backdoor Attacks on Neural Code Models via Entropy-based Purification](https://arxiv.org//abs/2410.20136)

	Fangwen Mu, Junjie Wang, Zhuohao Yu, Lin Shi, Song Wang, Mingyang Li, Qing Wang



# 2024-10-25
+ [Expose Before You Defend: Unifying and Enhancing Backdoor Defenses via Exposed Models](https://arxiv.org//abs/2410.19427)

	Yige Li, Hanxun Huang, Jiaming Zhang, Xingjun Ma, Yu-Gang Jiang


+ [Investigating the Role of Prompting and External Tools in Hallucination Rates of Large Language Models](https://arxiv.org//abs/2410.19385)

	Liam Barkley, Brink van der Merwe


+ [Adversarial Environment Design via Regret-Guided Diffusion Models](https://arxiv.org//abs/2410.19715)

	Hojun Chung, Junseo Lee, Minsoo Kim, Dohyeong Kim, Songhwai Oh


+ [The Reopening of Pandora's Box: Analyzing the Role of LLMs in the Evolving Battle Against AI-Generated Fake News](https://arxiv.org//abs/2410.19250)

	Xinyu Wang, Wenbo Zhang, Sai Koneru, Hangzhi Guo, Bonam Mingole, S. Shyam Sundar, Sarah Rajtmajer, Amulya Yadav


+ [A Debate-Driven Experiment on LLM Hallucinations and Accuracy](https://arxiv.org//abs/2410.19485)

	Ray Li, Tanishka Bagade, Kevin Martinez, Flora Yasmin, Grant Ayala, Michael Lam, Kevin Zhu


+ [Humanizing the Machine: Proxy Attacks to Mislead LLM Detectors](https://arxiv.org//abs/2410.19230)

	Tianchun Wang, Yuanzhou Chen, Zichuan Liu, Zhanwen Chen, Haifeng Chen, Xiang Zhang, Wei Cheng



# 2024-10-24
+ [GADT: Enhancing Transferable Adversarial Attacks through Gradient-guided Adversarial Data Transformation](https://arxiv.org//abs/2410.18648)

	Yating Ma, Xiaogang Xu, Liming Fang, Zhe Liu


+ [Complexity Matters: Effective Dimensionality as a Measure for Adversarial Robustness](https://arxiv.org//abs/2410.18556)

	David Khachaturov, Robert Mullins


+ [DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations](https://arxiv.org//abs/2410.18860)

	Aryo Pradipta Gema, Chen Jin, Ahmed Abdulaal, Tom Diethe, Philip Teare, Beatrice Alex, Pasquale Minervini, Amrutha Saseendran


+ [Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities](https://arxiv.org//abs/2410.18469)

	Chung-En Sun, Xiaodong Liu, Weiwei Yang, Tsui-Wei Weng, Hao Cheng, Aidan San, Michel Galley, Jianfeng Gao


+ [Weak-to-Strong Preference Optimization: Stealing Reward from Weak Aligned Model](https://arxiv.org//abs/2410.18640)

	Wenhong Zhu, Zhiwei He, Xiaofeng Wang, Pengfei Liu, Rui Wang


+ [Adversarial Attacks on Large Language Models Using Regularized Relaxation](https://arxiv.org//abs/2410.19160)

	Samuel Jacob Chacko, Sajib Biswas, Chashi Mahiul Islam, Fatema Tabassum Liza, Xiuwen Liu



# 2024-10-23
+ [Advancing NLP Security by Leveraging LLMs as Adversarial Engines](https://arxiv.org//abs/2410.18215)

	Sudarshan Srinivasan, Maria Mahbub, Amir Sadovnik


+ [Backdoor in Seconds: Unlocking Vulnerabilities in Large Pre-trained Models via Model Editing](https://arxiv.org//abs/2410.18267)

	Dongliang Guo, Mengxuan Hu, Zihan Guan, Junfeng Guo, Thomas Hartvigsen, Sheng Li


+ [Towards Understanding the Fragility of Multilingual LLMs against Fine-Tuning Attacks](https://arxiv.org//abs/2410.18210)

	Samuele Poppi, Zheng-Xin Yong, Yifei He, Bobbie Chern, Han Zhao, Aobo Yang, Jianfeng Chi


+ [Large Language Models Still Exhibit Bias in Long Text](https://arxiv.org//abs/2410.17519)

	Wonje Jeung, Dongjae Jeon, Ashkan Yousefpour, Jonghyun Choi



# 2024-10-22
+ [Breaking ReAct Agents: Foot-in-the-Door Attack Will Get You In](https://arxiv.org//abs/2410.16950)

	Itay Nakash, George Kour, Guy Uziel, Ateret Anaby-Tavor


+ [Context-aware Prompt Tuning: Advancing In-Context Learning with Adversarial Methods](https://arxiv.org//abs/2410.17222)

	Tsachi Blau, Moshe Kimhi, Yonatan Belinkov, Alexander Bronstein, Chaim Baskin


+ [DENOASR: Debiasing ASRs through Selective Denoising](https://arxiv.org//abs/2410.16712)

	Anand Kumar Rai, Siddharth D Jaiswal, Shubham Prakash, Bendi Pragnya Sree, Animesh Mukherjee


+ [Evaluating the Effectiveness of Attack-Agnostic Features for Morphing Attack Detection](https://arxiv.org//abs/2410.16802)

	Laurent Colbois, Sébastien Marcel


+ [Dual-Model Defense: Safeguarding Diffusion Models from Membership Inference Attacks through Disjoint Data Splitting](https://arxiv.org//abs/2410.16657)

	Bao Q. Tran, Viet Nguyen, Anh Tran, Toan Tran


+ [LLM-Assisted Red Teaming of Diffusion Models through "Failures Are Fated, But Can Be Faded"](https://arxiv.org//abs/2410.16738)

	Som Sagar, Aditya Taparia, Ransalu Senanayake


+ [Test-time Adversarial Defense with Opposite Adversarial Path and High Attack Time Cost](https://arxiv.org//abs/2410.16805)

	Cheng-Han Yeh, Kuanchun Yu, Chun-Shien Lu


+ [Optimal Robust Estimation under Local and Global Corruptions: Stronger Adversary and Smaller Error](https://arxiv.org//abs/2410.17230)

	Thanasis Pittas, Ankit Pensia


+ [BETA: Automated Black-box Exploration for Timing Attacks in Processors](https://arxiv.org//abs/2410.16648)

	Congcong Chen, Jinhua Cui, Jiliang Zhang


+ [On the Vulnerability of Text Sanitization](https://arxiv.org//abs/2410.17052)

	Meng Tong, Kejiang Chen, Xiaojian Yuang, Jiayang Liu, Weiming Zhang, Nenghai Yu, Jie Zhang



# 2024-10-21
+ [Boosting Jailbreak Transferability for Large Language Models](https://arxiv.org//abs/2410.15645)

	Hanqing Liu, Lifeng Zhou, Huanqian Yan


+ [Reducing Hallucinations in Vision-Language Models via Latent Space Steering](https://arxiv.org//abs/2410.15778)

	Sheng Liu, Haotian Ye, James Zou


+ [Model Mimic Attack: Knowledge Distillation for Provably Transferable Adversarial Examples](https://arxiv.org//abs/2410.15889)

	Kirill Lukyanov, Andrew Perminov, Denis Turdakov, Mikhail Pautov


+ [SMILES-Prompting: A Novel Approach to LLM Jailbreak Attacks in Chemical Synthesis](https://arxiv.org//abs/2410.15641)

	Aidan Wong, He Cao, Zijing Liu, Yu Li


+ [A Troublemaker with Contagious Jailbreak Makes Chaos in Honest Towns](https://arxiv.org//abs/2410.16155)

	Tianyi Men, Pengfei Cao, Zhuoran Jin, Yubo Chen, Kang Liu, Jun Zhao


+ [Can Knowledge Editing Really Correct Hallucinations?](https://arxiv.org//abs/2410.16251)

	Baixiang Huang, Canyu Chen, Xiongxiao Xu, Ali Payani, Kai Shu


+ [Erasing Undesirable Concepts in Diffusion Models with Adversarial Preservation](https://arxiv.org//abs/2410.15618)

	Anh Bui, Long Vuong, Khanh Doan, Trung Le, Paul Montague, Tamas Abraham, Dinh Phung


+ [A Realistic Threat Model for Large Language Model Jailbreaks](https://arxiv.org//abs/2410.16222)

	Valentyn Boreiko, Alexander Panfilov, Vaclav Voracek, Matthias Hein, Jonas Geiping


+ [On the Geometry of Regularization in Adversarial Training: High-Dimensional Asymptotics and Generalization Bounds](https://arxiv.org//abs/2410.16073)

	Matteo Vilucchio, Nikolaos Tsilivis, Bruno Loureiro, Julia Kempe


+ [Conflict-Aware Adversarial Training](https://arxiv.org//abs/2410.16579)

	Zhiyu Xue, Haohan Wang, Yao Qin, Ramtin Pedarsani


+ [Simplicity Bias via Global Convergence of Sharpness Minimization](https://arxiv.org//abs/2410.16401)

	Khashayar Gatmiry, Zhiyuan Li, Sashank J. Reddi, Stefanie Jegelka


+ [Enhancing PAC Learning of Half spaces Through Robust Optimization Techniques](https://arxiv.org//abs/2410.16573)

	Shirmohammad Tavangari, Zahra Shakarami, Aref Yelghi, Asef Yelghi



# 2024-10-20
+ [Jailbreaking and Mitigation of Vulnerabilities in Large Language Models](https://arxiv.org//abs/2410.15236)

	Benji Peng, Ziqian Bi, Qian Niu, Ming Liu, Pohsun Feng, Tianyang Wang, Lawrence K.Q. Yan, Yizhu Wen, Yichao Zhang, Caitlyn Heqi Yin


+ [Faster-GCG: Efficient Discrete Optimization Jailbreak Attacks against Aligned Large Language Models](https://arxiv.org//abs/2410.15362)

	Xiao Li, Zhuhong Li, Qiongxiu Li, Bingze Lee, Jinghao Cui, Xiaolin Hu


+ [The Best Defense is a Good Offense: Countering LLM-Powered Cyberattacks](https://arxiv.org//abs/2410.15396)

	Daniel Ayzenshteyn, Roy Weiss, Yisroel Mirsky


+ [PEAS: A Strategy for Crafting Transferable Adversarial Examples](https://arxiv.org//abs/2410.15409)

	Bar Avraham, Yisroel Mirsky


+ [CalibraEval: Calibrating Prediction Distribution to Mitigate Selection Bias in LLMs-as-Judges](https://arxiv.org//abs/2410.15393)

	Haitao Li, Junjie Chen, Qingyao Ai, Zhumin Chu, Yujia Zhou, Qian Dong, Yiqun Liu


+ [Modality-Fair Preference Optimization for Trustworthy MLLM Alignment](https://arxiv.org//abs/2410.15334)

	Songtao Jiang, Yan Zhang, Ruizhe Chen, Yeying Jin, Zuozhu Liu


+ [DynaVINS++: Robust Visual-Inertial State Estimator in Dynamic Environments by Adaptive Truncated Least Squares and Stable State Recovery](https://arxiv.org//abs/2410.15373)

	Seungwon Song, Hyungtae Lim, Alex Junho Lee, Hyun Myung



# 2024-10-19
+ [Bias Amplification: Language Models as Increasingly Biased Media](https://arxiv.org//abs/2410.15234)

	Ze Wang, Zekun Wu, Jeremy Zhang, Navya Jain, Xin Guan, Adriano Koshiyama


+ [Adversarial Training: A Survey](https://arxiv.org//abs/2410.15042)

	Mengnan Zhao, Lihe Zhang, Jingwen Ye, Huchuan Lu, Baocai Yin, Xinchao Wang


+ [Mind the Remaining: Mechanism Design for Robust Federated Unlearning](https://arxiv.org//abs/2410.15045)

	Jiaqi Shao, Tao Lin, Bing Luo


+ [SLIC: Secure Learned Image Codec through Compressed Domain Watermarking to Defend Image Manipulation](https://arxiv.org//abs/2410.15075)

	Chen-Hsiu Huang, Ja-Ling Wu


+ [Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large Language Models](https://arxiv.org//abs/2410.15116)

	Qitan Lv, Jie Wang, Hanzhu Chen, Bin Li, Yongdong Zhang, Feng Wu


+ [Toward Robust RALMs: Revealing the Impact of Imperfect Retrieval on Retrieval-Augmented Language Models](https://arxiv.org//abs/2410.15107)

	Seong-Il Park, Jay-Yoon Lee


+ [Attack as Defense: Run-time Backdoor Implantation for Image Content Protection](https://arxiv.org//abs/2410.14966)

	Haichuan Zhang, Meiyu Lin, Zhaoyi Liu, Renyuan Li, Zhiyuan Cheng, Carl Yang, Mingjie Tang



# 2024-10-18
+ [Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation](https://arxiv.org//abs/2410.14425)

	Shuai Zhao, Xiaobao Wu, Cong-Duy Nguyen, Meihuizi Jia, Yichao Feng, Luu Anh Tuan


+ [Real-time Fake News from Adversarial Feedback](https://arxiv.org//abs/2410.14651)

	Sanxing Chen, Yukun Huang, Bhuwan Dhingra


+ [NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples](https://arxiv.org//abs/2410.14669)

	Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja, Ranjay Krishna, Graham Neubig, Deva Ramanan


+ [DMGNN: Detecting and Mitigating Backdoor Attacks in Graph Neural Networks](https://arxiv.org//abs/2410.14105)

	Hao Sui, Bing Chen, Jiale Zhang, Chengcheng Zhu, Di Wu, Qinghua Lu, Guodong Long


+ [Backdoored Retrievers for Prompt Injection Attacks on Retrieval Augmented Generation of Large Language Models](https://arxiv.org//abs/2410.14479)

	Cody Clop, Yannick Teglia


+ [SignAttention: On the Interpretability of Transformer Models for Sign Language Translation](https://arxiv.org//abs/2410.14506)

	Pedro Alejandro Dal Bianco, Oscar Agustín Stanchi, Facundo Manuel Quiroga, Franco Ronchetti, Enzo Ferrante


+ [On the Regularization of Learnable Embeddings for Time Series Processing](https://arxiv.org//abs/2410.14630)

	Luca Butera, Giovanni De Felice, Andrea Cini, Cesare Alippi


+ [MetaAlign: Align Large Language Models with Diverse Preferences during Inference Time](https://arxiv.org//abs/2410.14184)

	Mozhi Zhang, Pengyu Wang, Chenkun Tan, Mianqiu Huang, Dong Zhang, Yaqian Zhou, Xipeng Qiu


+ [MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image Description and Reasoning Steps](https://arxiv.org//abs/2410.14668)

	Xiongtao Zhou, Jie He, Lanyu Chen, jingyu li, Haojing Chen, Victor Gutierrez Basulto, Jeff Z. Pan, Hanjie Chen


+ [Zero-shot Action Localization via the Confidence of Large Vision-Language Models](https://arxiv.org//abs/2410.14340)

	Josiah Aklilu, Xiaohan Wang, Serena Yeung-Levy


+ [A Mirror Descent Perspective of Smoothed Sign Descent](https://arxiv.org//abs/2410.14158)

	Shuyang Wang, Diego Klabjan


+ [Decomposing The Dark Matter of Sparse Autoencoders](https://arxiv.org//abs/2410.14670)

	Joshua Engels, Logan Riggs, Max Tegmark


+ [Optimizing importance weighting in the presence of sub-population shifts](https://arxiv.org//abs/2410.14315)

	Floris Holstege, Bram Wouters, Noud van Giersbergen, Cees Diks


+ [A Lipschitz spaces view of infinitely wide shallow neural networks](https://arxiv.org//abs/2410.14591)

	Francesca Bartolucci, Marcello Carioni, José A. Iglesias, Yury Korolev, Emanuele Naldi, Stefano Vigogna


+ [Not Sure Your Car Withstands Cyberwarfare](https://arxiv.org//abs/2410.14320)

	Giampaolo Bella, Gianpietro Castiglione, Sergio Esposito, Mario Raciti, Salvatore Riccobene


+ [Safeguarding Blockchain Ecosystem: Understanding and Detecting Attack Transactions on Cross-chain Bridges](https://arxiv.org//abs/2410.14493)

	Jiajing Wu, Kaixin Lin, Dan Lin, Bozhao Zhang, Zhiying Wu, Jianzhong Su


+ [Soft-Label Integration for Robust Toxicity Classification](https://arxiv.org//abs/2410.14894)

	Zelei Cheng, Xian Wu, Jiahao Yu, Shuo Han, Xin-Qiang Cai, Xinyu Xing


+ [Making LLMs Vulnerable to Prompt Injection via Poisoning Alignment](https://arxiv.org//abs/2410.14827)

	Zedian Shao, Hongbin Liu, Jaden Mu, Neil Zhenqiang Gong


+ [A Hybrid Defense Strategy for Boosting Adversarial Robustness in Vision-Language Models](https://arxiv.org//abs/2410.14911)

	Yuhan Liang, Yijun Li, Yumeng Niu, Qianhe Shen, Hangyu Liu



# 2024-10-17
+ [From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization](https://arxiv.org//abs/2410.13961)

	Catarina G. Belem, Pouya Pezeskhpour, Hayate Iso, Seiji Maekawa, Nikita Bhutani, Estevam Hruschka


+ [MMAD-Purify: A Precision-Optimized Framework for Efficient and Scalable Multi-Modal Attacks](https://arxiv.org//abs/2410.14089)

	Xinxin Liu, Zhongliang Guo, Siyuan Huang, Chun Pong Lau


+ [Trojan Prompt Attacks on Graph Neural Networks](https://arxiv.org//abs/2410.13974)

	Minhua Lin, Zhiwei Zhang, Enyan Dai, Zongyu Wu, Yilong Wang, Xiang Zhang, Suhang Wang


+ [Adversarial Inception for Bounded Backdoor Poisoning in Deep Reinforcement Learning](https://arxiv.org//abs/2410.13995)

	Ethan Rathbun, Christopher Amato, Alina Oprea


+ [Mitigating the Backdoor Effect for Multi-Task Model Merging via Safety-Aware Subspace](https://arxiv.org//abs/2410.13910)

	Jinluan Yang, Anke Tang, Didi Zhu, Zhengyu Chen, Li Shen, Fei Wu


+ [Cyber Attacks Prevention Towards Prosumer-based EV Charging Stations: An Edge-assisted Federated Prototype Knowledge Distillation Approach](https://arxiv.org//abs/2410.13260)

	Luyao Zou, Quang Hieu Vo, Kitae Kim, Huy Q. Le, Chu Myaet Thwal, Chaoning Zhang, Choong Seon Hong



# 2024-10-16
+ [Iter-AHMCL: Alleviate Hallucination for Large Language Model via Iterative Model-level Contrastive Learning](https://arxiv.org//abs/2410.12130)

	Huiwen Wu, Xiaohan Li, Xiaogang Xu, Jiafei Wu, Deyi Zhang, Zhe Liu


+ [Robust RL with LLM-Driven Data Synthesis and Policy Adaptation for Autonomous Driving](https://arxiv.org//abs/2410.12568)

	Sihao Wu, Jiaxu Liu, Xiangyu Yin, Guangliang Cheng, Meng Fang, Xingyu Zhao, Xinping Yi, Xiaowei Huang


+ [Low-Rank Adversarial PGD Attack](https://arxiv.org//abs/2410.12607)

	Dayana Savostianova, Emanuele Zangrando, Francesco Tudisco


+ [SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And Video Generation](https://arxiv.org//abs/2410.12761)

	Jaehong Yoon, Shoubin Yu, Vaidehi Patil, Huaxiu Yao, Mohit Bansal


+ [Vaccinating Federated Learning for Robust Modulation Classification in Distributed Wireless Networks](https://arxiv.org//abs/2410.12772)

	Hunmin Lee, Hongju Seong, Wonbin Kim, Hyeokchan Kwon, Daehee Seo


+ [DAT: Improving Adversarial Robustness via Generative Amplitude Mix-up in Frequency Domain](https://arxiv.org//abs/2410.12307)

	Fengpeng Li, Kemou Li, Haiwei Wu, Jinyu Tian, Jiantao Zhou



# 2024-10-15
+ [Towards General Deepfake Detection with Dynamic Curriculum](https://arxiv.org//abs/2410.11162)

	Wentang Song, Yuzhen Lin, Bin Li


+ [Archilles' Heel in Semi-open LLMs: Hiding Bottom against Recovery Attacks](https://arxiv.org//abs/2410.11182)

	Hanbo Huang, Yihan Li, Bowen Jiang, Lin Liu, Ruoyu Sun, Zhuotao Liu, Shiyu Liang


+ [Backdoor Attack on Vertical Federated Graph Neural Network Learning](https://arxiv.org//abs/2410.11290)

	Jirui Yang, Peng Chen, Zhihui Lu, Ruijun Deng, Qiang Duan, Jianping Zeng


+ [Multi-round jailbreak attack on large language models](https://arxiv.org//abs/2410.11533)

	Yihua Zhou, Xiaochuan Shi


+ [Magnifier Prompt: Tackling Multimodal Hallucination via Extremely Simple Instructions](https://arxiv.org//abs/2410.11701)

	Yuhan Fu, Ruobing Xie, Jiazhen Liu, Bangxiang Lan, Xingwu Sun, Zhanhui Kang, Xirong Li


+ [Cognitive Overload Attack:Prompt Injection for Long Context](https://arxiv.org//abs/2410.11272)

	Bibek Upadhayay, Vahid Behzadan, Amin Karbasi


+ [Deciphering the Chaos: Enhancing Jailbreak Attacks via Adversarial Prompt Translation](https://arxiv.org//abs/2410.11317)

	Qizhang Li, Xiaochen Yang, Wangmeng Zuo, Yiwen Guo


+ [Efficient and Effective Universal Adversarial Attack against Vision-Language Pre-training Models](https://arxiv.org//abs/2410.11639)

	Fan Yang, Yihao Huang, Kailong Wang, Ling Shi, Geguang Pu, Yang Liu, Haoyu Wang


+ [Adversarially Guided Stateful Defense Against Backdoor Attacks in Federated Deep Learning](https://arxiv.org//abs/2410.11205)

	Hassan Ali, Surya Nepal, Salil S. Kanhere, Sanjay Jha


+ [AdvBDGen: Adversarially Fortified Prompt-Specific Fuzzy Backdoor Generator Against LLM Alignment](https://arxiv.org//abs/2410.11283)

	Pankayaraj Pathmanathan, Udari Madhushani Sehwag, Michael-Andrei Panaitescu-Liess, Furong Huang


+ [Bias Similarity Across Large Language Models](https://arxiv.org//abs/2410.12010)

	Hyejun Jeong, Shiqing Ma, Amir Houmansadr


+ [Concept-Reversed Winograd Schema Challenge: Evaluating and Improving Robust Reasoning in Large Language Models via Abstraction](https://arxiv.org//abs/2410.12040)

	Kaiqiao Han, Tianqing Fang, Zhaowei Wang, Yangqiu Song, Mark Steedman


+ [Data-adaptive Differentially Private Prompt Synthesis for In-Context Learning](https://arxiv.org//abs/2410.12085)

	Fengyu Gao, Ruida Zhou, Tianhao Wang, Cong Shen, Jing Yang


+ [Taking off the Rose-Tinted Glasses: A Critical Look at Adversarial ML Through the Lens of Evasion Attacks](https://arxiv.org//abs/2410.12076)

	Kevin Eykholt, Farhan Ahmed, Pratik Vaishnavi, Amir Rahmati


+ [BeniFul: Backdoor Defense via Middle Feature Analysis for Deep Neural Networks](https://arxiv.org//abs/2410.14723)

	Xinfu Li, Junying Zhang, Xindi Ma



# 2024-10-14
+ [Jailbreak Instruction-Tuned LLMs via end-of-sentence MLP Re-weighting](https://arxiv.org//abs/2410.10150)

	Yifan Luo, Zhennan Zhou, Meitan Wang, Bin Dong


+ [Eliminating the Language Bias for Visual Question Answering with fine-grained Causal Intervention](https://arxiv.org//abs/2410.10184)

	Ying Liu, Ge Bai, Chenji Lu, Shilong Li, Zhang Zhang, Ruifang Liu, Wenbin Guo


+ [ROSAR: An Adversarial Re-Training Framework for Robust Side-Scan Sonar Object Detection](https://arxiv.org//abs/2410.10554)

	Martin Aubard, László Antal, Ana Madureira, Luis F. Teixeira, Erika Ábrahám


+ [Enhancing Robustness in Deep Reinforcement Learning: A Lyapunov Exponent Approach](https://arxiv.org//abs/2410.10674)

	Rory Young, Nicolas Pugeault


+ [Derail Yourself: Multi-turn LLM Jailbreak Attack through Self-discovered Clues](https://arxiv.org//abs/2410.10700)

	Qibing Ren, Hao Li, Dongrui Liu, Zhanxu Xie, Xiaoya Lu, Yu Qiao, Lei Sha, Junchi Yan, Lizhuang Ma, Jing Shao


+ [Locking Down the Finetuned LLMs Safety](https://arxiv.org//abs/2410.10343)

	Minjun Zhu, Linyi Yang, Yifan Wei, Ningyu Zhang, Yue Zhang


+ [Out-of-Bounding-Box Triggers: A Stealthy Approach to Cheat Object Detectors](https://arxiv.org//abs/2410.10091)

	Tao Lin, Lijia Yu, Gaojie Jin, Renjue Li, Peng Wu, Lijun Zhang


+ [Identity-Focused Inference and Extraction Attacks on Diffusion Models](https://arxiv.org//abs/2410.10177)

	Jayneel Vora, Aditya Krishnan, Nader Bouacida, Prabhu RV Shankar, Prasant Mohapatra


+ [Capture Artifacts via Progressive Disentangling and Purifying Blended Identities for Deepfake Detection](https://arxiv.org//abs/2410.10244)

	Weijie Zhou, Xiaoqing Luo, Zhancheng Zhang, Jiachen He, Xiaojun Wu


+ [Adversarially Robust Out-of-Distribution Detection Using Lyapunov-Stabilized Embeddings](https://arxiv.org//abs/2410.10744)

	Hossein Mirzaei, Mackenzie W. Mathis


+ [Regularized Robustly Reliable Learners and Instance Targeted Attacks](https://arxiv.org//abs/2410.10572)

	Avrim Blum, Donya Saless


+ [Towards Calibrated Losses for Adversarial Robust Reject Option Classification](https://arxiv.org//abs/2410.10736)

	Vrund Shah, Tejas Chaudhari, Naresh Manwani



# 2024-10-13
+ [Robust 3D Point Clouds Classification based on Declarative Defenders](https://arxiv.org//abs/2410.09691)

	Kaidong Li, Tianxiao Zhang, Chuncong Zhong, Ziming Zhang, Guanghui Wang


+ [BlackDAN: A Black-Box Multi-Objective Approach for Effective and Contextual Jailbreaking of Large Language Models](https://arxiv.org//abs/2410.09804)

	Xinyuan Wang, Victor Shea-Jay Huang, Renmiao Chen, Hao Wang, Chengwei Pan, Lei Sha, Minlie Huang


+ [Uncovering, Explaining, and Mitigating the Superficial Safety of Backdoor Defense](https://arxiv.org//abs/2410.09838)

	Rui Min, Zeyu Qin, Nevin L. Zhang, Li Shen, Minhao Cheng


+ [Understanding Robustness of Parameter-Efficient Tuning for Image Classification](https://arxiv.org//abs/2410.09845)

	Jiacheng Ruan, Xian Gao, Suncheng Xiang, Mingye Xie, Ting Liu, Yuzhuo Fu


+ [LongHalQA: Long-Context Hallucination Evaluation for MultiModal Large Language Models](https://arxiv.org//abs/2410.09962)

	Han Qiu, Jiaxing Huang, Peng Gao, Qin Qi, Xiaoqin Zhang, Ling Shao, Shijian Lu


+ [Targeted Vaccine: Safety Alignment for Large Language Models against Harmful Fine-Tuning via Layer-wise Perturbation](https://arxiv.org//abs/2410.09760)

	Guozhi Liu, Weiwei Lin, Tiansheng Huang, Ruichao Mo, Qi Mu, Li Shen


+ [Uncovering Attacks and Defenses in Secure Aggregation for Federated Deep Learning](https://arxiv.org//abs/2410.09676)

	Yiwei Zhang, Rouzbeh Behnia, Attila A. Yavuz, Reza Ebrahimi, Elisa Bertino



# 2024-10-12
+ [Are You Human? An Adversarial Benchmark to Expose LLMs](https://arxiv.org//abs/2410.09569)

	Gilad Gressel, Rahul Pankajakshan, Yisroel Mirsky


+ [Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbations](https://arxiv.org//abs/2410.09318)

	Saiful Islam Salim, Rubin Yuchan Yang, Alexander Cooper, Suryashree Ray, Saumya Debray, Sazzadur Rahaman


+ [A Speaker Turn-Aware Multi-Task Adversarial Network for Joint User Satisfaction Estimation and Sentiment Analysis](https://arxiv.org//abs/2410.09556)

	Kaisong Song, Yangyang Kang, Jiawei Liu, Xurui Li, Changlong Sun, Xiaozhong Liu


+ [Debiasing Vison-Language Models with Text-Only Training](https://arxiv.org//abs/2410.09365)

	Yunfan Yang, Chaoquan Jiang, Zhiyu Lin, Jinlin Xiao, Jiaming Zhang, Jitao Sang


+ [Decision-Point Guided Safe Policy Improvement](https://arxiv.org//abs/2410.09361)

	Abhishek Sharma, Leo Benac, Sonali Parbhoo, Finale Doshi-Velez


+ [Unlearn and Burn: Adversarial Machine Unlearning Requests Destroy Model Accuracy](https://arxiv.org//abs/2410.09591)

	Yangsibo Huang, Daogao Liu, Lynn Chua, Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Milad Nasr, Amer Sinha, Chiyuan Zhang



# 2024-10-11
+ [Kaleidoscope: Learnable Masks for Heterogeneous Multi-agent Reinforcement Learning](https://arxiv.org//abs/2410.08540)

	Xinran Li, Ling Pan, Jun Zhang


+ [RePD: Defending Jailbreak Attack through a Retrieval-based Prompt Decomposition Process](https://arxiv.org//abs/2410.08660)

	Peiran Wang, Xiaogeng Liu, Chaowei Xiao


+ [PoisonBench: Assessing Large Language Model Vulnerability to Data Poisoning](https://arxiv.org//abs/2410.08811)

	Tingchen Fu, Mrinank Sharma, Philip Torr, Shay B. Cohen, David Krueger, Fazl Barez


+ [The Good, the Bad and the Ugly: Watermarks, Transferable Attacks and Adversarial Defenses](https://arxiv.org//abs/2410.08864)

	Grzegorz Głuch, Berkant Turan, Sai Ganesh Nagarajan, Sebastian Pokutta


+ [On the Adversarial Transferability of Generalized "Skip Connections"](https://arxiv.org//abs/2410.08950)

	Yisen Wang, Yichuan Mo, Dongxian Wu, Mingjie Li, Xingjun Ma, Zhouchen Lin


+ [Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements](https://arxiv.org//abs/2410.08968)

	Jingyu Zhang, Ahmed Elgohary, Ahmed Magooda, Daniel Khashabi, Benjamin Van Durme


+ [NoVo: Norm Voting off Hallucinations with Attention Heads in Large Language Models](https://arxiv.org//abs/2410.08970)

	Zheng Yi Ho, Siyuan Liang, Sen Zhang, Yibing Zhan, Dacheng Tao


+ [AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents](https://arxiv.org//abs/2410.09024)

	Maksym Andriushchenko, Alexandra Souly, Mateusz Dziemian, Derek Duenas, Maxwell Lin, Justin Wang, Dan Hendrycks, Andy Zou, Zico Kolter, Matt Fredrikson, Eric Winsor, Jerome Wynne, Yarin Gal, Xander Davies


+ [RoRA-VLM: Robust Retrieval-Augmented Vision Language Models](https://arxiv.org//abs/2410.08876)

	Jingyuan Qi, Zhiyang Xu, Rulin Shao, Yang Chen, Jing Di, Yu Cheng, Qifan Wang, Lifu Huang


+ [AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation](https://arxiv.org//abs/2410.09040)

	Zijun Wang, Haoqin Tu, Jieru Mei, Bingchen Zhao, Yisen Wang, Cihang Xie


+ [Natural Language Induced Adversarial Images](https://arxiv.org//abs/2410.08620)

	Xiaopei Zhu, Peiyang Xu, Guanning Zeng, Yingpeng Dong, Xiaolin Hu


+ [Gradients Stand-in for Defending Deep Leakage in Federated Learning](https://arxiv.org//abs/2410.08734)

	H. Yi, H. Ren, C. Hu, Y. Li, J. Deng, X. Xie


+ [Adversarial Training Can Provably Improve Robustness: Theoretical Analysis of Feature Learning Process Under Structured Data](https://arxiv.org//abs/2410.08503)

	Binghui Li, Yuanzhi Li


+ [Fragile Giants: Understanding the Susceptibility of Models to Subpopulation Attacks](https://arxiv.org//abs/2410.08872)

	Isha Gupta, Hidde Lycklama, Emanuel Opel, Evan Rose, Anwar Hithnawi


+ [Training on Fake Labels: Mitigating Label Leakage in Split Learning via Secure Dimension Transformation](https://arxiv.org//abs/2410.09125)

	Yukun Jiang, Peiran Wang, Chengguo Lin, Ziyue Huang, Yong Cheng


+ [Multi-Agent Actor-Critics in Autonomous Cyber Defense](https://arxiv.org//abs/2410.09134)

	Mingjun Wang, Remington Dechene


+ [Quantum-Trained Convolutional Neural Network for Deepfake Audio Detection](https://arxiv.org//abs/2410.09250)

	Chu-Hsuan Abraham Lin, Chen-Yu Liu, Samuel Yen-Chi Chen, Kuan-Cheng Chen



# 2024-10-10
+ [Adversarial Robustness Overestimation and Instability in TRADES](https://arxiv.org//abs/2410.07675)

	Jonathan Weiping Li, Ren-Wei Liang, Cheng-Han Yeh, Cheng-Chang Tsai, Kuanchun Yu, Chun-Shien Lu, Shang-Tse Chen


+ [Private Language Models via Truncated Laplacian Mechanism](https://arxiv.org//abs/2410.08027)

	Tianhao Huang, Tao Yang, Ivan Habernal, Lijie Hu, Di Wang


+ [DPL: Cross-quality DeepFake Detection via Dual Progressive Learning](https://arxiv.org//abs/2410.07633)

	Dongliang Zhang, Yunfei Li, Jiaran Zhou, Yuezun Li


+ [Poison-splat: Computation Cost Attack on 3D Gaussian Splatting](https://arxiv.org//abs/2410.08190)

	Jiahao Lu, Yifan Zhang, Qiuhong Shen, Xinchao Wang, Shuicheng Yan


+ [Provable Privacy Attacks on Trained Shallow Neural Networks](https://arxiv.org//abs/2410.07632)

	Guy Smorodinsky, Gal Vardi, Itay Safran


+ [Understanding Adversarially Robust Generalization via Weight-Curvature Index](https://arxiv.org//abs/2410.07719)

	Yuelin Xu, Xiao Zhang


+ [Rethinking Adversarial Inverse Reinforcement Learning: From the Angles of Policy Imitation and Transferable Reward Recovery](https://arxiv.org//abs/2410.07643)

	Yangchun Zhang, Wang Zhou, Yirui Zhou


+ [Invisibility Cloak: Disappearance under Human Pose Estimation via Backdoor Attacks](https://arxiv.org//abs/2410.07670)

	Minxing Zhang, Michael Backes, Xiao Zhang


+ [Catastrophic Cyber Capabilities Benchmark (3CB): Robustly Evaluating LLM Agent Cyber Offense Capabilities](https://arxiv.org//abs/2410.09114)

	Andrey Anurin, Jonathan Ng, Kibo Schaffer, Ziyue Wang, Jason Schreiber, Esben Kran



# 2024-10-09
+ [Hallucinating AI Hijacking Attack: Large Language Models and Malicious Code Recommenders](https://arxiv.org//abs/2410.06462)

	David Noever, Forrest McKee


+ [Break the Visual Perception: Adversarial Attacks Targeting Encoded Visual Tokens of Large Vision-Language Models](https://arxiv.org//abs/2410.06699)

	Yubo Wang, Chaohu Liu, Yanqiu Qu, Haoyu Cao, Deqiang Jiang, Linli Xu


+ [Defending Membership Inference Attacks via Privacy-aware Sparsity Tuning](https://arxiv.org//abs/2410.06814)

	Qiang Hu, Hengxiang Zhang, Hongxin Wei


+ [Understanding Model Ensemble in Transferable Adversarial Attack](https://arxiv.org//abs/2410.06851)

	Wei Yao, Zeliang Zhang, Huayi Tang, Yong Liu


+ [Secure Video Quality Assessment Resisting Adversarial Attacks](https://arxiv.org//abs/2410.06866)

	Ao-Xiang Zhang, Yu Ran, Weixuan Tang, Yuan-Gen Wang, Qingxiao Guan, Chunsheng Yang


+ [PFAttack: Stealthy Attack Bypassing Group Fairness in Federated Learning](https://arxiv.org//abs/2410.06509)

	Jiashi Gao, Ziwei Wang, Xiangyu Zhao, Xin Yao, Xuetao Wei


+ [Average Certified Radius is a Poor Metric for Randomized Smoothing](https://arxiv.org//abs/2410.06895)

	Chenhao Sun, Yuhao Mao, Mark Niklas Müller, Martin Vechev


+ [Adversarial Vulnerability as a Consequence of On-Manifold Inseparibility](https://arxiv.org//abs/2410.06921)

	Rajdeep Haldar, Yue Xing, Qifan Song, Guang Lin


+ [Bots can Snoop: Uncovering and Mitigating Privacy Risks of Bots in Group Chats](https://arxiv.org//abs/2410.06587)

	Kai-Hsiang Chou, Yi-Min Lin, Yi-An Wang, Jonathan Weiping Li, Tiffany Hyun-Jin Kim, Hsu-Chun Hsiao


+ [Mind Your Questions Towards Backdoor Attacks on Text-to-Visualization Models](https://arxiv.org//abs/2410.06782)

	Shuaimin Li, Yuanfeng Song, Xuanang Chen, Anni Peng, Zhuoyue Wan, Chen Jason Zhang, Raymond Chi-Wing Wong


+ [Prompt Infection: LLM-to-LLM Prompt Injection within Multi-Agent Systems](https://arxiv.org//abs/2410.07283)

	Donghyun Lee, Mo Tiwari


+ [Recent advancements in LLM Red-Teaming: Techniques, Defenses, and Ethical Considerations](https://arxiv.org//abs/2410.09097)

	Tarun Raheja, Nilay Pochhi


+ [Instructional Segment Embedding: Improving LLM Safety with Instruction Hierarchy](https://arxiv.org//abs/2410.09102)

	Tong Wu, Shujian Zhang, Kaiqiang Song, Silei Xu, Sanqiang Zhao, Ravi Agrawal, Sathish Reddy Indurthi, Chong Xiang, Prateek Mittal, Wenxuan Zhou


+ [Data Taggants: Dataset Ownership Verification via Harmless Targeted Data Poisoning](https://arxiv.org//abs/2410.09101)

	Wassim Bouaziz, El-Mahdi El-Mhamdi, Nicolas Usunier



# 2024-10-08
+ [Fine-grained Hallucination Detection and Mitigation in Language Model Mathematical Reasoning](https://arxiv.org//abs/2410.06304)

	Ruosen Li, Ziming Luo, Xinya Du


+ [DiffusionGuard: A Robust Defense Against Malicious Diffusion-based Image Editing](https://arxiv.org//abs/2410.05694)

	June Suk Choi, Kyungmin Lee, Jongheon Jeong, Saining Xie, Jinwoo Shin, Kimin Lee


+ [Hyper Adversarial Tuning for Boosting Adversarial Robustness of Pretrained Large Vision Models](https://arxiv.org//abs/2410.05951)

	Kangtao Lv, Huangsen Cao, Kainan Tu, Yihuai Xu, Zhimeng Zhang, Xin Ding, Yongwei Wang


+ [$\textit{X}^2$-DFD: A framework for e${X}$plainable and e${X}$tendable Deepfake Detection](https://arxiv.org//abs/2410.06126)

	Yize Chen, Zhiyuan Yan, Siwei Lyu, Baoyuan Wu


+ [CALoR: Towards Comprehensive Model Inversion Defense](https://arxiv.org//abs/2410.05814)

	Hongyao Yu, Yixiang Qiu, Hao Fang, Bin Chen, Sijin Yu, Bin Wang, Shu-Tao Xia, Ke Xu


+ [Solving robust MDPs as a sequence of static RL problems](https://arxiv.org//abs/2410.06212)

	Adil Zouitine, Matthieu Geist, Emmanuel Rachelson


+ [Filtered Randomized Smoothing: A New Defense for Robust Modulation Classification](https://arxiv.org//abs/2410.06339)

	Wenhan Zhang, Meiyu Zhong, Ravi Tandon, Marwan Krunz



# 2024-10-07
+ [Patch is Enough: Naturalistic Adversarial Patch against Vision-Language Pre-training Models](https://arxiv.org//abs/2410.04884)

	Dehong Kong, Siyuan Liang, Xiaopeng Zhu, Yuansheng Zhong, Wenqi Ren


+ [Defense-as-a-Service: Black-box Shielding against Backdoored Graph Models](https://arxiv.org//abs/2410.04916)

	Xiao Yang, Kai Zhou, Yuni Lai, Gaolei Li


+ [Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality](https://arxiv.org//abs/2410.04780)

	Guanyu Zhou, Yibo Yan, Xin Zou, Kun Wang, Aiwei Liu, Xuming Hu


+ [CAT: Concept-level backdoor ATtacks for Concept Bottleneck Models](https://arxiv.org//abs/2410.04823)

	Songning Lai, Jiayu Yang, Yu Huang, Lijie Hu, Tianlang Xue, Zhangyi Hu, Jiaxu Li, Haicheng Liao, Yutao Yue


+ [MIBench: A Comprehensive Benchmark for Model Inversion Attack and Defense](https://arxiv.org//abs/2410.05159)

	Yixiang Qiu, Hongyao Yu, Hao Fang, Wenbo Yu, Bin Chen, Xuan Wang, Shu-Tao Xia, Ke Xu


+ [On the Adversarial Risk of Test Time Adaptation: An Investigation into Realistic Test-Time Data Poisoning](https://arxiv.org//abs/2410.04682)

	Yongyi Su, Yushu Li, Nanqing Liu, Kui Jia, Xulei Yang, Chuan-Sheng Foo, Xun Xu


+ [FRIDA: Free-Rider Detection using Privacy Attacks](https://arxiv.org//abs/2410.05020)

	Pol G. Recasens, Ádám Horváth, Alberto Gutierrez-Torre, Jordi Torres, Josep Ll.Berral, Balázs Pejó


+ [LOTOS: Layer-wise Orthogonalization for Training Robust Ensembles](https://arxiv.org//abs/2410.05136)

	Ali Ebrahimpour-Boroojeny, Hari Sundaram, Varun Chandrasekaran


+ [AnyAttack: Towards Large-scale Self-supervised Generation of Targeted Adversarial Examples for Vision-Language Models](https://arxiv.org//abs/2410.05346)

	Jiaming Zhang, Junhong Ye, Xingjun Ma, Yige Li, Yunfan Yang, Jitao Sang, Dit-Yan Yeung



# 2024-10-06
+ [DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination](https://arxiv.org//abs/2410.04514)

	Xuan Gong, Tianshi Ming, Xinpeng Wang, Zhihua Wei


+ [Towards Secure Tuning: Mitigating Security Risks Arising from Benign Instruction Fine-Tuning](https://arxiv.org//abs/2410.04524)

	Yanrui Du, Sendong Zhao, Jiawei Cao, Ming Ma, Danyang Zhao, Fenglei Fan, Ting Liu, Bing Qin


+ [Suspiciousness of Adversarial Texts to Human](https://arxiv.org//abs/2410.04377)

	Shakila Mahjabin Tonni, Pedro Faustini, Mark Dras


+ [DiffusionFake: Enhancing Generalization in Deepfake Detection via Guided Stable Diffusion](https://arxiv.org//abs/2410.04372)

	Ke Sun, Shen Chen, Taiping Yao, Hong Liu, Xiaoshuai Sun, Shouhong Ding, Rongrong Ji


+ [Robustness Reprogramming for Representation Learning](https://arxiv.org//abs/2410.04577)

	Zhichao Hou, MohamadAli Torkamani, Hamid Krim, Xiaorui Liu



# 2024-10-05
+ [Harnessing Task Overload for Scalable Jailbreak Attacks on Large Language Models](https://arxiv.org//abs/2410.04190)

	Yiting Dong, Guobin Shen, Dongcheng Zhao, Xiang He, Yi Zeng



# 2024-10-04
+ [Model-Based Reward Shaping for Adversarial Inverse Reinforcement Learning in Stochastic Environments](https://arxiv.org//abs/2410.03847)

	Simon Sinong Zhan, Qingyuan Wu, Philip Wang, Yixuan Wang, Ruochen Jiao, Chao Huang, Qi Zhu


+ [Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step](https://arxiv.org//abs/2410.03869)

	Wenxuan Wang, Kuiyi Gao, Zihan Jia, Youliang Yuan, Jen-tse Huang, Qiuzhi Liu, Shuai Wang, Wenxiang Jiao, Zhaopeng Tu


+ [A Brain-Inspired Regularizer for Adversarial Robustness](https://arxiv.org//abs/2410.03952)

	Elie Attias, Cengiz Pehlevan, Dina Obeid


+ [You Know What I'm Saying -- Jailbreak Attack via Implicit Reference](https://arxiv.org//abs/2410.03857)

	Tianyu Wu, Lingrui Mei, Ruibin Yuan, Lujun Li, Wei Xue, Yike Guo



# 2024-10-03
+ [Beyond correlation: The impact of human uncertainty in measuring the effectiveness of automatic evaluation and LLM-as-a-judge](https://arxiv.org//abs/2410.03775)

	Aparna Elangovan, Jongwoo Ko, Lei Xu, Mahsa Elyasi, Ling Liu, Sravan Bodapati, Dan Roth


+ [LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations](https://arxiv.org//abs/2410.02707)

	Hadas Orgad, Michael Toker, Zorik Gekhman, Roi Reichart, Idan Szpektor, Hadas Kotek, Yonatan Belinkov


+ [DaWin: Training-free Dynamic Weight Interpolation for Robust Adaptation](https://arxiv.org//abs/2410.03782)

	Changdae Oh, Yixuan Li, Kyungwoo Song, Sangdoo Yun, Dongyoon Han


+ [CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification](https://arxiv.org//abs/2410.03038)

	Jinghao Shi, Xiang Shen, Kaili Zhao, Xuedong Wang, Vera Wen, Zixuan Wang, Yifan Wu, Zhixin Zhang



# 2024-10-02
+ [Hidden in Plain Text: Emergence & Mitigation of Steganographic Collusion in LLMs](https://arxiv.org//abs/2410.03768)

	Yohan Mathew, Ollie Matthews, Robert McCarthy, Joan Velja, Christian Schroeder de Witt, Dylan Cope, Nandi Schoots



# 2024-09-26
+ [Showing Many Labels in Multi-label Classification Models: An Empirical Study of Adversarial Examples](https://arxiv.org//abs/2409.17568)

	Yujiang Liu, Wenjian Luo, Zhijian Chen, Muhammad Luqman Naseem


+ [DarkSAM: Fooling Segment Anything Model to Segment Nothing](https://arxiv.org//abs/2409.17874)

	Ziqi Zhou, Yufei Song, Minghui Li, Shengshan Hu, Xianlong Wang, Leo Yu Zhang, Dezhong Yao, Hai Jin


+ [Improving Fast Adversarial Training via Self-Knowledge Guidance](https://arxiv.org//abs/2409.17589)

	Chengze Jiang, Junkai Wang, Minjing Dong, Jie Gui, Xinli Shi, Yuan Cao, Yuan Yan Tang, James Tin-Yau Kwok


+ [TA-Cleaner: A Fine-grained Text Alignment Backdoor Defense Strategy for Multimodal Contrastive Learning](https://arxiv.org//abs/2409.17601)

	Yuan Xun, Siyuan Liang, Xiaojun Jia, Xinwei Liu, Xiaochun Cao


+ [Efficient Bias Mitigation Without Privileged Information](https://arxiv.org//abs/2409.17691)

	Mateo Espinosa Zarlenga, Swami Sankaranarayanan, Jerone T. A. Andrews, Zohreh Shams, Mateja Jamnik, Alice Xiang


+ [MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for Prompt Attacks](https://arxiv.org//abs/2409.17699)

	Giandomenico Cornacchia, Giulio Zizzo, Kieran Fraser, Muhammad Zaid Hamed, Ambrish Rawat, Mark Purcell


+ [Federated Learning under Attack: Improving Gradient Inversion for Batch of Images](https://arxiv.org//abs/2409.17767)

	Luiz Leite, Yuri Santo, Bruno L. Dalmazo, André Riker


+ [Faithfulness and the Notion of Adversarial Sensitivity in NLP Explanations](https://arxiv.org//abs/2409.17774)

	Supriya Manna, Niladri Sett


+ [PhantomLiDAR: Cross-modality Signal Injection Attacks against LiDAR](https://arxiv.org//abs/2409.17907)

	Zizhi Jin, Qinhong Jiang, Xuancun Lu, Chen Yan, Xiaoyu Ji, Wenyuan Xu


+ [Weak-To-Strong Backdoor Attacks for LLMs with Contrastive Knowledge Distillation](https://arxiv.org//abs/2409.17946)

	Shuai Zhao, Leilei Gan, Zhongliang Guo, Xiaobao Wu, Luwei Xiao, Xiaoyu Xu, Cong-Duy Nguyen, Luu Anh Tuan


+ [An Adversarial Perspective on Machine Unlearning for AI Safety](https://arxiv.org//abs/2409.18025)

	Jakub Łucki, Boyi Wei, Yangsibo Huang, Peter Henderson, Florian Tramèr, Javier Rando


+ [DARE: Diverse Visual Question Answering with Robustness Evaluation](https://arxiv.org//abs/2409.18023)

	Hannah Sterz, Jonas Pfeiffer, Ivan Vulić


+ [RED QUEEN: Safeguarding Large Language Models against Concealed Multi-Turn Jailbreaking](https://arxiv.org//abs/2409.17458)

	Yifan Jiang, Kriti Aggarwal, Tanmay Laud, Kashif Munir, Jay Pujara, Subhabrata Mukherjee


+ [HaloScope: Harnessing Unlabeled LLM Generations for Hallucination Detection](https://arxiv.org//abs/2409.17504)

	Xuefeng Du, Chaowei Xiao, Yixuan Li


+ [Dark Miner: Defend against unsafe generation for text-to-image diffusion models](https://arxiv.org//abs/2409.17682)

	Zheling Meng, Bo Peng, Xiaochuan Jin, Yue Jiang, Jing Dong, Wei Wang, Tieniu Tan


+ [Perturb, Attend, Detect and Localize (PADL): Robust Proactive Image Defense](https://arxiv.org//abs/2409.17941)

	Filippo Bartolucci, Iacopo Masi, Giuseppe Lisanti


+ [CNCA: Toward Customizable and Natural Generation of Adversarial Camouflage for Vehicle Detectors](https://arxiv.org//abs/2409.17963)

	Linye Lyu, Jiawei Zhou, Daojing He, Yu Li


+ [Cross-Modality Attack Boosted by Gradient-Evolutionary Multiform Optimization](https://arxiv.org//abs/2409.17977)

	Yunpeng Gong, Qingyuan Zeng, Dejun Xu, Zhenzhong Wang, Min Jiang



# 2024-09-25
+ [Claim-Guided Textual Backdoor Attack for Practical Applications](https://arxiv.org//abs/2409.16618)

	Minkyoo Song, Hanna Kim, Jaehan Kim, Youngjin Jin, Seungwon Shin


+ [RoleBreak: Character Hallucination as a Jailbreak Attack in Role-Playing Systems](https://arxiv.org//abs/2409.16727)

	Yihong Tang, Bo Wang, Xu Wang, Dongming Zhao, Jing Liu, Jijun Zhang, Ruifang He, Yuexian Hou


+ [EventHallusion: Diagnosing Event Hallucinations in Video LLMs](https://arxiv.org//abs/2409.16597)

	Jiacheng Zhang, Yang Jiao, Shaoxiang Chen, Jingjing Chen, Yu-Gang Jiang


+ [Verified Relative Safety Margins for Neural Network Twins](https://arxiv.org//abs/2409.16726)

	Anahita Baninajjar, Kamran Hosseini, Ahmed Rezine, Amir Aminifar


+ [RESAA: A Removal and Structural Analysis Attack Against Compound Logic Locking](https://arxiv.org//abs/2409.16959)

	Felipe Almeida, Levent Aksoy, Samuel Pagliarini


+ [Transient Adversarial 3D Projection Attacks on Object Detection in Autonomous Driving](https://arxiv.org//abs/2409.17403)

	Ce Zhou, Qiben Yan, Sijia Liu


+ [Optical Lens Attack on Deep Learning Based Monocular Depth Estimation](https://arxiv.org//abs/2409.17376)

	Ce Zhou, Qiben Yan, Daniel Kent, Guangjing Wang, Ziqi Zhang, Hayder Radha


+ [SHEATH: Defending Horizontal Collaboration for Distributed CNNs against Adversarial Noise](https://arxiv.org//abs/2409.17279)

	Muneeba Asif, Mohammad Kumail Kazmi, Mohammad Ashiqur Rahman, Syed Rafay Hasan, Soamar Homsi



# 2024-09-24
+ [Revisiting Acoustic Features for Robust ASR](https://arxiv.org//abs/2409.16399)

	Muhammad A. Shah, Bhiksha Raj


+ [A Unified Hallucination Mitigation Framework for Large Vision-Language Models](https://arxiv.org//abs/2409.16494)

	Yue Chang, Liqiang Jing, Xiaopeng Zhang, Yue Zhang


+ [Proactive Schemes: A Survey of Adversarial Attacks for Social Good](https://arxiv.org//abs/2409.16491)

	Vishal Asnani, Xi Yin, Xiaoming Liu



# 2024-09-23
+ [Toward Mixture-of-Experts Enabled Trustworthy Semantic Communication for 6G Networks](https://arxiv.org//abs/2409.15695)

	Jiayi He, Xiaofeng Luo, Jiawen Kang, Hongyang Du, Zehui Xiong, Ci Chen, Dusit Niyato, Xuemin Shen


+ [Adversarial Federated Consensus Learning for Surface Defect Classification Under Data Heterogeneity in IIoT](https://arxiv.org//abs/2409.15711)

	Jixuan Cui, Jun Li, Zhen Mei, Yiyang Ni, Wen Chen, Zengxiang Li


+ [Adversarial Watermarking for Face Recognition](https://arxiv.org//abs/2409.16056)

	Yuguang Yao, Anil Jain, Sijia Liu


+ [Towards Robust Object Detection: Identifying and Removing Backdoors via Module Inconsistency Analysis](https://arxiv.org//abs/2409.16057)

	Xianda Zhang, Siyuan Liang


+ [Adversarial Backdoor Defense in CLIP](https://arxiv.org//abs/2409.15968)

	Junhao Kuang, Siyuan Liang, Jiawei Liang, Kuanrong Liu, Xiaochun Cao


+ [Benchmarking Robustness of Endoscopic Depth Estimation with Synthetically Corrupted Data](https://arxiv.org//abs/2409.16063)

	An Wang, Haochen Yin, Beilei Cui, Mengya Xu, Hongliang Ren


+ [Data Poisoning-based Backdoor Attack Framework against Supervised Learning Rules of Spiking Neural Networks](https://arxiv.org//abs/2409.15670)

	Lingxin Jin, Meiyu Lin, Wei Jiang, Jinyu Zhan


+ [Smart Grid Security: A Verified Deep Reinforcement Learning Framework to Counter Cyber-Physical Attacks](https://arxiv.org//abs/2409.15757)

	Suman Maiti, Soumyajit Dey



# 2024-09-23
+ [PROMPTFUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs](https://arxiv.org//abs/2409.14729)

	Jiahao Yu, Yangguang Shao, Hanwen Miao, Junzheng Shi, Xinyu Xing


+ [Effective and Evasive Fuzz Testing-Driven Jailbreaking Attacks against LLMs](https://arxiv.org//abs/2409.14866)

	Xueluan Gong, Mingzhe Li, Yilin Zhang, Fengyuan Ran, Chen Chen, Yanjiao Chen, Qian Wang, Kwok-Yan Lam


+ [Robust Federated Learning Over the Air: Combating Heavy-Tailed Noise with Median Anchored Clipping](https://arxiv.org//abs/2409.15100)

	Jiaxing Li, Zihan Chen, Kai Fong Ernest Chong, Bikramjit Das, Tony Q. S. Quek, Howard H. Yang


+ [Towards Efficient and Robust VQA-NLE Data Generation with Large Vision-Language Models](https://arxiv.org//abs/2409.14785)

	Patrick Amadeus Irawan, Genta Indra Winata, Samuel Cahyawijaya, Ayu Purwarianti


+ [Evaluating the Usability of LLMs in Threat Intelligence Enrichment](https://arxiv.org//abs/2409.15072)

	Sanchana Srikanth, Mohammad Hasanuzzaman, Farah Tasnur Meem


+ [Improving Adversarial Robustness for 3D Point Cloud Recognition at Test-Time through Purified Self-Training](https://arxiv.org//abs/2409.14940)

	Jinpeng Lin, Xulei Yang, Tianrui Li, Xun Xu


+ [Interpretability-Guided Test-Time Adversarial Defense](https://arxiv.org//abs/2409.15190)

	Akshay Kulkarni, Tsui-Wei Weng


+ [RoWSFormer: A Robust Watermarking Framework with Swin Transformer for Enhanced Geometric Attack Resilience](https://arxiv.org//abs/2409.14829)

	Weitong Chen, Yuheng Li


+ [SDBA: A Stealthy and Long-Lasting Durable Backdoor Attack in Federated Learning](https://arxiv.org//abs/2409.14805)

	Minyeong Choe, Cheolhee Park, Changho Seo, Hyunil Kim


+ [Attack Atlas: A Practitioner's Perspective on Challenges and Pitfalls in Red Teaming GenAI](https://arxiv.org//abs/2409.15398)

	Ambrish Rawat, Stefan Schoepf, Giulio Zizzo, Giandomenico Cornacchia, Muhammad Zaid Hameed, Kieran Fraser, Erik Miehling, Beat Buesser, Elizabeth M. Daly, Mark Purcell, Prasanna Sattigeri, Pin-Yu Chen, Kush R. Varshney


+ [In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models](https://arxiv.org//abs/2409.15454)

	Pengrui Han, Peiyang Song, Haofei Yu, Jiaxuan You


# 2024-09-22
+ [Dormant: Defending against Pose-driven Human Image Animation](https://arxiv.org//abs/2409.14424)

	Jiachen Zhou, Mingsi Wang, Tianlin Li, Guozhu Meng, Kai Chen


+ [Enhancing LLM-based Autonomous Driving Agents to Mitigate Perception Attacks](https://arxiv.org//abs/2409.14488)

	Ruoyu Song, Muslum Ozgur Ozmen, Hyungsub Kim, Antonio Bianchi, Z. Berkay Celik


+ [Evaluating the Performance and Robustness of LLMs in Materials Science Q&A and Property Predictions](https://arxiv.org//abs/2409.14572)

	Hongchen Wang, Kangming Li, Scott Ramsay, Yao Fehlis, Edward Kim, Jason Hattrick-Simpers


+ [Backtracking Improves Generation Safety](https://arxiv.org//abs/2409.14586)

	Yiming Zhang, Jianfeng Chi, Hailey Nguyen, Kartikeya Upasani, Daniel M. Bikel, Jason Weston, Eric Michael Smith



# 2024-09-21
+ [PathSeeker: Exploring LLM Security Vulnerabilities with a Reinforcement Learning-Based Jailbreak Approach](https://arxiv.org//abs/2409.14177)

	Zhihao Lin, Wei Ma, Mingyi Zhou, Yanjie Zhao, Haoyu Wang, Yang Liu, Jun Wang, Li Li


+ [Data-centric NLP Backdoor Defense from the Lens of Memorization](https://arxiv.org//abs/2409.14200)

	Zhenting Wang, Zhizhi Wang, Mingyu Jin, Mengnan Du, Juan Zhai, Shiqing Ma


+ [When Witnesses Defend: A Witness Graph Topological Layer for Adversarial Graph Learning](https://arxiv.org//abs/2409.14161)

	Naheed Anjum Arafat, Debabrota Basu, Yulia Gel, Yuzhou Chen


+ [Adversarial Attacks on Parts of Speech: An Empirical Study in Text-to-Image Generation](https://arxiv.org//abs/2409.15381)

	G M Shahariar, Jia Chen, Jiachen Li, Yue Dong



# 2024-09-20
+ [Relationship between Uncertainty in DNNs and Adversarial Attacks](https://arxiv.org//abs/2409.13232)

	Abigail Adeniran, Adewale Adeyemo


+ [ID-Guard: A Universal Framework for Combating Facial Manipulation via Breaking Identification](https://arxiv.org//abs/2409.13349)

	Zuomin Qu, Wei Lu, Xiangyang Luo, Qian Wang, Xiaochun Cao


+ [Certified Adversarial Robustness via Partition-based Randomized Smoothing](https://arxiv.org//abs/2409.13546)

	Hossein Goli, Farzan Farnia


+ [Efficient Visualization of Neural Networks with Generative Models and Adversarial Perturbations](https://arxiv.org//abs/2409.13559)

	Athanasios Karagounis


+ [Neurosymbolic Conformal Classification](https://arxiv.org//abs/2409.13585)

	Arthur Ledaguenel, Céline Hudelot, Mostepha Khouadjia


+ [Applying Pre-trained Multilingual BERT in Embeddings for Improved Malicious Prompt Injection Attacks Detection](https://arxiv.org//abs/2409.13331)

	Md Abdur Rahman, Hossain Shahriar, Fan Wu, Alfredo Cuzzocrea


+ [Robust Salient Object Detection on Compressed Images Using Convolutional Neural Networks](https://arxiv.org//abs/2409.13464)

	Guibiao Liao, Wei Gao


+ [PureDiffusion: Using Backdoor to Counter Backdoor in Generative Diffusion Models](https://arxiv.org//abs/2409.13945)

	Vu Tuan Truong, Long Bao Le


+ [MAGICS: Adversarial RL with Minimax Actors Guided by Implicit Critic Stackelberg for Convergent Neural Synthesis of Robot Safety](https://arxiv.org//abs/2409.13867)

	Justin Wang, Haimin Hu, Duy Phuong Nguyen, Jaime Fernández Fisac


+ [ViTGuard: Attention-aware Detection against Adversarial Examples for Vision Transformer](https://arxiv.org//abs/2409.13828)

	Shihua Sun, Kenechukwu Nwodo, Shridatt Sugrim, Angelos Stavrou, Haining Wang


+ [Persistent Backdoor Attacks in Continual Learning](https://arxiv.org//abs/2409.13864)

	Zhen Guo, Abhinav Kumar, Reza Tourani



# 2024-09-19
+ [Privacy-Preserving Student Learning with Differentially Private Data-Free Distillation](https://arxiv.org//abs/2409.12384)

	Bochao Liu, Jianghu Lu, Pengju Wang, Junjie Zhang, Dan Zeng, Zhenxing Qian, Shiming Ge


+ [Channel-Aware Domain-Adaptive Generative Adversarial Network for Robust Speech Recognition](https://arxiv.org//abs/2409.12386)

	Chien-Chun Wang, Li-Wei Chen, Cheng-Kang Chou, Hung-Shin Lee, Berlin Chen, Hsin-Min Wang


+ [ITPatch: An Invisible and Triggered Physical Adversarial Patch against Traffic Sign Recognition](https://arxiv.org//abs/2409.12394)

	Shuai Yuan, Hongwei Li, Xingshuo Han, Guowen Xu, Wenbo Jiang, Tao Ni, Qingchuan Zhao, Yuguang Fang


+ [TEAM: Temporal Adversarial Examples Attack Model against Network Intrusion Detection System Applied to RNN](https://arxiv.org//abs/2409.12472)

	Ziyi Liu, Dengpan Ye, Long Tang, Yunming Zhang, Jiacheng Deng


+ [The Robustness of Spiking Neural Networks in Communication and its Application towards Network Efficiency in Federated Learning](https://arxiv.org//abs/2409.12769)

	Manh V. Nguyen, Liang Zhao, Bobin Deng, William Severa, Honghui Xu, Shaoen Wu


+ [Defending against Reverse Preference Attacks is Difficult](https://arxiv.org//abs/2409.12914)

	Domenic Rosati, Giles Edkins, Harsh Raj, David Atanasov, Subhabrata Majumdar, Janarthanan Rajendran, Frank Rudzicz, Hassan Sajjad


+ [Enhancing 3D Robotic Vision Robustness by Minimizing Adversarial Mutual Information through a Curriculum Training Approach](https://arxiv.org//abs/2409.12379)

	Nastaran Darabi, Dinithi Jayasuriya, Devashri Naik, Theja Tulabandhula, Amit Ranjan Trivedi


+ [Revisiting Semi-supervised Adversarial Robustness via Noise-aware Online Robust Distillation](https://arxiv.org//abs/2409.12946)

	Tsung-Han Wu, Hung-Ting Su, Shang-Tse Chen, Winston H. Hsu


+ [On the Regret of Coded Caching with Adversarial Requests](https://arxiv.org//abs/2409.12387)

	Anupam Nayak, Kota Srinivas Reddy, Nikhil Karamchandani


+ [On the Hardness of Decentralized Multi-Agent Policy Evaluation under Byzantine Attacks](https://arxiv.org//abs/2409.12882)

	Hairi, Minghong Fang, Zifan Zhang, Alvaro Velasquez, Jia Liu


+ [VCAT: Vulnerability-aware and Curiosity-driven Adversarial Training for Enhancing Autonomous Vehicle Robustness](https://arxiv.org//abs/2409.12997)

	Xuan Cai, Zhiyong Cui, Xuesong Bai, Ruimin Ke, Zhenshu Ma, Haiyang Yu, Yilong Ren


+ [FedAT: Federated Adversarial Training for Distributed Insider Threat Detection](https://arxiv.org//abs/2409.13083)

	R G Gayathri, Atul Sajjanhar, Md Palash Uddin, Yong Xiang



# 2024-09-18
+ [GReDP: A More Robust Approach for Differential Privacy Training with Gradient-Preserving Noise Reduction](https://arxiv.org//abs/2409.11663)

	Haodi Wang, Tangyu Jiang, Yu Guo, Xiaohua Jia, Chengjun Cai


+ [NPAT Null-Space Projected Adversarial Training Towards Zero Deterioration](https://arxiv.org//abs/2409.11754)

	Hanyi Hu, Qiao Han, Kui Chen, Yao Yang


+ [PAD-FT: A Lightweight Defense for Backdoor Attacks via Data Purification and Fine-Tuning](https://arxiv.org//abs/2409.12072)

	Yukai Xu, Yujie Gu, Kouichi Sakurai



# 2024-09-17
+ [Jailbreaking Large Language Models with Symbolic Mathematics](https://arxiv.org//abs/2409.11445)

	Emet Bethany, Mazal Bethany, Juan Arturo Nolazco Flores, Sumit Kumar Jha, Peyman Najafirad


+ [Golden Ratio Search: A Low-Power Adversarial Attack for Deep Learning based Modulation Classification](https://arxiv.org//abs/2409.11454)

	Deepsayan Sadhukhan, Nitin Priyadarshini Shankar, Sheetal Kalyani


# 2024-09-16
+ [HALO: Hallucination Analysis and Learning Optimization to Empower LLMs with Retrieval-Augmented Context for Guided Clinical Decision Making](https://arxiv.org//abs/2409.10011)

	Sumera Anjum, Hanzhi Zhang, Wenjun Zhou, Eun Jin Paek, Xiaopeng Zhao, Yunhe Feng


+ [Towards Physically-Realizable Adversarial Attacks in Embodied Vision Navigation](https://arxiv.org//abs/2409.10071)

	Meng Chen, Jiawei Tu, Chao Qi, Yonghao Dang, Feng Zhou, Wei Wei, Jianqin Yin



# 2024-09-12
+ [A Spatiotemporal Stealthy Backdoor Attack against Cooperative Multi-Agent Deep Reinforcement Learning](https://arxiv.org//abs/2409.07775)

	Yinbo Yu, Saihao Yan, Jiajia Liu


+ [Attack End-to-End Autonomous Driving through Module-Wise Noise](https://arxiv.org//abs/2409.07706)

	Lu Wang, Tianyuan Zhang, Yikai Han, Muyang Fang, Ting Jin, Jiaqi Kang


+ [Unleashing Worms and Extracting Data: Escalating the Outcome of Attacks against RAG-based Inference in Scale and Severity Using Jailbreaking](https://arxiv.org//abs/2409.08045)

	Stav Cohen, Ron Bitton, Ben Nassi


+ [LoRID: Low-Rank Iterative Diffusion for Adversarial Purification](https://arxiv.org//abs/2409.08255)

	Geigh Zollicoffer, Minh Vu, Ben Nebgen, Juan Castorena, Boian Alexandrov, Manish Bhattarai


+ [GAZEploit: Remote Keystroke Inference Attack by Gaze Estimation from Avatar Views in VR/MR Devices](https://arxiv.org//abs/2409.08122)

	Hanqiu Wang, Zihao Zhan, Haoqi Shan, Siqi Dai, Max Panoff, Shuo Wang


+ [Efficient Privacy-Preserving KAN Inference Using Homomorphic Encryption](https://arxiv.org//abs/2409.07751)

	Zhizheng Lai, Yufei Zhou, Peijia Zheng, Lin Chen


+ [DFDG: Data-Free Dual-Generator Adversarial Distillation for One-Shot Federated Learning](https://arxiv.org//abs/2409.07734)

	Kangyang Luo, Shuai Wang, Yexuan Fu, Renrong Shao, Xiang Li, Yunshi Lan, Ming Gao, Jinlong Shu


+ [Detecting and Defending Against Adversarial Attacks on Automatic Speech Recognition via Diffusion Models](https://arxiv.org/abs/2409.07936)

	Nikolai L. Kühne, Astrid H. F. Kitchen, Marie S. Jensen, Mikkel S. L. Brøndt, Martin Gonzalez, Christophe Biscio, Zheng-Hua Tan


+ [LogoRA: Local-Global Representation Alignment for Robust Time Series Classification](https://arxiv.org//abs/2409.12169)

	Huanyu Zhang, Yi-Fan Zhang, Zhang Zhang, Qingsong Wen, Liang Wang


+ [Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large Language Models on Generated Data](https://arxiv.org//abs/2409.11423)

	Atilla Akkus, Mingjie Li, Junjie Chu, Michael Backes, Yang Zhang, Sinem Sav



# 2024-09-11
+ [Cyber Deception: State of the art, Trends and Open challenges](https://arxiv.org//abs/2409.07194)

	Pedro Beltrán López, Manuel Gil Pérez, Pantaleone Nespoli


+ [Exploring User-level Gradient Inversion with a Diffusion Prior](https://arxiv.org//abs/2409.07291)

	Zhuohang Li, Andrew Lowy, Jing Liu, Toshiaki Koike-Akino, Bradley Malin, Kieran Parsons, Ye Wang


+ [Module-wise Adaptive Adversarial Training for End-to-end Autonomous Driving](https://arxiv.org//abs/2409.07321)

	Tianyuan Zhang, Lu Wang, Jiaqi Kang, Xinwei Zhang, Siyuan Liang, Yuwei Chen, Aishan Liu, Xianglong Liu


+ [Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks](https://arxiv.org//abs/2409.07353)

	Md Zarif Hossain, Ahmed Imteaj


+ [Introducing Perturb-ability Score (PS) to Enhance Robustness Against Evasion Adversarial Attacks on ML-NIDS](https://arxiv.org//abs/2409.07448)

	Mohamed elShehaby, Ashraf Matrawy


+ [AdvLogo: Adversarial Patch Attack against Object Detectors based on Diffusion Models](https://arxiv.org//abs/2409.07002)

	Boming Miao, Chunxiao Li, Yao Zhu, Weixiang Sun, Zizhe Wang, Xiaoyi Wang, Chuanlong Xie


+ [AdaPPA: Adaptive Position Pre-Fill Jailbreak Attack Approach Targeting LLMs](https://arxiv.org//abs/2409.07503)

	Lijia Lv, Weigang Zhang, Xuehai Tang, Jie Wen, Feng Liu, Jizhong Han, Songlin Hu


+ [A Cost-Aware Approach to Adversarial Robustness in Neural Networks](https://arxiv.org//abs/2409.07609)

	Charles Meyers, Mohammad Reza Saleh Sedghpour, Tommy Löfstedt, Erik Elmroth


+ [Context-Aware Membership Inference Attacks against Pre-trained Large Language Models](https://arxiv.org//abs/2409.13745)

	Hongyan Chang, Ali Shahin Shamsabadi, Kleomenis Katevas, Hamed Haddadi, Reza Shokri



# 2024-09-10
+ [On the Weaknesses of Backdoor-based Model Watermarking: An Information-theoretic Perspective](https://arxiv.org//abs/2409.06130)

	Aoting Hu, Yanzhi Chen, Renjie Xie, Adrian Weller


+ [Towards Robust Uncertainty-Aware Incomplete Multi-View Classification](https://arxiv.org//abs/2409.06270)

	Mulin Chen, Haojian Huang, Qiang Li


+ [LAMP: Learnable Meta-Path Guided Adversarial Contrastive Learning for Heterogeneous Graphs](https://arxiv.org//abs/2409.06323)

	Siqing Li, Jin-Duk Park, Wei Huang, Xin Cao, Won-Yong Shin, Zhiqiang Xu


+ [Unrevealed Threats: A Comprehensive Study of the Adversarial Robustness of Underwater Image Enhancement Models](https://arxiv.org//abs/2409.06420)

	Siyu Zhai, Zhibo He, Xiaofeng Cong, Junming Hou, Jie Gui, Jian Wei You, Xin Gong, James Tin-Yau Kwok, Yuan Yan Tang


+ [BACKRUNNER: Mitigating Smart Contract Attacks in the Real World](https://arxiv.org//abs/2409.06213)

	Chaofan Shou, Yuanyu Ke, Yupeng Yang, Qi Su, Or Dadosh, Assaf Eli, David Benchimol, Doudou Lu, Daniel Tong, Dex Chen, Zoey Tan, Jacob Chia, Koushik Sen, Wenke Lee


+ [Adversary Resilient Learned Bloom Filters](https://arxiv.org//abs/2409.06556)

	Allison Bishop, Hayder Tirmazi


+ [Adversarial Attacks to Multi-Modal Models](https://arxiv.org//abs/2409.06793)

	Zhihao Dou, Xin Hu, Haibo Yang, Zhuqing Liu, Minghong Fang


+ [DV-FSR: A Dual-View Target Attack Framework for Federated Sequential Recommendation](https://arxiv.org//abs/2409.07500)

	Qitao Qin, Yucong Luo, Mingyue Cheng, Qingyang Mao, Chenyi Lei



# 2024-09-09
+ [TERD: A Unified Framework for Safeguarding Diffusion Models Against Backdoors](https://arxiv.org//abs/2409.05294)

	Yichuan Mo, Hui Huang, Mingjie Li, Ang Li, Yisen Wang


+ [Seeing Through the Mask: Rethinking Adversarial Examples for CAPTCHAs](https://arxiv.org//abs/2409.05558)

	Yahya Jabary, Andreas Plesner, Turlan Kuzhagaliyev, Roger Wattenhofer


+ [DSDFormer: An Innovative Transformer-Mamba Framework for Robust High-Precision Driver Distraction Identification](https://arxiv.org//abs/2409.05587)

	Junzhou Chen, Zirui Zhang, Jing Yu, Heqiang Huang, Ronghui Zhang, Xuemiao Xu, Bin Sheng, Hong Yan


+ [Adversarial Attacks on Data Attribution](https://arxiv.org//abs/2409.05657)

	Xinhe Wang, Pingbang Hu, Junwei Deng, Jiaqi W. Ma



# 2024-09-08
+ [PIP: Detecting Adversarial Examples in Large Vision-Language Models via Attention Patterns of Irrelevant Probe Questions](https://arxiv.org//abs/2409.05076)

	Yudong Zhang, Ruobing Xie, Jiansheng Chen, Xingwu Sun, Yu Wang


+ [Vision-fused Attack: Advancing Aggressive and Stealthy Adversarial Text against Neural Machine Translation](https://arxiv.org//abs/2409.05021)

	Yanni Xue, Haojie Hao, Jiakai Wang, Qiang Sheng, Renshuai Tao, Yu Liang, Pu Feng, Xianglong Liu


+ [Natias: Neuron Attribution based Transferable Image Adversarial Steganography](https://arxiv.org//abs/2409.04968)

	Zexin Fan, Kejiang Chen, Kai Zeng, Jiansong Zhang, Weiming Zhang, Nenghai Yu


+ [Sight View Constraint for Robust Point Cloud Registration](https://arxiv.org//abs/2409.05065)

	Yaojie Zhang, Weijun Wang, Tianlun Huang, Zhiyong Wang, Wei Feng


+ [Can OOD Object Detectors Learn from Foundation Models?](https://arxiv.org//abs/2409.05162)

	Jiahui Liu, Xin Wen, Shizhen Zhao, Yingxian Chen, Xiaojuan Qi


+ [Balancing Security and Accuracy: A Novel Federated Learning Approach for Cyberattack Detection in Blockchain Networks](https://arxiv.org//abs/2409.04972)

	Tran Viet Khoa, Mohammad Abu Alsheikh, Yibeltal Alem, Dinh Thai Hoang



# 2024-09-07
+ [Swin Transformer for Robust Differentiation of Real and Synthetic Images: Intra- and Inter-Dataset Analysis](https://arxiv.org//abs/2409.04734)

	Preetu Mehta, Aman Sagar, Suchi Kumari


+ [PANTS: Practical Adversarial Network Traffic Samples against ML-powered Networking Classifiers](https://arxiv.org//abs/2409.04691)

	Minhao Jin, Maria Apostolaki


+ [PIXHELL Attack: Leaking Sensitive Information from Air-Gap Computers via `Singing Pixels'](https://arxiv.org//abs/2409.04930)

	Mordechai Guri



# 2024-09-06
+ [A First Look At Efficient And Secure On-Device LLM Inference Against KV Leakage](https://arxiv.org//abs/2409.04040)

	Huan Yang, Deyu Zhang, Yudong Zhao, Yuanchun Li, Yunxin Liu


+ [Context is the Key: Backdoor Attacks for In-Context Learning with Vision Transformers](https://arxiv.org//abs/2409.04142)

	Gorka Abad, Stjepan Picek, Lorenzo Cavallaro, Aitor Urbieta


+ [AGR: Age Group fairness Reward for Bias Mitigation in LLMs](https://arxiv.org//abs/2409.04340)

	Shuirong Cao, Ruoxi Cheng, Zhiqiang Wang


+ [Learning to Learn Transferable Generative Attack for Person Re-Identification](https://arxiv.org//abs/2409.04208)

	Yuan Bian, Min Liu, Xueping Wang, Yunfeng Ma, Yaonan Wang



# 2024-09-05
+ [Bypassing DARCY Defense: Indistinguishable Universal Adversarial Triggers](https://arxiv.org//abs/2409.03183)

	Zuquan Peng, Yuanyuan He, Jianbing Ni, Ben Niu


+ [Limited but consistent gains in adversarial robustness by co-training object recognition models with human EEG](https://arxiv.org//abs/2409.03646)

	Manshan Guo, Bhavin Choksi, Sari Sadiya, Alessandro T. Gifford, Martina G. Vilas, Radoslaw M. Cichy, Gemma Roig


+ [Active Fake: DeepFake Camouflage](https://arxiv.org//abs/2409.03200)

	Pu Sun, Honggang Qi, Yuezun Li


+ [Non-Uniform Illumination Attack for Fooling Convolutional Neural Networks](https://arxiv.org//abs/2409.03458)

	Akshay Jain, Shiv Ram Dubey, Satish Kumar Singh, KC Santosh, Bidyut Baran Chaudhuri


+ [A practical approach to evaluating the adversarial distance for machine learning classifiers](https://arxiv.org//abs/2409.03598)

	Georg Siedel, Ekagra Gupta, Andrey Morozov


+ [Simplex-enabled Safe Continual Learning Machine](https://arxiv.org//abs/2409.05898)

	Yihao Cai, Hongpeng Cao, Yanbing Mao, Lui Sha, Marco Caccamo




# 2024-09-04
+ [TASAR: Transferable Attack on Skeletal Action Recognition](https://arxiv.org//abs/2409.02483)

	Yunfeng Diao, Baiqi Wu, Ruixuan Zhang, Ajian Liu, Xingxing Wei, Meng Wang, He Wang


+ [Adversarial Attacks on Machine Learning-Aided Visualizations](https://arxiv.org//abs/2409.02485)

	Takanori Fujiwara, Kostiantyn Kucher, Junpeng Wang, Rafael M. Martins, Andreas Kerren, Anders Ynnerman


+ [AdvSecureNet: A Python Toolkit for Adversarial Machine Learning](https://arxiv.org//abs/2409.02629)

	Melih Catal, Manuel Günther


+ [Alignment-Aware Model Extraction Attacks on Large Language Models](https://arxiv.org//abs/2409.02718)

	Zi Liang, Qingqing Ye, Yanyun Wang, Sen Zhang, Yaxin Xiao, Ronghua Li, Jianliang Xu, Haibo Hu


+ [Benchmarking Spurious Bias in Few-Shot Image Classifiers](https://arxiv.org//abs/2409.02882)

	Guangtao Zheng, Wenqian Ye, Aidong Zhang


+ [Robust Federated Finetuning of Foundation Models via Alternating Minimization of LoRA](https://arxiv.org//abs/2409.02346)

	Shuangyi Chen, Yue Ju, Hardik Dalal, Zhongwen Zhu, Ashish Khisti


+ [Boosting Certificate Robustness for Time Series Classification with Efficient Self-Ensemble](https://arxiv.org//abs/2409.02802)

	Chang Dong, Zhengyang Li, Liangwei Zheng, Weitong Chen, Wei Emma Zhang


+ [Transfer-based Adversarial Poisoning Attacks for Online (MIMO-)Deep Receviers](https://arxiv.org//abs/2409.02430)

	Kunze Wu, Weiheng Jiang, Dusit Niyato, Yinghuan Li, Chuang Luo



# 2024-09-03
+ [Exploiting the Vulnerability of Large Language Models via Defense-Aware Architectural Backdoor](https://arxiv.org//abs/2409.01952)

	Abdullah Arafat Miah, Yu Bi


+ [In Defense of RAG in the Era of Long-Context Language Models](https://arxiv.org//abs/2409.01666)

	Tan Yu, Anbang Xu, Rama Akkiraju


+ [Dynamic Guidance Adversarial Distillation with Enhanced Teacher Knowledge](https://arxiv.org//abs/2409.01627)

	Hyejin Park, Dongbo Min


+ [NoiseAttack: An Evasive Sample-Specific Multi-Targeted Backdoor Attack Through White Gaussian Noise](https://arxiv.org//abs/2409.02251)

	Abdullah Arafat Miah, Kaan Icer, Resit Sendag, Yu Bi


+ [Safeguarding AI Agents: Developing and Analyzing Safety Architectures](https://arxiv.org//abs/2409.03793)

	Ishaan Domkundwar, Mukunda N S



# 2024-09-02
+ [CLIBE: Detecting Dynamic Backdoors in Transformer-based NLP Models](https://arxiv.org//abs/2409.01193)

	Rui Zeng, Xi Chen, Yuwen Pu, Xuhong Zhang, Tianyu Du, Shouling Ji


+ [Towards Robust Online Domain Adaptive Semantic Segmentation under Adverse Weather Conditions](https://arxiv.org//abs/2409.01072)

	Taorong Liu, Jing Xiao, Liang Liao, Chia-Wen Lin


+ [Defending against Model Inversion Attacks via Random Erasing](https://arxiv.org//abs/2409.01062)

	Viet-Hung Tran, Ngoc-Bao Nguyen, Son T. Mai, Hans Vandierendonck, Ngai-man Cheung


+ [Adversarial Pruning: A Survey and Benchmark of Pruning Methods for Adversarial Robustness](https://arxiv.org//abs/2409.01249)

	Giorgio Piras, Maura Pintor, Ambra Demontis, Battista Biggio, Giorgio Giacinto, Fabio Roli


+ [Backdoor Defense through Self-Supervised and Generative Learning](https://arxiv.org//abs/2409.01185)

	Ivan Sabolić, Ivan Grubišić, Siniša Šegvić


+ [Unveiling the Vulnerability of Private Fine-Tuning in Split-Based Frameworks for Large Language Models: A Bidirectionally Enhanced Attack](https://arxiv.org//abs/2409.00960)

	Guanzhong Chen, Zhenhan Qin, Mingxin Yang, Yajie Zhou, Tao Fan, Tianyu Du, Zenglin Xu


+ [No Peer, no Cry: Network Application Fuzzing via Fault Injection](https://arxiv.org//abs/2409.01059)

	Nils Bars, Moritz Schloegel, Nico Schiller, Lukas Bernhard, Thorsten Holz


+ [Phantom: Untargeted Poisoning Attacks on Semi-Supervised Learning (Full Version)](https://arxiv.org//abs/2409.01470)

	Jonathan Knauer, Phillip Rieger, Hossein Fereidooni, Ahmad-Reza Sadeghi



# 2024-09-01
+ [The Dark Side of Human Feedback: Poisoning Large Language Models via User Inputs](https://arxiv.org//abs/2409.00787)

	Bocheng Chen, Hanqing Guo, Guangjing Wang, Yuanda Wang, Qiben Yan


+ [Fisher Information guided Purification against Backdoor Attacks](https://arxiv.org//abs/2409.00863)

	Nazmul Karim, Abdullah Al Arafat, Adnan Siraj Rakin, Zhishan Guo, Nazanin Rahnavard



# 2024-08-31
+ [Robust off-policy Reinforcement Learning via Soft Constrained Adversary](https://arxiv.org//abs/2409.00418)

	Kosuke Nakanishi, Akihiro Kubo, Yuji Yasui, Shin Ishii


+ [Rethinking Backdoor Detection Evaluation for Language Models](https://arxiv.org//abs/2409.00399)

	Jun Yan, Wenjie Jacky Mo, Xiang Ren, Robin Jia


+ [LightPure: Realtime Adversarial Image Purification for Mobile Devices Using Diffusion Models](https://arxiv.org//abs/2409.00340)

	Hossein Khalili, Seongbin Park, Vincent Li, Brandan Bright, Ali Payani, Ramana Rao Kompella, Nader Sehatbakhsh


+ [HSF: Defending against Jailbreak Attacks with Hidden State Filtering](https://arxiv.org//abs/2409.03788)

	Cheng Qian, Hainan Zhang, Lei Sha, Zhiming Zheng



# 2024-08-30
+ [Safety Layers of Aligned Large Language Models: The Key to LLM Security](https://arxiv.org//abs/2408.17003)

	Shen Li, Liuyi Yao, Lan Zhang, Yaliang Li


+ [Instant Adversarial Purification with Adversarial Consistency Distillation](https://arxiv.org//abs/2408.17064)

	Chun Tong Lei, Hon Ming Yam, Zhongliang Guo, Chun Pong Lau


+ [Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language Models for Privacy Leakage](https://arxiv.org//abs/2408.17354)

	Md Rafi Ur Rashid, Jing Liu, Toshiaki Koike-Akino, Shagufta Mehnaz, Ye Wang


+ [Can We Leave Deepfake Data Behind in Training Deepfake Detector?](https://arxiv.org//abs/2408.17052)

	Jikang Cheng, Zhiyuan Yan, Ying Zhang, Yuhao Luo, Zhongyuan Wang, Chen Li



# 2024-08-29
+ [DetectBERT: Towards Full App-Level Representation Learning to Detect Android Malware](https://arxiv.org//abs/2408.16353)

	Tiezhu Sun, Nadia Daoudi, Kisub Kim, Kevin Allix, Tegawendé F. Bissyandé, Jacques Klein


+ [SFR-GNN: Simple and Fast Robust GNNs against Structural Attacks](https://arxiv.org//abs/2408.16537)

	Xing Ai, Guanyu Zhu, Yulin Zhu, Yu Zheng, Gaolei Li, Jianhua Li, Kai Zhou


+ [PromptSmooth: Certifying Robustness of Medical Vision-Language Models via Prompt Learning](https://arxiv.org//abs/2408.16769)

	Noor Hussein, Fahad Shamshad, Muzammal Naseer, Karthik Nandakumar


+ [GL-TSVM: A robust and smooth twin support vector machine with guardian loss function](https://arxiv.org//abs/2408.16336)

	Mushir Akhtar, M. Tanveer, Mohd. Arshad


+ [STEREO: Towards Adversarially Robust Concept Erasing from Text-to-Image Generation Models](https://arxiv.org//abs/2408.16807)

	Koushik Srivatsan, Fahad Shamshad, Muzammal Naseer, Karthik Nandakumar



# 2024-08-28
+ [Evaluating Model Robustness Using Adaptive Sparse L0 Regularization](https://arxiv.org//abs/2408.15702)

	Weiyou Liu, Zhenyang Li, Weitong Chen


+ [Defending Text-to-image Diffusion Models: Surprising Efficacy of Textual Perturbations Against Backdoor Attacks](https://arxiv.org//abs/2408.15721)

	Oscar Chew, Po-Yi Lu, Jayden Lin, Hsuan-Tien Lin


+ [Network transferability of adversarial patches in real-time object detection](https://arxiv.org//abs/2408.15833)

	Jens Bayer, Stefan Becker, David Münch, Michael Arens


+ [Certified Causal Defense with Generalizable Robustness](https://arxiv.org//abs/2408.15451)

	Yiran Qiao, Yu Yin, Chen Chen, Jing Ma


+ [VFLIP: A Backdoor Defense for Vertical Federated Learning via Identification and Purification](https://arxiv.org//abs/2408.15591)

	Yungi Cho, Woorim Han, Miseon Yu, Ho Bae, Yunheung Paek



# 2024-08-27
+ [TART: Boosting Clean Accuracy Through Tangent Direction Guided Adversarial Training](https://arxiv.org//abs/2408.14728)

	Bongsoo Yi, Rongjie Lai, Yao Li


+ [Detecting AI Flaws: Target-Driven Attacks on Internal Faults in Language Models](https://arxiv.org//abs/2408.14853)

	Yuhao Du, Zhuo Li, Pengyu Cheng, Xiang Wan, Anningzhe Gao


+ [Adversarial Attacks and Defenses in Multivariate Time-Series Forecasting for Smart and Connected Infrastructures](https://arxiv.org//abs/2408.14875)

	Pooja Krishan, Rohan Mohapatra, Saptarshi Sengupta


+ [Evidence-Enhanced Triplet Generation Framework for Hallucination Alleviation in Generative Question Answering](https://arxiv.org//abs/2408.15037)

	Haowei Du, Huishuai Zhang, Dongyan Zhao


+ [LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet](https://arxiv.org//abs/2408.15221)

	Nathaniel Li, Ziwen Han, Ian Steneker, Willow Primack, Riley Goodside, Hugh Zhang, Zifan Wang, Cristina Menghini, Summer Yue


+ [Adversarial Manhole: Challenging Monocular Depth Estimation and Semantic Segmentation Models with Patch Attack](https://arxiv.org//abs/2408.14879)

	Naufal Suryanto, Andro Aprila Adiputra, Ahmada Yusril Kadiptya, Yongsu Kim, Howon Kim


+ [Learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation](https://arxiv.org//abs/2408.14738)

	Bochao Liu, Pengju Wang, Shiming Ge


+ [Improving Adversarial Robustness in Android Malware Detection by Reducing the Impact of Spurious Correlations](https://arxiv.org//abs/2408.16025)

	Hamid Bostani, Zhengyu Zhao, Veelasha Moonsamy




# 2024-08-26
+ [Celtibero: Robust Layered Aggregation for Federated Learning](https://arxiv.org//abs/2408.14240)

	Borja Molina-Coronado


+ [MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR Errors with LLM-generated Synthetic Dialogues](https://arxiv.org//abs/2408.14418)

	Kuluhan Binici, Abhinav Ramesh Kashyap, Viktor Schlegel, Andy T. Liu, Vijay Prakash Dwivedi, Thanh-Tung Nguyen, Xiaoxue Gao, Nancy F. Chen, Stefan Winkler


+ [TF-Attack: Transferable and Fast Adversarial Attacks on Large Language Models](https://arxiv.org//abs/2408.13985)

	Zelin Li, Kehai Chen, Xuefeng Bai, Lemao Liu, Mingming Yang, Yang Xiang, Min Zhang


+ [Dual-Path Adversarial Lifting for Domain Shift Correction in Online Test-time Adaptation](https://arxiv.org//abs/2408.13983)

	Yushun Tang, Shuoshuo Chen, Zhihe Lu, Xinchao Wang, Zhihai He


+ [2D-Malafide: Adversarial Attacks Against Face Deepfake Detection Systems](https://arxiv.org//abs/2408.14143)

	Chiara Galdi, Michele Panariello, Massimiliano Todisco, Nicholas Evans



# 2024-08-25
+ [SAB:A Stealing and Robust Backdoor Attack based on Steganographic Algorithm against Federated Learning](https://arxiv.org//abs/2408.13773)

	Weida Xu, Yang Xu, Sicong Zhang


+ [On the Robustness of Kolmogorov-Arnold Networks: An Adversarial Perspective](https://arxiv.org//abs/2408.13809)

	Tal Alter, Raz Lapid, Moshe Sipper


+ [RT-Attack: Jailbreaking Text-to-Image Models via Random Token](https://arxiv.org//abs/2408.13896)

	Sensen Gao, Xiaojun Jia, Yihao Huang, Ranjie Duan, Jindong Gu, Yang Liu, Qing Guo


+ [CAMH: Advancing Model Hijacking Attack in Machine Learning](https://arxiv.org//abs/2408.13741)

	Xing He, Jiahao Chen, Yuwen Pu, Qingming Li, Chunyi Zhou, Yingcai Wu, Jinbao Li, Shouling Ji


+ [Sample-Independent Federated Learning Backdoor Attack](https://arxiv.org//abs/2408.13849)

	Weida Xu, Yang Xu, Sicong Zhang



# 2024-08-24
+ [Probing the Robustness of Vision-Language Pretrained Models: A Multimodal Adversarial Attack Approach](https://arxiv.org//abs/2408.13461)

	Jiwei Guan, Tianyu Ding, Longbing Cao, Lei Pan, Chen Wang, Xi Zheng



# 2024-08-23
+ [On the Credibility of Backdoor Attacks Against Object Detectors in the Physical World](https://arxiv.org//abs/2408.12122)

	Bao Gia Doan, Dang Quang Nguyen, Callum Lindquist, Paul Montague, Tamas Abraham, Olivier De Vel, Seyit Camtepe, Salil S. Kanhere, Ehsan Abbasnejad, Damith C. Ranasinghe


+ [BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on Large Language Models](https://arxiv.org//abs/2408.12798)

	Yige Li, Hanxun Huang, Yunhan Zhao, Xingjun Ma, Jun Sun


+ [Is Generative AI the Next Tactical Cyber Weapon For Threat Actors? Unforeseen Implications of AI Generated Cyber Attacks](https://arxiv.org//abs/2408.12806)

	Yusuf Usman, Aadesh Upadhyay, Prashnna Gyawali, Robin Chataut


+ [Dynamic Label Adversarial Training for Deep Learning Robustness Against Adversarial Attacks](https://arxiv.org//abs/2408.13102)

	Zhenyu Liu, Haoran Duan, Huizhi Liang, Yang Long, Vaclav Snasel, Guiseppe Nicosia, Rajiv Ranjan, Varun Ojha


+ [Protecting against simultaneous data poisoning attacks](https://arxiv.org//abs/2408.13221)

	Neel Alex, Shoaib Ahmed Siddiqui, Amartya Sanyal, David Krueger



# 2024-08-22
+ [Query-Efficient Video Adversarial Attack with Stylized Logo](https://arxiv.org//abs/2408.12099)

	Duoxun Tang, Yuxin Cao, Xi Xiao, Derui Wang, Sheng Wen, Tianqing Zhu


+ [MakeupAttack: Feature Space Black-box Backdoor Attack on Face Recognition via Makeup Transfer](https://arxiv.org//abs/2408.12312)

	Ming Sun, Lihua Jing, Zixuan Zhu, Rui Wang


+ [Enhancing Transferability of Adversarial Attacks with GE-AdvGAN+: A Comprehensive Framework for Gradient Editing](https://arxiv.org//abs/2408.12673)

	Zhibo Jin, Jiayu Zhang, Zhiyu Zhu, Yuchen Zhang, Jiahao Huang, Jianlong Zhou, Fang Chen


+ [Leveraging Information Consistency in Frequency and Spatial Domain for Adversarial Attacks](https://arxiv.org//abs/2408.12670)

	Zhibo Jin, Jiayu Zhang, Zhiyu Zhu, Xinyi Wang, Yiyun Huang, Huaming Chen


+ [BankTweak: Adversarial Attack against Multi-Object Trackers by Manipulating Feature Banks](https://arxiv.org//abs/2408.12727)

	Woojin Shin, Donghwa Kang, Daejin Choi, Brent Kang, Jinkyu Lee, Hyeongboo Baek



# 2024-08-21
+ [Unlocking Adversarial Suffix Optimization Without Affirmative Phrases: Efficient Black-box Jailbreaking via LLM as Optimizer](https://arxiv.org//abs/2408.11313)

	Weipeng Jiang, Zhenting Wang, Juan Zhai, Shiqing Ma, Zhengyu Zhao, Chao Shen


+ [Nothing in Excess: Mitigating the Exaggerated Safety for LLMs via Safety-Conscious Activation Steering](https://arxiv.org//abs/2408.11491)

	Zouying Cao, Yifei Yang, Hai Zhao


+ [Efficient Detection of Toxic Prompts in Large Language Models](https://arxiv.org//abs/2408.11727)

	Yi Liu, Junzhe Yu, Huijia Sun, Ling Shi, Gelei Deng, Yuqi Chen, Yang Liu


+ [Large Language Models are Good Attackers: Efficient and Stealthy Textual Backdoor Attacks](https://arxiv.org//abs/2408.11587)

	Ziqiang Li, Yueqi Zeng, Pengfei Xia, Lei Liu, Zhangjie Fu, Bin Li


+ [Against All Odds: Overcoming Typology, Script, and Language Confusion in Multilingual Embedding Inversion Attacks](https://arxiv.org//abs/2408.11749)

	Yiyi Chen, Russa Biswas, Heather Lent, Johannes Bjerva


+ [Latent Feature and Attention Dual Erasure Attack against Multi-View Diffusion Models for 3D Assets Protection](https://arxiv.org//abs/2408.11408)

	Jingwei Sun, Xuchong Zhang, Changfeng Sun, Qicheng Bai, Hongbin Sun


+ [Exploring Robustness of Visual State Space model against Backdoor Attacks](https://arxiv.org//abs/2408.11679)

	Cheng-Yi Lee, Cheng-Chang Tsai, Chia-Mu Yu, Chun-Shien Lu


+ [Pixel Is Not A Barrier: An Effective Evasion Attack for Pixel-Domain Diffusion Models](https://arxiv.org//abs/2408.11810)

	Chun-Yen Shih, Li-Xuan Peng, Jia-Wei Liao, Ernie Chu, Cheng-Fu Chou, Jun-Cheng Chen


+ [First line of defense: A robust first layer mitigates adversarial attacks](https://arxiv.org//abs/2408.11680)

	Janani Suresh, Nancy Nayak, Sheetal Kalyani


+ [A Practical Trigger-Free Backdoor Attack on Neural Networks](https://arxiv.org//abs/2408.11444)

	Jiahao Wang, Xianglong Zhang, Xiuzhen Cheng, Pengfei Hu, Guoming Zhang



# 2024-08-20
+ [Prompt-Agnostic Adversarial Perturbation for Customized Diffusion Models](https://arxiv.org//abs/2408.10571)

	Cong Wan, Yuhang He, Xiang Song, Yihong Gong


+ [Privacy-preserving Universal Adversarial Defense for Black-box Models](https://arxiv.org//abs/2408.10647)

	Qiao Li, Cong Wu, Jing Chen, Zijun Zhang, Kun He, Ruiying Du, Xinxin Wang, Qingchuang Zhao, Yang Liu


+ [Probing the Safety Response Boundary of Large Language Models via Unsafe Decoding Path Generation](https://arxiv.org//abs/2408.10668)

	Haoyu Wang, Bingzhe Wu, Yatao Bian, Yongzhe Chang, Xueqian Wang, Peilin Zhao


+ [Towards Robust Knowledge Unlearning: An Adversarial Framework for Assessing and Improving Unlearning Robustness in Large Language Models](https://arxiv.org//abs/2408.10682)

	Hongbang Yuan, Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao


+ [MEGen: Generative Backdoor in Large Language Models via Model Editing](https://arxiv.org//abs/2408.10722)

	Jiyang Qiu, Xinbei Ma, Zhuosheng Zhang, Hai Zhao


+ [Security Assessment of Hierarchical Federated Deep Learning](https://arxiv.org//abs/2408.10752)

	D Alqattan, R Sun, H Liang, G Nicosia, V Snasel, R Ranjan, V Ojha


+ [Does Current Deepfake Audio Detection Model Effectively Detect ALM-based Deepfake Audio?](https://arxiv.org//abs/2408.10853)

	Yuankun Xie, Chenxu Xiong, Xiaopeng Wang, Zhiyong Wang, Yi Lu, Xin Qi, Ruibo Fu, Yukun Liu, Zhengqi Wen, Jianhua Tao, Guanjun Li, Long Ye


+ [A Grey-box Attack against Latent Diffusion Model-based Image Editing by Posterior Collapse](https://arxiv.org//abs/2408.10901)

	Zhongliang Guo, Lei Fang, Jingyu Lin, Yifei Qian, Shuai Zhao, Zeyu Wang, Junhao Dong, Cunjian Chen, Ognjen Arandjelović, Chun Pong Lau


+ [GAIM: Attacking Graph Neural Networks via Adversarial Influence Maximization](https://arxiv.org//abs/2408.10948)

	Xiaodong Yang, Xiaoting Li, Huiyuan Chen, Yiwei Cai


+ [Adversarial Attack for Explanation Robustness of Rationalization Models](https://arxiv.org//abs/2408.10795)

	Yuankai Zhang, Lingxiao Kong, Haozhao Wang, Ruixuan Li, Jun Wang, Yuhua Li, Wei Liu


+ [MsMemoryGAN: A Multi-scale Memory GAN for Palm-vein Adversarial Purification](https://arxiv.org//abs/2408.10694)

	Huafeng Qin, Yuming Fu, Huiyan Zhang, Mounim A. El-Yacoubi, Xinbo Gao, Qun Song, Jun Wang


+ [Adversarial training of Keyword Spotting to Minimize TTS Data Overfitting](https://arxiv.org//abs/2408.10463)

	Hyun Jin Park, Dhruuv Agarwal, Neng Chen, Rentao Sun, Kurt Partridge, Justin Chen, Harry Zhang, Pai Zhu, Jacob Bartel, Kyle Kastner, Gary Wang, Andrew Rosenberg, Quan Wang


+ [Iterative Window Mean Filter: Thwarting Diffusion-based Adversarial Purification](https://arxiv.org//abs/2408.10673)

	Hanrui Wang, Ruoxi Sun, Cunjian Chen, Minhui Xue, Lay-Ki Soon, Shuo Wang, Zhe Jin


+ [Hide Your Malicious Goal Into Benign Narratives: Jailbreak Large Language Models through Neural Carrier Articles](https://arxiv.org//abs/2408.11182)

	Zhilong Wang, Haizhou Wang, Nanqing Luo, Lan Zhang, Xiaoyan Sun, Yebo Cao, Peng Liu


+ [Revisiting Min-Max Optimization Problem in Adversarial Training](https://arxiv.org//abs/2408.11218)

	Sina Hajer Ahmadi, Hassan Bahrami


+ [Robust Image Classification: Defensive Strategies against FGSM and PGD Adversarial Attacks](https://arxiv.org//abs/2408.13274)

	Hetvi Waghela, Jaydip Sen, Sneha Rakshit



# 2024-08-19
+- [Detecting Adversarial Attacks in Semantic Segmentation via Uncertainty Estimation: A Deep Analysis](https://arxiv.org//abs/2408.10021)

	Kira Maag, Roman Resner, Asja Fischer


+ [Regularization for Adversarial Robust Learning](https://arxiv.org//abs/2408.09672)

	Jie Wang, Rui Gao, Yao Xie


+ [Enhance Modality Robustness in Text-Centric Multimodal Alignment with Adversarial Prompting](https://arxiv.org//abs/2408.09798)

	Yun-Da Tsai, Ting-Yu Yen, Keng-Te Liao, Shou-De Lin


+ [Transferring Backdoors between Large Language Models by Knowledge Distillation](https://arxiv.org//abs/2408.09878)

	Pengzhou Cheng, Zongru Wu, Tianjie Ju, Wei Du, Zhuosheng Zhang Gongshen Liu


+ [The Brittleness of AI-Generated Image Watermarking Techniques: Examining Their Robustness Against Visual Paraphrasing Attacks](https://arxiv.org//abs/2408.10446)

	Niyar R Barman, Krish Sharma, Ashhar Aziz, Shashwat Bajpai, Shwetangshu Biswas, Vasu Sharma, Vinija Jain, Aman Chadha, Amit Sheth, Amitava Das


+ [Differentially Private Stochastic Gradient Descent with Fixed-Size Minibatches: Tighter RDP Guarantees with or without Replacement](https://arxiv.org//abs/2408.10456)

	Jeremiah Birrell, Reza Ebrahimi, Rouzbeh Behnia, Jason Pacheco



# 2024-08-18
+ [Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning](https://arxiv.org//abs/2408.09600)

	Tiansheng Huang, Gautam Bhattacharya, Pratik Joshi, Josh Kimball, Ling Liu


+ [Characterizing and Evaluating the Reliability of LLMs against Jailbreak Attacks](https://arxiv.org//abs/2408.09326)

	Kexin Chen, Yi Liu, Dongxia Wang, Jiaying Chen, Wenhai Wang


+ [Adversarial Attacked Teacher for Unsupervised Domain Adaptive Object Detection](https://arxiv.org//abs/2408.09431)

	Kaiwen Wang, Yinzhe Shen, Martin Lauer


+ [Enhancing Adversarial Transferability with Adversarial Weight Tuning](https://arxiv.org//abs/2408.09469)

	Jiahao Chen, Zhou Feng, Rui Zeng, Yuwen Pu, Chunyi Zhou, Yi Jiang, Yuyou Gan, Jinbao Li, Shouling Ji, Shouling_Ji


+ [NoRA: Nested Low-Rank Adaptation for Efficient Fine-Tuning Large Models](https://arxiv.org//abs/2408.10280)

	Cheng Lin, Lujun Li, Dezhi Li, Jie Zou, Wenhan Luo, Wei Xue, Yike Guo


+ [DiffZOO: A Purely Query-Based Black-Box Attack for Red-teaming Text-to-Image Generative Model via Zeroth Order Optimization](https://arxiv.org//abs/2408.11071)

	Pucheng Dang, Xing Hu, Dong Li, Rui Zhang, Qi Guo, Kaidi Xu



# 2024-08-17
+ [Mitigating Backdoor Attacks in Federated Learning via Flipping Weight Updates of Low-Activation Input Neurons](https://arxiv.org//abs/2408.08655)

	Binbin Ding, Penghui Yang, Zeqing Ge, Shengjun Huang


+ [Can Large Language Models Improve the Adversarial Robustness of Graph Neural Networks?](https://arxiv.org//abs/2408.08685)

	Zhongjian Zhang, Xiao Wang, Huichi Zhou, Yue Yu, Mengmei Zhang, Cheng Yang, Chuan Shi


+ [Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions](https://arxiv.org//abs/2408.08780)

	Chenming Tang, Zhixiang Wang, Yunfang Wu


+ [Efficient Image-to-Image Diffusion Classifier for Adversarial Robustness](https://arxiv.org//abs/2408.08502)

	Hefei Mei, Minjing Dong, Chang Xu


+ [Visual-Friendly Concept Protection via Selective Adversarial Perturbations](https://arxiv.org//abs/2408.08518)

	Xiaoyue Mi, Fan Tang, Juan Cao, Peng Li, Yang Liu


+ [Towards Physical World Backdoor Attacks against Skeleton Action Recognition](https://arxiv.org//abs/2408.08671)

	Qichen Zheng, Yi Yu, Siyuan Yang, Jun Liu, Kwok-Yan Lam, Alex Kot


+ [\textit{MMJ-Bench}: A Comprehensive Study on Jailbreak Attacks and Defenses for Vision Language Models](https://arxiv.org//abs/2408.08464)

	Fenghua Weng, Yue Xu, Chengyan Fu, Wenjie Wang


+ [Gradient-Variation Online Learning under Generalized Smoothness](https://arxiv.org//abs/2408.09074)

	Yan-Feng Xie, Peng Zhao, Zhi-Hua Zhou


+ [Scalable and Certifiable Graph Unlearning via Lazy Local Propagation](https://arxiv.org//abs/2408.09212)

	Lu Yi, Zhewei Wei


+ [Malacopula: adversarial automatic speaker verification attacks using a neural-based generalised Hammerstein model](https://arxiv.org//abs/2408.09300)

	Massimiliano Todisco, Michele Panariello, Xin Wang, Héctor Delgado, Kong Aik Lee, Nicholas Evans


+ [BaThe: Defense against the Jailbreak Attack in Multimodal Large Language Models by Treating Harmful Instruction as Backdoor Trigger](https://arxiv.org//abs/2408.09093)

	Yulin Chen, Haoran Li, Zihao Zheng, Yangqiu Song


+ [Attack Anything: Blind DNNs via Universal Background Adversarial Attack](https://arxiv.org//abs/2409.00029)

	Jiawei Lian, Shaohui Mei, Xiaofei Wang, Yi Wang, Lefan Wang, Yingjie Lu, Mingyang Ma, Lap-Pui Chau



# 2024-08-16
+ [KOALA: Enhancing Speculative Decoding for LLM via Multi-Layer Draft Heads with Adversarial Learning](https://arxiv.org//abs/2408.08146)

	Kaiqi Zhang, Jing Zhao, Rui Chen


+ [A Multi-task Adversarial Attack Against Face Authentication](https://arxiv.org//abs/2408.08205)

	Hanrui Wang, Shuo Wang, Cunjian Chen, Massimo Tistarelli, Zhe Jin


+ [Evaluating Text Classification Robustness to Part-of-Speech Adversarial Examples](https://arxiv.org//abs/2408.08374)

	Anahita Samadi, Allison Sullivan


+ [Penny-Wise and Pound-Foolish in Deepfake Detection](https://arxiv.org//abs/2408.08412)

	Yabin Wang, Zhiwu Huang, Su Zhou, Adam Prugel-Bennett, Xiaopeng Hong


+ [Prefix Guidance: A Steering Wheel for Large Language Models to Defend Against Jailbreak Attacks](https://arxiv.org//abs/2408.08924)

	Jiawei Zhao, Kejiang Chen, Xiaojian Yuan, Weiming Zhang


+ [Ask, Attend, Attack: A Effective Decision-Based Black-Box Targeted Attack for Image-to-Text Models](https://arxiv.org//abs/2408.08989)

	Qingyuan Zeng, Zhenzhong Wang, Yiu-ming Cheung, Min Jiang


+ [See What LLMs Cannot Answer: A Self-Challenge Framework for Uncovering LLM Weaknesses](https://arxiv.org//abs/2408.08978)

	Yulong Chen, Yang Liu, Jianhao Yan, Xuefeng Bai, Ming Zhong, Yinghao Yang, Ziyi Yang, Chenguang Zhu, Yue Zhang



# 2024-08-14
+ [UAHOI: Uncertainty-aware Robust Interaction Learning for HOI Detection](https://arxiv.org//abs/2408.07430)

	Mu Chen, Minghan Chen, Yi Yang


+ [BAPLe: Backdoor Attacks on Medical Foundational Models using Prompt Learning](https://arxiv.org//abs/2408.07440)

	Asif Hanif, Fahad Shamshad, Muhammad Awais, Muzammal Naseer, Fahad Shahbaz Khan, Karthik Nandakumar, Salman Khan, Rao Muhammad Anwer


+ [Robust Active Learning (RoAL): Countering Dynamic Adversaries in Active Learning with Elastic Weight Consolidation](https://arxiv.org//abs/2408.07364)

	Ricky Maulana Fajri, Yulong Pei, Lu Yin, Mykola Pechenizkiy


+ [TabularBench: Benchmarking Adversarial Robustness for Tabular Deep Learning in Real-world Use-cases](https://arxiv.org//abs/2408.07579)

	Thibault Simonetto, Salah Ghamizi, Maxime Cordy


+ [BadMerging: Backdoor Attacks Against Model Merging](https://arxiv.org//abs/2408.07362)

	Jinghuai Zhang, Jianfeng Chi, Zheng Li, Kunlin Cai, Yang Zhang, Yuan Tian


+ [Enhancing Adversarial Attacks via Parameter Adaptive Adversarial Attack](https://arxiv.org//abs/2408.07733)

	Zhibo Jin, Jiayu Zhang, Zhiyu Zhu, Chenyu Zhang, Jiahao Huang, Jianlong Zhou, Fang Chen


+ [CodeMirage: Hallucinations in Code Generated by Large Language Models](https://arxiv.org//abs/2408.08333)

	Vibhor Agarwal, Yulong Pei, Salwa Alamir, Xiaomo Liu



# 2024-08-13
+ [RW-NSGCN: A Robust Approach to Structural Attacks via Negative Sampling](https://arxiv.org//abs/2408.06665)

	Shuqi He, Jun Zhuang, Ding Wang, Jun Song


+ [DePatch: Towards Robust Adversarial Patch for Evading Person Detectors in the Real World](https://arxiv.org//abs/2408.06625)

	Jikang Cheng, Ying Zhang, Zhongyuan Wang, Zou Qin, Chen Li


+ [VulCatch: Enhancing Binary Vulnerability Detection through CodeT5 Decompilation and KAN Advanced Feature Extraction](https://arxiv.org//abs/2408.07181)

	Abdulrahman Hamman Adama Chukkol, Senlin Luo, Kashif Sharif, Yunusa Haruna, Muhammad Muhammad Abdullahi


+ [FedMADE: Robust Federated Learning for Intrusion Detection in IoT Networks Using a Dynamic Aggregation Method](https://arxiv.org//abs/2408.07152)

	Shihua Sun, Pragya Sharma, Kenechukwu Nwodo, Angelos Stavrou, Haining Wang



# 2024-08-12
+ [Understanding Byzantine Robustness in Federated Learning with A Black-box Server](https://arxiv.org//abs/2408.06042)

	Fangyuan Zhao, Yuexiang Xie, Xuebin Ren, Bolin Ding, Shusen Yang, Yaliang Li


+ [Classifier Guidance Enhances Diffusion-based Adversarial Purification by Preserving Predictive Information](https://arxiv.org//abs/2408.05900)

	Mingkun Zhang, Jianing Li, Wei Chen, Jiafeng Guo, Xueqi Cheng


+ [Towards Adversarial Robustness via Debiased High-Confidence Logit Alignment](https://arxiv.org//abs/2408.06079)

	Kejia Zhang, Juanjuan Weng, Zhiming Luo, Shaozi Li


+ [LEARN: An Invex Loss for Outlier Oblivious Robust Online Optimization](https://arxiv.org//abs/2408.06297)

	Adarsh Barik, Anand Krishna, Vincent Y. F. Tan


+ [Nob-MIAs: Non-biased Membership Inference Attacks Assessment on Large Language Models with Ex-Post Dataset Construction](https://arxiv.org//abs/2408.05968)

	Cédric Eichler, Nathan Champeil, Nicolas Anciaux, Alexandra Bensamoun, Heber Hwang Arcolezi, José Maria De Fuentes


+ [Lancelot: Towards Efficient and Privacy-Preserving Byzantine-Robust Federated Learning within Fully Homomorphic Encryption](https://arxiv.org//abs/2408.06197)

	Siyang Jiang, Hao Yang, Qipeng Xie, Chuan Ma, Sen Wang, Guoliang Xing


+ [Fooling SHAP with Output Shuffling Attacks](https://arxiv.org//abs/2408.06509)

	Jun Yuan, Aritra Dasgupta



# 2024-08-11
+ [StealthDiffusion: Towards Evading Diffusion Forensic Detection through Diffusion Model](https://arxiv.org//abs/2408.05669)

	Ziyin Zhou, Ke Sun, Zhongxi Chen, Huafeng Kuang, Xiaoshuai Sun, Rongrong Ji


+ [Improving Adversarial Transferability with Neighbourhood Gradient Information](https://arxiv.org//abs/2408.05745)

	Haijing Guo, Jiafeng Wang, Zhaoyu Chen, Kaixun Jiang, Lingyi Hong, Pinxue Guo, Jinglun Li, Wenqiang Zhang



# 2024-08-10
+ [MABR: A Multilayer Adversarial Bias Removal Approach Without Prior Bias Knowledge](https://arxiv.org//abs/2408.05497)

	Maxwell J. Yin, Boyu Wang, Charles Ling


+ [ReToMe-VA: Recursive Token Merging for Video Diffusion-based Unrestricted Adversarial Attack](https://arxiv.org//abs/2408.05479)

	Ziyi Gao, Kai Chen, Zhipeng Wei, Tingshu Mou, Jingjing Chen, Zhiyu Tan, Hao Li, Yu-Gang Jiang



# 2024-08-09
+ [h4rm3l: A Dynamic Benchmark of Composable Jailbreak Attacks for LLM Safety Assessment](https://arxiv.org//abs/2408.04811)

	Moussa Koulako Bala Doumbouya, Ananjan Nandi, Gabriel Poesia, Davide Ghilardi, Anna Goldie, Federico Bianchi, Dan Jurafsky, Christopher D. Manning


+ [Counterfactual Explanations with Probabilistic Guarantees on their Robustness to Model Change](https://arxiv.org//abs/2408.04842)

	Ignacy Stępka, Mateusz Lango, Jerzy Stefanowski


+ [A Jailbroken GenAI Model Can Cause Substantial Harm: GenAI-powered Applications are Vulnerable to PromptWares](https://arxiv.org//abs/2408.05061)

	Stav Cohen, Ron Bitton, Ben Nassi


+ [Surgical-VQLA++: Adversarial Contrastive Learning for Calibrated Robust Visual Question-Localized Answering in Robotic Surgery](https://arxiv.org//abs/2408.04958)

	Long Bai, Guankun Wang, Mobarakol Islam, Lalithkumar Seenivasan, An Wang, Hongliang Ren


+ [Adversarially Robust Industrial Anomaly Detection Through Diffusion Model](https://arxiv.org//abs/2408.04839)

	Yuanpu Cao, Lu Lin, Jinghui Chen


+ [Model Debiasing by Learnable Data Augmentation](https://arxiv.org//abs/2408.04955)

	Pietro Morerio, Ruggero Ragonesi, Vittorio Murino


+ [Range Membership Inference Attacks](https://arxiv.org//abs/2408.05131)

	Jiashu Tao, Reza Shokri



# 2024-08-08
+ [Towards Resilient and Efficient LLMs: A Comparative Study of Efficiency, Performance, and Adversarial Robustness](https://arxiv.org//abs/2408.04585)

	Xiaojing Fan, Chunliang Tao


+ [Constructing Adversarial Examples for Vertical Federated Learning: Optimal Client Corruption through Multi-Armed Bandit](https://arxiv.org//abs/2408.04310)

	Duanyi Yao, Songze Li, Ye Xue, Jin Liu


+ [FDI: Attack Neural Code Generation Systems through User Feedback Channel](https://arxiv.org//abs/2408.04194)

	Zhensu Sun, Xiaoning Du, Xiapu Luo, Fu Song, David Lo, Li Li


+ [Eliminating Backdoors in Neural Code Models via Trigger Inversion](https://arxiv.org//abs/2408.04683)

	Weisong Sun, Yuchen Chen, Chunrong Fang, Yebo Feng, Yuan Xiao, An Guo, Quanjun Zhang, Yang Liu, Baowen Xu, Zhenyu Chen


+ [Multi-Turn Context Jailbreak Attack on Large Language Models From First Principles](https://arxiv.org//abs/2408.04686)

	Xiongtao Sun, Deyue Zhang, Dongdong Yang, Quanchen Zou, Hui Li


+ [Ensemble everything everywhere: Multi-scale aggregation for adversarial robustness](https://arxiv.org//abs/2408.05446)

	Stanislav Fort, Balaji Lakshminarayanan



# 2024-08-07
+ [EnJa: Ensemble Jailbreak on Large Language Models](https://arxiv.org//abs/2408.03603)

	Jiahao Zhang, Zilong Wang, Ruofan Wang, Xingjun Ma, Yu-Gang Jiang


+ [LaFA: Latent Feature Attacks on Non-negative Matrix Factorization](https://arxiv.org//abs/2408.03909)

	Minh Vu, Ben Nebgen, Erik Skau, Geigh Zollicoffer, Juan Castorena, Kim Rasmussen, Boian Alexandrov, Manish Bhattarai


+ [TALE: Training-free Cross-domain Image Composition via Adaptive Latent Manipulation and Energy-guided Optimization](https://arxiv.org//abs/2408.03637)

	Kien T. Pham, Jingye Chen, Qifeng Chen


+ [Enhancing Output Diversity Improves Conjugate Gradient-based Adversarial Attacks](https://arxiv.org//abs/2408.03972)

	Keiichiro Yamamura, Issa Oe, Hiroki Ishikura, Katsuki Fujisawa


+ [PushPull-Net: Inhibition-driven ResNet robust to image corruptions](https://arxiv.org//abs/2408.04077)

	Guru Swaroop Bennabhaktula, Enrique Alegre, Nicola Strisciuglio, George Azzopardi


+ [Exploring RAG-based Vulnerability Augmentation with LLMs](https://arxiv.org//abs/2408.04125)

	Seyed Shayan Daneshvar, Yu Nong, Xu Yang, Shaowei Wang, Haipeng Cai


# 2024-08-06
+ [Mitigating Hallucinations in Large Vision-Language Models (LVLMs) via Language-Contrastive Decoding (LCD)](https://arxiv.org//abs/2408.04664)

	Avshalom Manevich, Reut Tsarfaty


# 2024-08-05
+ [RCDM: Enabling Robustness for Conditional Diffusion Model](https://arxiv.org//abs/2408.02710)

	Weifeng Xu, Xiang Zhu, Xiaoyong Li


+ [Mitigating Malicious Attacks in Federated Learning via Confidence-aware Defense](https://arxiv.org//abs/2408.02813)

	Qilei Li, Ahmed M. Abdelmoniem


+ [Sample-agnostic Adversarial Perturbation for Vision-Language Pre-training Models](https://arxiv.org//abs/2408.02980)

	Haonan Zheng, Wen Jiang, Xinyang Deng, Wenrui Li


+ [DisCoM-KD: Cross-Modal Knowledge Distillation via Disentanglement Representation and Adversarial Learning](https://arxiv.org//abs/2408.07080)

	Dino Ienco (EVERGREEN, UMR TETIS, INRAE), Cassio Fraga Dantas (UMR TETIS, INRAE, EVERGREEN)



# 2024-08-02
+ [Interpreting Global Perturbation Robustness of Image Models using Axiomatic Spectral Importance Decomposition](https://arxiv.org//abs/2408.01139)

	Róisín Luo, James McDermott, Colm O'Riordan


+ [Mission Impossible: A Statistical Perspective on Jailbreaking LLMs](https://arxiv.org//abs/2408.01420)

	Jingtong Su, Julia Kempe, Karen Ullrich


+ [Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models within Perturbed Inputs](https://arxiv.org//abs/2408.01355)

	Peng Ding, Jingyu Wu, Jun Kuang, Dan Ma, Xuezhi Cao, Xunliang Cai, Shi Chen, Jiajun Chen, Shujian Huang


+ [Assessing Robustness of Machine Learning Models using Covariate Perturbations](https://arxiv.org//abs/2408.01300)

	Arun Prakash R, Anwesha Bhattacharyya, Joel Vaughan, Vijayan N. Nair


+ [EmoBack: Backdoor Attacks Against Speaker Identification Using Emotional Prosody](https://arxiv.org//abs/2408.01178)

	Coen Schoof, Stefanos Koffas, Mauro Conti, Stjepan Picek



# 2024-08-01
+ [Contrastive Graph Representation Learning with Adversarial Cross-view Reconstruction and Information Bottleneck](https://arxiv.org//abs/2408.00295)

	Yuntao Shou, Haozhi Lan, Xiangyong Cao


+ [ADBM: Adversarial diffusion bridge model for reliable adversarial purification](https://arxiv.org//abs/2408.00315)

	Xiao Li, Wenxuan Sun, Huanran Chen, Qiongxiu Li, Yining Liu, Yingzhe He, Jie Shi, Xiaolin Hu


+ [OTAD: An Optimal Transport-Induced Robust Model for Agnostic Adversarial Attack](https://arxiv.org//abs/2408.00329)

	Kuo Gai, Sicong Wang, Shihua Zhang


+ [CERT-ED: Certifiably Robust Text Classification for Edit Distance](https://arxiv.org//abs/2408.00728)

	Zhuoqun Huang, Neil G Marchant, Olga Ohrimenko, Benjamin I. P. Rubinstein


+ [Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion](https://arxiv.org//abs/2408.00352)

	Honglei Miao, Fan Ma, Ruijie Quan, Kun Zhan, Yi Yang


+ [Revocable Backdoor for Deep Model Trading](https://arxiv.org//abs/2408.00255)

	Yiran Xu, Nan Zhong, Zhenxing Qian, Xinpeng Zhang


+ [Adversarial Text Rewriting for Text-aware Recommender Systems](https://arxiv.org//abs/2408.00312)

	Sejoon Oh, Gaurav Verma, Srijan Kumar


+ [Benchmarking Attacks on Learning with Errors](https://arxiv.org//abs/2408.00882)

	Emily Wenger, Eshika Saxena, Mohamed Malhou, Ellie Thieu, Kristin Lauter



# 2024-07-31
+ [Measuring What Matters: Intrinsic Distance Preservation as a Robust Metric for Embedding Quality](https://arxiv.org//abs/2407.21590)

	Steven N. Hart, Thomas E. Tavolara


+ [Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?](https://arxiv.org//abs/2407.21792)

	Richard Ren, Steven Basart, Adam Khoja, Alice Gatti, Long Phan, Xuwang Yin, Mantas Mazeika, Alexander Pan, Gabriel Mukobi, Ryan H. Kim, Stephen Fitz, Dan Hendrycks


+ [Defending Jailbreak Attack in VLMs via Cross-modality Information Detector](https://arxiv.org//abs/2407.21659)

	Yue Xu, Xiuyuan Qi, Zhan Qin, Wenjie Wang


+ [Benchmarking AIGC Video Quality Assessment: A Dataset and Unified Model](https://arxiv.org//abs/2407.21408)

	Zhichao Zhang, Xinyue Li, Wei Sun, Jun Jia, Xiongkuo Min, Zicheng Zhang, Chunyi Li, Zijian Chen, Puyi Wang, Zhongpeng Ji, Fengyu Sun, Shangling Jui, Guangtao Zhai


+ [Conditioned Prompt-Optimization for Continual Deepfake Detection](https://arxiv.org//abs/2407.21554)

	Francesco Laiti, Benedetta Liberatori, Thomas De Min, Elisa Ricci


+ [Paying More Attention to Image: A Training-Free Method for Alleviating Hallucination in LVLMs](https://arxiv.org//abs/2407.21771)

	Shi Liu, Kecheng Zheng, Wei Chen


+ [Diff-Cleanse: Identifying and Mitigating Backdoor Attacks in Diffusion Models](https://arxiv.org//abs/2407.21316)

	Jiang Hao, Xiao Jin, Hu Xiaoguang, Chen Tianyou


+ [Resilience and Security of Deep Neural Networks Against Intentional and Unintentional Perturbations: Survey and Research Challenges](https://arxiv.org//abs/2408.00193)

	Sazzad Sayyed, Milin Zhang, Shahriar Rifat, Ananthram Swami, Michael De Lucia, Francesco Restuccia



# 2024-07-30
+ [FACL-Attack: Frequency-Aware Contrastive Learning for Transferable Adversarial Attacks](https://arxiv.org//abs/2407.20653)

	Hunmin Yang, Jongoh Jeong, Kuk-Jin Yoon


+ [Prompt-Driven Contrastive Learning for Transferable Adversarial Attacks](https://arxiv.org//abs/2407.20657)

	Hunmin Yang, Jongoh Jeong, Kuk-Jin Yoon


+ [PIP: Prototypes-Injected Prompt for Federated Class Incremental Learning](https://arxiv.org//abs/2407.20705)

	Muhammad Anwar Ma'sum, Mahardhika Pratama, Savitha Ramasamy, Lin Liu, Habibullah Habibullah, Ryszard Kowalczyk


+ [Vulnerabilities in AI-generated Image Detection: The Challenge of Adversarial Attacks](https://arxiv.org//abs/2407.20836)

	Yunfeng Diao, Naixin Zhai, Changtao Miao, Xun Yang, Meng Wang


+ [Can LLMs be Fooled? Investigating Vulnerabilities in LLMs](https://arxiv.org//abs/2407.20529)

	Sara Abdali, Jia He, CJ Barberan, Richard Anarfi


+ [Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification](https://arxiv.org//abs/2407.20859)

	Boyang Zhang, Yicong Tan, Yun Shen, Ahmed Salem, Michael Backes, Savvas Zannettou, Yang Zhang


+ [DeepBaR: Fault Backdoor Attack on Deep Neural Network Layers](https://arxiv.org//abs/2407.21220)

	C. A. Martínez-Mejía, J. Solano, J. Breier, D. Bucko, X. Hou



# 2024-07-29
+ [Can Editing LLMs Inject Harm?](https://arxiv.org//abs/2407.20224)

	Canyu Chen, Baixiang Huang, Zekun Li, Zhaorun Chen, Shiyang Lai, Xiongxiao Xu, Jia-Chen Gu, Jindong Gu, Huaxiu Yao, Chaowei Xiao, Xifeng Yan, William Yang Wang, Philip Torr, Dawn Song, Kai Shu


+ [Detecting and Understanding Vulnerabilities in Language Models via Mechanistic Interpretability](https://arxiv.org//abs/2407.19842)

	Jorge García-Carrasco, Alejandro Maté, Juan Trujillo


+ [RSC-SNN: Exploring the Trade-off Between Adversarial Robustness and Accuracy in Spiking Neural Networks via Randomized Smoothing Coding](https://arxiv.org//abs/2407.20099)

	Keming Wu, Man Yao, Yuhong Chou, Xuerui Qiu, Rui Yang, Bo Xu, Guoqi Li


+ [BackdoorBench: A Comprehensive Benchmark and Analysis of Backdoor Learning](https://arxiv.org//abs/2407.19845)

	Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, Mingli Zhu, Ruotong Wang, Li Liu, Chao Shen


+ [Practical and Robust Safety Guarantees for Advanced Counterfactual Learning to Rank](https://arxiv.org//abs/2407.19943)

	Shashank Gupta, Harrie Oosterhuis, Maarten de Rijke


+ [Contrasting Deepfakes Diffusion via Contrastive Learning and Global-Local Similarities](https://arxiv.org//abs/2407.20337)

	Lorenzo Baraldi, Federico Cocchi, Marcella Cornia, Lorenzo Baraldi, Alessandro Nicolosi, Rita Cucchiara


+ [From ML to LLM: Evaluating the Robustness of Phishing Webpage Detection Models against Adversarial Attacks](https://arxiv.org//abs/2407.20361)

	Aditya Kulkarni, Vivek Balachandran, Dinil Mon Divakaran, Tamal Das


+ [Enhancing Adversarial Text Attacks on BERT Models with Projected Gradient Descent](https://arxiv.org//abs/2407.21073)

	Hetvi Waghela, Jaydip Sen, Sneha Rakshit


# 2024-07-28
+ [Exploring the Adversarial Robustness of CLIP for AI-generated Image Detection](https://arxiv.org//abs/2407.19553)

	Vincenzo De Rosa, Fabrizio Guillaro, Giovanni Poggi, Davide Cozzolino, Luisa Verdoliva



# 2024-07-27
+ [Towards Clean-Label Backdoor Attacks in the Physical World](https://arxiv.org//abs/2407.19203)

	Thinh Dao, Cuong Chi Le, Khoa D Doan, Kok-Seng Wong


+ [EaTVul: ChatGPT-based Evasion Attack Against Software Vulnerability Detection](https://arxiv.org//abs/2407.19216)

	Shigang Liu, Di Cao, Junae Kim, Tamas Abraham, Paul Montague, Seyit Camtepe, Jun Zhang, Yang Xiang


+ [Debiased Graph Poisoning Attack via Contrastive Surrogate Objective](https://arxiv.org//abs/2407.19155)

	Kanghoon Yoon, Yeonjun In, Namkyeong Lee, Kibum Kim, Chanyoung Park



# 2024-07-26
+ [Adversarial Robustification via Text-to-Image Diffusion Models](https://arxiv.org//abs/2407.18658)

	Daewon Choi, Jongheon Jeong, Huiwon Jang, Jinwoo Shin


+ [Robust VAEs via Generating Process of Noise Augmented Data](https://arxiv.org//abs/2407.18632)

	Hiroo Irobe, Wataru Aoki, Kimihiro Yamazaki, Yuhui Zhang, Takumi Nakagawa, Hiroki Waida, Yuichiro Wada, Takafumi Kanamori


+ [Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning](https://arxiv.org//abs/2407.19119)

	Sayyed Farid Ahamed, Soumya Banerjee, Sandip Roy, Devin Quinn, Marc Vucovich, Kevin Choi, Abdul Rahman, Alison Hu, Edward Bowen, Sachin Shetty



# 2024-07-25
+ [A Unified Understanding of Adversarial Vulnerability Regarding Unimodal Models and Vision-Language Pre-training Models](https://arxiv.org//abs/2407.17797)

	Haonan Zheng, Xinyang Deng, Wen Jiang, Wenrui Li


+ [The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models](https://arxiv.org//abs/2407.17915)

	Zihui Wu, Haichang Gao, Jianping He, Ping Wang


+ [Peak-Controlled Logits Poisoning Attack in Federated Distillation](https://arxiv.org//abs/2407.18039)

	Yuhan Tang, Aoxu Zhang, Zhiyuan Wu, Bo Gao, Tian Wen, Yuwei Wang, Sheng Sun


+ [Is the Digital Forensics and Incident Response Pipeline Ready for Text-Based Threats in LLM Era?](https://arxiv.org//abs/2407.17870)

	Avanti Bhandarkar, Ronald Wilson, Anushka Swarup, Mengdi Zhu, Damon Woodard


+ [Sparse vs Contiguous Adversarial Pixel Perturbations in Multimodal Models: An Empirical Analysis](https://arxiv.org//abs/2407.18251)

	Cristian-Alexandru Botocan, Raphael Meier, Ljiljana Dolamic


+ [RIDA: A Robust Attack Framework on Incomplete Graphs](https://arxiv.org//abs/2407.18170)

	Jianke Yu, Hanchen Wang, Chen Chen, Xiaoyang Wang, Wenjie Zhang, Ying Zhang


+ [Adversarial Robust Decision Transformer: Enhancing Robustness of RvS via Minimax Returns-to-go](https://arxiv.org//abs/2407.18414)

	Xiaohang Tang, Afonso Marques, Parameswaran Kamalaruban, Ilija Bogunovic



# 2024-07-24
+ [Robust Deep Hawkes Process under Label Noise of Both Event and Occurrence](https://arxiv.org//abs/2407.17164)

	Xiaoyu Tan, Bin Li, Xihe Qiu, Jingjing Huang, Yinghui Xu, Wei Chu


+ [How Good (Or Bad) Are LLMs at Detecting Misleading Visualizations?](https://arxiv.org//abs/2407.17291)

	Leo Yu-Ho Lo, Huamin Qu


+ [Physical Adversarial Attack on Monocular Depth Estimation via Shape-Varying Patches](https://arxiv.org//abs/2407.17312)

	Chenxing Zhao, Yang Li, Shihao Wu, Wenyi Tan, Shuangju Zhou, Quan Pan


+ [Can Watermarking Large Language Models Prevent Copyrighted Text Generation and Hide Training Data?](https://arxiv.org//abs/2407.17417)

	Michael-Andrei Panaitescu-Liess, Zora Che, Bang An, Yuancheng Xu, Pankayaraj Pathmanathan, Souradip Chakraborty, Sicheng Zhu, Tom Goldstein, Furong Huang


+ [From Sands to Mansions: Enabling Automatic Full-Life-Cycle Cyberattack Construction with LLM](https://arxiv.org//abs/2407.16928)

	Lingzhi Wang, Jiahui Wang, Kyle Jung, Kedar Thiagarajan, Emily Wei, Xiangmin Shen, Yan Chen, Zhenyuan Li



# 2024-07-23
+ [Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models](https://arxiv.org//abs/2407.16205)

	Shi Lin, Rongchang Li, Xun Wang, Changting Lin, Wenpeng Xing, Meng Han


+ [RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent](https://arxiv.org//abs/2407.16667)

	Huiyu Xu, Wenhui Zhang, Zhibo Wang, Feng Xiao, Rui Zheng, Yunhe Feng, Zhongjie Ba, Kui Ren


+ [Can Large Language Models Automatically Jailbreak GPT-4V?](https://arxiv.org//abs/2407.16686)

	Yuanwei Wu, Yue Huang, Yixin Liu, Xiang Li, Pan Zhou, Lichao Sun


+ [Algebraic Adversarial Attacks on Integrated Gradients](https://arxiv.org//abs/2407.16233)

	Lachlan Simpson, Federico Costanza, Kyle Millar, Adriel Cheng, Cheng-Chew Lim, Hong Gunn Chew


+ [STATE: A Robust ATE Estimator of Heavy-Tailed Metrics for Variance Reduction in Online Controlled Experiments](https://arxiv.org//abs/2407.16337)

	Hao Zhou, Kun Sun, Shaoming Li, Yangfeng Fan, Guibin Jiang, Jiaqi Zheng, Tao Li


+ [Backdoor Attacks against Hybrid Classical-Quantum Neural Networks](https://arxiv.org//abs/2407.16273)

	Ji Guo, Wenbo Jiang, Rui Zhang, Wenshu Fan, Jiachen Li, Guoming Lu


+ [Multimodal Unlearnable Examples: Protecting Data against Multimodal Contrastive Learning](https://arxiv.org//abs/2407.16307)

	Xinwei Liu, Xiaojun Jia, Yuan Xun, Siyuan Liang, Xiaochun Cao


+ [Theoretical Analysis of Privacy Leakage in Trustworthy Federated Learning: A Perspective from Linear Algebra and Optimization Theory](https://arxiv.org//abs/2407.16735)

	Xiaojin Zhang, Wei Chen


+ [S-E Pipeline: A Vision Transformer (ViT) based Resilient Classification Pipeline for Medical Imaging Against Adversarial Attacks](https://arxiv.org//abs/2407.17587)

	Neha A S, Vivek Chaturvedi, Muhammad Shafique



# 2024-07-22
+ [ImPress: Securing DRAM Against Data-Disturbance Errors via Implicit Row-Press Mitigation](https://arxiv.org//abs/2407.16006)

	Moinuddin Qureshi, Anish Saxena, Aamer Jaleel



# 2024-07-18
+ [DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving](https://arxiv.org//abs/2407.13690)

	Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, Junxian He


+ [Visually Robust Adversarial Imitation Learning from Videos with Contrastive Learning](https://arxiv.org//abs/2407.12792)

	Vittorio Giammarino, James Queeney, Ioannis Ch. Paschalidis


+ [Cross-Task Attack: A Self-Supervision Generative Framework Based on Attention Shift](https://arxiv.org//abs/2407.13700)

	Qingyuan Zeng, Yunpeng Gong, Min Jiang


+ [Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models](https://arxiv.org//abs/2407.13757)

	Zhuo Chen, Jiawei Liu, Haotan Liu, Qikai Cheng, Fan Zhang, Wei Lu, Xiaozhong Liu


+ [Unveiling Structural Memorization: Structural Membership Inference Attack for Text-to-Image Diffusion Models](https://arxiv.org//abs/2407.13252)

	Qiao Li, Xiaomeng Fu, Xi Wang, Jin Liu, Xingyu Gao, Jiao Dai, Jizhong Han


+ [PG-Attack: A Precision-Guided Adversarial Attack Framework Against Vision Foundation Models for Autonomous Driving](https://arxiv.org//abs/2407.13111)

	Jiyuan Fu, Zhaoyu Chen, Kaixun Jiang, Haijing Guo, Shuyong Gao, Wenqiang Zhang


+ [Krait: A Backdoor Attack Against Graph Prompt Tuning](https://arxiv.org//abs/2407.13068)

	Ying Song, Rita Singh, Balaji Palanisamy


+ [Motif-Consistent Counterfactuals with Adversarial Refinement for Graph-Level Anomaly Detection](https://arxiv.org//abs/2407.13251)

	Chunjing Xiao, Shikang Pang, Wenxin Tai, Yanlong Huang, Goce Trajcevski, Fan Zhou


+ [Distributionally and Adversarially Robust Logistic Regression via Intersecting Wasserstein Balls](https://arxiv.org//abs/2407.13625)

	Aras Selvi, Eleonora Kreacic, Mohsen Ghassemi, Vamsi Potluru, Tucker Balch, Manuela Veloso


+ [BiasDPO: Mitigating Bias in Language Models through Direct Preference Optimization](https://arxiv.org//abs/2407.13928)

	Ahmed Allam


+ [A Closer Look at GAN Priors: Exploiting Intermediate Features for Enhanced Model Inversion Attacks](https://arxiv.org//abs/2407.13863)

	Yixiang Qiu, Hao Fang, Hongyao Yu, Bin Chen, MeiKang Qiu, Shu-Tao Xia


# 2024-07-17
+ [Turning Generative Models Degenerate: The Power of Data Poisoning Attacks](https://arxiv.org//abs/2407.12281)

	Shuli Jiang, Swanand Ravindra Kadhe, Yi Zhou, Farhan Ahmed, Ling Cai, Nathalie Baracaldo


+ [Any Target Can be Offense: Adversarial Example Generation via Generalized Latent Infection](https://arxiv.org//abs/2407.12292)

	Youheng Sun, Shengming Yuan, Xuanhan Wang, Lianli Gao, Jingkuan Song


+ [Benchmarking Robust Self-Supervised Learning Across Diverse Downstream Tasks](https://arxiv.org//abs/2407.12588)

	Antoni Kowalczuk, Jan Dubiński, Atiyeh Ashari Ghomi, Yi Sui, George Stein, Jiapeng Wu, Jesse C. Cresswell, Franziska Boenisch, Adam Dziedzic


+ [LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models](https://arxiv.org//abs/2407.12772)

	Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, Ziwei Liu


+ [Preventing Catastrophic Overfitting in Fast Adversarial Training: A Bi-level Optimization Perspective](https://arxiv.org//abs/2407.12443)

	Zhaoxin Wang, Handing Wang, Cong Tian, Yaochu Jin


+ [Contrastive Adversarial Training for Unsupervised Domain Adaptation](https://arxiv.org//abs/2407.12782)

	Jiahong Chen, Zhilin Zhang, Lucy Li, Behzad Shahrasbi, Arjun Mishra


+ [AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases](https://arxiv.org//abs/2407.12784)

	Zhaorun Chen, Zhen Xiang, Chaowei Xiao, Dawn Song, Bo Li


+ [Safeguard Text-to-Image Diffusion Models with Human Feedback Inversion](https://arxiv.org//abs/2407.21032)

	Sanghyun Kim, Seohyeon Jung, Balhae Kim, Moonseok Choi, Jinwoo Shin, Juho Lee


+ [Direct Unlearning Optimization for Robust and Safe Text-to-Image Models](https://arxiv.org//abs/2407.21035)

	Yong-Hyun Park, Sangdoo Yun, Jin-Hwa Kim, Junho Kim, Geonhui Jang, Yonghyun Jeong, Junghyo Jo, Gayoung Lee



# 2024-07-16
+ [EARN Fairness: Explaining, Asking, Reviewing and Negotiating Artificial Intelligence Fairness Metrics Among Stakeholders](https://arxiv.org//abs/2407.11442)

	Lin Luo, Yuri Nakao, Mathieu Chollet, Hiroya Inakoshi, Simone Stumpf


+ [Feature Inference Attack on Shapley Values](https://arxiv.org//abs/2407.11359)

	Xinjian Luo, Yangfan Jiang, Xiaokui Xiao


+ [AEMIM: Adversarial Examples Meet Masked Image Modeling](https://arxiv.org//abs/2407.11537)

	Wenzhao Xiang, Chang Liu, Hang Su, Hongyang Yu


+ [Enhancing TinyML Security: Study of Adversarial Attack Transferability](https://arxiv.org//abs/2407.11599)

	Parin Shah, Yuvaraj Govindarajulu, Pavan Kulkarni, Manojkumar Parmar


+ [Variational Randomized Smoothing for Sample-Wise Adversarial Robustness](https://arxiv.org//abs/2407.11844)

	Ryo Hase, Ye Wang, Toshiaki Koike-Akino, Jing Liu, Kieran Parsons


+ [Does Refusal Training in LLMs Generalize to the Past Tense?](https://arxiv.org//abs/2407.11969)

	Maksym Andriushchenko, Nicolas Flammarion


+ [Model Inversion Attacks Through Target-Specific Conditional Diffusion Models](https://arxiv.org//abs/2407.11424)

	Ouxiang Li, Yanbin Hao, Zhicai Wang, Bin Zhu, Shuo Wang, Zaixi Zhang, Fuli Feng


+ [Cycle Contrastive Adversarial Learning for Unsupervised image Deraining](https://arxiv.org//abs/2407.11750)

	Chen Zhao, Weiling Cai, ChengWei Hu, Zheng Yuan


+ [SegSTRONG-C: Segmenting Surgical Tools Robustly On Non-adversarial Generated Corruptions -- An EndoVis'24 Challenge](https://arxiv.org//abs/2407.11906)

	Hao Ding, Tuxun Lu, Yuqian Zhang, Ruixing Liang, Hongchao Shu, Lalithkumar Seenivasan, Yonghao Long, Qi Dou, Cong Gao, Mathias Unberath


+ [IPA-NeRF: Illusory Poisoning Attack Against Neural Radiance Fields](https://arxiv.org//abs/2407.11921)

	Wenxiang Jiang, Hanwei Zhang, Shuo Zhao, Zhongwen Guo, Hao Wang


+ [UNIT: Backdoor Mitigation via Automated Neural Distribution Tightening](https://arxiv.org//abs/2407.11372)

	Siyuan Cheng, Guangyu Shen, Kaiyuan Zhang, Guanhong Tao, Shengwei An, Hanxi Guo, Shiqing Ma, Xiangyu Zhang


+ [Relaxing Graph Transformers for Adversarial Attacks](https://arxiv.org//abs/2407.11764)

	Philipp Foth, Lukas Gosch, Simon Geisler, Leo Schwinn, Stephan Günnemann


+ [One-Shot Unlearning of Personal Identities](https://arxiv.org//abs/2407.12069)

	Thomas De Min, Subhankar Roy, Massimiliano Mancini, Stéphane Lathuilière, Elisa Ricci


+ [Generalized Coverage for More Robust Low-Budget Active Learning](https://arxiv.org//abs/2407.12212)

	Wonho Bae, Junhyug Noh, Danica J. Sutherland


# 2024-07-15
+ [Backdoor Attacks against Image-to-Image Networks](https://arxiv.org//abs/2407.10445)

	Wenbo Jiang, Hongwei Li, Jiaming He, Rui Zhang, Guowen Xu, Tianwei Zhang, Rongxing Lu


+ [Learning to Unlearn for Robust Machine Unlearning](https://arxiv.org//abs/2407.10494)

	Mark He Huang, Lin Geng Foo, Jun Liu


+ [Wicked Oddities: Selectively Poisoning for Effective Clean-Label Backdoor Attacks](https://arxiv.org//abs/2407.10825)

	Quang H. Nguyen, Nguyen Ngoc-Hieu, The-Anh Ta, Thanh Nguyen-Tang, Hoang Thanh-Tung, Khoa D. Doan


+ [Provable Robustness of (Graph) Neural Networks Against Data Poisoning and Backdoor Attacks](https://arxiv.org//abs/2407.10867)

	Lukas Gosch, Mahalakshmi Sabanayagam, Debarghya Ghoshdastidar, Stephan Günnemann



# 2024-07-14
+ [Look Within, Why LLMs Hallucinate: A Causal Perspective](https://arxiv.org//abs/2407.10153)

	He Li, Haoang Chi, Mingyu Liu, Wenjing Yang


+ [Augmented Neural Fine-Tuning for Efficient Backdoor Purification](https://arxiv.org//abs/2407.10052)

	Nazmul Karim, Abdullah Al Arafat, Umar Khalid, Zhishan Guo, Nazanin Rahnavard


+ [CLIP-Guided Networks for Transferable Targeted Attacks](https://arxiv.org//abs/2407.10179)

	Hao Fang, Jiawei Kong, Bin Chen, Tao Dai, Hao Wu, Shu-Tao Xia


+ [SENTINEL: Securing Indoor Localization against Adversarial Attacks with Capsule Neural Networks](https://arxiv.org//abs/2407.11091)

	Danish Gufran, Pooja Anandathirtha, Sudeep Pasricha


+ [Towards Adversarially Robust Vision-Language Models: Insights from Design Choices and Prompt Formatting Techniques](https://arxiv.org//abs/2407.11121)

	Rishika Bhagwatkar, Shravan Nayak, Reza Bayat, Alexis Roger, Daniel Z Kaplan, Pouya Bashivan, Irina Rish


+ [Uncertainty is Fragile: Manipulating Uncertainty in Large Language Models](https://arxiv.org//abs/2407.11282)

	Qingcheng Zeng, Mingyu Jin, Qinkai Yu, Zhenting Wang, Wenyue Hua, Zihao Zhou, Guangyan Sun, Yanda Meng, Shiqing Ma, Qifan Wang, Felix Juefei-Xu, Kaize Ding, Fan Yang, Ruixiang Tang, Yongfeng Zhang



# 2024-07-13
+ [Partner in Crime: Boosting Targeted Poisoning Attacks against Federated Learning](https://arxiv.org//abs/2407.09958)

	Shihua Sun, Shridatt Sugrim, Angelos Stavrou, Haining Wang


+ [SemiAdv: Query-Efficient Black-Box Adversarial Attack with Unlabeled Images](https://arxiv.org//abs/2407.11073)

	Mingyuan Fan, Yang Liu, Cen Chen, Ximeng Liu



# 2024-07-12
+ [Towards More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations](https://arxiv.org//abs/2407.08983)

	David N. Palacio, Daniel Rodriguez-Cardenas, Alejandro Velasco, Dipin Khati, Kevin Moran, Denys Poshyvanyk


+ [Robustness of LLMs to Perturbations in Text](https://arxiv.org//abs/2407.08989)

	Ayush Singh, Navpreet Singh, Shubham Vatsal


+ [Refusing Safe Prompts for Multi-modal Large Language Models](https://arxiv.org//abs/2407.09050)

	Zedian Shao, Hongbin Liu, Yuepeng Hu, Neil Zhenqiang Gong


+ [Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training](https://arxiv.org//abs/2407.09121)

	Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Jiahao Xu, Tian Liang, Pinjia He, Zhaopeng Tu


+ [TAPI: Towards Target-Specific and Adversarial Prompt Injection against Code LLMs](https://arxiv.org//abs/2407.09164)

	Yuchen Yang, Hongwei Yao, Bingrun Yang, Yiling He, Yiming Li, Tianwei Zhang, Zhan Qin, Kui Ren


+ [Deep Adversarial Defense Against Multilevel-Lp Attacks](https://arxiv.org//abs/2407.09251)

	Ren Wang, Yuxuan Li, Alfred Hero


+ [Evaluating the Adversarial Robustness of Semantic Segmentation: Trying Harder Pays Off](https://arxiv.org//abs/2407.09150)

	Levente Halmosi, Bálint Mohos, Márk Jelasity


+ [Distributed Backdoor Attacks on Federated Graph Learning and Certified Defenses](https://arxiv.org//abs/2407.08935)

	Yuxin Yang, Qiang Li, Jinyuan Jia, Yuan Hong, Binghui Wang


+ [PriRoAgg: Achieving Robust Model Aggregation with Minimum Privacy Leakage for Federated Learning](https://arxiv.org//abs/2407.08954)

	Sizai Hou, Songze Li, Tayyebeh Jahani-Nezhad, Giuseppe Caire


+ [DeCE: Deceptive Cross-Entropy Loss Designed for Defending Backdoor Attacks](https://arxiv.org//abs/2407.08956)

	Guang Yang, Yu Zhou, Xiang Chen, Xiangyu Zhang, Terry Yue Zhuo, David Lo, Taolue Chen


+ [CEIPA: Counterfactual Explainable Incremental Prompt Attack Analysis on Large Language Models](https://arxiv.org//abs/2407.09292)

	Dong Shu, Mingyu Jin, Tianle Chen, Chong Zhang, Yongfeng Zhang


+ [BoBa: Boosting Backdoor Detection through Data Distribution Inference in Federated Learning](https://arxiv.org//abs/2407.09658)

	Ning Wang, Shanghao Shi, Yang Xiao, Yimin Chen, Y. Thomas Hou, Wenjing Lou


+ [MaPPing Your Model: Assessing the Impact of Adversarial Attacks on LLM-based Programming Assistants](https://arxiv.org//abs/2407.11072)

	John Heibel, Daniel Lowd



# 2024-07-11
+ [Prediction Exposes Your Face: Black-box Model Inversion via Prediction Alignment](https://arxiv.org//abs/2407.08127)

	Yufan Liu, Wanqian Zhang, Dayan Wu, Zheng Lin, Jingzi Gu, Weiping Wang


+ [Enhancing Robustness of Vision-Language Models through Orthogonality Learning and Cross-Regularization](https://arxiv.org//abs/2407.08374)

	Jinlong Li, Zequn Jie, Elisa Ricci, Lin Ma, Nicu Sebe


+ [Rethinking the Threat and Accessibility of Adversarial Attacks against Face Recognition Systems](https://arxiv.org//abs/2407.08514)

	Yuxin Cao, Yumeng Zhu, Derui Wang, Sheng Wen, Minhui Xue, Jin Lu, Hao Ge


+ [Boosting Adversarial Transferability for Skeleton-based Action Recognition via Exploring the Model Posterior Space](https://arxiv.org//abs/2407.08572)

	Yunfeng Diao, Baiqi Wu, Ruixuan Zhang, Xun Yang, Meng Wang, He Wang


+ [How to beat a Bayesian adversary](https://arxiv.org//abs/2407.08678)

	Zihan Ding, Kexin Jin, Jonas Latz, Chenguang Liu


+ [Model-agnostic clean-label backdoor mitigation in cybersecurity environments](https://arxiv.org//abs/2407.08159)

	Giorgio Severi, Simona Boboila, John Holodnak, Kendra Kratkiewicz, Rauf Izmailov, Alina Oprea


+ [Enhancing Privacy of Spatiotemporal Federated Learning against Gradient Inversion Attacks](https://arxiv.org//abs/2407.08529)

	Lele Zheng, Yang Cao, Renhe Jiang, Kenjiro Taura, Yulong Shen, Sheng Li, Masatoshi Yoshikawa


+ [A Survey on the Application of Generative Adversarial Networks in Cybersecurity: Prospective, Direction and Open Research Scopes](https://arxiv.org//abs/2407.08839)

	Md Mashrur Arifin, Md Shoaib Ahmed, Tanmai Kumar Ghosh, Jun Zhuang, Jyh-haw Yeh


+ [Deep Learning for Network Anomaly Detection under Data Contamination: Evaluating Robustness and Mitigating Performance Degradation](https://arxiv.org//abs/2407.08838)

	D'Jeff K. Nkashama, Jordan Masakuna Félicien, Arian Soltani, Jean-Charles Verdier, Pierre-Martin Tardif, Marc Frappier, Froduald Kabanza



# 2024-07-10
+ [Tuning Vision-Language Models with Candidate Labels by Prompt Alignment](https://arxiv.org//abs/2407.07638)

	Zhifang Zhang, Beibei Li


+ [Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization](https://arxiv.org//abs/2407.07880)

	Junkang Wu, Yuexiang Xie, Zhengyi Yang, Jiancan Wu, Jiawei Chen, Jinyang Gao, Bolin Ding, Xiang Wang, Xiangnan He


+ [Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent Communities](https://arxiv.org//abs/2407.07791)

	Tianjie Ju, Yiting Wang, Xinbei Ma, Pengzhou Cheng, Haodong Zhao, Yulong Wang, Lifeng Liu, Jian Xie, Zhuosheng Zhang, Gongshen Liu


+ [Decompose and Compare Consistency: Measuring VLMs' Answer Reliability via Task-Decomposition Consistency Comparison](https://arxiv.org//abs/2407.07840)

	Qian Yang, Weixiang Yan, Aishwarya Agrawal


+ [Mitigating Backdoor Attacks using Activation-Guided Model Editing](https://arxiv.org//abs/2407.07662)

	Felix Hsieh, Huy H. Nguyen, AprilPyone MaungMaung, Dmitrii Usynin, Isao Echizen


# 2024-07-09
+ [A Hybrid Training-time and Run-time Defense Against Adversarial Attacks in Modulation Classification](https://arxiv.org//abs/2407.06807)

	Lu Zhang, Sangarapillai Lambotharan, Gan Zheng, Guisheng Liao, Ambra Demontis, Fabio Roli


+ [Robust Neural Information Retrieval: An Adversarial and Out-of-distribution Perspective](https://arxiv.org//abs/2407.06992)

	Yu-An Liu, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, Xueqi Cheng


+ [Hiding Local Manipulations on SAR Images: a Counter-Forensic Attack](https://arxiv.org//abs/2407.07041)

	Sara Mandelli, Edoardo Daniele Cannas, Paolo Bestagini, Stefano Tebaldini, Stefano Tubaro


+ [Universal Multi-view Black-box Attack against Object Detectors via Layout Optimization](https://arxiv.org//abs/2407.06688)

	Donghua Wang, Wen Yao, Tingsong Jiang, Chao Li, Xiaoqian Chen


+ [Improving the Transferability of Adversarial Examples by Feature Augmentation](https://arxiv.org//abs/2407.06714)

	Donghua Wang, Wen Yao, Tingsong Jiang, Xiaohu Zheng, Junqi Wu, Xiaoqian Chen


+ [AstroSpy: On detecting Fake Images in Astronomy via Joint Image-Spectral Representations](https://arxiv.org//abs/2407.06817)

	Mohammed Talha Alam, Raza Imam, Mohsen Guizani, Fakhri Karray


+ [Towards Physics-informed Cyclic Adversarial Multi-PSF Lensless Imaging](https://arxiv.org//abs/2407.06727)

	Abeer Banerjee, Sanjay Singh


+ [Event Trojan: Asynchronous Event-based Backdoor Attacks](https://arxiv.org//abs/2407.06838)

	Ruofei Wang, Qing Guo, Haoliang Li, Renjie Wan


+ [Tracing Back the Malicious Clients in Poisoning Attacks to Federated Learning](https://arxiv.org//abs/2407.07221)

	Yuqi Jia, Minghong Fang, Hongbin Liu, Jinghuai Zhang, Neil Zhenqiang Gong


# 2024-07-08
+ [$R^2$-Guard: Robust Reasoning Enabled LLM Guardrail via Knowledge-Enhanced Logical Reasoning](https://arxiv.org//abs/2407.05557)

	Mintong Kang, Bo Li


+ [KG-FPQ: Evaluating Factuality Hallucination in LLMs with Knowledge Graph-based False Premise Questions](https://arxiv.org//abs/2407.05868)

	Yanxu Zhu, Jinlin Xiao, Yuhang Wang, Jitao Sang


+ [Active Label Refinement for Robust Training of Imbalanced Medical Image Classification Tasks in the Presence of High Label Noise](https://arxiv.org//abs/2407.05973)

	Bidur Khanal, Tianhong Dai, Binod Bhattarai, Cristian Linte


+ [Enhanced Model Robustness to Input Corruptions by Per-corruption Adaptation of Normalization Statistics](https://arxiv.org//abs/2407.06450)

	Elena Camuffo, Umberto Michieli, Simone Milani, Jijoong Moon, Mete Ozay


+ [FORAY: Towards Effective Attack Synthesis against Deep Logical Vulnerabilities in DeFi Protocols](https://arxiv.org//abs/2407.06348)

	Hongbo Wen, Hanzhi Liu, Jiaxin Song, Yanju Chen, Wenbo Guo, Yu Feng


# 2024-07-07
+ [Gradient Diffusion: A Perturbation-Resilient Gradient Leakage Attack](https://arxiv.org//abs/2407.05285)

	Xuan Liu, Siqi Cai, Qihua Zhou, Song Guo, Ruibin Li, Kaiwei Lin


+ [Evolutionary Trigger Detection and Lightweight Model Repair Based Backdoor Defense](https://arxiv.org//abs/2407.05396)

	Qi Zhou, Zipeng Ye, Yubo Tang, Wenjian Luo, Yuhui Shi, Yan Jia


# 2024-07-06
+ [BadCLM: Backdoor Attack in Clinical Language Models for Electronic Health Records](https://arxiv.org//abs/2407.05213)

	Weimin Lyu, Zexin Bi, Fusheng Wang, Chao Chen


# 2024-07-05
+ [Controlling Whisper: Universal Acoustic Adversarial Attacks to Control Speech Foundation Models](https://arxiv.org//abs/2407.04482)

	Vyas Raina, Mark Gales


+ [T2IShield: Defending Against Backdoors on Text-to-Image Diffusion Models](https://arxiv.org//abs/2407.04215)

	Zhongqi Wang, Jie Zhang, Shiguang Shan, Xilin Chen


+ [Self-Supervised Representation Learning for Adversarial Attack Detection](https://arxiv.org//abs/2407.04382)

	Yi Li, Plamen Angelov, Neeraj Suri


+ [Late Breaking Results: Fortifying Neural Networks: Safeguarding Against Adversarial Attacks with Stochastic Computing](https://arxiv.org//abs/2407.04861)

	Faeze S. Banitaba, Sercan Aygun, M. Hassan Najafi


+ [Non-Cooperative Backdoor Attacks in Federated Learning: A New Threat Landscape](https://arxiv.org//abs/2407.07917)

	Tuan Nguyen, Dung Thuy Nguyen, Khoa D Doan, Kok-Seng Wong



# 2024-07-04
+ [Adversarial Robustness of VAEs across Intersectional Subgroups](https://arxiv.org//abs/2407.03864)

	Chethan Krishnamurthy Ramanaik, Arjun Roy, Eirini Ntoutsi


+ [Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models](https://arxiv.org//abs/2407.04121)

	Yuyan Chen, Qiang Fu, Yichen Yuan, Zhihao Wen, Ge Fan, Dayiheng Liu, Dongmei Zhang, Zhixu Li, Yanghua Xiao


+ [Securing Multi-turn Conversational Language Models Against Distributed Backdoor Triggers](https://arxiv.org//abs/2407.04151)

	Terry Tong, Jiashu Xu, Qin Liu, Muhao Chen


+ [Defense Against Syntactic Textual Backdoor Attacks with Token Substitution](https://arxiv.org//abs/2407.04179)

	Xinglin Li, Xianwen He, Yao Li, Minhao Cheng


+ [DART: Deep Adversarial Automated Red Teaming for LLM Safety](https://arxiv.org//abs/2407.03876)

	Bojian Jiang, Yi Jing, Tianhao Shen, Qing Yang, Deyi Xiong


+ [TrackPGD: A White-box Attack using Binary Masks against Robust Transformer Trackers](https://arxiv.org//abs/2407.03946)

	Fatemeh Nourilenjan Nokabadi, Yann Batiste Pequignot, Jean-Francois Lalonde, Christian Gagné


+ [Mitigating Low-Frequency Bias: Feature Recalibration and Frequency Attention Regularization for Adversarial Robustness](https://arxiv.org//abs/2407.04016)

	Kejia Zhang, Juanjuan Weng, Yuanzheng Cai, Zhiming Luo, Shaozi Li


+ [Future Events as Backdoor Triggers: Investigating Temporal Vulnerabilities in LLMs](https://arxiv.org//abs/2407.04108)

	Sara Price, Arjun Panickssery, Sam Bowman, Asa Cooper Stickland



# 2024-07-03
+ [Self-Evaluation as a Defense Against Adversarial Attacks on LLMs](https://arxiv.org/abs/2407.03234)

	Hannah Brown, Leon Lin, Kenji Kawaguchi, Michael Shieh


+ [Safe Unlearning: A Surprisingly Effective and Generalizable Solution to Defend Against Jailbreak Attacks](https://arxiv.org/abs/2407.02855)

	Zhexin Zhang, Junxiao Yang, Pei Ke, Shiyao Cui, Chujie Zheng, Hongning Wang, Minlie Huang


+ [JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational Datasets](https://arxiv.org/abs/2407.03045)

	Zhihua Jin, Shiyi Liu, Haotian Li, Xun Zhao, Huamin Qu


+ [SPLITZ: Certifiable Robustness via Split Lipschitz Randomized Smoothing](https://arxiv.org/abs/2407.02811)

	Meiyu Zhong, Ravi Tandon


+ [A Wolf in Sheep's Clothing: Practical Black-box Adversarial Attacks for Evading Learning-based Windows Malware Detection in the Wild](https://arxiv.org/abs/2407.02886)

	Xiang Ling, Zhiyu Wu, Bin Wang, Wei Deng, Jingzheng Wu, Shouling Ji, Tianyue Luo, Yanjun Wu


+ [Soft Begging: Modular and Efficient Shielding of LLMs against Prompt Injection and Jailbreaking based on Prompt Tuning](https://arxiv.org//abs/2407.03391)

	Simon Ostermann, Kevin Baum, Christoph Endres, Julia Masloh, Patrick Schramowski


# 2024-07-02
+ [EvolBA: Evolutionary Boundary Attack under Hard-label Black Box condition](https://arxiv.org/abs/2407.02248)

	Ayane Tajima, Satoshi Ono


+ [Adversarial Magnification to Deceive Deepfake Detection through Super Resolution](https://arxiv.org/abs/2407.02670)

	Davide Alessandro Coccomini, Roberto Caldelli, Giuseppe Amato, Fabrizio Falchi, Claudio Gennaro


+ [Parameter Matching Attack: Enhancing Practical Applicability of Availability Attacks](https://arxiv.org/abs/2407.02437)

	Yu Zhe, Jun Sakuma


+ [Face Reconstruction Transfer Attack as Out-of-Distribution Generalization](https://arxiv.org/abs/2407.02403)

	Yoon Gyo Jung, Jaewoo Park, Xingbo Dong, Hojin Park, Andrew Beng Jin Teoh, Octavia Camps


+ [Towards More Realistic Extraction Attacks: An Adversarial Perspective](https://arxiv.org/abs/2407.02596)

	Yash More, Prakhar Ganesh, Golnoosh Farnadi


+ [Is Your AI-Generated Code Really Safe? Evaluating Large Language Models on Secure Code Generation with CodeSecEval](https://arxiv.org//abs/2407.02395)

	Jiexin Wang, Xitong Luo, Liuwen Cao, Hongkui He, Hailin Huang, Jiayuan Xie, Adam Jatowt, Yi Cai


# 2024-07-01
+ [Multi-View Black-Box Physical Attacks on Infrared Pedestrian Detectors Using Adversarial Infrared Grid](https://arxiv.org//abs/2407.01168)

	Kalibinuer Tiliwalidi, Chengyin Hu, Weiwen Shi


+ [Learning Robust 3D Representation from CLIP via Dual Denoising](https://arxiv.org//abs/2407.00905)

	Shuqing Luo, Bowen Qu, Wei Gao


+ [Semantic-guided Adversarial Diffusion Model for Self-supervised Shadow Removal](https://arxiv.org//abs/2407.01104)

	Ziqi Zeng, Chen Zhao, Weiling Cai, Chenyu Dong


+ [Unveiling the Unseen: Exploring Whitebox Membership Inference through the Lens of Explainability](https://arxiv.org//abs/2407.01306)

	Chenxi Li, Abhinav Kumar, Zhen Guo, Jie Hou, Reza Tourani



# 2024-06-30
+ [Unveiling Glitches: A Deep Dive into Image Encoding Bugs within CLIP](https://arxiv.org//abs/2407.00592)

	Ayush Ranjan, Daniel Wen, Karthik Bhat


+ [Consistency Purification: Effective and Efficient Diffusion Purification towards Certified Robustness](https://arxiv.org//abs/2407.00623)

	Yiquan Li, Zhongzhu Chen, Kun Jin, Jiongxiao Wang, Bo Li, Chaowei Xiao


+ [A Whole-Process Certifiably Robust Aggregation Method Against Backdoor Attacks in Federated Learning](https://arxiv.org//abs/2407.00719)

	Anqi Zhou, Yezheng Liu, Yidong Chai, Hongyi Zhu, Xinyue Ge, Yuanchun Jiang, Meng Wang



# 2024-06-29
+ [Query-Efficient Hard-Label Black-Box Attack against Vision Transformers](https://arxiv.org//abs/2407.00389)

	Chao Zhou, Xiaowen Shi, Yuan-Gen Wang



# 2024-06-28
+ [Data-Driven Lipschitz Continuity: A Cost-Effective Approach to Improve Adversarial Robustness](https://arxiv.org//abs/2406.19622)

	Erh-Chung Chen, Pin-Yu Chen, I-Hsin Chung, Che-Rung Lee


+ [Deceptive Diffusion: Generating Synthetic Adversarial Examples](https://arxiv.org//abs/2406.19807)

	Lucas Beerens, Catherine F. Higham, Desmond J. Higham


+ [Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation](https://arxiv.org//abs/2406.20053)

	Danny Halawi, Alexander Wei, Eric Wallace, Tony T. Wang, Nika Haghtalab, Jacob Steinhardt


+ [IDT: Dual-Task Adversarial Attacks for Privacy Protection](https://arxiv.org//abs/2406.19642)

	Pedro Faustini, Shakila Mahjabin Tonni, Annabelle McIver, Qiongkai Xu, Mark Dras


+ [NLPerturbator: Studying the Robustness of Code LLMs to Natural Language Variations](https://arxiv.org//abs/2406.19783)

	Junkai Chen, Zhenhao Li, Xing Hu, Xin Xia


+ [GM-DF: Generalized Multi-Scenario Deepfake Detection](https://arxiv.org//abs/2406.20078)

	Yingxin Lai, Zitong Yu, Jing Yang, Bin Li, Xiangui Kang, Linlin Shen


+ [AstMatch: Adversarial Self-training Consistency Framework for Semi-Supervised Medical Image Segmentation](https://arxiv.org//abs/2406.19649)

	Guanghao Zhu, Jing Zhang, Juanxiu Liu, Xiaohui Du, Ruqian Hao, Yong Liu, Lin Liu


+ [Backdoor Attack in Prompt-Based Continual Learning](https://arxiv.org//abs/2406.19753)

	Trang Nguyen, Anh Tran, Nhat Ho


+ [Virtual Context: Enhancing Jailbreak Attacks with Special Token Injection](https://arxiv.org//abs/2406.19845)

	Yuqi Zhou, Lin Lu, Hanchi Sun, Pan Zhou, Lichao Sun


+ [DiffuseDef: Improved Robustness to Adversarial Attacks](https://arxiv.org//abs/2407.00248)

	Zhenhao Li, Marek Rei, Lucia Specia



# 2024-06-27
+ [Rethinking harmless refusals when fine-tuning foundation models](https://arxiv.org//abs/2406.19552)

	Florin Pop, Judd Rosenblatt, Diogo Schwerz de Lucena, Michael Vaiana


+ [Data Poisoning Attacks to Locally Differentially Private Frequent Itemset Mining Protocols](https://arxiv.org//abs/2406.19466)

	Wei Tong, Haoyu Chen, Jiacheng Niu, Sheng Zhong



# 2024-06-26
+ [Poisoned LangChain: Jailbreak LLMs by LangChain](https://arxiv.org//abs/2406.18122)

	Ziqiu Wang, Jun Liu, Shengkai Zhang, Yang Yang


+ [MALSIGHT: Exploring Malicious Source Code and Benign Pseudocode for Iterative Binary Malware Summarization](https://arxiv.org//abs/2406.18379)

	Haolang Lu, Hongrui Peng, Guoshun Nan, Jiaoyang Cui, Cheng Wang, Weifei Jin


+ [WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models](https://arxiv.org//abs/2406.18510)

	Liwei Jiang, Kavel Rao, Seungju Han, Allyson Ettinger, Faeze Brahman, Sachin Kumar, Niloofar Mireshghallah, Ximing Lu, Maarten Sap, Yejin Choi, Nouha Dziri


+ [SafeAligner: Safety Alignment against Jailbreak Attacks via Response Disparity Guidance](https://arxiv.org//abs/2406.18118)

	Caishuang Huang, Wanxu Zhao, Rui Zheng, Huijie Lv, Shihan Dou, Sixian Li, Xiao Wang, Enyu Zhou, Junjie Ye, Yuming Yang, Tao Gui, Qi Zhang, Xuanjing Huang



# 2024-06-25
+ [Machine Unlearning Fails to Remove Data Poisoning Attacks](https://arxiv.org//abs/2406.17216)

	Martin Pawelczyk, Jimmy Z. Di, Yiwei Lu, Gautam Kamath, Ayush Sekhari, Seth Neel


+ [Diffusion-based Adversarial Purification for Intrusion Detection](https://arxiv.org//abs/2406.17606)

	Mohamed Amine Merzouk, Erwan Beurier, Reda Yaich, Nora Boulahia-Cuppens, Frédéric Cuppens


+ [A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns Well with The Key Tokens](https://arxiv.org//abs/2406.17378)

	Zhijie Nie, Richong Zhang, Zhanyu Wu


+ [Inherent Challenges of Post-Hoc Membership Inference for Large Language Models](https://arxiv.org//abs/2406.17975)

	Matthieu Meeus, Shubham Jain, Marek Rei, Yves-Alexandre de Montjoye



# 2024-06-24
+ [UNICAD: A Unified Approach for Attack Detection, Noise Reduction and Novel Class Identification](https://arxiv.org//abs/2406.16501)

	Alvaro Lopez Pellicer, Kittipos Giatgong, Yi Li, Neeraj Suri, Plamen Angelov


+ [Evaluating the Robustness of Deep-Learning Algorithm-Selection Models by Evolving Adversarial Instances](https://arxiv.org//abs/2406.16609)

	Emma Hart, Quentin Renau, Kevin Sim, Mohamad Alissa


+ [Adversarial Contrastive Decoding: Boosting Safety Alignment of Large Language Models via Opposite Prompt Optimization](https://arxiv.org//abs/2406.16743)

	Zhengyue Zhao, Xiaoyun Zhang, Kaidi Xu, Xing Hu, Rui Zhang, Zidong Du, Qi Guo, Yunji Chen


+ [Evaluating and Analyzing Relationship Hallucinations in LVLMs](https://arxiv.org//abs/2406.16449)

	Mingrui Wu, Jiayi Ji, Oucheng Huang, Jiale Li, Yuhang Wu, Xiaoshuai Sun, Rongrong Ji


+ [Improving robustness to corruptions with multiplicative weight perturbations](https://arxiv.org//abs/2406.16540)

	Trung Trinh, Markus Heinonen, Luigi Acerbi, Samuel Kaski


+ [Noisy Neighbors: Efficient membership inference attacks against LLMs](https://arxiv.org//abs/2406.16565)

	Filippo Galli, Luca Melis, Tommaso Cucinotta


+ [BEEAR: Embedding-based Adversarial Removal of Safety Backdoors in Instruction-tuned Language Models](https://arxiv.org//abs/2406.17092)

	Yi Zeng, Weiyu Sun, Tran Ngoc Huynh, Dawn Song, Bo Li, Ruoxi Jia


+ [Evaluating the Quality of Hallucination Benchmarks for Large Vision-Language Models](https://arxiv.org//abs/2406.17115)

	Bei Yan, Jie Zhang, Zheng Yuan, Shiguang Shan, Xilin Chen


+ [Automated Adversarial Discovery for Safety Classifiers](https://arxiv.org//abs/2406.17104)

	Yash Kumar Lal, Preethi Lahoti, Aradhana Sinha, Yao Qin, Ananth Balashankar


# 2024-06-23
+ [Towards unlocking the mystery of adversarial fragility of neural networks](https://arxiv.org//abs/2406.16200)

	Jingchao Gao, Raghu Mudumbai, Xiaodong Wu, Jirong Yi, Catherine Xu, Hui Xie, Weiyu Xu


+ [CBPF: Filtering Poisoned Data Based on Composite Backdoor Attack](https://arxiv.org//abs/2406.16125)

	Hanfeng Xia, Haibo Hong, Ruili Wang


+ [Blind Baselines Beat Membership Inference Attacks for Foundation Models](https://arxiv.org//abs/2406.16201)

	Debeshee Das, Jie Zhang, Florian Tramèr



# 2024-06-22
+ [Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs](https://arxiv.org//abs/2406.15927)

	Jannik Kossen, Jiatong Han, Muhammed Razzak, Lisa Schut, Shreshth Malik, Yarin Gal


+ [EmoAttack: Emotion-to-Image Diffusion Models for Emotional Backdoor Generation](https://arxiv.org//abs/2406.15863)

	Tianyu Wei, Shanmin Pang, Qi Guo, Yizhuo Ma, Qing Guo


+ [Federated Adversarial Learning for Robust Autonomous Landing Runway Detection](https://arxiv.org//abs/2406.15925)

	Yi Li, Plamen Angelov, Zhengxin Yu, Alvaro Lopez Pellicer, Neeraj Suri


+ [Large Language Models for Link Stealing Attacks Against Graph Neural Networks](https://arxiv.org//abs/2406.16963)

	Faqian Guan, Tianqing Zhu, Hui Sun, Wanlei Zhou, Philip S. Yu


+ [MOSSBench: Is Your Multimodal Language Model Oversensitive to Safe Queries?](https://arxiv.org//abs/2406.17806)

	Xirui Li, Hengguang Zhou, Ruochen Wang, Tianyi Zhou, Minhao Cheng, Cho-Jui Hsieh



# 2024-06-21
+ [From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking](https://arxiv.org//abs/2406.14859)

	Siyuan Wang, Zhuohan Long, Zhihao Fan, Zhongyu Wei


+ [Contextual Interaction via Primitive-based Adversarial Training For Compositional Zero-shot Learning](https://arxiv.org//abs/2406.14962)

	Suyi Li, Chenyi Jiang, Shidong Wang, Yang Long, Zheng Zhang, Haofeng Zhang


+ [DataFreeShield: Defending Adversarial Attacks without Training Data](https://arxiv.org//abs/2406.15635)

	Hyeyoon Lee, Kanghyun Choi, Dain Kwon, Sunjong Park, Mayoore Selvarasa Jaiswal, Noseong Park, Jonghyun Choi, Jinho Lee



# 2024-06-20
+ [Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective](https://arxiv.org//abs/2406.14023)

	Yuchen Wen, Keping Bi, Wei Chen, Jiafeng Guo, Xueqi Cheng


+ [Enhancing robustness of data-driven SHM models: adversarial training with circle loss](https://arxiv.org//abs/2406.14232)

	Xiangli Yang, Xijie Deng, Hanwei Zhang, Yang Zou, Jianxi Yang


+ [ObscurePrompt: Jailbreaking Large Language Models via Obscure Input](https://arxiv.org//abs/2406.13662)

	Yue Huang, Jingyu Tang, Dongping Chen, Bingda Tang, Yao Wan, Lichao Sun, Xiangliang Zhang


+ [MEAT: Median-Ensemble Adversarial Training for Improving Robustness and Generalization](https://arxiv.org//abs/2406.14259)

	Zhaozhe Hu, Jia-Li Yin, Bin Chen, Luojun Lin, Bo-Hao Chen, Ximeng Liu


+ [Explainable AI Security: Exploring Robustness of Graph Neural Networks to Adversarial Attacks](https://arxiv.org//abs/2406.13920)

	Tao Wu, Canyixing Cui, Xingping Xian, Shaojie Qiao, Chao Wang, Lin Yuan, Shui Yu


+ [Defending Against Sophisticated Poisoning Attacks with RL-based Aggregation in Federated Learning](https://arxiv.org//abs/2406.14217)

	Yujing Wang, Hainan Zhang, Sijia Wen, Wangjie Qiu, Binghui Guo


+ [Adaptive Adversarial Cross-Entropy Loss for Sharpness-Aware Minimization](https://arxiv.org//abs/2406.14329)

	Tanapat Ratchatorn, Masayuki Tanaka


+ [Adversaries Can Misuse Combinations of Safe Models](https://arxiv.org//abs/2406.14595)

	Erik Jones, Anca Dragan, Jacob Steinhardt



# 2024-06-19
+ [AGSOA:Graph Neural Network Targeted Attack Based on Average Gradient and Structure Optimization](https://arxiv.org//abs/2406.13228)

	Yang Chen, Bin Zhou


+ [Bayes' capacity as a measure for reconstruction attacks in federated learning](https://arxiv.org//abs/2406.13569)

	Sayan Biswas, Mark Dras, Pedro Faustini, Natasha Fernandes, Annabelle McIver, Catuscia Palamidessi, Parastoo Sadeghi


+ [Towards Trustworthy Unsupervised Domain Adaptation: A Representation Learning Perspective for Enhancing Robustness, Discrimination, and Generalization](https://arxiv.org//abs/2406.13180)

	Jia-Li Yin, Haoyuan Zheng, Ximeng Liu


+ [Benchmarking Unsupervised Online IDS for Masquerade Attacks in CAN](https://arxiv.org//abs/2406.13778)

	Pablo Moriano, Steven C. Hespeler, Mingyan Li, Robert A. Bridges



# 2024-06-18
+ [CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models](https://arxiv.org//abs/2406.12257)

	Yuetai Li, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Dinuka Sahabandu, Bhaskar Ramasubramanian, Radha Poovendran


+ [Adversarial Attacks on Large Language Models in Medicine](https://arxiv.org//abs/2406.12259)

	Yifan Yang, Qiao Jin, Furong Huang, Zhiyong Lu


+ [Stealth edits for provably fixing or attacking large language models](https://arxiv.org//abs/2406.12670)

	Oliver J. Sutton, Qinghua Zhou, Wei Wang, Desmond J. Higham, Alexander N. Gorban, Alexander Bastounis, Ivan Y. Tyukin


+ [UIFV: Data Reconstruction Attack in Vertical Federated Learning](https://arxiv.org//abs/2406.12588)

	Jirui Yang, Peng Chen, Zhihui Lu, Qiang Duan, Yubing Bao


+ [Can Go AIs be adversarially robust?](https://arxiv.org//abs/2406.12843)

	Tom Tseng, Euan McLean, Kellin Pelrine, Tony T. Wang, Adam Gleave


+ [ToxiCloakCN: Evaluating Robustness of Offensive Language Detection in Chinese with Cloaking Perturbations](https://arxiv.org//abs/2406.12223)

	Yunze Xiao, Yujia Hu, Kenny Tsu Wei Choo, Roy Ka-wei Lee


+ [Defending Against Social Engineering Attacks in the Age of LLMs](https://arxiv.org//abs/2406.12263)

	Lin Ai, Tharindu Kumarage, Amrita Bhattacharjee, Zizhou Liu, Zheng Hui, Michael Davinroy, James Cook, Laura Cassani, Kirill Trapeznikov, Matthias Kirchner, Arslan Basharat, Anthony Hoogs, Joshua Garland, Huan Liu, Julia Hirschberg


+ [Adversarial Attacks on Multimodal Agents](https://arxiv.org//abs/2406.12814)

	Chen Henry Wu, Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried, Aditi Raghunathan


+ [Attack and Defense of Deep Learning Models in the Field of Web Attack Detection](https://arxiv.org//abs/2406.12605)

	Lijia Shi, Shihao Dong


+ [MaskPure: Improving Defense Against Text Adversaries with Stochastic Purification](https://arxiv.org//abs/2406.13066)

	Harrison Gietz, Jugal Kalita


+ [NoiSec: Harnessing Noise for Security against Adversarial and Backdoor Attacks](https://arxiv.org//abs/2406.13073)

	Md Hasan Shahriar, Ning Wang, Y. Thomas Hou, Wenjing Lou


+ [DLP: towards active defense against backdoor attacks with decoupled learning process](https://arxiv.org//abs/2406.13098)

	Zonghao Ying, Bin Wu


+ [Saliency Attention and Semantic Similarity-Driven Adversarial Perturbation](https://arxiv.org//abs/2406.19413)

	Hetvi Waghela, Jaydip Sen, Sneha Rakshit



# 2024-06-17
+ [Adversarial Style Augmentation via Large Language Model for Robust Fake News Detection](https://arxiv.org//abs/2406.11260)

	Sungwon Park, Sungwon Han, Meeyoung Cha


+ [Knowledge-to-Jailbreak: One Knowledge Point Worth One Attack](https://arxiv.org//abs/2406.11682)

	Shangqing Tu, Zhuoran Pan, Wenxuan Wang, Zhexin Zhang, Yuliang Sun, Jifan Yu, Hongning Wang, Lei Hou, Juanzi Li


+ ["Not Aligned" is Not "Malicious": Being Careful about Hallucinations of Large Language Models' Jailbreak](https://arxiv.org//abs/2406.11668)

	Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Jiayi Mao, Xueqi Cheng


+ [Harmonizing Feature Maps: A Graph Convolutional Approach for Enhancing Adversarial Robustness](https://arxiv.org//abs/2406.11576)

	Kejia Zhang, Juanjuan Weng, Junwei Wu, Guoqing Yang, Shaozi Li, Zhiming Luo


+ [A First Physical-World Trajectory Prediction Attack via LiDAR-induced Deceptions in Autonomous Driving](https://arxiv.org//abs/2406.11707)

	Yang Lou, Yi Zhu, Qun Song, Rui Tan, Chunming Qiao, Wei-Bin Lee, Jianping Wang


+ [Adversaries With Incentives: A Strategic Alternative to Adversarial Robustness](https://arxiv.org//abs/2406.11458)

	Maayan Ehrenberg, Roy Ganz, Nir Rosenfeld


+ [Obfuscating IoT Device Scanning Activity via Adversarial Example Generation](https://arxiv.org//abs/2406.11515)

	Haocong Li, Yaxin Zhang, Long Cheng, Wenjia Niu, Haining Wang, Qiang Li


+ [Is poisoning a real threat to LLM alignment? Maybe more so than you think](https://arxiv.org//abs/2406.12091)

	Pankayaraj Pathmanathan, Souradip Chakraborty, Xiangyu Liu, Yongyuan Liang, Furong Huang


+ [Adversarial Perturbations Cannot Reliably Protect Artists From Generative AI](https://arxiv.org//abs/2406.12027)

	Robert Hönig, Javier Rando, Nicholas Carlini, Florian Tramèr


+ [ChatBug: A Common Vulnerability of Aligned LLMs Induced by Chat Templates](https://arxiv.org//abs/2406.12935)

	Fengqing Jiang, Zhangchen Xu, Luyao Niu, Bill Yuchen Lin, Radha Poovendran



# 2024-06-16
+ [KGPA: Robustness Evaluation for Large Language Models via Cross-Domain Knowledge Graphs](https://arxiv.org//abs/2406.10802)

	Aihua Pei, Zehua Yang, Shunan Zhu, Ruoxi Cheng, Ju Jia, Lina Wang


+ [Imperceptible Rhythm Backdoor Attacks: Exploring Rhythm Transformation for Embedding Undetectable Vulnerabilities on Speech Recognition](https://arxiv.org//abs/2406.10932)

	Wenhan Yao, Jiangkun Yang, Yongqiang He, Jia Liu, Weiping Wen


+ [RUPBench: Benchmarking Reasoning Under Perturbations for Robustness Evaluation in Large Language Models](https://arxiv.org//abs/2406.11020)

	Yuqing Wang, Yun Zhao


+ [Imperceptible Face Forgery Attack via Adversarial Semantic Mask](https://arxiv.org//abs/2406.10887)

	Decheng Liu, Qixuan Su, Chunlei Peng, Nannan Wang, Xinbo Gao


+ [Improving Adversarial Robustness via Decoupled Visual Representation Masking](https://arxiv.org//abs/2406.10933)

	Decheng Liu, Tao Chen, Chunlei Peng, Nannan Wang, Ruimin Hu, Xinbo Gao



# 2024-06-15
+ [Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions](https://arxiv.org//abs/2406.10573)

	Xiao Yang, Gaolei Li, Jianhua Li


+ [Emerging Safety Attack and Defense in Federated Instruction Tuning of Large Language Models](https://arxiv.org//abs/2406.10630)

	Rui Ye, Jingyi Chai, Xiangrui Liu, Yaodong Yang, Yanfeng Wang, Siheng Chen


+ [Trading Devil: Robust backdoor attack via Stochastic investment models and Bayesian approach](https://arxiv.org//abs/2406.10719)

	Orson Mengara


+ [E-SAGE: Explainability-based Defense Against Backdoor Attacks on Graph Neural Networks](https://arxiv.org//abs/2406.10655)

	Dingqiang Yuan, Xiaohua Xu, Lei Yu, Tongchang Han, Rongchang Li, Meng Han



# 2024-06-14
+ [Robust Model-Based Reinforcement Learning with an Adversarial Auxiliary Model](https://arxiv.org//abs/2406.09976)

	Siemen Herremans, Ali Anwar, Siegfried Mercelis


+ [Bag of Lies: Robustness in Continuous Pre-training BERT](https://arxiv.org//abs/2406.09967)

	Ine Gevers, Walter Daelemans


+ [Robustness-Inspired Defense Against Backdoor Attacks on Graph Neural Networks](https://arxiv.org//abs/2406.09836)

	Zhiwei Zhang, Minhua Lin, Junjie Xu, Zongyu Wu, Enyan Dai, Suhang Wang


+ [Over-parameterization and Adversarial Robustness in Neural Networks: An Overview and Empirical Analysis](https://arxiv.org//abs/2406.10090)

	Zhang Chen, Luca Demetrio, Srishti Gupta, Xiaoyi Feng, Zhaoqiang Xia, Antonio Emanuele Cinà, Maura Pintor, Luca Oneto, Ambra Demontis, Battista Biggio, Fabio Roli


+ [Semantic Membership Inference Attack against Large Language Models](https://arxiv.org//abs/2406.10218)

	Hamid Mozaffari, Virendra J. Marathe


+ [Watch the Watcher! Backdoor Attacks on Security-Enhancing Diffusion Models](https://arxiv.org//abs/2406.09669)

	Changjiang Li, Ren Pang, Bochuan Cao, Jinghui Chen, Fenglong Ma, Shouling Ji, Ting Wang


+ [PRISM: A Design Framework for Open-Source Foundation Model Safety](https://arxiv.org//abs/2406.10415)

	Terrence Neumann, Bryan Jones


+ [Adaptive Randomized Smoothing: Certifying Multi-Step Defences against Adversarial Examples](https://arxiv.org//abs/2406.10427)

	Saiyue Lyu, Shadab Shaikh, Frederick Shpilevskiy, Evan Shelhamer, Mathias Lécuyer



# 2024-06-13
+ [Is Diffusion Model Safe? Severe Data Leakage via Gradient-Guided Diffusion Model](https://arxiv.org//abs/2406.09484)

	Jiayang Meng, Tao Huang, Hong Chen, Cuiping Li



# 2024-06-12
+ [Graph Transductive Defense: a Two-Stage Defense for Graph Membership Inference Attacks](https://arxiv.org//abs/2406.07917)

	Peizhi Niu, Chao Pan, Siheng Chen, Olgica Milenkovic


+ [Asynchronous Voice Anonymization Using Adversarial Perturbation On Speaker Embedding](https://arxiv.org//abs/2406.08200)

	Rui Wang, Liping Chen, Kong AiK Lee, Zhen-Hua Ling


+ [Adversarial Evasion Attack Efficiency against Large Language Models](https://arxiv.org//abs/2406.08050)

	João Vitorino, Eva Maia, Isabel Praça


+ [Are Objective Explanatory Evaluation metrics Trustworthy? An Adversarial Analysis](https://arxiv.org//abs/2406.07820)

	Prithwijit Chowdhury, Mohit Prabhushankar, Ghassan AlRegib, Mohamed Deriche


+ [Adversarial Patch for 3D Local Feature Extractor](https://arxiv.org//abs/2406.08102)

	Yu Wen Pao, Li Chang Lai, Hong-Yi Lin


+ [Transformation-Dependent Adversarial Attacks](https://arxiv.org//abs/2406.08443)

	Yaoteng Tan, Zikui Cai, M. Salman Asif


+ [On Evaluating Adversarial Robustness of Volumetric Medical Segmentation Models](https://arxiv.org//abs/2406.08486)

	Hashmat Shadab Malik, Numan Saeed, Asif Hanif, Muzammal Naseer, Mohammad Yaqub, Salman Khan, Fahad Shahbaz Khan


+ [RRLS : Robust Reinforcement Learning Suite](https://arxiv.org//abs/2406.08406)

	Adil Zouitine, David Bertoin, Pierre Clavier, Matthieu Geist, Emmanuel Rachelson


+ [Genetic Column Generation for Computing Lower Bounds for Adversarial Classification](https://arxiv.org//abs/2406.08331)

	Maximilian Penka


+ [Graph Transductive Defense: a Two-Stage Defense for Graph Membership Inference Attacks](https://arxiv.org//abs/2406.07917)

	Peizhi Niu, Chao Pan, Siheng Chen, Olgica Milenkovic


+ [Adversarial Evasion Attack Efficiency against Large Language Models](https://arxiv.org//abs/2406.08050)

	João Vitorino, Eva Maia, Isabel Praça


+ [Are Objective Explanatory Evaluation metrics Trustworthy? An Adversarial Analysis](https://arxiv.org//abs/2406.07820)

	Prithwijit Chowdhury, Mohit Prabhushankar, Ghassan AlRegib, Mohamed Deriche


+ [Adversarial Patch for 3D Local Feature Extractor](https://arxiv.org//abs/2406.08102)

	Yu Wen Pao, Li Chang Lai, Hong-Yi Lin


+ [AdaNCA: Neural Cellular Automata As Adaptors For More Robust Vision Transformer](https://arxiv.org//abs/2406.08298)

	Yitao Xu, Tong Zhang, Sabine Süsstrunk


+ [Transformation-Dependent Adversarial Attacks](https://arxiv.org//abs/2406.08443)

	Yaoteng Tan, Zikui Cai, M. Salman Asif


+ [On Evaluating Adversarial Robustness of Volumetric Medical Segmentation Models](https://arxiv.org//abs/2406.08486)

	Hashmat Shadab Malik, Numan Saeed, Asif Hanif, Muzammal Naseer, Mohammad Yaqub, Salman Khan, Fahad Shahbaz Khan


+ [Genetic Column Generation for Computing Lower Bounds for Adversarial Classification](https://arxiv.org//abs/2406.08331)

	Maximilian Penka


+ [Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey](https://arxiv.org//abs/2406.07973)

	Shang Wang, Tianqing Zhu, Bo Liu, Ding Ming, Xu Guo, Dayong Ye, Wanlei Zhou


+ [I Don't Know You, But I Can Catch You: Real-Time Defense against Diverse Adversarial Patches for Object Detectors](https://arxiv.org//abs/2406.10285)

	Zijin Lin, Yue Zhao, Kai Chen, Jinwen He


+ [Transferable Embedding Inversion Attack: Uncovering Privacy Risks in Text Embeddings without Model Queries](https://arxiv.org//abs/2406.10280)

	Yu-Hsiang Huang, Yuche Tsai, Hsiang Hsiao, Hong-Yi Lin, Shou-De Lin



# 2024-06-11
+ [Dual Thinking and Perceptual Analysis of Deep Learning Models using Human Adversarial Examples](https://arxiv.org//abs/2406.06967)

	Kailas Dayanandan, Anand Sinha, Brejesh Lall


+ [Benchmarking Trustworthiness of Multimodal Large Language Models: A Comprehensive Study](https://arxiv.org//abs/2406.07057)

	Yichi Zhang, Yao Huang, Yitong Sun, Chang Liu, Zhe Zhao, Zhengwei Fang, Yifan Wang, Huanran Chen, Xiao Yang, Xingxing Wei, Hang Su, Yinpeng Dong, Jun Zhu


+ [Merging Improves Self-Critique Against Jailbreak Attacks](https://arxiv.org//abs/2406.07188)

	Victor Gallego


+ [AudioMarkBench: Benchmarking Robustness of Audio Watermarking](https://arxiv.org//abs/2406.06979)

	Hongbin Liu, Moyang Guo, Zhengyuan Jiang, Lun Wang, Neil Zhenqiang Gong


+ [Erasing Radio Frequency Fingerprinting via Active Adversarial Perturbation](https://arxiv.org//abs/2406.07349)

	Zhaoyi Lu, Wenchao Xu, Ming Tu, Xin Xie, Cunqing Hua, Nan Cheng


+ [Out-Of-Context Prompting Boosts Fairness and Robustness in Large Language Model Predictions](https://arxiv.org//abs/2406.07685)

	Leonardo Cotta, Chris J. Maddison


+ [Adversarial Machine Unlearning](https://arxiv.org//abs/2406.07687)

	Zonglin Di, Sixie Yu, Yevgeniy Vorobeychik, Yang Liu


+ [Out-Of-Context Prompting Boosts Fairness and Robustness in Large Language Model Predictions](https://arxiv.org//abs/2406.07685)

	Leonardo Cotta, Chris J. Maddison


+ [Adversarial Machine Unlearning](https://arxiv.org//abs/2406.07687)

	Zonglin Di, Sixie Yu, Yevgeniy Vorobeychik, Yang Liu



# 2024-06-10
+ [Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models](https://arxiv.org//abs/2406.05948)

	Xi Li, Yusen Zhang, Renze Lou, Chen Wu, Jiaqi Wang

+ [Lurking in the shadows: Unveiling Stealthy Backdoor Attacks against Personalized Federated Learning](https://arxiv.org//abs/2406.06207)

	Xiaoting Lyu, Yufei Han, Wei Wang, Jingkai Liu, Yongsheng Zhu, Guangquan Xu, Jiqiang Liu, Xiangliang Zhang


+ [Reinforced Compressive Neural Architecture Search for Versatile Adversarial Robustness](https://arxiv.org//abs/2406.06792)

	Dingrong Wang, Hitesh Sapkota, Zhiqiang Tao, Qi Yu


+ [An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities against Strong Detection](https://arxiv.org//abs/2406.06822)

	Shenao Yan, Shen Wang, Yue Duan, Hanbin Hong, Kiho Lee, Doowon Kim, Yuan Hong


+ [A Survey of Backdoor Attacks and Defenses on Large Language Models: Implications for Security Measures](https://arxiv.org//abs/2406.06852)

	Shuai Zhao, Meihuizi Jia, Zhongliang Guo, Leilei Gan, Jie Fu, Yichao Feng, Fengjun Pan, Luu Anh Tuan



# 2024-06-09
+ [PSBD: Prediction Shift Uncertainty Unlocks Backdoor Detection](https://arxiv.org//abs/2406.05826)

	Wei Li, Pin-Yu Chen, Sijia Liu, Ren Wang


+ [Injecting Undetectable Backdoors in Deep Learning and Language Models](https://arxiv.org//abs/2406.05660)

	Alkis Kalavasis, Amin Karbasi, Argyris Oikonomou, Katerina Sotiraki, Grigoris Velegkas, Manolis Zampetakis


+ [DMS: Addressing Information Loss with More Steps for Pragmatic Adversarial Attacks](https://arxiv.org//abs/2406.07580)

	Zhiyu Zhu, Jiayu Zhang, Xinyi Wang, Zhibo Jin, Huaming Chen


+ [Artificial Intelligence as the New Hacker: Developing Agents for Offensive Security](https://arxiv.org//abs/2406.07561)

	Leroy Jacob Valencia


+ [DMS: Addressing Information Loss with More Steps for Pragmatic Adversarial Attacks](https://arxiv.org//abs/2406.07580)

	Zhiyu Zhu, Jiayu Zhang, Xinyi Wang, Zhibo Jin, Huaming Chen


# 2024-06-08
+ [SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner](https://arxiv.org//abs/2406.05498)

	Xunguang Wang, Daoyuan Wu, Zhenlan Ji, Zongjie Li, Pingchuan Ma, Shuai Wang, Yingjiu Li, Yang Liu, Ning Liu, Juergen Rahmel


+ [Enhancing Adversarial Transferability via Information Bottleneck Constraints](https://arxiv.org//abs/2406.05531)

	Biqing Qi, Junqi Gao, Jianxing Liu, Ligang Wu, Bowen Zhou


+ [Exploring Adversarial Robustness of Deep State Space Models](https://arxiv.org//abs/2406.05532)

	Biqing Qi, Yang Luo, Junqi Gao, Pengfei Li, Kai Tian, Zhiyuan Ma, Bowen Zhou


+ [Perturbation Towards Easy Samples Improves Targeted Adversarial Transferability](https://arxiv.org//abs/2406.05535)

	Junqi Gao, Biqing Qi, Yao Li, Zhichang Guo, Dong Li, Yuming Xing, Dazhi Zhang


+ [Adversarial flows: A gradient flow characterization of adversarial attacks](https://arxiv.org//abs/2406.05376)

	Lukas Weigand, Tim Roith, Martin Burger



# 2024-06-07
+ [Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations](https://arxiv.org//abs/2406.04755)

	Weiran Lin, Anna Gerchanovsky, Omer Akgul, Lujo Bauer, Matt Fredrikson, Zifan Wang


+ [ADBA:Approximation Decision Boundary Approach for Black-Box Adversarial Attacks](https://arxiv.org//abs/2406.04998)

	Feiyang Wang, Xingquan Zuo, Hai Huang, Gang Chen


+ [Faster Than Lies: Real-time Deepfake Detection using Binary Neural Networks](https://arxiv.org//abs/2406.04932)

	Lanzino Romeo, Fontana Federico, Diko Anxhelo, Marini Marco Raoul, Cinque Luigi


+ [The Price of Implicit Bias in Adversarially Robust Generalization](https://arxiv.org//abs/2406.04981)

	Nikolaos Tsilivis, Natalie Frank, Nathan Srebro, Julia Kempe


+ [Adversarial Tuning: Defending Against Jailbreak Attacks for LLMs](https://arxiv.org//abs/2406.06622)

	Fan Liu, Zhao Xu, Hao Liu



# 2024-06-06
+ [Batch-in-Batch: a new adversarial training framework for initial perturbation and sample selection](https://arxiv.org//abs/2406.04070)

	Yinting Wu, Pai Peng, Bo Cai, Le Li


+ [Improving Alignment and Robustness with Short Circuiting](https://arxiv.org//abs/2406.04313)

	Andy Zou, Long Phan, Justin Wang, Derek Duenas, Maxwell Lin, Maksym Andriushchenko, Rowan Wang, Zico Kolter, Matt Fredrikson, Dan Hendrycks


+ [Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt](https://arxiv.org//abs/2406.04031)

	Zonghao Ying, Aishan Liu, Tianyuan Zhang, Zhengmin Yu, Siyuan Liang, Xianglong Liu, Dacheng Tao


+ [AutoJailbreak: Exploring Jailbreak Attacks and Defenses through a Dependency Lens](https://arxiv.org//abs/2406.03805)

	Lin Lu, Hai Yan, Zenghui Yuan, Jiawen Shi, Wenqi Wei, Pin-Yu Chen, Pan Zhou


+ [PromptFix: Few-shot Backdoor Removal via Adversarial Prompt Tuning](https://arxiv.org//abs/2406.04478)

	Tianrong Zhang, Zhaohan Xi, Ting Wang, Prasenjit Mitra, Jinghui Chen



# 2024-06-05
+ [FREA: Feasibility-Guided Generation of Safety-Critical Scenarios with Reasonable Adversariality](https://arxiv.org//abs/2406.02983)

	Keyu Chen, Yuheng Lei, Hao Cheng, Haoran Wu, Wenchao Sun, Sifa Zheng


+ [BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents](https://arxiv.org//abs/2406.03007)

	Yifei Wang, Dizhan Xue, Shengjie Zhang, Shengsheng Qian


+ [DifAttack++: Query-Efficient Black-Box Adversarial Attack via Hierarchical Disentangled Feature Space in Cross Domain](https://arxiv.org//abs/2406.03017)

	Jun Liu, Jiantao Zhou, Jiandian Zeng, Jinyu Tian


+ [VQUNet: Vector Quantization U-Net for Defending Adversarial Atacks by Regularizing Unwanted Noise](https://arxiv.org//abs/2406.03117)

	Zhixun He, Mukesh Singhal


+ [ZeroPur: Succinct Training-Free Adversarial Purification](https://arxiv.org//abs/2406.03143)

	Xiuli Bi, Zonglin Yang, Bo Liu, Xiaodong Cun, Chi-Man Pun, Pietro Lio, Bin Xiao


+ [Are Your Models Still Fair? Fairness Attacks on Graph Neural Networks via Node Injections](https://arxiv.org//abs/2406.03052)

	Zihan Luo, Hong Huang, Yongkang Zhou, Jiping Zhang, Nuo Chen


+ [Distributional Adversarial Loss](https://arxiv.org//abs/2406.03458)

	Saba Ahmadi, Siddharth Bhandari, Avrim Blum, Chen Dan, Prabhav Jain


+ [Defending Large Language Models Against Attacks With Residual Stream Activation Analysis](https://arxiv.org//abs/2406.03230)

	Amelia Kawasaki, Andrew Davis, Houssam Abbas


+ [Mutual Information Guided Backdoor Mitigation for Pre-trained Encoders](https://arxiv.org//abs/2406.03508)

	Tingxu Han, Weisong Sun, Ziqi Ding, Chunrong Fang, Hanwei Qian, Jiaxun Li, Zhenyu Chen, Xiangyu Zhang



# 2024-06-04
+ [Certifiably Byzantine-Robust Federated Conformal Prediction](https://arxiv.org//abs/2406.01960)

	Mintong Kang, Zhen Lin, Jimeng Sun, Cao Xiao, Bo Li


+ [CR-UTP: Certified Robustness against Universal Text Perturbations](https://arxiv.org//abs/2406.01873)

	Qian Lou, Xin Liang, Jiaqi Xue, Yancheng Zhang, Rui Xie, Mengxin Zheng


+ [QROA: A Black-Box Query-Response Optimization Attack on LLMs](https://arxiv.org//abs/2406.02044)

	Hussein Jawad, Nicolas J.-B. BRUNEL (LaMME)


+ [MoLA: Motion Generation and Editing with Latent Diffusion Enhanced by Adversarial Training](https://arxiv.org//abs/2406.01867)

	Kengo Uchida, Takashi Shibuya, Yuhta Takida, Naoki Murata, Shusuke Takahashi, Yuki Mitsufuji


+ [SVASTIN: Sparse Video Adversarial Attack via Spatio-Temporal Invertible Neural Networks](https://arxiv.org//abs/2406.01894)

	Yi Pan, Jun-Jie Huang, Zihan Chen, Wentao Zhao, Ziyue Wang


+ [Advancing Generalized Transfer Attack with Initialization Derived Bilevel Optimization and Dynamic Sequence Truncation](https://arxiv.org//abs/2406.02064)

	Yaohua Liu, Jiaxin Gao, Xuan Liu, Xianghao Jiao, Xin Fan, Risheng Liu


+ [Effects of Exponential Gaussian Distribution on (Double Sampling) Randomized Smoothing](https://arxiv.org//abs/2406.02309)

	Youwei Shu, Xi Xiao, Derui Wang, Yuxin Cao, Siji Chen, Jason Xue, Linyi Li, Bo Li


+ [Ai-Sampler: Adversarial Learning of Markov kernels with involutive maps](https://arxiv.org//abs/2406.02490)

	Evgenii Egorov, Ricardo Valperga, Efstratios Gavves


+ [Auditing Privacy Mechanisms via Label Inference Attacks](https://arxiv.org//abs/2406.02797)

	Róbert István Busa-Fekete, Travis Dick, Claudio Gentile, Andrés Muñoz Medina, Adam Smith, Marika Swanberg


# 2024-06-03
+ [BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models](https://arxiv.org//abs/2406.00083)

	Jiaqi Xue, Mengxin Zheng, Yebowen Hu, Fei Liu, Xun Chen, Qian Lou


+ [FedAdOb: Privacy-Preserving Federated Deep Learning with Adaptive Obfuscation](https://arxiv.org//abs/2406.01085)

	Hanlin Gu, Jiahuan Luo, Yan Kang, Yuan Yao, Gongxi Zhu, Bowen Li, Lixin Fan, Qiang Yang


+ [Are AI-Generated Text Detectors Robust to Adversarial Perturbations?](https://arxiv.org//abs/2406.01179)

	Guanhua Huang, Yuchen Zhang, Zhe Li, Yongjian You, Mingze Wang, Zhouwang Yang


+ [PrivacyRestore: Privacy-Preserving Inference in Large Language Models via Privacy Removal and Restoration](https://arxiv.org//abs/2406.01394)

	Ziqian Zeng, Jianwei Wang, Zhengdong Lu, Huiping Zhuang, Cen Chen


+ [Exploring Vulnerabilities and Protections in Large Language Models: A Survey](https://arxiv.org//abs/2406.00240)

	Frank Weizhen Liu, Chenhui Hu


+ [Are you still on track!? Catching LLM Task Drift with Activations](https://arxiv.org//abs/2406.00799)

	Sahar Abdelnabi, Aideen Fay, Giovanni Cherubin, Ahmed Salem, Mario Fritz, Andrew Paverd


+ [Robust Infidelity: When Faithfulness Measures on Masked Language Models Are Misleading](https://arxiv.org//abs/2308.06795)

	Evan Crothers, Herna Viktor, Nathalie Japkowicz


+ [Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast](https://arxiv.org//abs/2402.08567)

	Xiangming Gu, Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Ye Wang, Jing Jiang, Min Lin


+ [Adversarial 3D Virtual Patches using Integrated Gradients](https://arxiv.org//abs/2406.00282)

	Chengzeng You, Zhongyuan Hau, Binbin Xu, Soteris Demetriou


+ [Accent Conversion in Text-To-Speech Using Multi-Level VAE and Adversarial Training](https://arxiv.org//abs/2406.01018)

	Jan Melechovsky, Ambuj Mehrish, Berrak Sisman, Dorien Herremans


+ [Poisoning Attacks and Defenses in Recommender Systems: A Survey](https://arxiv.org//abs/2406.01022)

	Zongwei Wang, Junliang Yu, Min Gao, Guanhua Ye, Shazia Sadiq, Hongzhi Yin


+ [Constraint-based Adversarial Example Synthesis](https://arxiv.org//abs/2406.01219)

	Fang Yu, Ya-Yu Chi, Yu-Fang Chen


+ [Reproducibility Study on Adversarial Attacks Against Robust Transformer Trackers](https://arxiv.org//abs/2406.01765)

	Fatemeh Nourilenjan Nokabadi, Jean-François Lalonde, Christian Gagné


+ [Model for Peanuts: Hijacking ML Models without Training Access is Possible](https://arxiv.org//abs/2406.01708)

	Mahmoud Ghorbel, Halima Bouzidi, Ioan Marius Bilasco, Ihsen Alouani


+ [Safeguarding Large Language Models: A Survey](https://arxiv.org//abs/2406.02622)

	Yi Dong, Ronghui Mu, Yanghao Zhang, Siqi Sun, Tianle Zhang, Changshun Wu, Gaojie Jin, Yi Qi, Jinwei Hu, Jie Meng, Saddek Bensalem, Xiaowei Huang


+ [Unelicitable Backdoors in Language Models via Cryptographic Transformer Circuits](https://arxiv.org//abs/2406.02619)

	Andis Draguns, Andrew Gritsevskiy, Sumeet Ramesh Motwani, Charlie Rogers-Smith, Jeffrey Ladish, Christian Schroeder de Witt



# 2024-06-02
+ [Stealing Image-to-Image Translation Models With a Single Query](https://arxiv.org//abs/2406.00828)

	Nurit Spingarn-Eliezer, Tomer Michaeli


+ [Invisible Backdoor Attacks on Diffusion Models](https://arxiv.org//abs/2406.00816)

	Sen Li, Junchi Ma, Minhao Cheng


+ [Generalization Bound and New Algorithm for Clean-Label Backdoor Attack](https://arxiv.org//abs/2406.00588)

	Lijia Yu, Shuang Liu, Yibo Miao, Xiao-Shan Gao, Lijun Zhang


+ [Constrained Adaptive Attack: Effective Adversarial Attack Against Deep Neural Networks for Tabular Data](https://arxiv.org//abs/2406.00775)

	Thibault Simonetto, Salah Ghamizi, Maxime Cordy


+ [Teams of LLM Agents can Exploit Zero-Day Vulnerabilities](https://arxiv.org//abs/2406.01637)

	Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, Daniel Kang


+ [A Novel Defense Against Poisoning Attacks on Federated Learning: LayerCAM Augmented with Autoencoder](https://arxiv.org//abs/2406.02605)

	Jingjing Zheng, Xin Yuan, Kai Li, Wei Ni, Eduardo Tovar, Jon Crowcroft


# 2024-06-01
+ [Improving Accuracy-robustness Trade-off via Pixel Reweighted Adversarial Training](https://arxiv.org//abs/2406.00685)

	Jiacheng Zhang, Feng Liu, Dawei Zhou, Jingfeng Zhang, Tongliang Liu


+ [Robust Knowledge Distillation Based on Feature Variance Against Backdoored Teacher Model](https://arxiv.org//abs/2406.03409)

	Jinyin Chen, Xiaoming Zhao, Haibin Zheng, Xiao Li, Sheng Xiang, Haifeng Guo



# 2024-05-31
+ [Enhancing Jailbreak Attack Against Large Language Models through Silent Tokens](https://arxiv.org//abs/2405.20653)

	Jiahao Yu, Haozheng Luo, Jerry Yao-Chieh, Wenbo Guo, Han Liu, Xinyu Xing


+ [GI-NAS: Boosting Gradient Inversion Attacks through Adaptive Neural Architecture Search](https://arxiv.org//abs/2405.20725)

	Wenbo Yu, Hao Fang, Bin Chen, Xiaohang Sui, Chuan Chen, Hao Wu, Shu-Tao Xia, Ke Xu


+ [Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training](https://arxiv.org//abs/2405.20978)

	Feiteng Fang, Yuelin Bai, Shiwen Ni, Min Yang, Xiaojun Chen, Ruifeng Xu


+ [Certifying Global Robustness for Deep Neural Networks](https://arxiv.org//abs/2405.20556)

	You Li, Guannan Zhao, Shuyu Kong, Yunqi He, Hai Zhou


+ [Disrupting Diffusion: Token-Level Attention Erasure Attack against Diffusion-based Customization](https://arxiv.org//abs/2405.20584)

	Yisu Liu, Jinyang An, Wanqian Zhang, Dayan Wu, Jingzi Gu, Zheng Lin, Weiping Wang


+ [GANcrop: A Contrastive Defense Against Backdoor Attacks in Federated Learning](https://arxiv.org//abs/2405.20727)

	Xiaoyun Gan, Shanyu Gan, Taizhi Su, Peng Liu


+ [ACE: A Model Poisoning Attack on Contribution Evaluation Methods in Federated Learning](https://arxiv.org//abs/2405.20975)

	Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Bo Li, Radha Poovendran


+ [Improved Techniques for Optimization-Based Jailbreaking on Large Language Models](https://arxiv.org//abs/2405.21018)

	Xiaojun Jia, Tianyu Pang, Chao Du, Yihao Huang, Jindong Gu, Yang Liu, Xiaochun Cao, Min Lin


+ [Investigating and unmasking feature-level vulnerabilities of CNNs to adversarial perturbations](https://arxiv.org//abs/2405.20672)

	Davide Coppola, Hwee Kuan Lee


+ [Query Provenance Analysis for Robust and Efficient Query-based Black-box Attack Defense](https://arxiv.org//abs/2405.20641)

	Shaofei Li, Ziqi Zhang, Haomin Jia, Ding Li, Yao Guo, Xiangqun Chen


+ [BackdoorIndicator: Leveraging OOD Data for Proactive Backdoor Detection in Federated Learning](https://arxiv.org//abs/2405.20862)

	Songze Li, Yanbo Dai


+ [RASE: Efficient Privacy-preserving Data Aggregation against Disclosure Attacks for IoTs](https://arxiv.org//abs/2405.20914)

	Zuyan Wang, Jun Tao, Dika Zou


+ [Exfiltration of personal information from ChatGPT via prompt injection](https://arxiv.org//abs/2406.00199)

	Gregory Schwartzman



# 2024-05-30
+ [HOLMES: to Detect Adversarial Examples with Multiple Detectors](https://arxiv.org//abs/2405.19956)

	Jing Wen


+ [Efficient LLM-Jailbreaking by Introducing Visual Modality](https://arxiv.org//abs/2405.20015)

	Zhenxing Niu, Yuyao Sun, Haodong Ren, Haoxuan Ji, Quan Wang, Xiaoke Ma, Gang Hua, Rong Jin


+ [Context Injection Attacks on Large Language Models](https://arxiv.org//abs/2405.20234)

	Cheng'an Wei, Kai Chen, Yue Zhao, Yujia Gong, Lu Xiang, Shenchen Zhu


+ [Large Language Model Watermark Stealing With Mixed Integer Programming](https://arxiv.org//abs/2405.19677)

	Zhaoxi Zhang, Xiaomei Zhang, Yanjun Zhang, Leo Yu Zhang, Chao Chen, Shengshan Hu, Asif Gill, Shirui Pan


+ [AutoBreach: Universal and Adaptive Jailbreaking with Efficient Wordplay-Guided Optimization](https://arxiv.org//abs/2405.19668)

	Jiawei Chen, Xiao Yang, Zhengwei Fang, Yu Tian, Yinpeng Dong, Zhaoxia Yin, Hang Su


+ [DiffPhysBA: Diffusion-based Physical Backdoor Attack against Person Re-Identification in Real-World](https://arxiv.org//abs/2405.19990)

	Wenli Sun, Xinyang Jiang, Dongsheng Li, Cairong Zhao


+ [Typography Leads Semantic Diversifying: Amplifying Adversarial Transferability across Multimodal Large Language Models](https://arxiv.org//abs/2405.20090)

	Hao Cheng, Erjia Xiao, Jiahang Cao, Le Yang, Kaidi Xu, Jindong Gu, Renjing Xu


+ [Unveiling and Mitigating Backdoor Vulnerabilities based on Unlearning Weight Changes and Backdoor Activeness](https://arxiv.org//abs/2405.20291)

	Weilin Lin, Li Liu, Shaokui Wei, Jianze Li, Hui Xiong


+ [BAN: Detecting Backdoors Activated by Adversarial Neuron Noise](https://arxiv.org//abs/2405.19928)

	Xiaoyun Xu, Zhuoran Liu, Stefanos Koffas, Shujian Yu, Stjepan Picek


+ [Reconstruction Attacks on Machine Unlearning: Simple Models are Vulnerable](https://arxiv.org//abs/2405.20272)

	Martin Bertran, Shuai Tang, Michael Kearns, Jamie Morgenstern, Aaron Roth, Zhiwei Steven Wu


+ [Evaluating the Effectiveness and Robustness of Visual Similarity-based Phishing Detection Models](https://arxiv.org//abs/2405.19598)

	Fujiao Ji, Kiho Lee, Hyungjoon Koo, Wenhao You, Euijin Choo, Hyoungshick Kim, Doowon Kim


+ [Defensive Prompt Patch: A Robust and Interpretable Defense of LLMs against Jailbreak Attacks](https://arxiv.org//abs/2405.20099)

	Chen Xiong, Xiangyu Qi, Pin-Yu Chen, Tsung-Yi Ho


+ [Is My Data in Your Retrieval Database? Membership Inference Attacks Against Retrieval Augmented Generation](https://arxiv.org//abs/2405.20446)

	Maya Anderson, Guy Amit, Abigail Goldsteen


+ [Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters](https://arxiv.org//abs/2405.20413)

	Haibo Jin, Andy Zhou, Joe D. Menke, Haohan Wang


+ [Phantom: General Trigger Attacks on Retrieval Augmented Language Generation](https://arxiv.org//abs/2405.20485)

	Harsh Chaudhari, Giorgio Severi, John Abascal, Matthew Jagielski, Christopher A. Choquette-Choo, Milad Nasr, Cristina Nita-Rotaru, Alina Oprea


+ [Is Synthetic Data all We Need? Benchmarking the Robustness of Models Trained with Synthetic Images](https://arxiv.org//abs/2405.20469)

	Krishnakant Singh, Thanush Navaratnam, Jannik Holmer, Simone Schaub-Meyer, Stefan Roth


+ [Enhancing Adversarial Robustness in SNNs with Sparse Gradients](https://arxiv.org//abs/2405.20355)

	Yujia Liu, Tong Bu, Jianhao Ding, Zecheng Hao, Tiejun Huang, Zhaofei Yu


+ [SleeperNets: Universal Backdoor Poisoning Attacks Against Reinforcement Learning Agents](https://arxiv.org//abs/2405.20539)

	Ethan Rathbun, Christopher Amato, Alina Oprea



# 2024-05-29
+ [Leveraging Many-To-Many Relationships for Defending Against Visual-Language Adversarial Attacks](https://arxiv.org//abs/2405.18770)

	Futa Waseda, Antonio Tejero-de-Pablos


+ [EntProp: High Entropy Propagation for Improving Accuracy and Robustness](https://arxiv.org//abs/2405.18931)

	Shohei Enomoto


+ [Verifiably Robust Conformal Prediction](https://arxiv.org//abs/2405.18942)

	Linus Jeary, Tom Kuipers, Mehran Hosseini, Nicola Paoletti


+ [DiveR-CT: Diversity-enhanced Red Teaming with Relaxing Constraints](https://arxiv.org//abs/2405.19026)

	Andrew Zhao, Quentin Xu, Matthieu Lin, Shenzhi Wang, Yong-jin Liu, Zilong Zheng, Gao Huang


+ [Convex neural network synthesis for robustness in the 1-norm](https://arxiv.org//abs/2405.19029)

	Ross Drummond, Chris Guiver, Matthew C. Turner


+ [Efficient Black-box Adversarial Attacks via Bayesian Optimization Guided by a Function Prior](https://arxiv.org//abs/2405.19098)

	Shuyu Cheng, Yibo Miao, Yinpeng Dong, Xiao Yang, Xiao-Shan Gao, Jun Zhu


+ [Model Agnostic Defense against Adversarial Patch Attacks on Object Detection in Unmanned Aerial Vehicles](https://arxiv.org//abs/2405.19179)

	Saurabh Pathak, Samridha Shrestha, Abdelrahman AlMahmoud


+ [Robust Entropy Search for Safe Efficient Bayesian Optimization](https://arxiv.org//abs/2405.19059)

	Dorina Weichert, Alexander Kister, Patrick Link, Sebastian Houben, Gunar Ernis


+ [Voice Jailbreak Attacks Against GPT-4o](https://arxiv.org//abs/2405.19103)

	Xinyue Shen, Yixin Wu, Michael Backes, Yang Zhang


+ [PermLLM: Private Inference of Large Language Models within 3 Seconds under WAN](https://arxiv.org//abs/2405.18744)

	Fei Zheng, Chaochao Chen, Zhongxuan Han, Xiaolin Zheng


+ [Node Injection Attack Based on Label Propagation Against Graph Neural Network](https://arxiv.org//abs/2405.18824)

	Peican Zhu, Zechen Pan, Keke Tang, Xiaodong Cui, Jinhuan Wang, Qi Xuan


+ [AI Risk Management Should Incorporate Both Safety and Security](https://arxiv.org//abs/2405.19524)

	Xiangyu Qi, Yangsibo Huang, Yi Zeng, Edoardo Debenedetti, Jonas Geiping, Luxi He, Kaixuan Huang, Udari Madhushani, Vikash Sehwag, Weijia Shi, Boyi Wei, Tinghao Xie, Danqi Chen, Pin-Yu Chen, Jeffrey Ding, Ruoxi Jia, Jiaqi Ma, Arvind Narayanan, Weijie J Su, Mengdi Wang, Chaowei Xiao, Bo Li, Dawn Song, Peter Henderson, Prateek Mittal


+ [Diffusion Policy Attacker: Crafting Adversarial Attacks for Diffusion-based Policies](https://arxiv.org//abs/2405.19424)

	Yipu Chen, Haotian Xue, Yongxin Chen


# 2024-05-28
+ [Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing](https://arxiv.org//abs/2405.18166)

	Wei Zhao, Zhe Li, Yige Li, Ye Zhang, Jun Sun


+ [Safe Reinforcement Learning in Black-Box Environments via Adaptive Shielding](https://arxiv.org//abs/2405.18180)

	Daniel Bethell, Simos Gerasimou, Radu Calinescu, Calum Imrie


+ [Rethinking Pruning for Backdoor Mitigation: An Optimization Perspective](https://arxiv.org//abs/2405.17746)

	Nan Li, Haiyang Yu, Ping Yi


+ [Magnitude-based Neuron Pruning for Backdoor Defens](https://arxiv.org//abs/2405.17750)

	Nan Li, Haoyu Jiang, Ping Yi


+ [White-box Multimodal Jailbreaks Against Large Vision-Language Models](https://arxiv.org//abs/2405.17894)

	Ruofan Wang, Xingjun Ma, Hanxu Zhou, Chuanjun Ji, Guangnan Ye, Yu-Gang Jiang


+ [ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented Generator](https://arxiv.org//abs/2405.18111)

	Junda Zhu, Lingyong Yan, Haibo Shi, Dawei Yin, Lei Sha


+ [Towards Unified Robustness Against Both Backdoor and Adversarial Attacks](https://arxiv.org//abs/2405.17929)

	Zhenxing Niu, Yuyao Sun, Qiguang Miao, Rong Jin, Gang Hua


+ [Cross-Context Backdoor Attacks against Graph Prompt Learning](https://arxiv.org//abs/2405.17984)

	Xiaoting Lyu, Yufei Han, Wei Wang, Hangwei Qian, Ivor Tsang, Xiangliang Zhang


+ [Channel Reciprocity Based Attack Detection for Securing UWB Ranging by Autoencoder](https://arxiv.org//abs/2405.18255)

	Wenlong Gou, Chuanhang Yu, Juntao Ma, Gang Wu, Vladimir Mordachev


+ [Learning diverse attacks on large language models for robust red-teaming and safety tuning](https://arxiv.org//abs/2405.18540)

	Seanie Lee, Minsu Kim, Lynn Cherif, David Dobre, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, Moksh Jain


+ [Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning](https://arxiv.org//abs/2405.18641)

	Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Ling Liu


+ [Improved Generation of Adversarial Examples Against Safety-aligned LLMs](https://arxiv.org//abs/2405.20778)

	Qizhang Li, Yiwen Guo, Wangmeng Zuo, Hao Chen


+ [Stochastic Adversarial Networks for Multi-Domain Text Classification](https://arxiv.org//abs/2406.00044)

	Xu Wang, Yuan Wu



# 2024-05-27
+ [TrojFM: Resource-efficient Backdoor Attacks against Very Large Foundation Models](https://arxiv.org//abs/2405.16783)

	Yuzhou. Nie, Yanting. Wang, Jinyuan. Jia, Michael J. De Lucia, Nathaniel D. Bastian, Wenbo. Guo, Dawn. Song


+ [Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization](https://arxiv.org//abs/2405.17067)

	Dixuan Wang, Yanda Li, Junyuan Jiang, Zepeng Ding, Guochao Jiang, Jiaqing Liang, Deqing Yang


+ [A One-Layer Decoder-Only Transformer is a Two-Layer RNN: With an Application to Certified Robustness](https://arxiv.org//abs/2405.17361)

	Yuhao Zhang, Aws Albarghouthi, Loris D'Antoni


+ [Exploiting the Layered Intrinsic Dimensionality of Deep Models for Practical Adversarial Training](https://arxiv.org//abs/2405.17130)

	Enes Altinisik, Safa Messaoud, Husrev Taha Sencar, Hassan Sajjad, Sanjay Chawla


+ [Privacy-Aware Visual Language Models](https://arxiv.org//abs/2405.17423)

	Laurens Samson, Nimrod Barazani, Sennay Ghebreab, Yuki M. Asano


+ [Anonymization Prompt Learning for Facial Privacy-Preserving Text-to-Image Generation](https://arxiv.org//abs/2405.16895)

	Liang Shi, Jie Zhang, Shiguang Shan


+ [Adversarial Attacks on Both Face Recognition and Face Anti-spoofing Models](https://arxiv.org//abs/2405.16940)

	Fengfan Zhou, Qianyu Zhou, Xiangtai Li, Xuequan Lu, Lizhuang Ma, Hefei Ling


+ [Spectral regularization for adversarially-robust representation learning](https://arxiv.org//abs/2405.17181)

	Sheng Yang, Jacob A. Zavatone-Veth, Cengiz Pehlevan


+ [Safe LoRA: the Silver Lining of Reducing Safety Risks when Fine-tuning Large Language Models](https://arxiv.org//abs/2405.16833)

	Chia-Yi Hsu, Yu-Lin Tsai, Chih-Hsun Lin, Pin-Yu Chen, Chia-Mu Yu, Chun-Ying Huang


+ [The Uncanny Valley: Exploring Adversarial Robustness from a Flatness Perspective](https://arxiv.org//abs/2405.16918)

	Nils Philipp Walter, Linara Adilova, Jilles Vreeken, Michael Kamp


+ [OSLO: One-Shot Label-Only Membership Inference Attacks](https://arxiv.org//abs/2405.16978)

	Yuefeng Peng, Jaechul Roh, Subhransu Maji, Amir Houmansadr


+ [Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models](https://arxiv.org//abs/2405.17374)

	ShengYun Peng, Pin-Yu Chen, Matthew Hull, Duen Horng Chau


+ [TIMA: Text-Image Mutual Awareness for Balancing Zero-Shot Adversarial Robustness and Generalization Ability](https://arxiv.org//abs/2405.17678)

	Fengji Ma, Li Liu, Hei Victor Cheng


+ [Exploring Backdoor Attacks against Large Language Model-based Decision Making](https://arxiv.org//abs/2405.20774)

	Ruochen Jiao, Shaoyuan Xie, Justin Yue, Takami Sato, Lixu Wang, Yixuan Wang, Qi Alfred Chen, Qi Zhu



# 2024-05-26
+ [Automatic Jailbreaking of the Text-to-Image Generative AI Systems](https://arxiv.org//abs/2405.16567)

	Minseon Kim, Hyomin Lee, Boqing Gong, Huishuai Zhang, Sung Ju Hwang


+ [Visualizing the Shadows: Unveiling Data Poisoning Behaviors in Federated Learning](https://arxiv.org//abs/2405.16707)

	Xueqing Zhang, Junkai Zhang, Ka-Ho Chow, Juntao Chen, Ying Mao, Mohamed Rahouti, Xiang Li, Yuchen Liu, Wenqi Wei


+ [Intruding with Words: Towards Understanding Graph Injection Attacks at the Text Level](https://arxiv.org//abs/2405.16405)

	Runlin Lei, Yuwei Hu, Yuchen Ren, Zhewei Wei


+ [Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer](https://arxiv.org//abs/2405.16436)

	Zhihan Liu, Miao Lu, Shenao Zhang, Boyi Liu, Hongyi Guo, Yingxiang Yang, Jose Blanchet, Zhaoran Wang


+ [Partial train and isolate, mitigate backdoor attack](https://arxiv.org//abs/2405.16488)

	Yong Li, Han Gao


+ [Pruning for Robust Concept Erasing in Diffusion Models](https://arxiv.org//abs/2405.16534)

	Tianyun Yang, Juan Cao, Chang Xu


+ [Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large Language Models](https://arxiv.org//abs/2405.20775)

	Xijie Huang, Xinyuan Wang, Hantao Zhang, Jiawen Xi, Jingkun An, Hao Wang, Chengwei Pan


# 2024-05-25
+ [Diffusion-Reward Adversarial Imitation Learning](https://arxiv.org//abs/2405.16194)

	Chun-Mao Lai, Hsiang-Chun Wang, Ping-Chun Hsieh, Yu-Chiang Frank Wang, Min-Hung Chen, Shao-Hua Sun


+ [Breaking the False Sense of Security in Backdoor Defense through Re-Activation Attack](https://arxiv.org//abs/2405.16134)

	Mingli Zhu, Siyuan Liang, Baoyuan Wu


+ [Enhancing Adversarial Transferability Through Neighborhood Conditional Sampling](https://arxiv.org//abs/2405.16181)

	Chunlin Qiu, Yiheng Duan, Lingchen Zhao, Qian Wang


+ [Detecting Adversarial Data via Perturbation Forgery](https://arxiv.org//abs/2405.16226)

	Qian Wang, Chen Li, Yuchen Luo, Hefei Ling, Ping Li, Jiazhong Chen, Shijuan Huang, Ning Yu


+ [Enhancing Consistency-Based Image Generation via Adversarialy-Trained Classification and Energy-Based Discrimination](https://arxiv.org//abs/2405.16260)

	Shelly Golan, Roy Ganz, Michael Elad


+ [R.A.C.E.: Robust Adversarial Concept Erasure for Secure Text-to-Image Diffusion Model](https://arxiv.org//abs/2405.16341)

	Changhoon Kim, Kyle Min, Yezhou Yang


+ [Certifying Adapters: Enabling and Enhancing the Certification of Classifier Adversarial Robustness](https://arxiv.org//abs/2405.16036)

	Jieren Deng, Hanbin Hong, Aaron Palmer, Xin Zhou, Jinbo Bi, Kaleel Mahmood, Yuan Hong, Derek Aguiar


+ [Mitigating Backdoor Attack by Injecting Proactive Defensive Backdoor](https://arxiv.org//abs/2405.16112)

	Shaokui Wei, Hongyuan Zha, Baoyuan Wu


+ [Layer-Aware Analysis of Catastrophic Overfitting: Revealing the Pseudo-Robust Shortcut Dependency](https://arxiv.org//abs/2405.16262)

	Runqi Lin, Chaojian Yu, Bo Han, Hang Su, Tongliang Liu


+ [Secure Hierarchical Federated Learning in Vehicular Networks Using Dynamic Client Selection and Anomaly Detection](https://arxiv.org//abs/2405.17497)

	M. Saeid HaghighiFard, Sinem Coleri


+ [Towards Black-Box Membership Inference Attack for Diffusion Models](https://arxiv.org//abs/2405.20771)

	Jingwei Li, Jing Dong, Tianxing He, Jingzhao Zhang


+ [Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Characte](https://arxiv.org//abs/2405.20773)

	Siyuan Ma, Weidi Luo, Yu Wang, Xiaogeng Liu, Muhao Chen, Bo Li, Chaowei Xiao



# 2024-05-24
+ [How Does Bayes Error Limit Probabilistic Robust Accuracy](https://arxiv.org//abs/2405.14923)

	Ruihan Zhang, Jun Sun


+ [RFLPA: A Robust Federated Learning Framework against Poisoning Attacks with Secure Aggregation](https://arxiv.org//abs/2405.15182)

	Peihua Mai, Ran Yan, Yan Pang


+ [Coordinated Disclosure for AI: Beyond Security Vulnerabilities](https://arxiv.org//abs/2402.07039)

	Sven Cattell, Avijit Ghosh, Lucie-Aimée Kaffee


+ [Robust Diffusion Models for Adversarial Purification](https://arxiv.org//abs/2403.16067)

	Guang Lin, Zerui Tao, Jianhai Zhang, Toshihisa Tanaka, Qibin Zhao


+ [Certifiably Robust RAG against Retrieval Corruption](https://arxiv.org//abs/2405.15556)

	Chong Xiang, Tong Wu, Zexuan Zhong, David Wagner, Danqi Chen, Prateek Mittal


+ [Defensive Unlearning with Adversarial Training for Robust Concept Erasure in Diffusion Models](https://arxiv.org//abs/2405.15234)

	Yimeng Zhang, Xin Chen, Jinghan Jia, Yihua Zhang, Chongyu Fan, Jiancheng Liu, Mingyi Hong, Ke Ding, Sijia Liu


+ [BDetCLIP: Multimodal Prompting Contrastive Test-Time Backdoor Detection](https://arxiv.org//abs/2405.15269)

	Yuwei Niu, Shuo He, Qi Wei, Feng Liu, Lei Feng


+ [Scale-Invariant Feature Disentanglement via Adversarial Learning for UAV-based Object Detection](https://arxiv.org//abs/2405.15465)

	Fan Liu, Liang Yao, Chuanyi Zhang, Ting Wu, Xinlei Zhang, Jun Zhou, Xiruo Jiang


+ [Better Membership Inference Privacy Measurement through Discrepancy](https://arxiv.org//abs/2405.15140)

	Ruihan Wu, Pengrun Huang, Kamalika Chaudhuri


+ [Adversarial Attacks on Hidden Tasks in Multi-Task Learning](https://arxiv.org//abs/2405.15244)

	Yu Zhe, Rei Nagaike, Daiki Nishiyama, Kazuto Fukuchi, Jun Sakuma


+ [Decaf: Data Distribution Decompose Attack against Federated Learning](https://arxiv.org//abs/2405.15316)

	Zhiyang Dai, Chunyi Zhou, Anmin Fu


+ [Lost in the Averages: A New Specific Setup to Evaluate Membership Inference Attacks Against Machine Learning Models](https://arxiv.org//abs/2405.15423)

	Florent Guépin, Nataša Krčo, Matthieu Meeus, Yves-Alexandre de Montjoye


+ [DAGER: Exact Gradient Inversion for Large Language Models](https://arxiv.org//abs/2405.15586)

	Ivo Petrov, Dimitar I. Dimitrov, Maximilian Baader, Mark Niklas Müller, Martin Vechev


+ [Efficient Adversarial Training in LLMs with Continuous Attacks](https://arxiv.org//abs/2405.15589)

	Sophie Xhonneux, Alessandro Sordoni, Stephan Günnemann, Gauthier Gidel, Leo Schwinn


+ [TrojanForge: Adversarial Hardware Trojan Examples with Reinforcement Learning](https://arxiv.org//abs/2405.15184)

	Amin Sarihi, Peter Jamieson, Ahmad Patooghy, Abdel-Hameed A. Badawy


+ [Adversarial Imitation Learning from Visual Observations using Latent Information](https://arxiv.org//abs/2309.17371)

	Vittorio Giammarino, James Queeney, Ioannis Ch. Paschalidis


+ [Evaluating the Adversarial Robustness of Retrieval-Based In-Context Learning for Large Language Models](https://arxiv.org//abs/2405.15984)

	Simon Chi Lok Yu, Jie He, Pasquale Minervini, Jeff Z. Pan


+ [A Neurosymbolic Framework for Bias Correction in CNNs](https://arxiv.org//abs/2405.15886)

	Parth Padalkar, Natalia Ślusarz, Ekaterina Komendantskaya, Gopal Gupta


+ [Robust width: A lightweight and certifiable adversarial defense](https://arxiv.org//abs/2405.15971)

	Jonathan Peck, Bart Goossens


+ [Can Implicit Bias Imply Adversarial Robustness?](https://arxiv.org//abs/2405.15942)

	Hancheng Min, René Vidal


+ [BadGD: A unified data-centric framework to identify gradient descent vulnerabilities](https://arxiv.org//abs/2405.15979)

	Chi-Hua Wang, Guang Cheng


+ [Robustifying Safety-Aligned Large Language Models through Clean Data Curation](https://arxiv.org//abs/2405.19358)

	Xiaoqun Liu, Jiacheng Liang, Muchao Ye, Zhaohan Xi


+ [ART: Automatic Red-teaming for Text-to-Image Models to Protect Benign Users](https://arxiv.org//abs/2405.19360)

	Guanlin Li, Kangjie Chen, Shudong Zhang, Jie Zhang, Tianwei Zhang


+ [Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent](https://arxiv.org//abs/2405.20770)

	Guang Lin, Qibin Zhao



# 2024-05-23
+ [Learning to Transform Dynamically for Better Adversarial Transferability](https://arxiv.org//abs/2405.14077)

	Rongyi Zhu, Zeliang Zhang, Susan Liang, Zhuo Liu, Chenliang Xu


+ [Certified Robustness against Sparse Adversarial Perturbations via Data Localization](https://arxiv.org//abs/2405.14176)

	Ambar Pal, René Vidal, Jeremias Sulam


+ [SLIFER: Investigating Performance and Robustness of Malware Detection Pipelines](https://arxiv.org//abs/2405.14478)

	Andrea Ponte, Dmitrijs Trizna, Luca Demetrio, Battista Biggio, Fabio Roli


+ [Semantic-guided Prompt Organization for Universal Goal Hijacking against LLMs](https://arxiv.org//abs/2405.14189)

	Yihao Huang, Chong Wang, Xiaojun Jia, Qing Guo, Felix Juefei-Xu, Jian Zhang, Geguang Pu, Yang Liu


+ [MoGU: A Framework for Enhancing Safety of Open-Sourced LLMs While Preserving Their Usability](https://arxiv.org//abs/2405.14488)

	Yanrui Du, Sendong Zhao, Danyang Zhao, Ming Ma, Yuhan Chen, Liangyu Huo, Qing Yang, Dongliang Xu, Bing Qin


+ [Impact of Non-Standard Unicode Characters on Security and Comprehension in Large Language Models](https://arxiv.org//abs/2405.14490)

	Johan S Daniel, Anand Pal


+ [Unveiling the Achilles' Heel of NLG Evaluators: A Unified Adversarial Framework Driven by Large Language Models](https://arxiv.org//abs/2405.14646)

	Yiming Chen, Chen Zhang, Danqing Luo, Luis Fernando D'Haro, Robby T. Tan, Haizhou Li


+ [Towards Transferable Attacks Against Vision-LLMs in Autonomous Driving with Typography](https://arxiv.org//abs/2405.14169)

	Nhat Chung, Sensen Gao, Tuan-Anh Vu, Jie Zhang, Aishan Liu, Yun Lin, Jin Song Dong, Qing Guo


+ [Eidos: Efficient, Imperceptible Adversarial 3D Point Clouds](https://arxiv.org//abs/2405.14210)

	Hanwei Zhang, Luo Cheng, Qisong He, Wei Huang, Renjue Li, Ronan Sicre, Xiaowei Huang, Holger Hermanns, Lijun Zhang


+ [Towards Imperceptible Backdoor Attack in Self-supervised Learning](https://arxiv.org//abs/2405.14672)

	Hanrong Zhang, Zhenting Wang, Tingxu Han, Mingyu Jin, Chenlu Zhan, Mengnan Du, Hongwei Wang, Shiqing Ma


+ [Membership Inference on Text-to-Image Diffusion Models via Conditional Likelihood Discrepancy](https://arxiv.org//abs/2405.14800)

	Shengfang Zhai, Huanran Chen, Yinpeng Dong, Jiajun Li, Qingni Shen, Yansong Gao, Hang Su, Yang Liu


+ [Boosting Robustness by Clipping Gradients in Distributed Learning](https://arxiv.org//abs/2405.14432)

	Youssef Allouah, Rachid Guerraoui, Nirupam Gupta, Ahmed Jellouli, Geovani Rizk, John Stephan


+ [Identity Inference from CLIP Models using Only Textual Data](https://arxiv.org//abs/2405.14517)

	Songze Li, Ruoxi Cheng, Xiaojun Jia


+ [A New Formulation for Zeroth-Order Optimization of Adversarial EXEmples in Malware Detection](https://arxiv.org//abs/2405.14519)

	Marco Rando, Luca Demetrio, Lorenzo Rosasco, Fabio Roli


+ [Nearly Tight Black-Box Auditing of Differentially Private Machine Learning](https://arxiv.org//abs/2405.14106)

	Meenatchi Sundaram Muthu Selva Annamalai, Emiliano De Cristofaro


+ [Generating camera failures as a class of physics-based adversarial examples](https://arxiv.org//abs/2405.15033)

	Manav Prabhakar, Jwalandhar Girnar, Arpan Kusari


+ [Universal Robustness via Median Randomized Smoothing for Real-World Super-Resolution](https://arxiv.org//abs/2405.14934)

	Zakariya Chaouai, Mohamed Tamaazousti



# 2024-05-22
+ [Adversarial Training via Adaptive Knowledge Amalgamation of an Ensemble of Teachers](https://arxiv.org//abs/2405.13324)

	Shayan Mohajer Hamidi, Linfeng Ye


+ [Safety Alignment for Vision Language Models](https://arxiv.org//abs/2405.13581)

	Zhendong Liu, Yuanbi Nie, Yingshui Tan, Xiangyu Yue, Qiushi Cui, Chongjun Wang, Xiaoyong Zhu, Bo Zheng


+ [DeepNcode: Encoding-Based Protection against Bit-Flip Attacks on Neural Networks](https://arxiv.org//abs/2405.13891)

	Patrik Velčický, Jakub Breier, Xiaolu Hou, Mladen Kovačević


+ [Towards Comprehensive and Efficient Post Safety Alignment of Large Language Models via Safety Patching](https://arxiv.org//abs/2405.13820)

	Weixiang Zhao, Yulin Hu, Zhuojun Li, Yang Deng, Yanyan Zhao, Bing Qin, Tat-Seng Chua


+ [TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in Large Language Models](https://arxiv.org//abs/2405.13401)

	Pengzhou Cheng, Yidong Ding, Tianjie Ju, Zongru Wu, Wei Du, Ping Yi, Zhuosheng Zhang, Gongshen Liu


+ [Towards Certification of Uncertainty Calibration under Adversarial Attacks](https://arxiv.org//abs/2405.13922)

	Cornelius Emde, Francesco Pinto, Thomas Lukasiewicz, Philip H.S. Torr, Adel Bibi


+ [WordGame: Efficient & Effective LLM Jailbreak via Simultaneous Obfuscation in Query and Response](https://arxiv.org//abs/2405.14023)

	Tianrong Zhang, Bochuan Cao, Yuanpu Cao, Lu Lin, Prasenjit Mitra, Jinghui Chen


+ [Adversarial Training of Two-Layer Polynomial and ReLU Activation Networks via Convex Optimization](https://arxiv.org//abs/2405.14033)

	Daniel Kuelbs, Sanjay Lall, Mert Pilanci


+ [Memory Scraping Attack on Xilinx FPGAs: Private Data Extraction from Terminated Processes](https://arxiv.org//abs/2405.13927)

	Bharadwaj Madabhushi, Sandip Kundu, Daniel Holcomb


# 2024-05-21
+ [Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models](https://arxiv.org//abs/2405.12523)

	Jiaqi Li, Qianshan Wei, Chuanyi Zhang, Guilin Qi, Miaozeng Du, Yongrui Chen, Sheng Bi


+ [Tiny Refinements Elicit Resilience: Toward Efficient Prefix-Model Against LLM Red-Teaming](https://arxiv.org//abs/2405.12604)

	Jiaxu Liu, Xiangyu Yin, Sihao Wu, Jianhong Wang, Meng Fang, Xinping Yi, Xiaowei Huang


+ [Generative AI and Large Language Models for Cyber Security: All Insights You Need](https://arxiv.org//abs/2405.12750)

	Mohamed Amine Ferrag, Fatima Alwahedi, Ammar Battah, Bilel Cherif, Abdechakour Mechri, Norbert Tihanyi


+ [Transparency Distortion Robustness for SOTA Image Segmentation Tasks](https://arxiv.org//abs/2405.12864)

	Volker Knauthe, Arne Rak, Tristan Wirth, Thomas Pöllabauer, Simon Metzler, Arjan Kuijper, Dieter W. Fellner


+ [Nearest is Not Dearest: Towards Practical Defense against Quantization-conditioned Backdoor Attacks](https://arxiv.org//abs/2405.12725)

	Boheng Li, Yishuo Cai, Haowei Li, Feng Xue, Zhifeng Li, Yiming Li


+ [Spatial-aware Attention Generative Adversarial Network for Semi-supervised Anomaly Detection in Medical Image](https://arxiv.org//abs/2405.12872)

	Zerui Zhang, Zhichao Sun, Zelong Liu, Bo Du, Rui Yu, Zhou Zhao, Yongchao Xu


+ [Robust Classification via a Single Diffusion Model](https://arxiv.org//abs/2305.15241)

	Huanran Chen, Yinpeng Dong, Zhengyi Wang, Xiao Yang, Chengqi Duan, Hang Su, Jun Zhu


+ [Rethinking Robustness Assessment: Adversarial Attacks on Learning-based Quadrupedal Locomotion Controllers](https://arxiv.org//abs/2405.12424)

	Fan Shi, Chong Zhang, Takahiro Miki, Joonho Lee, Marco Hutter, Stelian Coros


+ [How to Train a Backdoor-Robust Model on a Poisoned Dataset without Auxiliary Data?](https://arxiv.org//abs/2405.12719)

	Yuwen Pu, Jiahao Chen, Chunyi Zhou, Zhou Feng, Qingming Li, Chunqiang Hu, Shouling Ji


+ [A Stealthy Backdoor Attack for Without-Label-Sharing Split Learning](https://arxiv.org//abs/2405.12751)

	Yuwen Pu, Zhuoyuan Ding, Jiahao Chen, Chunyi Zhou, Qingming Li, Chunqiang Hu, Shouling Ji


+ [Rethinking the Vulnerabilities of Face Recognition Systems:From a Practical Perspective](https://arxiv.org//abs/2405.12786)

	Jiahao Chen, Zhiqiang Shen, Yuwen Pu, Chunyi Zhou, Shouling Ji


+ [GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation](https://arxiv.org//abs/2405.13077)

	Govind Ramesh, Yao Dou, Wei Xu


+ [Interactive Simulations of Backdoors in Neural Networks](https://arxiv.org//abs/2405.13217)

	Peter Bajcsy, Maxime Bros


+ [EmInspector: Combating Backdoor Attacks in Federated Self-Supervised Learning Through Embedding Inspection](https://arxiv.org//abs/2405.13080)

	Yuwen Qian, Shuchi Wu, Kang Wei, Ming Ding, Di Xiao, Tao Xiang, Chuan Ma, Song Guo


+ [A novel reliability attack of Physical Unclonable Functions](https://arxiv.org//abs/2405.13147)

	Gaoxiang Li, Yu Zhuang


# 2024-05-20
+ [Fed-Credit: Robust Federated Learning with Credibility Management](https://arxiv.org//abs/2405.11758)

	Jiayan Chen, Zhirong Qian, Tianhui Meng, Xitong Gao, Tian Wang, Weijia Jia


+ [Robust Deep Reinforcement Learning with Adaptive Adversarial Perturbations in Action Space](https://arxiv.org//abs/2405.11982)

	Qianmei Liu, Yufei Kuang, Jie Wang


+ [Adaptive Batch Normalization Networks for Adversarial Robustness](https://arxiv.org//abs/2405.11708)

	Shao-Yuan Lo, Vishal M. Patel


+ [Adversarially Diversified Rehearsal Memory (ADRM): Mitigating Memory Overfitting Challenge in Continual Learning](https://arxiv.org//abs/2405.11829)

	Hikmat Khan, Ghulam Rasool, Nidhal Carla Bouaynaya


+ [Data Contamination Calibration for Black-box LLMs](https://arxiv.org//abs/2405.11930)

	Wentao Ye, Jiaqi Hu, Liyao Li, Haobo Wang, Gang Chen, Junbo Zhao


+ [Decentralized Privacy Preservation for Critical Connections in Graphs](https://arxiv.org//abs/2405.11713)

	Conggai Li, Wei Ni, Ming Ding, Youyang Qu, Jianjun Chen, David Smith, Wenjie Zhang, Thierry Rakotoarivelo


+ [GAN-GRID: A Novel Generative Attack on Smart Grid Stability Prediction](https://arxiv.org//abs/2405.12076)

	Emad Efatinasab, Alessandro Brighente, Mirco Rampazzo, Nahal Azadi, Mauro Conti


+ [Adaptive Batch Normalization Networks for Adversarial Robustness](https://arxiv.org//abs/2405.11708)

	Shao-Yuan Lo, Vishal M. Patel


+ [Efficient Model-Stealing Attacks Against Inductive Graph Neural Networks](https://arxiv.org//abs/2405.12295)

	Marcin Podhajski, Jan Dubiński, Franziska Boenisch, Adam Dziedzic, Agnieszka Pregowska, Tomasz Michalak


+ [Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation](https://arxiv.org//abs/2405.13068)

	Yuxi Li, Yi Liu, Yuekang Li, Ling Shi, Gelei Deng, Shengquan Chen, Kailong Wang



# 2024-05-19
+ [An Invisible Backdoor Attack Based On Semantic Feature](https://arxiv.org//abs/2405.11551)

	Yangming Chen


+ [Searching Realistic-Looking Adversarial Objects For Autonomous Driving Systems](https://arxiv.org//abs/2405.11629)

	Shengxiang Sun, Shenzhe Zhu


+ [A Constraint-Enforcing Reward for Adversarial Attacks on Text Classifiers](https://arxiv.org//abs/2405.11904)

	Tom Roth, Inigo Jauregi Unanue, Alsharif Abuadbba, Massimo Piccardi


+ [On Robust Reinforcement Learning with Lipschitz-Bounded Policy Networks](https://arxiv.org//abs/2405.11432)

	Nicholas H. Barbara, Ruigang Wang, Ian R. Manchester


+ [Certified Robust Accuracy of Neural Networks Are Bounded due to Bayes Errors](https://arxiv.org//abs/2405.11547)

	Ruihan Zhang, Jun Sun


+ [A GAN-Based Data Poisoning Attack Against Federated Learning Systems and Its Countermeasure](https://arxiv.org//abs/2405.11440)

	Wei Sun, Bo Gao, Ke Xiong, Yuwei Wang, Pingyi Fan, Khaled Ben Letaief


+ [Sketches-based join size estimation under local differential privacy](https://arxiv.org//abs/2405.11419)

	Meifan Zhang, Xin Liu, Lihua Yin


+ [BOSC: A Backdoor-based Framework for Open Set Synthetic Image Attribution](https://arxiv.org//abs/2405.11491)

	Jun Wang, Benedetta Tondi, Mauro Barni



# 2024-05-18
+ [Revisiting the Robust Generalization of Adversarial Prompt Tuning](https://arxiv.org//abs/2405.11154)

	Fan Yang, Mingxuan Xia, Sangzhou Xia, Chicheng Ma, Hui Hui


+ [Trustworthy Actionable Perturbations](https://arxiv.org//abs/2405.11195)

	Jesse Friedbaum, Sudarshan Adiga, Ravi Tandon


+ [Towards Robust Policy: Enhancing Offline Reinforcement Learning with Adversarial Attacks and Defenses](https://arxiv.org//abs/2405.11206)

	Thanh Nguyen, Tung M. Luu, Tri Ton, Chang D. Yoo


+ [SEEP: Training Dynamics Grounds Latent Representation Search for Mitigating Backdoor Poisoning Attacks](https://arxiv.org//abs/2405.11575)

	Xuanli He, Qiongkai Xu, Jun Wang, Benjamin I. P. Rubinstein, Trevor Cohn


+ [BadActs: A Universal Backdoor Defense in the Activation Space](https://arxiv.org//abs/2405.11227)

	Biao Yi, Sishuo Chen, Yiming Li, Tong Li, Baolei Zhang, Zheli Liu


+ [UPAM: Unified Prompt Attack in Text-to-Image Generation Models Against Both Textual Filters and Visual Checkers](https://arxiv.org//abs/2405.11336)

	Duo Peng, Qiuhong Ke, Jun Liu


 [AquaLoRA: Toward White-box Protection for Customized Stable Diffusion Models via Watermark LoRA](https://arxiv.org//abs/2405.11135)

	Weitao Feng, Wenbo Zhou, Jiyan He, Jie Zhang, Tianyi Wei, Guanlin Li, Tianwei Zhang, Weiming Zhang, Nenghai Yu


+ [Few-Shot API Attack Detection: Overcoming Data Scarcity with GAN-Inspired Learning](https://arxiv.org//abs/2405.11258)

	Udi Aharon, Revital Marbel, Ran Dubin, Amit Dvir, Chen Hajaj


+ [Detecting Complex Multi-step Attacks with Explainable Graph Neural Network](https://arxiv.org//abs/2405.11335)

	Wei Liu, Peng Gao, Haotian Zhang, Ke Li, Weiyong Yang, Xingshen Wei, Shuji Wu


+ [MediCLIP: Adapting CLIP for Few-shot Medical Image Anomaly Detection](https://arxiv.org//abs/2405.11315)

	Ximiao Zhang, Min Xu, Dehui Qiu, Ruixin Yan, Ning Lang, Xiuzhuang Zhou


# 2024-05-17
+ [Safeguarding Vision-Language Models Against Patched Visual Prompt Injectors](https://arxiv.org//abs/2405.10529)

	Jiachen Sun, Changsheng Wang, Jiongxiao Wang, Yiwei Zhang, Chaowei Xiao


+ [Not All Prompts Are Secure: A Switchable Backdoor Attack Against Pre-trained Vision Transformers](https://arxiv.org//abs/2405.10612)

	Sheng Yang, Jiawang Bai, Kuofeng Gao, Yong Yang, Yiming Li, Shu-tao Xia


+ [Multicenter Privacy-Preserving Model Training for Deep Learning Brain Metastases Autosegmentation](https://arxiv.org//abs/2405.10870)

	Yixing Huang, Zahra Khodabakhshi, Ahmed Gomaa, Manuel Schmidt, Rainer Fietkau, Matthias Guckenberger, Nicolaus Andratschke, Christoph Bert, Stephanie Tanadini-Lang, Florian Putz


+ [Rethinking Graph Backdoor Attacks: A Distribution-Preserving Perspective](https://arxiv.org//abs/2405.10757)

	Zhiwei Zhang, Minhua Lin, Enyan Dai, Suhang Wang


+ [Boosting Few-Pixel Robustness Verification via Covering Verification Designs](https://arxiv.org//abs/2405.10924)

	Yuval Shapira, Naor Wiesel, Shahar Shabelman, Dana Drachsler-Cohen


+ [Generative AI for Secure and Privacy-Preserving Mobile Crowdsensing](https://arxiv.org//abs/2405.10521)

	Yaoqi Yang, Bangning Zhang, Daoxing Guo, Hongyang Du, Zehui Xiong, Dusit Niyato, Zhu Han



# 2024-05-16
+ [Box-Free Model Watermarks Are Prone to Black-Box Removal Attacks](https://arxiv.org//abs/2405.09863)

	Haonan An, Guang Hua, Zhiping Lin, Yuguang Fang


+ [DiffAM: Diffusion-based Adversarial Makeup Transfer for Facial Privacy Protection](https://arxiv.org//abs/2405.09882)

	Yuhao Sun, Lingyun Yu, Hongtao Xie, Jiaming Li, Yongdong Zhang


+ [Keep It Private: Unsupervised Privatization of Online Text](https://arxiv.org//abs/2405.10260)

	Calvin Bao, Marine Carpuat


+ [SecureLLM: Using Compositionality to Build Provably Secure Language Models for Private, Sensitive, and Secret Data](https://arxiv.org//abs/2405.09805)

	Abdulrahman Alabdulakreem, Christian M Arnold, Yerim Lee, Pieter M Feenstra, Boris Katz, Andrei Barbu


+ [Infrared Adversarial Car Stickers](https://arxiv.org//abs/2405.09924)

	Xiaopei Zhu, Yuqiu Liu, Zhanhao Hu, Jianmin Li, Xiaolin Hu


+ [Adversarial Robustness for Visual Grounding of Multimodal Large Language Models](https://arxiv.org//abs/2405.09981)

	Kuofeng Gao, Yang Bai, Jiawang Bai, Yong Yang, Shu-Tao Xia


+ [IBD-PSC: Input-level Backdoor Detection via Parameter-oriented Scaling Consistency](https://arxiv.org//abs/2405.09786)

	Linshan Hou, Ruili Feng, Zhongyun Hua, Wei Luo, Leo Yu Zhang, Yiming Li


+ [Manifold Integrated Gradients: Riemannian Geometry for Feature Attribution](https://arxiv.org//abs/2405.09800)

	Eslam Zaher, Maciej Trzaskowski, Quan Nguyen, Fred Roosta


+ [Dealing Doubt: Unveiling Threat Models in Gradient Inversion Attacks under Federated Learning, A Survey and Taxonomy](https://arxiv.org//abs/2405.10376)

	Yichuan Shi, Olivera Kotevska, Viktor Reshniak, Abhishek Singh, Ramesh Raskar


+ [Adversarial Robustness Guarantees for Quantum Classifiers](https://arxiv.org//abs/2405.10360)

	Neil Dowling, Maxwell T. West, Angus Southwell, Azar C. Nakhl, Martin Sevior, Muhammad Usman, Kavan Modi


+ [Learnable Privacy Neurons Localization in Language Models](https://arxiv.org//abs/2405.10989)

	Ruizhe Chen, Tianxiang Hu, Yang Feng, Zuozhu Liu


+ ["What do you want from theory alone?" Experimenting with Tight Auditing of Differentially Private Synthetic Data Generation](https://arxiv.org//abs/2405.10994)

	Meenatchi Sundaram Muthu Selva Annamalai, Georgi Ganev, Emiliano De Cristofaro



# 2024-05-15
+ [Identity Overlap Between Face Recognition Train/Test Data: Causing Optimistic Bias in Accuracy Measurement](https://arxiv.org//abs/2405.09403)

	Haiyu Wu, Sicong Tian, Jacob Gutierrez, Aman Bhatta, Kağan Öztürk, Kevin W. Bowyer


+ [Efficient LLM Jailbreak via Adaptive Dense-to-sparse Constrained Optimization](https://arxiv.org//abs/2405.09113)

	Kai Hu, Weichen Yu, Tianjun Yao, Xiang Li, Wenhe Liu, Lijun Yu, Yining Li, Kai Chen, Zhiqiang Shen, Matt Fredrikson


+ [Cross-Input Certified Training for Universal Perturbations](https://arxiv.org//abs/2405.09176)

	Changming Xu, Gagandeep Singh


+ [Towards Evaluating the Robustness of Automatic Speech Recognition Systems via Audio Style Transfer](https://arxiv.org//abs/2405.09470)

	Weifei Jin, Yuxin Cao, Junjie Su, Qi Shen, Kai Ye, Derui Wang, Jie Hao, Ziyao Liu


+ [Words Blending Boxes. Obfuscating Queries in Information Retrieval using Differential Privacy](https://arxiv.org//abs/2405.09306)

	Francesco Luigi De Faveri, Guglielmo Faggioli, Nicola Ferro


+ [Training Deep Learning Models with Hybrid Datasets for Robust Automatic Target Detection on real SAR images](https://arxiv.org//abs/2405.09588)

	Benjamin Camus, Théo Voillemin, Corentin Le Barbu, Jean-Christophe Louvigné, Carole Belloni, Emmanuel Vallée


+ [Properties that allow or prohibit transferability of adversarial attacks among quantized networks](https://arxiv.org//abs/2405.09598)

	Abhishek Shrestha, Jürgen Großmann


+ [DP-RuL: Differentially-Private Rule Learning for Clinical Decision Support Systems](https://arxiv.org//abs/2405.09721)

	Josephine Lamp, Lu Feng, David Evans


+ [Benchmark Early and Red Team Often: A Framework for Assessing and Managing Dual-Use Hazards of AI Foundation Models](https://arxiv.org//abs/2405.10986)

	Anthony M. Barrett, Krystal Jackson, Evan R. Murphy, Nada Madkour, Jessica Newman



# 2024-05-14
+ [Can we Defend Against the Unknown? An Empirical Study About Threshold Selection for Neural Network Monitoring](https://arxiv.org//abs/2405.08654)

	Khoi Tran Dang, Kevin Delmas, Jérémie Guiochet, Joris Guérin


+ [Towards Safe Large Language Models for Medicine](https://arxiv.org//abs/2403.03744)

	Tessa Han, Aounon Kumar, Chirag Agarwal, Himabindu Lakkaraju


+ [SpeechGuard: Exploring the Adversarial Robustness of Multimodal Large Language Models](https://arxiv.org//abs/2405.08317)

	Raghuveer Peri, Sai Muralidhar Jayanthi, Srikanth Ronanki, Anshu Bhatia, Karel Mundnich, Saket Dingliwal, Nilaksh Das, Zejiang Hou, Goeric Huybrechts, Srikanth Vishnubhotla, Daniel Garcia-Romero, Sundararajan Srinivasan, Kyu J Han, Katrin Kirchhoff


+ [UnMarker: A Universal Attack on Defensive Watermarking](https://arxiv.org//abs/2405.08363)

	Andre Kassis, Urs Hengartner


+ [Certifying Robustness of Graph Convolutional Networks for Node Perturbation with Polyhedra Abstract Interpretation](https://arxiv.org//abs/2405.08645)

	Boqi Chen, Kristóf Marussy, Oszkár Semeráth, Gunter Mussbacher, Dániel Varró


+ [Differentially Private Federated Learning: A Systematic Review](https://arxiv.org//abs/2405.08299)

	Jie Fu, Yuan Hong, Xinpeng Ling, Leixia Wang, Xun Ran, Zhiyu Sun, Wendy Hui Wang, Zhili Chen, Yang Cao


+ [Work-in-Progress: Crash Course: Can (Under Attack) Autonomous Driving Beat Human Drivers?](https://arxiv.org//abs/2405.08466)

	Francesco Marchiori, Alessandro Brighente, Mauro Conti


+ [Adversarial Machine Learning Threats to Spacecraft](https://arxiv.org//abs/2405.08834)

	Rajiv Thummala, Shristi Sharma, Matteo Calabrese, Gregory Falco


+ [Neural Collapse Meets Differential Privacy: Curious Behaviors of NoisyGD with Near-perfect Representation Learning](https://arxiv.org//abs/2405.08920)

	Chendi Wang, Yuqing Zhu, Weijie J. Su, Yu-Xiang Wang


+ [The Pitfalls and Promise of Conformal Inference Under Adversarial Attacks](https://arxiv.org//abs/2405.08886)

	Ziquan Liu, Yufei Cui, Yan Yan, Yi Xu, Xiangyang Ji, Xue Liu, Antoni B. Chan


+ [RS-Reg: Probabilistic and Robust Certified Regression Through Randomized Smoothing](https://arxiv.org//abs/2405.08892)

	Aref Miri Rekavandi, Olga Ohrimenko, Benjamin I.P. Rubinstein


+ [Private Data Leakage in Federated Human Activity Recognition for Wearable Healthcare Devices](https://arxiv.org//abs/2405.10979)

	Kongyang Chen, Dongping Zhang, Bing Mi


# 2024-05-13
+ [GLiRA: Black-Box Membership Inference Attack via Knowledge Distillation](https://arxiv.org//abs/2405.07562)

	Andrey V. Galichin, Mikhail Pautov, Alexey Zhavoronkin, Oleg Y. Rogov, Ivan Oseledets


+ [Environmental Matching Attack Against Unmanned Aerial Vehicles Object Detection](https://arxiv.org//abs/2405.07595)

	Dehong Kong, Siyuan Liang, Wenqi Ren


+ [CrossCert: A Cross-Checking Detection Approach to Patch Robustness Certification for Deep Learning Models](https://arxiv.org//abs/2405.07668)

	Qilin Zhou, Zhengyuan Wei, Haipeng Wang, Bo Jiang, W.K. Chan


+ [RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors](https://arxiv.org//abs/2405.07940)

	Liam Dugan, Alyssa Hwang, Filip Trhlik, Josh Magnus Ludan, Andrew Zhu, Hainiu Xu, Daphne Ippolito, Chris Callison-Burch


+ [Backdoor Removal for Generative Large Language Models](https://arxiv.org//abs/2405.07667)

	Haoran Li, Yulin Chen, Zihao Zheng, Qi Hu, Chunkit Chan, Heshan Liu, Yangqiu Song


+ [Evaluating Google's Protected Audience Protocol](https://arxiv.org//abs/2405.08102)

	Minjun Long, David Evans



# 2024-05-12
+ [VALID: a Validated Algorithm for Learning in Decentralized Networks with Possible Adversarial Presence](https://arxiv.org//abs/2405.07316)

	Mayank Bakshi, Sara Ghasvarianjahromi, Yauhen Yakimenka, Allison Beemer, Oliver Kosut, Joerg Kliewer



# 2024-05-11
+ [Shadow-Free Membership Inference Attacks: Recommender Systems Are More Vulnerable Than You Thought](https://arxiv.org//abs/2405.07018)

	Xiaoxiao Chi, Xuyun Zhang, Yan Wang, Lianyong Qi, Amin Beheshti, Xiaolong Xu, Kim-Kwang Raymond Choo, Shuo Wang, Hongsheng Hu


+ [Disrupting Style Mimicry Attacks on Video Imagery](https://arxiv.org//abs/2405.06865)

	Josephine Passananti, Stanley Wu, Shawn Shan, Haitao Zheng, Ben Y. Zhao


# 2024-05-10
+ [Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems](https://arxiv.org//abs/2405.06624)

	David "davidad" Dalrymple, Joar Skalse, Yoshua Bengio, Stuart Russell, Max Tegmark, Sanjit Seshia, Steve Omohundro, Christian Szegedy, Ben Goldhaber, Nora Ammann, Alessandro Abate, Joe Halpern, Clark Barrett, Ding Zhao, Tan Zhi-Xuan, Jeannette Wing, Joshua Tenenbaum


+ [Concealing Backdoor Model Updates in Federated Learning by Trigger-Optimized Data Poisoning](https://arxiv.org//abs/2405.06206)

	Yujie Zhang, Neil Gong, Michael K. Reiter


+ [Disttack: Graph Adversarial Attacks Toward Distributed GNN Training](https://arxiv.org//abs/2405.06247)

	Yuxiang Zhang, Xin Liu, Meng Wu, Wei Yan, Mingyu Yan, Xiaochun Ye, Dongrui Fan


+ [Exploring the Interplay of Interpretability and Robustness in Deep Neural Networks: A Saliency-guided Approach](https://arxiv.org//abs/2405.06278)

	Amira Guesmi, Nishant Suresh Aswani, Muhammad Shafique


+ [Improving Transferable Targeted Adversarial Attack via Normalized Logit Calibration and Truncated Feature Mixing](https://arxiv.org//abs/2405.06340)

	Juanjuan Weng, Zhiming Luo, Shaozi Li


+ [Evaluating Adversarial Robustness in the Spatial Frequency Domain](https://arxiv.org//abs/2405.06345)

	Keng-Hsin Liao, Chin-Yuan Yeh, Hsi-Wen Chen, Ming-Syan Chen


+ [Certified $\ell_2$ Attribution Robustness via Uniformly Smoothed Attributions](https://arxiv.org//abs/2405.06361)

	Fan Wang, Adams Wai-Kin Kong


+ [Risks of Practicing Large Language Models in Smart Grid: Threat Modeling and Validation](https://arxiv.org//abs/2405.06237)

	Jiangnan Li, Yingyuan Yang, Jinyuan Sun


+ [PLeak: Prompt Leaking Attacks against Large Language Model Applications](https://arxiv.org//abs/2405.06823)

	Bo Hui, Haolin Yuan, Neil Gong, Philippe Burlina, Yinzhi Cao


+ [LLM-Generated Black-box Explanations Can Be Adversarially Helpful](https://arxiv.org//abs/2405.06800)

	Rohan Ajwani, Shashidhar Reddy Javaji, Frank Rudzicz, Zining Zhu


# 2024-05-09
+ [Towards Robust Physical-world Backdoor Attacks on Lane Detection](https://arxiv.org//abs/2405.05553)

	Xinwei Zhang, Aishan Liu, Tianyuan Zhang, Siyuan Liang, Xianglong Liu


+ [Trustworthy AI-Generative Content in Intelligent 6G Network: Adversarial, Privacy, and Fairness](https://arxiv.org//abs/2405.05930)

	Siyuan Li, Xi Lin, Yaju Liu, Jianhua Li


+ [Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM](https://arxiv.org//abs/2405.05610)

	Xikang Yang, Xuehai Tang, Songlin Hu, Jizhong Han


+ [Towards Accurate and Robust Architectures via Neural Architecture Search](https://arxiv.org//abs/2405.05502)

	Yuwei Ou, Yuqi Feng, Yanan Sun


+ [Universal Adversarial Perturbations for Vision-Language Pre-trained Models](https://arxiv.org//abs/2405.05524)

	Peng-Fei Zhang, Zi Huang, Guangdong Bai


+ [Poisoning-based Backdoor Attacks for Arbitrary Target Label with Positive Triggers](https://arxiv.org//abs/2405.05573)

	Binxiao Huang, Jason Chun Lok, Chang Liu, Ngai Wong


+ [Model Inversion Robustness: Can Transfer Learning Help?](https://arxiv.org//abs/2405.05588)

	Sy-Tuyen Ho, Koh Jun Hao, Keshigeyan Chandrasegaran, Ngoc-Bao Nguyen, Ngai-Man Cheung


+ [Privacy-Preserving Edge Federated Learning for Intelligent Mobile-Health Systems](https://arxiv.org//abs/2405.05611)

	Amin Aminifar, Matin Shokri, Amir Aminifar


+ [Link Stealing Attacks Against Inductive Graph Neural Networks](https://arxiv.org//abs/2405.05784)

	Yixin Wu, Xinlei He, Pascal Berrang, Mathias Humbert, Michael Backes, Neil Zhenqiang Gong, Yang Zhang


+ [A Linear Reconstruction Approach for Attribute Inference Attacks against Synthetic Data](https://arxiv.org//abs/2301.10053)

	Meenatchi Sundaram Muthu Selva Annamalai, Andrea Gadotti, Luc Rocher


+ [High-Performance Privacy-Preserving Matrix Completion for Trajectory Recovery](https://arxiv.org//abs/2405.05789)

	Jiahao Guo, An-Bao Xu


+ [Special Characters Attack: Toward Scalable Training Data Extraction From Large Language Models](https://arxiv.org//abs/2405.05990)

	Yang Bai, Ge Pei, Jindong Gu, Yong Yang, Xingjun Ma


+ [Muting Whisper: A Universal Acoustic Adversarial Attack on Speech Foundation Models](https://arxiv.org//abs/2405.06134)

	Vyas Raina, Rao Ma, Charles McGhee, Kate Knill, Mark Gales


+ [BB-Patch: BlackBox Adversarial Patch-Attack using Zeroth-Order Optimization](https://arxiv.org//abs/2405.06049)

	Satyadwyoom Kumar, Saurabh Gupta, Arun Balaji Buduru


+ [Hard Work Does Not Always Pay Off: Poisoning Attacks on Neural Architecture Search](https://arxiv.org//abs/2405.06073)

	Zachary Coalson, Huazheng Wang, Qingyun Wu, Sanghyun Hong


# 2024-05-08
+ [Explanation as a Watermark: Towards Harmless and Multi-bit Model Ownership Verification via Watermarking Feature Attribution](https://arxiv.org//abs/2405.04825)

	Shuo Shao, Yiming Li, Hongwei Yao, Yiling He, Zhan Qin, Kui Ren


+ [The Codecfake Dataset and Countermeasures for the Universally Detection of Deepfake Audio](https://arxiv.org//abs/2405.04880)

	Yuankun Xie, Yi Lu, Ruibo Fu, Zhengqi Wen, Zhiyong Wang, Jianhua Tao, Xin Qi, Xiaopeng Wang, Yukun Liu, Haonan Cheng, Long Ye, Yi Sun


+ [BiasKG: Adversarial Knowledge Graphs to Induce Bias in Large Language Models](https://arxiv.org//abs/2405.04756)

	Chu Fei Luo, Ahmad Ghawanmeh, Xiaodan Zhu, Faiza Khan Khattak


+ [Mitigating Bias Using Model-Agnostic Data Attribution](https://arxiv.org//abs/2405.05031)

	Sander De Coninck, Wei-Cheng Wang, Sam Leroux, Pieter Simoens


+ [Espresso: Robust Concept Filtering in Text-to-Image Models](https://arxiv.org//abs/2404.19227)

	Anudeep Das, Vasisht Duddu, Rui Zhang, N. Asokan


+ [Towards Efficient Training and Evaluation of Robust Models against $l_0$ Bounded Adversarial Perturbations](https://arxiv.org//abs/2405.05075)

	Xuyang Zhong, Yixiao Huang, Chen Liu


+ [Adversarial Threats to Automatic Modulation Open Set Recognition in Wireless Networks](https://arxiv.org//abs/2405.05022)

	Yandie Yang, Sicheng Zhang, Kuixian Li, Qiao Tian, Yun Lin


+ [HackCar: a test platform for attacks and defenses on a cost-contained automotive architecture](https://arxiv.org//abs/2405.05023)

	Dario Stabili, Filip Valgimigli, Edoardo Torrini, Mirco Marchetti


+ [Systematic Use of Random Self-Reducibility against Physical Attacks](https://arxiv.org//abs/2405.05193)

	Ferhat Erata, TingHung Chiu, Anthony Etim, Srilalith Nampally, Tejas Raju, Rajashree Ramu, Ruzica Piskac, Timos Antonopoulos, Wenjie Xiong, Jakub Szefer


+ [Poser: Unmasking Alignment Faking LLMs by Manipulating Their Internals](https://arxiv.org//abs/2405.05466)

	Joshua Clymer, Caden Juang, Severin Field


+ [Adversary-Guided Motion Retargeting for Skeleton Anonymization](https://arxiv.org//abs/2405.05428)

	Thomas Carr, Depeng Xu, Aidong Lu


+ [Model Reconstruction Using Counterfactual Explanations: Mitigating the Decision Boundary Shift](https://arxiv.org//abs/2405.05369)

	Pasan Dissanayake, Sanghamitra Dutta


+ [Untargeted Adversarial Attack on Knowledge Graph Embeddings](https://arxiv.org//abs/2405.10970)

	Tianzhe Zhao, Jiaoyan Chen, Yanchi Ru, Qika Lin, Yuxia Geng, Jun Liu


# 2024-05-07
+ [Enhancing Scalability of Metric Differential Privacy via Secret Dataset Partitioning and Benders Decomposition](https://arxiv.org//abs/2405.04344)

	Chenxi Qiu


+ [Locally Differentially Private In-Context Learning](https://arxiv.org//abs/2405.04032)

	Chunyan Zheng, Keke Sun, Wenhao Zhao, Haibo Zhou, Lixin Jiang, Shaoyang Song, Chunlai Zhou


+ [A2-DIDM: Privacy-preserving Accumulator-enabled Auditing for Distributed Identity of DNN Model](https://arxiv.org//abs/2405.04108)

	Tianxiu Xie, Keke Gai, Jing Yu, Liehuang Zhu, Kim-Kwang Raymond Choo


+ [Revisiting character-level adversarial attacks](https://arxiv.org//abs/2405.04346)

	Elias Abad Rocamora, Yongtao Wu, Fanghui Liu, Grigorios G. Chrysos, Volkan Cevher


+ [IPFed: Identity protected federated learning for user authentication](https://arxiv.org//abs/2405.03955)

	Yosuke Kaga, Yusei Suzuki, Kenta Takahashi


+ [Exposing AI-generated Videos: A Benchmark Dataset and a Local-and-Global Temporal Defect Based Detection Method](https://arxiv.org//abs/2405.04133)

	Peisong He, Leyao Zhu, Jiaxing Li, Shiqi Wang, Haoliang Li


+ [Breast Histopathology Image Retrieval by Attention-based Adversarially Regularized Variational Graph Autoencoder with Contrastive Learning-Based Feature Extraction](https://arxiv.org//abs/2405.04211)

	Nematollah Saeidi, Hossein Karshenas, Bijan Shoushtarian, Sepideh Hatamikia, Ramona Woitek, Amirreza Mahbod


+ [Effective and Robust Adversarial Training against Data and Label Corruptions](https://arxiv.org//abs/2405.04191)

	Peng-Fei Zhang, Zi Huang, Xin-Shun Xu, Guangdong Bai


+ [Unlearning Backdoor Attacks through Gradient-Based Model Pruning](https://arxiv.org//abs/2405.03918)

	Kealan Dunnett, Reza Arablouei, Dimity Miller, Volkan Dedeoglu, Raja Jurdak


+ [Explainability-Informed Targeted Malware Misclassification](https://arxiv.org//abs/2405.04010)

	Quincy Card, Kshitiz Aryal, Maanak Gupta


+ [Enabling Privacy-Preserving and Publicly Auditable Federated Learning](https://arxiv.org//abs/2405.04029)

	Huang Zeng, Anjia Yang, Jian Weng, Min-Rong Chen, Fengjun Xiao, Yi Liu, Ye Yao


+ [A Stealthy Wrongdoer: Feature-Oriented Reconstruction Attack against Split Learning](https://arxiv.org//abs/2405.04115)

	Xiaoyang Xu, Mengda Yang, Wenzhe Yi, Ziang Li, Juan Wang, Hongxin Hu, Yong Zhuang, Yaxin Liu


# 2024-05-06
+ [ To Each (Textual Sequence) Its Own: Improving Memorized-Data Unlearning  in Large Language Models](https://arxiv.org//abs/2405.03097)

	George-Octavian Barbulescu, Peter Triantafillou


+ [ Assessing Adversarial Robustness of Large Language Models: An Empirical  Study](https://arxiv.org//abs/2405.02764)

	Zeyu Yang, Zhao Meng, Xiaochen Zheng, Roger Wattenhofer


+ [ Exploring Frequencies via Feature Mixing and Meta-Learning for Improving  Adversarial Transferability](https://arxiv.org//abs/2405.03193)

	Juanjuan Weng, Zhiming Luo, Shaozi Li


+ [ UnsafeBench: Benchmarking Image Safety Classifiers on Real-World and  AI-Generated Images](https://arxiv.org//abs/2405.03486)

	Yiting Qu, Xinyue Shen, Yixin Wu, Michael Backes, Savvas Zannettou, Yang Zhang


+ [ Learning Robust Classifiers with Self-Guided Spurious Correlation  Mitigation](https://arxiv.org//abs/2405.03649)

	Guangtao Zheng, Wenqian Ye, Aidong Zhang


+ [ Provably Unlearnable Examples](https://arxiv.org//abs/2405.03316)

	Derui Wang, Minhui Xue, Bo Li, Seyit Camtepe, Liming Zhu


+ [ GI-SMN: Gradient Inversion Attack against Federated Learning without  Prior Knowledge](https://arxiv.org//abs/2405.03516)

	Jin Qian, Kaimin Wei, Yongdong Wu, Jilian Zhang, Jipeng Chen, Huan Bao


+ [ Federated Learning Privacy: Attacks, Defenses, Applications, and Policy  Landscape - A Survey](https://arxiv.org//abs/2405.03636)

	Joshua C. Zhao, Saurabh Bagchi, Salman Avestimehr, Kevin S. Chan, Somali Chaterji, Dimitris Dimitriadis, Jiacheng Li, Ninghui Li, Arash Nourian, Holger R. Roth


+ [ Cutting through buggy adversarial example defenses: fixing 1 line of  code breaks Sabre](https://arxiv.org//abs/2405.03672)

	Nicholas Carlini


+ [ FOBNN: Fast Oblivious Binarized Neural Network Inference](https://arxiv.org//abs/2405.03136)

	Xin Chen, Zhili Chen, Benchang Dong, Shiwen Wei, Lin Chen, Daojing He


+ [ DarkFed: A Data-Free Backdoor Attack in Federated Learning](https://arxiv.org//abs/2405.03299)

	Minghui Li, Wei Wan, Yuxuan Ning, Shengshan Hu, Lulu Xue, Leo Yu Zhang, Yichen Wang


+ [ LaserEscape: Detecting and Mitigating Optical Probing Attacks](https://arxiv.org//abs/2405.03632)

	Saleh Khalaj Monfared, Kyle Mitard, Andrew Cannon, Domenic Forte, Shahin Tajik


+ [Is ReLU Adversarially Robust?](https://arxiv.org//abs/2405.03777)

	Korn Sooksatra, Greg Hamerly, Pablo Rivas


+ [On Adversarial Examples for Text Classification by Perturbing Latent Representations](https://arxiv.org//abs/2405.03789)

	Korn Sooksatra, Bikram Khanal, Pablo Rivas


+ [Enhancing O-RAN Security: Evasion Attacks and Robust Defenses for Graph Reinforcement Learning-based Connection Management](https://arxiv.org//abs/2405.03891)

	Ravikumar Balakrishnan, Marius Arvinte, Nageen Himayat, Hosein Nikopour, Hassnaa Moustafa


+ [Is ReLU Adversarially Robust?](https://arxiv.org//abs/2405.03777)

	Korn Sooksatra, Greg Hamerly, Pablo Rivas


+ [On Adversarial Examples for Text Classification by Perturbing Latent Representations](https://arxiv.org//abs/2405.03789)

	Korn Sooksatra, Bikram Khanal, Pablo Rivas


+ [Enhancing O-RAN Security: Evasion Attacks and Robust Defenses for Graph Reinforcement Learning-based Connection Management](https://arxiv.org//abs/2405.03891)

	Ravikumar Balakrishnan, Marius Arvinte, Nageen Himayat, Hosein Nikopour, Hassnaa Moustafa


+ [Generative adversarial learning with optimal input dimension and its adaptive generator architecture](https://arxiv.org//abs/2405.03723)

	Zhiyao Tan, Ling Zhou, Huazhen Lin


+ [Secure Inference for Vertically Partitioned Data Using Multiparty Homomorphic Encryption](https://arxiv.org//abs/2405.03775)

	Shuangyi Chen, Yue Ju, Zhongwen Zhu, Ashish Khisti


+ [Differentially Private Federated Learning without Noise Addition: When is it Possible?](https://arxiv.org//abs/2405.04551)

	Jiang Zhang, Yahya H Ezzeldin, Ahmed Roushdy Elkordy, Konstantinos Psounis, Salman Avestimehr


+ [Differentially Private Synthetic Data with Private Density Estimation](https://arxiv.org//abs/2405.04554)

	Nikolija Bojkovic, Po-Ling Loh


# 2024-05-05
+ [ Safe Reinforcement Learning with Learned Non-Markovian Safety  Constraints](https://arxiv.org//abs/2405.03005)

	Siow Meng Low, Akshat Kumar


+ [ AnoGAN for Tabular Data: A Novel Approach to Anomaly Detection](https://arxiv.org//abs/2405.03075)

	Aditya Singh, Pavan Reddy


+ [ Confidential and Protected Disease Classifier using Fully Homomorphic  Encryption](https://arxiv.org//abs/2405.02790)

	Aditya Malik, Nalini Ratha, Bharat Yalavarthi, Tilak Sharma, Arjun Kaushik, Charanjit Jutla


+ [ Trojans in Large Language Models of Code: A Critical Review through a  Trigger-Based Taxonomy](https://arxiv.org//abs/2405.02828)

	Aftab Hussain, Md Rafiqul Islam Rabin, Toufique Ahmed, Bowen Xu, Premkumar Devanbu, Mohammad Amin Alipour


+ [ Defense against Joint Poison and Evasion Attacks: A Case Study of DERMS](https://arxiv.org//abs/2405.02989)

	Zain ul Abdeen, Padmaksha Roy, Ahmad Al-Tawaha, Rouxi Jia, Laura Freeman, Peter Beling, Chen-Ching Liu, Alberto Sangiovanni-Vincentelli, Ming Jin



# 2024-05-04
+ [ Leveraging the Human Ventral Visual Stream to Improve Neural Network  Robustness](https://arxiv.org//abs/2405.02564)

	Zhenan Shao, Linjian Ma, Bo Li, Diane M. Beck


+ [ Detecting Edited Knowledge in Language Models](https://arxiv.org//abs/2405.02765)

	Paul Youssef, Zhixue Zhao, Jörg Schlötterer, Christin Seifert


+ [ Can LLMs Deeply Detect Complex Malicious Queries? A Framework for  Jailbreaking via Obfuscating Intent](https://arxiv.org//abs/2405.03654)

	Shang Shang, Xinqiang Zhao, Zhongjiang Yao, Yepeng Yao, Liya Su, Zijing Fan, Xiaodan Zhang, Zhengwei Jiang


+ [ PrivSGP-VR: Differentially Private Variance-Reduced Stochastic Gradient  Push with Tight Utility Bounds](https://arxiv.org//abs/2405.02638)

	Zehan Zhu, Yan Huang, Xin Wang, Jinming Xu


+ [ Updating Windows Malware Detectors: Balancing Robustness and Regression  against Adversarial EXEmples](https://arxiv.org//abs/2405.02646)

	Matous Kozak, Luca Demetrio, Dmitrijs Trizna, Fabio Roli


+ [ Metric Differential Privacy at the User-Level](https://arxiv.org//abs/2405.02665)

	Jacob Imola, Amrita Roy Chowdhury, Kamalika Chaudhuri


+ [Your Network May Need to Be Rewritten: Network Adversarial Based on High-Dimensional Function Graph Decomposition](https://arxiv.org//abs/2405.03712)

	Xiaoyan Su, Yinghao Zhu, Run Li



# 2024-05-03
+ [ Impact of Architectural Modifications on Deep Learning Adversarial  Robustness](https://arxiv.org//abs/2405.01934)

	Firuz Juraev, Mohammed Abuhamad, Simon S. Woo, George K Thiruvathukal, Tamer Abuhmed


+ [ From Attack to Defense: Insights into Deep Learning Security Measures in  Black-Box Settings](https://arxiv.org//abs/2405.01963)

	Firuz Juraev, Mohammed Abuhamad, Eric Chan-Tin, George K. Thiruvathukal, Tamer Abuhmed


+ [ Adversarial Botometer: Adversarial Analysis for Social Bot Detection](https://arxiv.org//abs/2405.02016)

	Shaghayegh Najari, Davood Rafiee, Mostafa Salehi, Reza Farahbakhsh


+ [ Zero-Sum Positional Differential Games as a Framework for Robust  Reinforcement Learning: Deep Q-Learning Approach](https://arxiv.org//abs/2405.02044)

	Anton Plaksin, Vitaly Kalev


+ [ Uniformly Stable Algorithms for Adversarial Training and Beyond](https://arxiv.org//abs/2405.01817)

	Jiancong Xiao, Jiawei Zhang, Zhi-Quan Luo, Asuman Ozdaglar


+ [ A Novel Approach to Guard from Adversarial Attacks using Stable  Diffusion](https://arxiv.org//abs/2405.01838)

	Trinath Sai Subhash Reddy Pittala, Uma Maheswara Rao Meleti, Geethakrishna Puligundla


+ [ Optimistic Regret Bounds for Online Learning in Adversarial Markov  Decision Processes](https://arxiv.org//abs/2405.02188)

	Sang Bin Moon, Abolfazl Hashemi


+ [ ProFLingo: A Fingerprinting-based Copyright Protection Scheme for Large  Language Models](https://arxiv.org//abs/2405.02466)

	Heng Jin, Chaoyu Zhang, Shanghao Shi, Wenjing Lou, Y. Thomas Hou


+ [ Adaptive and robust watermark against model extraction attack](https://arxiv.org//abs/2405.02365)

	Kaiyi Pang, Tao Qi, Chuhan Wu, Minhao Bai



# 2024-05-02
+ [ PVF (Parameter Vulnerability Factor): A Quantitative Metric Measuring AI  Vulnerability and Resilience Against Parameter Corruptions](https://arxiv.org//abs/2405.01741)

	Xun Jiao, Fred Lin, Harish D. Dixit, Joel Coburn, Abhinav Pandey, Han Wang, Jianyu Huang, Venkat Ramesh, Wang Xu, Daniel Moore, Sriram Sankar


+ [ Privacy-aware Berrut Approximated Coded Computing for Federated Learning](https://arxiv.org//abs/2405.01704)

	Xavier Martínez Luaña, Rebeca P. Díaz Redondo, Manuel Fernández Veiga


+ [ Robust Risk-Sensitive Reinforcement Learning with Conditional  Value-at-Risk](https://arxiv.org//abs/2405.01718)

	Xinyi Ni, Lifeng Lai


+ [ Adversarial Attacks on Reinforcement Learning Agents for Command and  Control](https://arxiv.org//abs/2405.01693)

	Ahaan Dabholkar, James Z. Hare, Mark Mittrick, John Richardson, Nicholas Waytowich, Priya Narayanan, Saurabh Bagchi


+ [ ATTAXONOMY: Unpacking Differential Privacy Guarantees Against Practical  Adversaries](https://arxiv.org//abs/2405.01716)

	Rachel Cummings, Shlomi Hod, Jayshree Sarathy, Marika Swanberg


+ [ Explainability Guided Adversarial Evasion Attacks on Malware Detectors](https://arxiv.org//abs/2405.01728)

	Kshitiz Aryal, Maanak Gupta, Mahmoud Abdelsalam, Moustafa Saleh


+ [ Backdoor-based Explainable AI Benchmark for High Fidelity Evaluation of  Attribution Methods](https://arxiv.org//abs/2405.02344)

	Peiyu Yang, Naveed Akhtar, Jiantong Jiang, Ajmal Mian


+ [ Improved Communication-Privacy Trade-offs in $L_2$ Mean Estimation under  Streaming Differential Privacy](https://arxiv.org//abs/2405.02341)

	Wei-Ning Chen, Berivan Isik, Peter Kairouz, Albert No, Sewoong Oh, Zheng Xu


+ [ Temporal assessment of malicious behaviors: application to turnout field  data monitoring](https://arxiv.org//abs/2405.02346)

	Sara Abdellaoui, Emil Dumitrescu, Cédric Escudero, Eric Zamaï


# 2024-05-01
+ [ Robustness of graph embedding methods for community detection](https://arxiv.org/abs/2405.00636)

	Zhi-Feng Wei, Pablo Moriano, Ramakrishnan Kannan


+ [ Certified Adversarial Robustness of Machine Learning-based Malware Detectors via (De)Randomized Smoothing](https://arxiv.org/abs/2405.00392)

	Daniel Gibert, Luca Demetrio, Giulio Zizzo, Quan Le, Jordi Planes, Battista Biggio


# 2024-04-30
+ [ Transferring Troubles: Cross-Lingual Transferability of Backdoor Attacks in LLMs with Instruction Tuning](https://arxiv.org/abs/2404.19597)

	Xuanli He, Jun Wang, Qiongkai Xu, Pasquale Minervini, Pontus Stenetorp, Benjamin I. P. Rubinstein, Trevor Cohn


+ [ AttackBench: Evaluating Gradient-based Attacks for Adversarial Examples](https://arxiv.org/abs/2404.19460)

	Antonio Emanuele Cinà, Jérôme Rony, Maura Pintor, Luca Demetrio, Ambra Demontis, Battista Biggio, Ismail Ben Ayed, Fabio Roli


+ [ ASAM: Boosting Segment Anything Model with Adversarial Tuning](https://arxiv.org/abs/2405.00256)

	Bo Li, Haoke Xiao, Lv Tang


+ [ Probing Unlearned Diffusion Models: A Transferable Adversarial Attack Perspective](https://arxiv.org/abs/2404.19382)

	Xiaoxuan Han, Songlin Yang, Wei Wang, Yang Li, Jing Dong


+ [Revisiting the Adversarial Robustness of Vision Language Models: a Multimodal Perspective](https://arxiv.org/abs/2404.19287)

	Wanqi Zhou, Shuanghao Bai, Qibin Zhao, Badong Chen


# 2024-04-29
+ [ Do Neutral Prompts Produce Insecure Code? FormAI-v2 Dataset: Labelling  Vulnerabilities in Code Generated by Large Language Models](https://arxiv.org//abs/2404.18353)

	Norbert Tihanyi, Tamas Bisztray, Mohamed Amine Ferrag, Ridhi Jain, Lucas C. Cordeiro


+ [ Certification of Speaker Recognition Models to Additive Perturbations](https://arxiv.org//abs/2404.18791)

	Dmitrii Korzh, Elvir Karimov, Mikhail Pautov, Oleg Y. Rogov, Ivan Oseledets


+ [ Harmonic Machine Learning Models are Robust](https://arxiv.org//abs/2404.18825)

	Nicholas S. Kersting, Yi Li, Aman Mohanty, Oyindamola Obisesan, Raphael Okochu


+ [ Uncertainty-boosted Robust Video Activity Anticipation](https://arxiv.org//abs/2404.18648)

	Zhaobo Qi, Shuhui Wang, Weigang Zhang, Qingming Huang


+ [ Why You Should Not Trust Interpretations in Machine Learning:  Adversarial Attacks on Partial Dependence Plots](https://arxiv.org//abs/2404.18702)

	Xi Xin, Fei Huang, Giles Hooker


+ [ A Systematic Evaluation of Adversarial Attacks against Speech Emotion  Recognition Models](https://arxiv.org//abs/2404.18514)

	Nicolas Facchinetti, Federico Simonetta, Stavros Ntalampiras


+ [ Assessing Cybersecurity Vulnerabilities in Code Large Language Models](https://arxiv.org//abs/2404.18567)

	Md Imran Hossen, Jianyi Zhang, Yinzhi Cao, Xiali Hei



# 2024-04-27
+ [ Adversarial Examples: Generation Proposal in the Context of Facial  Recognition Systems](https://arxiv.org//abs/2404.17760)

	Marina Fuster, Ignacio Vidaurreta


+ [ Bounding the Expected Robustness of Graph Neural Networks Subject to  Node Feature Attacks](https://arxiv.org//abs/2404.17947)

	Yassine Abbahaddou, Sofiane Ennadir, Johannes F. Lutzeyer, Michalis Vazirgiannis, Henrik Boström


+ [ Privacy-Preserving Aggregation for Decentralized Learning with  Byzantine-Robustness](https://arxiv.org//abs/2404.17970)

	Ali Reza Ghavamipour, Benjamin Zi Hao Zhao, Oguzhan Ersoy, Fatih Turkmen


+ [ Are Watermarks Bugs for Deepfake Detectors? Rethinking Proactive  Forensics](https://arxiv.org//abs/2404.17867)

	Xiaoshuai Wu, Xin Liao, Bo Ou, Yuling Liu, Zheng Qin


+ [ Improving Smart Contract Security with Contrastive Learning-based  Vulnerability Detection](https://arxiv.org//abs/2404.17839)

	Yizhou Chen, Zeyu Sun, Zhihao Gong, Dan Hao



# 2024-04-26
+ [ Talking Nonsense: Probing Large Language Models' Understanding of  Adversarial Gibberish Inputs](https://arxiv.org//abs/2404.17120)

	Valeriia Cherepanova, James Zou


+ [ Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered  Applications](https://arxiv.org//abs/2404.17196)

	Quan Zhang, Binqi Zeng, Chijin Zhou, Gwihwan Go, Heyuan Shi, Yu Jiang


+ [ Enhancing Privacy and Security of Autonomous UAV Navigation](https://arxiv.org//abs/2404.17225)

	Vatsal Aggarwal, Arjun Ramesh Kaushik, Charanjit Jutla, Nalini Ratha


+ [ M3BAT: Unsupervised Domain Adaptation for Multimodal Mobile Sensing with  Multi-Branch Adversarial Training](https://arxiv.org//abs/2404.17391)

	Lakmal Meegahapola, Hamza Hassoune, Daniel Gatica-Perez


+ [ Defending Spiking Neural Networks against Adversarial Attacks through  Image Purification](https://arxiv.org//abs/2404.17092)

	Weiran Chen, Qi Sun, Qi Xu


+ [ Adversarial Reweighting with $α$-Power Maximization for Domain  Adaptation](https://arxiv.org//abs/2404.17275)

	Xiang Gu, Xi Yu, Yan Yang, Jian Sun, Zongben Xu


+ [ Estimating the Robustness Radius for Randomized Smoothing with  100$\times$ Sample Efficiency](https://arxiv.org//abs/2404.17371)

	Emmanouil Seferis, Stefanos Kollias, Chih-Hong Cheng


+ [ Adversarial Consistency and the Uniqueness of the Adversarial Bayes  Classifier](https://arxiv.org//abs/2404.17358)

	Natalie S. Frank


+ [ Evaluations of Machine Learning Privacy Defenses are Misleading](https://arxiv.org//abs/2404.17399)

	Michael Aerni, Jie Zhang, Florian Tramèr


+ [ Beyond Traditional Threats: A Persistent Backdoor Attack on Federated  Learning](https://arxiv.org//abs/2404.17617)

	Tao Liu, Yuhang Zhang, Zhu Feng, Zhiqin Yang, Chen Xu, Dapeng Man, Wu Yang


+ [ Center-Based Relaxed Learning Against Membership Inference Attacks](https://arxiv.org//abs/2404.17674)

	Xingli Fang, Jung-Eun Kim


+ [ Efficient Exploration of Image Classifier Failures with Bayesian  Optimization and Text-to-Image Models](https://arxiv.org//abs/2405.02332)

	Adrien Le Coz, Houssem Ouertatani, Stéphane Herbin, Faouzi Adjed



# 2024-04-25
+ [ Constructing Optimal Noise Channels for Enhanced Robustness in Quantum  Machine Learning](https://arxiv.org//abs/2404.16417)

	David Winderl, Nicola Franco, Jeanette Miriam Lorenz


+ [ Towards Precise Observations of Neural Model Robustness in  Classification](https://arxiv.org//abs/2404.16457)

	Wenchuan Mu, Kwan Hui Lim


+ [ Energy-Latency Manipulation of Multi-modal Large Language Models via  Verbose Samples](https://arxiv.org//abs/2404.16557)

	Kuofeng Gao, Jindong Gu, Yang Bai, Shu-Tao Xia, Philip Torr, Wei Liu, Zhifeng Li


+ [ Understanding Privacy Risks of Embeddings Induced by Large Language  Models](https://arxiv.org//abs/2404.16587)

	Zhihao Zhu, Ninglu Shao, Defu Lian, Chenwang Wu, Zheng Liu, Yi Yang, Enhong Chen


+ [ Don't Say No: Jailbreaking LLM by Suppressing Refusal](https://arxiv.org//abs/2404.16369)

	Yukai Zhou, Wenjie Wang


+ [ PAD: Patch-Agnostic Defense against Adversarial Patch Attacks](https://arxiv.org//abs/2404.16452)

	Lihua Jing, Rui Wang, Wenqi Ren, Xin Dong, Cong Zou


+ [ Differentially Private Federated Learning: Servers Trustworthiness,  Estimation, and Statistical Inference](https://arxiv.org//abs/2404.16287)

	Zhe Zhang, Ryumei Nakada, Linjun Zhang


+ [ Boosting Model Resilience via Implicit Adversarial Data Augmentation](https://arxiv.org//abs/2404.16307)

	Xiaoling Zhou, Wei Ye, Zhemg Lee, Rui Xie, Shikun Zhang


+ [ Generating Minimalist Adversarial Perturbations to Test Object-Detection  Models: An Adaptive Multi-Metric Evolutionary Search Approach](https://arxiv.org//abs/2404.17020)

	Cristopher McIntyre-Garcia, Adrien Heymans, Beril Borali, Won-Sook Lee, Shiva Nejati


+ [ A Notion of Uniqueness for the Adversarial Bayes Classifier](https://arxiv.org//abs/2404.16956)

	Natalie S. Frank



# 2024-04-24
+ [ A General Black-box Adversarial Attack on Graph-based Fake News  Detectors](https://arxiv.org//abs/2404.15744)

	Peican Zhu, Zechen Pan, Yang Liu, Jiwei Tian, Keke Tang, Zhen Wang


+ [ Steal Now and Attack Later: Evaluating Robustness of Object Detection  against Black-box Adversarial Attacks](https://arxiv.org//abs/2404.15881)

	Erh-Chung Chen, Pin-Yu Chen, I-Hsin Chung, Che-Rung Lee


+ [ Universal Adversarial Triggers Are Not Universal](https://arxiv.org//abs/2404.16020)

	Nicholas Meade, Arkil Patel, Siva Reddy


+ [ 3D Face Morphing Attack Generation using Non-Rigid Registration](https://arxiv.org//abs/2404.15765)

	Jag Mohan Singh, Raghavendra Ramachandra


+ [ Vision Transformer-based Adversarial Domain Adaptation](https://arxiv.org//abs/2404.15817)

	Yahan Li, Yuan Wu


+ [ Beyond Deepfake Images: Detecting AI-Generated Videos](https://arxiv.org//abs/2404.15955)

	Danial Samadi Vahdati, Tai D. Nguyen, Aref Azizpour, Matthew C. Stamm


+ [ MISLEAD: Manipulating Importance of Selected features for Learning  Epsilon in Evasion Attack Deception](https://arxiv.org//abs/2404.15656)

	Vidit Khazanchi, Pavan Kulkarni, Yuvaraj Govindarajulu, Manojkumar Parmar


+ [ CLAD: Robust Audio Deepfake Detection Against Manipulation Attacks with  Contrastive Learning](https://arxiv.org//abs/2404.15854)

	Haolin Wu, Jing Chen, Ruiying Du, Cong Wu, Kun He, Xingcan Shang, Hao Ren, Guowen Xu


+ [ Security Analysis of WiFi-based Sensing Systems: Threats from  Perturbation Attacks](https://arxiv.org//abs/2404.15587)

	Hangcheng Cao, Wenbin Huang, Guowen Xu, Xianhao Chen, Ziyang He, Jingyang Hu, Hongbo Jiang, Yuguang Fang


+ [ PoisonedFL: Model Poisoning Attacks to Federated Learning via  Multi-Round Consistency](https://arxiv.org//abs/2404.15611)

	Yueqi Xie, Minghong Fang, Neil Zhenqiang Gong


+ [ Noise Variance Optimization in Differential Privacy: A Game-Theoretic  Approach Through Per-Instance Differential Privacy](https://arxiv.org//abs/2404.15686)

	Sehyun Ryu, Jonggyu Jang, Hyun Jong Yang


+ [ Advancing Recommender Systems by mitigating Shilling attacks](https://arxiv.org//abs/2404.16177)

	Aditya Chichani, Juzer Golwala, Tejas Gundecha, Kiran Gawande


+ [ Investigating the prompt leakage effect and black-box defenses for  multi-turn LLM interactions](https://arxiv.org//abs/2404.16251)

	Divyansh Agarwal, Alexander R. Fabbri, Philippe Laban, Shafiq Joty, Caiming Xiong, Chien-Sheng Wu


+ [ An Analysis of Recent Advances in Deepfake Image Detection in an  Evolving Threat Landscape](https://arxiv.org//abs/2404.16212)

	Sifat Muhammad Abdullah, Aravind Cheruvu, Shravya Kanchi, Taejoong Chung, Peng Gao, Murtuza Jadliwala, Bimal Viswanath


+ [ Enhancing Privacy in Face Analytics Using Fully Homomorphic Encryption](https://arxiv.org//abs/2404.16255)

	Bharat Yalavarthi, Arjun Ramesh Kaushik, Arun Ross, Vishnu Boddeti, Nalini Ratha


+ [ A Comparative Analysis of Adversarial Robustness for Quantum and  Classical Machine Learning Models](https://arxiv.org//abs/2404.16154)

	Maximilian Wendlinger, Kilian Tscharke, Pascal Debus


+ [ Attacks on Third-Party APIs of Large Language Models](https://arxiv.org//abs/2404.16891)

	Wanru Zhao, Vidit Khazanchi, Haodi Xing, Xuanli He, Qiongkai Xu, Nicholas Donald Lane



# 2024-04-23
+ [ Talk Too Much: Poisoning Large Language Models under Token Limit](https://arxiv.org//abs/2404.14795)

	Jiaming He, Wenbo Jiang, Guanyu Hou, Wenshu Fan, Rui Zhang, Hongwei Li


+ [ A Customer Level Fraudulent Activity Detection Benchmark for Enhancing  Machine Learning Model Research and Evaluation](https://arxiv.org//abs/2404.14746)

	Phoebe Jing, Yijing Gao, Xianlong Zeng


+ [ Leverage Variational Graph Representation For Model Poisoning on  Federated Learning](https://arxiv.org//abs/2404.15042)

	Kai Li, Xin Yuan, Jingjing Zheng, Wei Ni, Falko Dressler, Abbas Jamalipour


+ [ Formal Verification of Graph Convolutional Networks with Uncertain Node  Features and Uncertain Graph Structure](https://arxiv.org//abs/2404.15065)

	Tobias Ladner, Michael Eichelbeck, Matthias Althoff


+ [ Manipulating Recommender Systems: A Survey of Poisoning Attacks and  Countermeasures](https://arxiv.org//abs/2404.14942)

	Thanh Toan Nguyen, Quoc Viet Hung Nguyen, Thanh Tam Nguyen, Thanh Trung Huynh, Thanh Thi Nguyen, Matthias Weidlich, Hongzhi Yin


+ [ Double Privacy Guard: Robust Traceable Adversarial Watermarking against  Face Recognition](https://arxiv.org//abs/2404.14693)

	Yunming Zhang, Dengpan Ye, Sipeng Shen, Caiyun Xie, Ziyi Liu, Jiacheng Deng, Long Tang


+ [ Every Breath You Don't Take: Deepfake Speech Detection Using Breath](https://arxiv.org//abs/2404.15143)

	Seth Layton, Thiago De Andrade, Daniel Olszewski, Kevin Warren, Carrie Gates, Kevin Butler, Patrick Traynor


+ [ Rethinking LLM Memorization through the Lens of Adversarial Compression](https://arxiv.org//abs/2404.15146)

	Avi Schwarzschild, Zhili Feng, Pratyush Maini, Zachary C. Lipton, J. Zico Kolter


+ [ Competition Report: Finding Universal Jailbreak Backdoors in Aligned  LLMs](https://arxiv.org//abs/2404.14461)

	Javier Rando, Francesco Croce, Kryštof Mitka, Stepan Shabalin, Maksym Andriushchenko, Nicolas Flammarion, Florian Tramèr


+ [ The Adversarial AI-Art: Understanding, Generation, Detection, and  Benchmarking](https://arxiv.org//abs/2404.14581)

	Yuying Li, Zeyan Liu, Junyi Zhao, Liangqin Ren, Fengjun Li, Jiebo Luo, Bo Luo


+ [ Insufficient Statistics Perturbation: Stable Estimators for Private  Least Squares](https://arxiv.org//abs/2404.15409)

	Gavin Brown, Jonathan Hayase, Samuel Hopkins, Weihao Kong, Xiyang Liu, Sewoong Oh, Juan C. Perdomo, Adam Smith



# 2024-04-22
+ [ Protecting Your LLMs with Information Bottleneck](https://arxiv.org//abs/2404.13968)

	Zichuan Liu, Zefan Wang, Linjie Xu, Jinyu Wang, Lei Song, Tianchun Wang, Chunlin Chen, Wei Cheng, Jiang Bian


+ [ Detecting and Mitigating Hallucination in Large Vision Language Models  via Fine-Grained AI Feedback](https://arxiv.org//abs/2404.14233)

	Wenyi Xiao, Ziwei Huang, Leilei Gan, Wanggui He, Haoyuan Li, Zhelun Yu, Hao Jiang, Fei Wu, Linchao Zhu


+ [ Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by  Simulating Documents in the Wild via Low-level Perturbations](https://arxiv.org//abs/2404.13948)

	Sukmin Cho, Soyeong Jeong, Jeongyeon Seo, Taeho Hwang, Jong C. Park


+ [ Zero-shot Cross-lingual Stance Detection via Adversarial Language  Adaptation](https://arxiv.org//abs/2404.14339)

	Bharathi A, Arkaitz Zubiaga


+ [ Swap It Like Its Hot: Segmentation-based spoof attacks on eye-tracking  images](https://arxiv.org//abs/2404.13827)

	Anish S. Narkar, Brendan David-John


+ [ FreqBlender: Enhancing DeepFake Detection by Blending Frequency  Knowledge](https://arxiv.org//abs/2404.13872)

	Hanzhe Li, Jiaran Zhou, Bin Li, Junyu Dong, Yuezun Li


+ [ CloudFort: Enhancing Robustness of 3D Point Cloud Classification Against  Backdoor Attacks via Spatial Partitioning and Ensemble Prediction](https://arxiv.org//abs/2404.14042)

	Wenhao Lan, Yijun Yang, Haihua Shen, Shan Li


+ [ Towards Better Adversarial Purification via Adversarial Denoising  Diffusion Training](https://arxiv.org//abs/2404.14309)

	Yiming Liu, Kezhao Liu, Yao Xiao, Ziyi Dong, Xiaogang Xu, Pengxu Wei, Liang Lin


+ [ Improving Group Robustness on Spurious Correlation Requires Preciser  Group Inference](https://arxiv.org//abs/2404.13815)

	Yujin Han, Difan Zou


+ [ Distributional Black-Box Model Inversion Attack with Multi-Agent  Reinforcement Learning](https://arxiv.org//abs/2404.13860)

	Huan Bao, Kaimin Wei, Yongdong Wu, Jin Qian, Robert H. Deng


+ [ Explicit Lipschitz Value Estimation Enhances Policy Robustness Against  Perturbation](https://arxiv.org//abs/2404.13879)

	Xulin Chen, Ruipeng Liu, Garrett E. Katz


+ [ Dual Model Replacement:invisible Multi-target Backdoor Attack based on  Federal Learning](https://arxiv.org//abs/2404.13946)

	Rong Wang, Guichen Zhou, Mingjun Gao, Yunpeng Xiao


+ [ Poisoning Attacks on Federated Learning-based Wireless Traffic  Prediction](https://arxiv.org//abs/2404.14389)

	Zifan Zhang, Minghong Fang, Jiayuan Huang, Yuchen Liu


+ [ A mean curvature flow arising in adversarial training](https://arxiv.org//abs/2404.14402)

	Leon Bungert, Tim Laux, Kerrek Stinson


+ [ Offensive AI: Enhancing Directory Brute-forcing Attack with the Use of  Language Models](https://arxiv.org//abs/2404.14138)

	Alberto Castagnaro, Mauro Conti, Luca Pajola



# 2024-04-21
+ [ Reliable Model Watermarking: Defending Against Theft without  Compromising on Evasion](https://arxiv.org//abs/2404.13518)

	Hongyu Zhu, Sichu Liang, Wentao Hu, Fangqi Li, Ju Jia, Shilin Wang


+ [ FedMPQ: Secure and Communication-Efficient Federated Learning with  Multi-codebook Product Quantization](https://arxiv.org//abs/2404.13575)

	Xu Yang, Jiapeng Zhang, Qifeng Zhang, Zhuo Tang


+ [ Interval Abstractions for Robust Counterfactual Explanations](https://arxiv.org//abs/2404.13736)

	Junqi Jiang, Francesco Leofante, Antonio Rago, Francesca Toni


+ [ Towards General Conceptual Model Editing via Adversarial Representation  Engineering](https://arxiv.org//abs/2404.13752)

	Yihao Zhang, Zeming Wei, Jun Sun, Meng Sun


+ [ Trojan Detection in Large Language Models: Insights from The Trojan  Detection Challenge](https://arxiv.org//abs/2404.13660)

	Narek Maloyan, Ekansh Verma, Bulat Nutfullin, Bislan Ashinov


+ [ Attack on Scene Flow using Point Clouds](https://arxiv.org//abs/2404.13621)

	Haniyeh Ehsani Oskouie, Mohammad-Shahram Moin, Shohreh Kasaei


+ [ Mean Aggregator Is More Robust Than Robust Aggregators Under Label  Poisoning Attacks](https://arxiv.org//abs/2404.13647)

	Jie Peng, Weiyu Li, Qing Ling


+ [ LLMs in Web-Development: Evaluating LLM-Generated PHP code unveiling  vulnerabilities and limitations](https://arxiv.org//abs/2404.14459)

	Rebeka Tóth, Tamas Bisztray, László Erdodi


+ [ Robust EEG-based Emotion Recognition Using an Inception and Two-sided  Perturbation Model](https://arxiv.org//abs/2404.15373)

	Shadi Sartipi, Mujdat Cetin


+ [ AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs](https://arxiv.org//abs/2404.16873)

	Anselm Paulus, Arman Zharmagambetov, Chuan Guo, Brandon Amos, Yuandong Tian


# 2024-04-20
+ [ Pixel is a Barrier: Diffusion Models Are More Adversarially Robust Than  We Think](https://arxiv.org//abs/2404.13320)

	Haotian Xue, Yongxin Chen


+ [ AdvLoRA: Adversarial Low-Rank Adaptation of Vision-Language Models](https://arxiv.org//abs/2404.13425)

	Yuheng Ji, Yue Liu, Zhicheng Zhang, Zhao Zhang, Yuting Zhao, Gang Zhou, Xingwei Zhang, Xinwang Liu, Xiaolong Zheng


+ [ PristiQ: A Co-Design Framework for Preserving Data Security of Quantum  Learning in the Cloud](https://arxiv.org//abs/2404.13475)

	Zhepeng Wang, Yi Sheng, Nirajan Koirala, Kanad Basu, Taeho Jung, Cheng-Chang Lu, Weiwen Jiang


+ [ Beyond Score Changes: Adversarial Attack on No-Reference Image Quality  Assessment from Two Perspectives](https://arxiv.org//abs/2404.13277)

	Chenxi Yang, Yujia Liu, Dingquan Li, Yan Zhong, Tingting Jiang


+ [ Backdoor Attacks and Defenses on Semantic-Symbol Reconstruction in  Semantic Communications](https://arxiv.org//abs/2404.13279)

	Yuan Zhou, Rose Qingyang Hu, Yi Qian



+ [ Generative Subspace Adversarial Active Learning for Outlier Detection in  Multiple Views of High-dimensional Data](https://arxiv.org//abs/2404.14451)

	Jose Cribeiro-Ramallo, Vadim Arzamasov, Federico Matteucci, Denis Wambold, Klemens Böhm


# 2024-04-19
+ [ How Real Is Real? A Human Evaluation Framework for Unrestricted  Adversarial Examples](https://arxiv.org//abs/2404.12653)

	Dren Fazlija, Arkadij Orlov, Johanna Schrader, Monty-Maximilian Zühlke, Michael Rohs, Daniel Kudenko


+ [ A Clean-graph Backdoor Attack against Graph Convolutional Networks with  Poisoned Label Only](https://arxiv.org//abs/2404.12704)

	Jiazhu Dai, Haoyu Sun


+ [ AED-PADA:Improving Generalizability of Adversarial Example Detection via  Principal Adversarial Domain Adaptation](https://arxiv.org//abs/2404.12635)

	Heqi Peng, Yunhong Wang, Ruijie Yang, Beichen Li, Rui Wang, Yuanfang Guo


+ [ MLSD-GAN -- Generating Strong High Quality Face Morphing Attacks using  Latent Semantic Disentanglement](https://arxiv.org//abs/2404.12679)

	Aravinda Reddy PN, Raghavendra Ramachandra, Krothapalli Sreenivasa Rao, Pabitra Mitra


+ [ PATE-TripleGAN: Privacy-Preserving Image Synthesis with Gaussian  Differential Privacy](https://arxiv.org//abs/2404.12730)

	Zepeng Jiang, Weiwei Ni, Yifan Zhang


+ [ Robust CLIP-Based Detector for Exposing Diffusion Model-Generated Images](https://arxiv.org//abs/2404.12908)

	Santosh, Li Lin, Irene Amerini, Xin Wang, Shu Hu


+ [ SA-Attack: Speed-adaptive stealthy adversarial attack on trajectory  prediction](https://arxiv.org//abs/2404.12612)

	Huilin Yin, Jiaxiang Li, Pengju Zhen, Jun Yan


+ [ LSP Framework: A Compensatory Model for Defeating Trigger Reverse  Engineering via Label Smoothing Poisoning](https://arxiv.org//abs/2404.12852)

	Beichen Li, Yuanfang Guo, Heqi Peng, Yangxi Li, Yunhong Wang


+ [ Defending against Data Poisoning Attacks in Federated Learning via User  Elimination](https://arxiv.org//abs/2404.12778)

	Nick Galanis


+ [ The Power of Words: Generating PowerShell Attacks from Natural Language](https://arxiv.org//abs/2404.12893)

	Pietro Liguori, Christian Marescalco, Roberto Natella, Vittorio Orbinato, Luciano Pianese


+ [ Physical Backdoor Attack can Jeopardize Driving with  Vision-Large-Language Models](https://arxiv.org//abs/2404.12916)

	Zhenyang Ni, Rui Ye, Yuxi Wei, Zhen Xiang, Yanfeng Wang, Siheng Chen


+ [ Privacy-Preserving Debiasing using Data Augmentation and Machine  Unlearning](https://arxiv.org//abs/2404.13194)

	Zhixin Pan, Emma Andrews, Laura Chang, Prabhat Mishra


+ [ DeepFake-O-Meter v2.0: An Open Platform for DeepFake Detection](https://arxiv.org//abs/2404.13146)

	Shuwei Hou, Yan Ju, Chengzhe Sun, Shan Jia, Lipeng Ke, Riky Zhou, Anita Nikolich, Siwei Lyu


+ [ CyberSecEval 2: A Wide-Ranging Cybersecurity Evaluation Suite for Large  Language Models](https://arxiv.org//abs/2404.13161)

	Manish Bhatt, Sahana Chennabasappa, Yue Li, Cyrus Nikolaidis, Daniel Song, Shengye Wan, Faizan Ahmad, Cornelius Aschermann, Yaohui Chen, Dhaval Kapil, David Molnar, Spencer Whitman, Joshua Saxe


# 2024-04-18
+ [ Fortify the Guardian, Not the Treasure: Resilient Adversarial Detectors](https://arxiv.org//abs/2404.12120)

	Raz Lapid, Almog Dubin, Moshe Sipper


+ [ Proteus: Preserving Model Confidentiality during Graph Optimizations](https://arxiv.org//abs/2404.12512)

	Yubo Gao, Maryam Haghifam, Christina Giannoula, Renbo Tu, Gennady Pekhimenko, Nandita Vijaykumar


+ [ Introducing v0.5 of the AI Safety Benchmark from MLCommons](https://arxiv.org//abs/2404.12241)

	Bertie Vidgen, Adarsh Agrawal, Ahmed M. Ahmed, Victor Akinwande, Namir Al-Nuaimi, Najla Alfaraj, Elie Alhajjar, Lora Aroyo, Trupti Bavalatti, Borhane Blili-Hamelin, Kurt Bollacker, Rishi Bomassani, Marisa Ferrara Boston, Siméon Campos, Kal Chakra, Canyu Chen, Cody Coleman, Zacharie Delpierre Coudert, Leon Derczynski, Debojyoti Dutta, Ian Eisenberg, James Ezick, Heather Frase, Brian Fuller, Ram Gandikota, Agasthya Gangavarapu, Ananya Gangavarapu, James Gealy, Rajat Ghosh, James Goel, Usman Gohar, Sujata Goswami, Scott A. Hale, Wiebke Hutiri, Joseph Marvin Imperial, Surgan Jandial, Nick Judd, Felix Juefei-Xu, Foutse Khomh, Bhavya Kailkhura, Hannah Rose Kirk, Kevin Klyman, Chris Knotz, Michael Kuchnik, Shachi H. Kumar, Chris Lengerich, Bo Li, Zeyi Liao, Eileen Peters Long, Victor Lu, Yifan Mai, et al. (46 additional authors not shown)


+ [ Advancing the Robustness of Large Language Models through Self-Denoised  Smoothing](https://arxiv.org//abs/2404.12274)

	Jiabao Ji, Bairu Hou, Zhen Zhang, Guanhua Zhang, Wenqi Fan, Qing Li, Yang Zhang, Gaowen Liu, Sijia Liu, Shiyu Chang


+ [ Enhance Robustness of Language Models Against Variation Attack through  Graph Integration](https://arxiv.org//abs/2404.12014)

	Zi Xiong, Lizhi Qing, Yangyang Kang, Jiawei Liu, Hongsong Li, Changlong Sun, Xiaozhong Liu, Wei Lu


+ [ Uncovering Safety Risks in Open-source LLMs through Concept Activation  Vector](https://arxiv.org//abs/2404.12038)

	Zhihao Xu, Ruixuan Huang, Xiting Wang, Fangzhao Wu, Jing Yao, Xing Xie


+ [ Utilizing Adversarial Examples for Bias Mitigation and Accuracy  Enhancement](https://arxiv.org//abs/2404.11819)

	Pushkar Shukla, Dhruv Srikanth, Lee Cohen, Matthew Turk


+ [ FedMID: A Data-Free Method for Using Intermediate Outputs as a Defense  Mechanism Against Poisoning Attacks in Federated Learning](https://arxiv.org//abs/2404.11905)

	Sungwon Han, Hyeonho Song, Sungwon Park, Meeyoung Cha


+ [ A Mean-Field Analysis of Neural Gradient Descent-Ascent: Applications to  Functional Conditional Moment Equations](https://arxiv.org//abs/2404.12312)

	Yuchen Zhu, Yufeng Zhang, Zhaoran Wang, Zhuoran Yang, Xiaohong Chen


+ [ KDk: A Defense Mechanism Against Label Inference Attacks in Vertical  Federated Learning](https://arxiv.org//abs/2404.12369)

	Marco Arazzi, Serena Nicolazzo, Antonino Nocera



# 2024-04-17
+ [ TransLinkGuard: Safeguarding Transformer Models Against Model Stealing  in Edge Deployment](https://arxiv.org//abs/2404.11121)

	Qinfeng Li, Zhiqiang Shen, Zhenghan Qin, Yangfan Xie, Xuhong Zhang, Tianyu Du, Jianwei Yin


+ [ Sampling-based Pseudo-Likelihood for Membership Inference Attacks](https://arxiv.org//abs/2404.11262)

	Masahiro Kaneko, Youmi Ma, Yuki Wata, Naoaki Okazaki


+ [ A Federated Learning Approach to Privacy Preserving Offensive Language  Identification](https://arxiv.org//abs/2404.11470)

	Marcos Zampieri, Damith Premasiri, Tharindu Ranasinghe


+ [ GenFighter: A Generative and Evolutive Textual Attack Removal](https://arxiv.org//abs/2404.11538)

	Md Athikul Islam, Edoardo Serra, Sushil Jajodia


+ [ The Victim and The Beneficiary: Exploiting a Poisoned Model to Train a  Clean Model on Poisoned Data](https://arxiv.org//abs/2404.11265)

	Zixuan Zhu, Rui Wang, Cong Zou, Lihua Jing


+ [ Detector Collapse: Backdooring Object Detection to Catastrophic Overload  or Blindness](https://arxiv.org//abs/2404.11357)

	Hangtao Zhang, Shengshan Hu, Yichen Wang, Leo Yu Zhang, Ziqi Zhou, Xianlong Wang, Yanjun Zhang, Chao Chen


+ [ Clipped SGD Algorithms for Privacy Preserving Performative Prediction:  Bias Amplification and Remedies](https://arxiv.org//abs/2404.10995)

	Qiang Li, Michal Yemini, Hoi-To Wai


+ [ Exploring DNN Robustness Against Adversarial Attacks Using Approximate  Multipliers](https://arxiv.org//abs/2404.11665)

	Mohammad Javad Askarizadeh, Ebrahim Farahmand, Jorge Castro-Godinez, Ali Mahani, Laura Cabrera-Quiros, Carlos Salazar-Garcia


+ [ A Secure and Trustworthy Network Architecture for Federated Learning  Healthcare Applications](https://arxiv.org//abs/2404.11698)

	Antonio Boiano, Marco Di Gennaro, Luca Barbieri, Michele Carminati, Monica Nicoli, Alessandro Redondi, Stefano Savazzi, Albert Sund Aillet, Diogo Reis Santos, Luigi Serio



# 2024-04-16
+ [ Private Attribute Inference from Images with Vision-Language Models](https://arxiv.org//abs/2404.10618)

	Batuhan Tömekçe, Mark Vero, Robin Staab, Martin Vechev


+ [ Towards a Novel Perspective on Adversarial Examples Driven by Frequency](https://arxiv.org//abs/2404.10202)

	Zhun Zhang, Yi Zeng, Qihe Liu, Shijie Zhou


+ [ Unveiling the Misuse Potential of Base Large Language Models via  In-Context Learning](https://arxiv.org//abs/2404.10552)

	Xiao Wang, Tianze Chen, Xianjun Yang, Qi Zhang, Xun Zhao, Dahua Lin


+ [ Self-playing Adversarial Language Game Enhances LLM Reasoning](https://arxiv.org//abs/2404.10642)

	Pengyu Cheng, Tianhao Hu, Han Xu, Zhisong Zhang, Yong Dai, Lei Han, Nan Du


+ [ Efficiently Adversarial Examples Generation for Visual-Language Models  under Targeted Transfer Scenarios using Diffusion Models](https://arxiv.org//abs/2404.10335)

	Qi Guo, Shanmin Pang, Xiaojun Jia, Qing Guo


+ [ Adversarial Identity Injection for Semantic Face Image Synthesis](https://arxiv.org//abs/2404.10408)

	Giuseppe Tarollo, Tomaso Fontanini, Claudio Ferrari, Guido Borghi, Andrea Prati


+ [ Do Counterfactual Examples Complicate Adversarial Training?](https://arxiv.org//abs/2404.10588)

	Eric Yeats, Cameron Darwin, Eduardo Ortega, Frank Liu, Hai Li


+ [ Nearly Optimal Algorithms for Contextual Dueling Bandits from  Adversarial Feedback](https://arxiv.org//abs/2404.10776)

	Qiwei Di, Jiafan He, Quanquan Gu


+ [ Differentially Private Optimization with Sparse Gradients](https://arxiv.org//abs/2404.10881)

	Badih Ghazi, Cristóbal Guzmán, Pritish Kamath, Ravi Kumar, Pasin Manurangsi



# 2024-04-15
+ [ Privacy at a Price: Exploring its Dual Impact on AI Fairness](https://arxiv.org//abs/2404.09391)

	Mengmeng Yang, Ming Ding, Youyang Qu, Wei Ni, David Smith, Thierry Rakotoarivelo


+ [ Watermark-embedded Adversarial Examples for Copyright Protection against  Diffusion Models](https://arxiv.org//abs/2404.09401)

	Peifei Zhu, Tsubasa Takahashi, Hirokatsu Kataoka


+ [ Improving Weakly-Supervised Object Localization Using Adversarial  Erasing and Pseudo Label](https://arxiv.org//abs/2404.09475)

	Byeongkeun Kang, Sinhae Cha, Yeejin Lee


+ [ Beyond Noise: Privacy-Preserving Decentralized Learning with Virtual  Nodes](https://arxiv.org//abs/2404.09536)

	Sayan Biswas, Mathieu Even, Anne-Marie Kermarrec, Laurent Massoulie, Rafael Pires, Rishi Sharma, Martijn de Vos


+ [ Privacy-Preserving Intrusion Detection using Convolutional Neural  Networks](https://arxiv.org//abs/2404.09625)

	Martin Kodys, Zhongmin Dai, Vrizlynn L. L. Thing


+ [ Mitigating the Curse of Dimensionality for Certified Robustness via Dual  Randomized Smoothing](https://arxiv.org//abs/2404.09586)

	Song Xia, Yu Yi, Xudong Jiang, Henghui Ding


+ [ Ti-Patch: Tiled Physical Adversarial Patch for no-reference video  quality metrics](https://arxiv.org//abs/2404.09961)

	Victoria Leonenkova, Ekaterina Shumitskaya, Anastasia Antsiferova, Dmitriy Vatolin


+ [ On the Efficiency of Privacy Attacks in Federated Learning](https://arxiv.org//abs/2404.09430)

	Nawrin Tabassum, Ka-Ho Chow, Xuyu Wang, Wenbin Zhang, Yanzhao Wu


+ [ Privacy-Preserving Federated Unlearning with Certified Client Removal](https://arxiv.org//abs/2404.09724)

	Ziyao Liu, Huanyi Ye, Yu Jiang, Jiyuan Shen, Jiale Guo, Ivan Tjuawinata, Kwok-Yan Lam


+ [ Deceiving to Enlighten: Coaxing LLMs to Self-Reflection for Enhanced  Bias Detection and Mitigation](https://arxiv.org//abs/2404.10160)

	Ruoxi Cheng, Haoxuan Ma, Shuirong Cao


+ [ AIGeN: An Adversarial Approach for Instruction Generation in VLN](https://arxiv.org//abs/2404.10054)

	Niyati Rawal, Roberto Bigazzi, Lorenzo Baraldi, Rita Cucchiara


+ [ Black-box Adversarial Transferability: An Empirical Study in  Cybersecurity Perspective](https://arxiv.org//abs/2404.10796)

	Khushnaseeb Roshan, Aasim Zafar


# 2024-04-14
+ [ Make Split, not Hijack: Preventing Feature-Space Hijacking Attacks in  Split Learning](https://arxiv.org//abs/2404.09265)

	Tanveer Khan, Mindaugas Budzys, Antonis Michalas


+ [ FaceCat: Enhancing Face Recognition Security with a Unified Generative  Model Framework](https://arxiv.org//abs/2404.09193)

	Jiawei Chen, Xiao Yang, Yinpeng Dong, Hang Su, Jianteng Peng, Zhaoxia Yin


+ [ Adversarial Robustness Limits via Scaling-Law and Human-Alignment  Studies](https://arxiv.org//abs/2404.09349)

	Brian R. Bartoldson, James Diffenderfer, Konstantinos Parasyris, Bhavya Kailkhura


# 2024-04-13
+ [ Proof-of-Learning with Incentive Security](https://arxiv.org//abs/2404.09005)

	Zishuo Zhao, Zhixuan Fang, Xuechao Wang, Yuan Zhou


+ [ CodeCloak: A Method for Evaluating and Mitigating Code Leakage by LLM  Code Assistants](https://arxiv.org//abs/2404.09066)

	Amit Finkman, Eden Bar-Kochva, Avishag Shapira, Dudu Mimran, Yuval Elovici, Asaf Shabtai


+ [ Stability and Generalization in Free Adversarial Training](https://arxiv.org//abs/2404.08980)

	Xiwei Cheng, Kexin Fu, Farzan Farnia


+ [ Multimodal Attack Detection for Action Recognition Models](https://arxiv.org//abs/2404.10790)

	Furkan Mumcu, Yasin Yilmaz



# 2024-04-12
+ [ A Survey of Neural Network Robustness Assessment in Image Recognition](https://arxiv.org//abs/2404.08285)

	Jie Wang, Jun Ai, Minyan Lu, Haoran Su, Dan Yu, Yutao Zhang, Junda Zhu, Jingyu Liu


+ [ Adversarial Imitation Learning via Boosting](https://arxiv.org//abs/2404.08513)

	Jonathan D. Chang, Dhruv Sreenivas, Yingbing Huang, Kianté Brantley, Wen Sun


+ [ VertAttack: Taking advantage of Text Classifiers' horizontal vision](https://arxiv.org//abs/2404.08538)

	Jonathan Rusert


+ [ Practical Region-level Attack against Segment Anything Models](https://arxiv.org//abs/2404.08255)

	Yifan Shen, Zhengyuan Li, Gang Wang


+ [ Struggle with Adversarial Defense? Try Diffusion](https://arxiv.org//abs/2404.08273)

	Yujie Li, Yanbin Wang, Haitao xu, Bin Liu, Jianguo Sun, Zhenhao Guo, Wenrui Ma


+ [ Counterfactual Explanations for Face Forgery Detection via Adversarial  Removal of Artifacts](https://arxiv.org//abs/2404.08341)

	Yang Li, Songlin Yang, Wei Wang, Ziwen He, Bo Peng, Jing Dong


+ [ Joint Physical-Digital Facial Attack Detection Via Simulating Spoofing  Clues](https://arxiv.org//abs/2404.08450)

	Xianhua He, Dashuang Liang, Song Yang, Zhanlong Hao, Hui Ma, Binjie Mao, Xi Li, Yao Wang, Pengfei Yan, Ajian Liu


+ [ On the Robustness of Language Guidance for Low-Level Vision Tasks:  Findings from Depth Estimation](https://arxiv.org//abs/2404.08540)

	Agneet Chatterjee, Tejas Gokhale, Chitta Baral, Yezhou Yang


+ [ Anti-Byzantine Attacks Enabled Vehicle Selection for Asynchronous  Federated Learning in Vehicular Edge Computing](https://arxiv.org//abs/2404.08444)

	Cui Zhang, Xiao Xu, Qiong Wu, Pingyi Fan, Qiang Fan, Huiling Zhu, Jiangzhou Wang


+ [ FCert: Certifiably Robust Few-Shot Classification in the Era of  Foundation Models](https://arxiv.org//abs/2404.08631)

	Yanting Wang, Wei Zou, Jinyuan Jia


+ [ LazyDP: Co-Designing Algorithm-Software for Scalable Training of  Differentially Private Recommendation Models](https://arxiv.org//abs/2404.08847)

	Juntaek Lim, Youngeun Kwon, Ranggi Hwang, Kiwan Maeng, G. Edward Suh, Minsoo Rhu


+ [ PASA: Attack Agnostic Unsupervised Adversarial Detection using  Prediction & Attribution Sensitivity Analysis](https://arxiv.org//abs/2404.10789)

	Dipkamal Bhusal, Md Tanvirul Alam, Monish K. Veerabhadran, Michael Clifford, Sara Rampazzi, Nidhi Rastogi


# 2024-04-11
+ [ Differentially Private GANs for Generating Synthetic Indoor Location  Data](https://arxiv.org//abs/2404.07366)

	Vahideh Moghtadaiee, Mina Alishahi, Milad Rabiei


+ [ Differentially Private Reinforcement Learning with Self-Play](https://arxiv.org//abs/2404.07559)

	Dan Qiao, Yu-Xiang Wang


+ [ Fragile Model Watermark for integrity protection: leveraging boundary  volatility and sensitive sample-pairing](https://arxiv.org//abs/2404.07572)

	ZhenZhe Gao, Zhenjun Tang, Zhaoxia Yin, Baoyuan Wu, Yue Lu


+ [ AmpleGCG: Learning a Universal and Transferable Generative Model of  Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs](https://arxiv.org//abs/2404.07921)

	Zeyi Liao, Huan Sun


+ [ Privacy preserving layer partitioning for Deep Neural Network models](https://arxiv.org//abs/2404.07437)

	Kishore Rajasekar, Randolph Loh, Kar Wai Fok, Vrizlynn L. L. Thing


+ [ Enhancing Network Intrusion Detection Performance using Generative  Adversarial Networks](https://arxiv.org//abs/2404.07464)

	Xinxing Zhao, Kar Wai Fok, Vrizlynn L. L. Thing


+ [ Backdoor Contrastive Learning via Bi-level Trigger Optimization](https://arxiv.org//abs/2404.07863)

	Weiyu Sun, Xinyu Zhang, Hao Lu, Yingcong Chen, Ting Wang, Jinghui Chen, Lu Lin


+ [ Latent Guard: a Safety Framework for Text-to-image Generation](https://arxiv.org//abs/2404.08031)

	Runtao Liu, Ashkan Khakzar, Jindong Gu, Qifeng Chen, Philip Torr, Fabio Pizzati


+ [ LLM Agents can Autonomously Exploit One-day Vulnerabilities](https://arxiv.org//abs/2404.08144)

	Richard Fang, Rohan Bindu, Akul Gupta, Daniel Kang


+ [ Persistent Classification: A New Approach to Stability of Data and  Adversarial Examples](https://arxiv.org//abs/2404.08069)

	Brian Bell, Michael Geyer, David Glickenstein, Keaton Hamm, Carlos Scheidegger, Amanda Fernandez, Juston Moore


+ [ Eliminating Catastrophic Overfitting Via Abnormal Adversarial Examples  Regularization](https://arxiv.org//abs/2404.08154)

	Runqi Lin, Chaojian Yu, Tongliang Liu


+ [ CodeFort: Robust Training for Code Generation Models](https://arxiv.org//abs/2405.01567)

	Yuhao Zhang, Shiqi Wang, Haifeng Qian, Zijian Wang, Mingyue Shang, Linbo Liu, Sanjay Krishna Gouda, Baishakhi Ray, Murali Krishna Ramanathan, Xiaofei Ma, Anoop Deoras


# 2024-04-10
+ [ Towards a Game-theoretic Understanding of Explanation-based Membership  Inference Attacks](https://arxiv.org//abs/2404.07139)

	Kavita Kumari, Murtuza Jadliwala, Sumit Kumar Jha, Anindya Maiti


+ [ SafeGen: Mitigating Unsafe Content Generation in Text-to-Image Models](https://arxiv.org//abs/2404.06666)

	Xinfeng Li, Yuchen Yang, Jiangyi Deng, Chen Yan, Yanjiao Chen, Xiaoyu Ji, Wenyuan Xu


+ [ How to Craft Backdoors with Unlabeled Data Alone?](https://arxiv.org//abs/2404.06694)

	Yifei Wang, Wenhan Ma, Yisen Wang


+ [ Logit Calibration and Feature Contrast for Robust Federated Learning on  Non-IID Data](https://arxiv.org//abs/2404.06776)

	Yu Qiao, Chaoning Zhang, Apurba Adhikary, Choong Seon Hong


+ [ Adversarial purification for no-reference image-quality metrics:  applicability study and new methods](https://arxiv.org//abs/2404.06957)

	Aleksandr Gushchin, Anna Chistyakova, Vladislav Minashkin, Anastasia Antsiferova, Dmitriy Vatolin


+ [ Simpler becomes Harder: Do LLMs Exhibit a Coherent Behavior on  Simplified Corpora?](https://arxiv.org//abs/2404.06838)

	Miriam Anschütz, Edoardo Mosca, Georg Groh


+ [ Poisoning Prevention in Federated Learning and Differential Privacy via  Stateful Proofs of Execution](https://arxiv.org//abs/2404.06721)

	Norrathep Rattanavipanon, Ivan de Oliviera Nunes



# 2024-04-09
+ [ Differential Privacy for Anomaly Detection: Analyzing the Trade-off  Between Privacy and Explainability](https://arxiv.org//abs/2404.06144)

	Fatima Ezzeddine, Mirna Saad, Omran Ayoub, Davide Andreoletti, Martin Gjoreski, Ihab Sbeity, Marc Langheinrich, Silvia Giordano


+ [ LRR: Language-Driven Resamplable Continuous Representation against  Adversarial Tracking Attacks](https://arxiv.org//abs/2404.06247)

	Jianlang Chen, Xuhong Ren, Qing Guo, Felix Juefei-Xu, Di Lin, Wei Feng, Lei Ma, Jianjun Zhao


+ [ On adversarial training and the 1 Nearest Neighbor classifier](https://arxiv.org//abs/2404.06313)

	Amir Hagai, Yair Weiss


+ [ Towards Robust Domain Generation Algorithm Classification](https://arxiv.org//abs/2404.06236)

	Arthur Drichel, Marc Meyer, Ulrike Meyer


+ [ Privacy-preserving Scanpath Comparison for Pervasive Eye Tracking](https://arxiv.org//abs/2404.06216)

	Suleyman Ozdel, Efe Bozkir, Enkelejda Kasneci


+ [ Sandwich attack: Multi-language Mixture Adaptive Attack on LLMs](https://arxiv.org//abs/2404.07242)

	Bibek Upadhayay, Vahid Behzadan


+ [ Towards Building a Robust Toxicity Predictor](https://arxiv.org//abs/2404.08690)

	Dmitriy Bespalov, Sourav Bhabesh, Yi Xiang, Liutong Zhou, Yanjun Qi



# 2024-04-08
+ [ SoK: Gradient Leakage in Federated Learning](https://arxiv.org//abs/2404.05403)

	Jiacheng Du, Jiahui Hu, Zhibo Wang, Peng Sun, Neil Zhenqiang Gong, Kui Ren


+ [ Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data](https://arxiv.org//abs/2404.05530)

	Tim Baumgärtner, Yang Gao, Dana Alon, Donald Metzler


+ [ Investigating the Impact of Quantization on Adversarial Robustness](https://arxiv.org//abs/2404.05639)

	Qun Li, Yuan Meng, Chen Tang, Jiacheng Jiang, Zhi Wang


+ [ David and Goliath: An Empirical Evaluation of Attacks and Defenses for  QNNs at the Deep Edge](https://arxiv.org//abs/2404.05688)

	Miguel Costa, Sandro Pinto


+ [ Semantic Stealth: Adversarial Text Attacks on NLP Using Several Methods](https://arxiv.org//abs/2404.05159)

	Roopkatha Dey, Aivy Debnath, Sayak Kumar Dutta, Kaustav Ghosh, Arijit Mitra, Arghya Roy Chowdhury, Jaydip Sen


+ [ Out-of-Distribution Data: An Acquaintance of Adversarial Examples -- A  Survey](https://arxiv.org//abs/2404.05219)

	Naveen Karunanayake, Ravin Gunawardena, Suranga Seneviratne, Sanjay Chawla


+ [ BruSLeAttack: A Query-Efficient Score-Based Black-Box Sparse Adversarial  Attack](https://arxiv.org//abs/2404.05311)

	Viet Quoc Vo, Ehsan Abbasnejad, Damith C. Ranasinghe


+ [ Certified PEFTSmoothing: Parameter-Efficient Fine-Tuning with Randomized  Smoothing](https://arxiv.org//abs/2404.05350)

	Chengyan Fu, Wenjie Wang


+ [ Flexible Fairness Learning via Inverse Conditional Permutation](https://arxiv.org//abs/2404.05678)

	Yuheng Lai, Leying Guan


+ [ Enabling Privacy-Preserving Cyber Threat Detection with Federated  Learning](https://arxiv.org//abs/2404.05130)

	Yu Bi, Yekai Li, Xuan Feng, Xianghang Mi


+ [ Negative Preference Optimization: From Catastrophic Collapse to  Effective Unlearning](https://arxiv.org//abs/2404.05868)

	Ruiqi Zhang, Licong Lin, Yu Bai, Song Mei


+ [ Eraser: Jailbreaking Defense in Large Language Models via Unlearning  Harmful Knowledge](https://arxiv.org//abs/2404.05880)

	Weikai Lu, Ziqian Zeng, Jianwei Wang, Zhengdong Lu, Zelin Chen, Huiping Zhuang, Cen Chen


+ [ Privacy-Preserving Deep Learning Using Deformable Operators for Secure  Task Learning](https://arxiv.org//abs/2404.05828)

	Fabian Perez, Jhon Lopez, Henry Arguello


+ [ Quantum Adversarial Learning for Kernel Methods](https://arxiv.org//abs/2404.05824)

	Giuseppe Montalbano, Leonardo Banchi


# 2024-04-07
+ [ Inference-Time Rule Eraser: Distilling and Removing Bias Rules to  Mitigate Bias in Deployed Models](https://arxiv.org//abs/2404.04814)

	Yi Zhang, Jitao Sang


+ [ Hidden You Malicious Goal Into Benigh Narratives: Jailbreak Large  Language Models through Logic Chain Injection](https://arxiv.org//abs/2404.04849)

	Zhilong Wang, Yebo Cao, Peng Liu


+ [ SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for  Clinical Trials](https://arxiv.org//abs/2404.04963)

	Mael Jullien, Marco Valentino, André Freitas


+ [ How much reliable is ChatGPT's prediction on Information Extraction  under Input Perturbations?](https://arxiv.org//abs/2404.05088)

	Ishani Mondal, Abhilasha Sancheti


+ [ Privacy-Preserving Traceable Functional Encryption for Inner Product](https://arxiv.org//abs/2404.04861)

	Muyao Qiu, Jinguang Han



# 2024-04-06
+ [ Trustless Audits without Revealing Data or Models](https://arxiv.org//abs/2404.04500)

	Suppakit Waiwitlikhit, Ion Stoica, Yi Sun, Tatsunori Hashimoto, Daniel Kang


+ [ Data Poisoning Attacks on Off-Policy Policy Evaluation Methods](https://arxiv.org//abs/2404.04714)

	Elita Lobo, Harvineet Singh, Marek Petrik, Cynthia Rudin, Himabindu Lakkaraju


+ [ D$^3$: Scaling Up Deepfake Detection by Learning from Discrepancy](https://arxiv.org//abs/2404.04584)

	Yongqi Yang, Zhihao Qian, Ye Zhu, Yu Wu


+ [ Structured Gradient-based Interpretations via Norm-Regularized  Adversarial Training](https://arxiv.org//abs/2404.04647)

	Shizhan Gong, Qi Dou, Farzan Farnia


+ [ CANEDERLI: On The Impact of Adversarial Training and Transferability on  CAN Intrusion Detection Systems](https://arxiv.org//abs/2404.04648)

	Francesco Marchiori, Mauro Conti


+ [ Goal-guided Generative Prompt Injection Attack on Large Language Models](https://arxiv.org//abs/2404.07234)

	Chong Zhang, Mingyu Jin, Qinkai Yu, Chengzhi Liu, Haochen Xue, Xiaobo Jin


+ [ ALERT: A Comprehensive Benchmark for Assessing Large Language Models'  Safety through Red Teaming](https://arxiv.org//abs/2404.08676)

	Simone Tedeschi, Felix Friedrich, Patrick Schramowski, Kristian Kersting, Roberto Navigli, Huu Nguyen, Bo Li


## 2024-04-05
+ [ Precision Guided Approach to Mitigate Data Poisoning Attacks in  Federated Learning](https://arxiv.org//abs/2404.04139)

	K Naveen Kumar, C Krishna Mohan, Aravind Machiry


+ [ Watermark-based Detection and Attribution of AI-Generated Content](https://arxiv.org//abs/2404.04254)

	Zhengyuan Jiang, Moyang Guo, Yuepeng Hu, Neil Zhenqiang Gong


+ [ Evaluating Adversarial Robustness: A Comparison Of FGSM, Carlini-Wagner  Attacks, And The Role of Distillation as Defense Mechanism](https://arxiv.org//abs/2404.04245)

	Trilokesh Ranjan Sarkar, Nilanjan Das, Pralay Sankar Maitra, Bijoy Some, Ritwik Saha, Orijita Adhikary, Bishal Bose, Jaydip Sen


+ [ Re-pseudonymization Strategies for Smart Meter Data Are Not Robust to  Deep Learning Profiling Attacks](https://arxiv.org//abs/2404.03948)

	Ana-Maria Cretu, Miruna Rusu, Yves-Alexandre de Montjoye


+ [ You Can Use But Cannot Recognize: Preserving Visual Privacy in Deep  Neural Networks](https://arxiv.org//abs/2404.04098)

	Qiushi Li, Yan Zhang, Ju Ren, Qi Li, Yaoxue Zhang


+ [ Increased LLM Vulnerabilities from Fine-tuning and Quantization](https://arxiv.org//abs/2404.04392)

	Divyanshu Kumar, Anurakt Kumar, Sahil Agarwal, Prashanth Harshangi



## 2024-04-04
+ [ Knowledge Distillation-Based Model Extraction Attack using Private  Counterfactual Explanations](https://arxiv.org/abs/2404.03348)

	Fatima Ezzeddine, Omran Ayoub, Silvia Giordano


+ [ Learn When (not) to Trust Language Models: A Privacy-Centric Adaptive  Model-Aware Approach](https://arxiv.org/abs/2404.03514)

	Chengkai Huang, Rui Wang, Kaige Xie, Tong Yu, Lina Yao


+ [ A Comparative Analysis of Word-Level Metric Differential Privacy:  Benchmarking The Privacy-Utility Trade-off](https://arxiv.org/abs/2404.03324)

	Stephen Meisenbacher, Nihildev Nandakumar, Alexandra Klymenko, Florian Matthes


+ [ Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak  Attacks?](https://arxiv.org/abs/2404.03411)

	Shuo Chen, Zhen Han, Bailan He, Zifeng Ding, Wenqian Yu, Philip Torr, Volker Tresp, Jindong Gu


+ [ Meta Invariance Defense Towards Generalizable Robustness to Unknown  Adversarial Attacks](https://arxiv.org/abs/2404.03340)

	Lei Zhang, Yuhang Zhou, Yi Yang, Xinbo Gao


+ [ Learn What You Want to Unlearn: Unlearning Inversion Attacks against  Machine Unlearning](https://arxiv.org/abs/2404.03233)

	Hongsheng Hu, Shuo Wang, Tian Dong, Minhui Xue


+ [ Privacy-Enhancing Technologies for Artificial Intelligence-Enabled  Systems](https://arxiv.org/abs/2404.03509)

	Liv d'Aliberti, Evan Gronberg, Joseph Kovba


+ [ Accurate Low-Degree Polynomial Approximation of Non-polynomial Operators  for Fast Private Inference in Homomorphic Encryption](https://arxiv.org/abs/2404.03216)

	Jianming Tong, Jingtian Dang, Anupam Golder, Callie Hao, Arijit Raychowdhury, Tushar Krishna


## 2024-04-03
+ [ JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal  Large Language Models against Jailbreak Attacks](https://arxiv.org/abs/2404.03027)

	Weidi Luo, Siyuan Ma, Xiaogeng Liu, Xiaoyu Guo, Chaowei Xiao


+ [ Adversarial Attacks and Dimensionality in Text Classifiers](https://arxiv.org/abs/2404.02660)

	Nandish Chattopadhyay, Atreya Goswami, Anupam Chattopadhyay


+ [ Learn to Disguise: Avoid Refusal Responses in LLM's Defense via a Multi-agent Attacker-Disguiser Game](https://arxiv.org/abs/2404.02532)

	Qianqiao Xu, Zhiliang Tian, Hongyan Wu, Zhen Huang, Yiping Song, Feng Liu, Dongsheng Li


## 2024-04-02
+ [ Jailbreaking Prompt Attack: A Controllable Adversarial Attack against  Diffusion Models](https://arxiv.org/abs/2404.02928)

	Jiachen Ma, Anda Cao, Zhiqing Xiao, Jie Zhang, Chao Ye, Junbo Zhao


+ [ Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks](https://arxiv.org/abs/2404.02151)

	Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion


+ [ Humanizing Machine-Generated Content: Evading AI-Text Detection through Adversarial Attack](https://arxiv.org/abs/2404.01907)

	Ying Zhou, Ben He, Le Sun


+ [ Red-Teaming Segment Anything Model](https://arxiv.org/abs/2404.02067)

	Krzysztof Jankowski, Bartlomiej Sobieski, Mateusz Kwiatkowski, Jakub Szulc, Michal Janik, Hubert Baniecki, Przemyslaw Biecek


+ [ Towards Robust 3D Pose Transfer with Adversarial Learning](https://arxiv.org/abs/2404.02242)

	Haoyu Chen, Hao Tang, Ehsan Adeli, Guoying Zhao


+ [ Exploring Backdoor Vulnerabilities of Chat Models](https://arxiv.org/abs/2404.02406)

	Yunzhuo Hao, Wenkai Yang, Yankai Lin


## 2024-04-01
+ [ BadPart: Unified Black-box Adversarial Patch Attacks against Pixel-wise Regression Tasks](https://arxiv.org/abs/2404.00924)

	Zhiyuan Cheng, Zhaoyi Liu, Tengda Guo, Shiwei Feng, Dongfang Liu, Mingjie Tang, Xiangyu Zhang


+ [ Multi-granular Adversarial Attacks against Black-box Neural Ranking Models](https://arxiv.org/abs/2404.01574)

	Yu-An Liu, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, Xueqi Cheng


+ [ UFID: A Unified Framework for Input-level Backdoor Detection on Diffusion Models](https://arxiv.org/abs/2404.01101)

	Zihan Guan, Mengxuan Hu, Sheng Li, Anil Vullikanti


## 2024-03-31
+ [ An Unsupervised Adversarial Autoencoder for Cyber Attack Detection in  Power Distribution Grids](https://arxiv.org/abs/2404.02923)

	Mehdi Jabbari Zideh, Mohammad Reza Khalghani, Sarika Khushalani Solanki


## 2024-03-30
+ [ STBA: Towards Evaluating the Robustness of DNNs for Query-Limited Black-box Scenario](https://arxiv.org/abs/2404.00362)

	Renyang Liu, Kwok-Yan Lam, Wei Zhou, Sixing Wu, Jun Zhao, Dongting Hu, Mingming Gong


## 2024-03-29
+ [ Benchmarking the Robustness of Temporal Action Detection Models Against  Temporal Corruptions](https://arxiv.org/abs/2403.20254)

	Runhao Zeng, Xiaoyong Chen, Jiaming Liang, Huisi Wu, Guangzhong Cao, Yong Guo


## 2024-03-28
+ [ MMCert: Provable Defense against Adversarial Attacks to Multi-modal  Models](https://arxiv.org/abs/2403.19080)

	Yanting Wang, Hongye Fu, Wei Zou, Jinyuan Jia


+ [ MedBN: Robust Test-Time Adaptation against Malicious Test Samples](https://arxiv.org/abs/2403.19326)

	Hyejin Park, Jeongyeon Hwang, Sunung Mun, Sangdon Park, Jungseul Ok


+ [ Towards Understanding Dual BN In Hybrid Adversarial Training](https://arxiv.org/abs/2403.19150)

	Chenshuang Zhang, Chaoning Zhang, Kang Zhang, Axi Niu, Junmo Kim, In So Kweon


+ [ Improving Adversarial Data Collection by Supporting Annotators: Lessons  from GAHD, a German Hate Speech Dataset](https://arxiv.org/abs/2403.19559)

	Janis Goldzycher, Paul Röttger, Gerold Schneider


## 2024-03-27
+ [ Manipulating Neural Path Planners via Slight Perturbations](https://arxiv.org/abs/2403.18256)

	Zikang Xiong, Suresh Jagannathan


+ [ CosalPure: Learning Concept from Group Images for Robust Co-Saliency  Detection](https://arxiv.org/abs/2403.18554)

	Jiayi Zhu, Qing Guo, Felix Juefei-Xu, Yihao Huang, Yang Liu, Geguang Pu


+ [ Safe and Robust Reinforcement-Learning: Principles and Practice](https://arxiv.org/abs/2403.18539)

	Taku Yamagata, Raul Santos-Rodriguez


+ [ Bayesian Learned Models Can Detect Adversarial Malware For Free](https://arxiv.org/abs/2403.18309)

	Bao Gia Doan, Dang Quang Nguyen, Paul Montague, Tamas Abraham, Olivier De Vel, Seyit Camtepe, Salil S. Kanhere, Ehsan Abbasnejad, Damith C. Ranasinghe


+ [ MisGUIDE : Defense Against Data-Free Deep Learning Model Extraction](https://arxiv.org/abs/2403.18580)

	Mahendra Gurve, Sankar Behera, Satyadev Ahlawat, Yamuna Prasad


+ [ Robustness and Visual Explanation for Black Box Image, Video, and ECG  Signal Classification with Reinforcement Learning](https://arxiv.org/abs/2403.18985)

	Soumyendu Sarkar, Ashwin Ramesh Babu, Sajad Mousavi, Vineet Gundecha, Avisek Naug, Sahand Ghorbanpour


## 2024-03-26
+ [ Out-of-distribution Rumor Detection via Test-Time Adaptation](https://arxiv.org/abs/2403.17735)

	Xiang Tao, Mingqing Zhang, Qiang Liu, Shu Wu, Liang Wang


+ [ DataCook: Crafting Anti-Adversarial Examples for Healthcare Data  Copyright Protection](https://arxiv.org/abs/2403.17755)

	Sihan Shang, Jiancheng Yang, Zhenglong Sun, Pascal Fua


+ [ UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object  Detection with Sparse LiDAR and Large Domain Gaps](https://arxiv.org/abs/2403.17633)

	Maciej K Wozniak, Mattias Hansson, Marko Thiel, Patric Jensfelt


+ [ Optimization-based Prompt Injection Attack to LLM-as-a-Judge](https://arxiv.org/abs/2403.17710)

	Jiawen Shi, Zenghui Yuan, Yinuo Liu, Yue Huang, Pan Zhou, Lichao Sun, Neil Zhenqiang Gong


+ [ Physical 3D Adversarial Attacks against Monocular Depth Estimation in  Autonomous Driving](https://arxiv.org/abs/2403.17301)

	Junhao Zheng, Chenhao Lin, Jiahao Sun, Zhengyu Zhao, Qian Li, Chao Shen


+ [ Boosting Adversarial Training via Fisher-Rao Norm-based Regularization](https://arxiv.org/abs/2403.17520)

	Xiangyu Yin, Wenjie Ruan


+ [ Securing GNNs: Explanation-Based Identification of Backdoored Training  Graphs](https://arxiv.org/abs/2403.18136)

	Jane Downer, Ren Wang, Binghui Wang


+ [ Targeted Visualization of the Backbone of Encoder LLMs](https://arxiv.org/abs/2403.18872)

	Isaac Roberts, Alexander Schulz, Luca Hermes, Barbara Hammer


## 2024-03-25
+ [ $\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on  Prompt-based Language Models](https://arxiv.org/abs/2403.16432)

	Yue Xu, Wenjie Wang


+ [ The Anatomy of Adversarial Attacks: Concept-based XAI Dissection](https://arxiv.org/abs/2403.16782)

	Georgii Mikriukov, Gesina Schwalbe, Franz Motzkus, Korinna Bade


+ [ Generating Potent Poisons and Backdoors from Scratch with Guided  Diffusion](https://arxiv.org/abs/2403.16365)

	Hossein Souri, Arpit Bansal, Hamid Kazemi, Liam Fowl, Aniruddha Saha, Jonas Geiping, Andrew Gordon Wilson, Rama Chellappa, Tom Goldstein, Micah Goldblum


+ [ Ensemble Adversarial Defense via Integration of Multiple Dispersed Low  Curvature Models](https://arxiv.org/abs/2403.16405)

	Kaikang Zhao, Xi Chen, Wei Huang, Liuxin Ding, Xianglong Kong, Fan Zhang


+ [ Revealing Vulnerabilities of Neural Networks in Parameter Learning and  Defense Against Explanation-Aware Backdoors](https://arxiv.org/abs/2403.16569)

	Md Abdul Kadir, GowthamKrishna Addluri, Daniel Sonntag


+ [ CipherFormer: Efficient Transformer Private Inference with Low Round  Complexity](https://arxiv.org/abs/2403.16860)

	Weize Wang, Yi Kuang


+ [ Task-Agnostic Detector for Insertion-Based Backdoor Attacks](https://arxiv.org/abs/2403.17155)

	Weimin Lyu, Xiao Lin, Songzhu Zheng, Lu Pang, Haibin Ling, Susmit Jha, Chao Chen


+ [ LOTUS: Evasive and Resilient Backdoor Attacks through Sub-Partitioning](https://arxiv.org/abs/2403.17188)

	Siyuan Cheng, Guanhong Tao, Yingqi Liu, Guangyu Shen, Shengwei An, Shiwei Feng, Xiangzhe Xu, Kaiyuan Zhang, Shiqing Ma, Xiangyu Zhang


## 2024-03-24
+ [ Unlearning Backdoor Threats: Enhancing Backdoor Defense in Multimodal  Contrastive Learning via Local Token Unlearning](https://arxiv.org/abs/2403.16257)

	Siyuan Liang, Kuanrong Liu, Jiajun Gong, Jiawei Liang, Yuan Xun, Ee-Chien Chang, Xiaochun Cao


+ [ Robust Diffusion Models for Adversarial Purification](https://arxiv.org/abs/2403.16067)

	Guang Lin, Zerui Tao, Jianhai Zhang, Toshihisa Tanaka, Qibin Zhao


+ [ Subspace Defense: Discarding Adversarial Perturbations by Learning a  Subspace for Clean Signals](https://arxiv.org/abs/2403.16176)

	Rui Zheng, Yuhao Zhou, Zhiheng Xi, Tao Gui, Qi Zhang, Xuanjing Huang



## 2024-03-23
+ [ Adversarial Defense Teacher for Cross-Domain Object Detection under Poor  Visibility Conditions](https://arxiv.org/abs/2403.15786)

	Kaiwen Wang, Yinzhe Shen, Martin Lauer


+ [ An Embarrassingly Simple Defense Against Backdoor Attacks On SSL](https://arxiv.org/abs/2403.15918)

	Aryan Satpathy, Nilaksh, Dhruva Rajwade


## 2024-03-22
+ [ A Transfer Attack to Image Watermarks](https://arxiv.org/abs/2403.15365)

	Yuepeng Hu, Zhengyuan Jiang, Moyang Guo, Neil Gong


+ [ Clean-image Backdoor Attacks](https://arxiv.org/abs/2403.15010)

	Dazhong Rong, Shuheng Shen, Xinyi Fu, Peng Qian, Jianhai Chen, Qinming He, Xing Fu, Weiqiang Wang


+ [ Robust optimization for adversarial learning with finite sample  complexity guarantees](https://arxiv.org/abs/2403.15207)

	André Bertolace, Konstatinos Gatsis, Kostas Margellos


+ [ Twin Auto-Encoder Model for Learning Separable Representation in  Cyberattack Detection](https://arxiv.org/abs/2403.15509)

	Phai Vu Dinh, Quang Uy Nguyen, Thai Hoang Dinh, Diep N. Nguyen, Bao Son Pham, Eryk Dutkiewicz


+ [ Differentially Private Next-Token Prediction of Large Language Models](https://arxiv.org/abs/2403.15638)

	James Flemings, Meisam Razaviyayn, Murali Annavaram


## 2024-03-21
+ [ SoftPatch: Unsupervised Anomaly Detection with Noisy Data](https://arxiv.org/abs/2403.14233)

	Xi Jiang, Ying Chen, Qiang Nie, Yong Liu, Jianlin Liu, Bin-Bin Gao, Jun Liu, Chengjie Wang, Feng Zheng


+ [ Locating and Mitigating Gender Bias in Large Language Models](https://arxiv.org/abs/2403.14409)

	Yuchen Cai, Ding Cao, Rongxi Guo, Yaqin Wen, Guiquan Liu, Enhong Chen


+ [ Detoxifying Large Language Models via Knowledge Editing](https://arxiv.org/abs/2403.14472)

	Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, Huajun Chen


+ [ MMIDR: Teaching Large Language Model to Interpret Multimodal  Misinformation via Knowledge Distillation](https://arxiv.org/abs/2403.14171)

	Longzheng Wang, Xiaohan Xu, Lei Zhang, Jiarui Lu, Yongxiu Xu, Hongbo Xu, Chuang Zhang


+ [ Adversary-Robust Graph-Based Learning of WSIs](https://arxiv.org/abs/2403.14489)

	Saba Heidari Gheshlaghi, Milan Aryal, Nasim Yahyasoltani, Masoud Ganji


+ [ Rethinking Adversarial Inverse Reinforcement Learning: From the Angles  of Policy Imitation and Transferable Reward Recovery](https://arxiv.org/abs/2403.14593)

	Yangchun Zhang, Yirui Zhou


+ [ Improving the Robustness of Large Language Models via Consistency  Alignment](https://arxiv.org/abs/2403.14221)

	Zhao Yukun, Yan Lingyong, Sun Weiwei, Xing Guoliang, Wang Shuaiqiang, Meng Chong, Cheng Zhicong, Ren Zhaochun, Yin Dawei


+ [ Rethinking Adversarial Inverse Reinforcement Learning: From the Angles  of Policy Imitation and Transferable Reward Recovery](https://arxiv.org/abs/2403.14593)

	Yangchun Zhang, Yirui Zhou


+ [ Adversary-Augmented Simulation to evaluate client-fairness on  HyperLedger Fabric](https://arxiv.org/abs/2403.14342)

	Erwan Mahe, Rouwaida Abdallah, Sara Tucci-Piergiovanni, Pierre-Yves Piriou


+ [ FIT-RAG: Black-Box RAG with Factual Information and Token Reduction](https://arxiv.org/abs/2403.14374)

	Yuren Mao, Xuemei Dong, Wenyi Xu, Yunjun Gao, Bin Wei, Ying Zhang


+ [ Adversary-Robust Graph-Based Learning of WSIs](https://arxiv.org/abs/2403.14489)

	Saba Heidari Gheshlaghi, Milan Aryal, Nasim Yahyasoltani, Masoud Ganji


+ [ Safeguarding Medical Image Segmentation Datasets against Unauthorized  Training via Contour- and Texture-Aware Perturbations](https://arxiv.org/abs/2403.14250)

	Xun Lin, Yi Yu, Song Xia, Jue Jiang, Haoran Wang, Zitong Yu, Yizhong Liu, Ying Fu, Shuai Wang, Wenzhong Tang, Alex Kot


+ [ HETAL: Efficient Privacy-preserving Transfer Learning with Homomorphic  Encryption](https://arxiv.org/abs/2403.14111)

	Seewoo Lee, Garam Lee, Jung Woo Kim, Junbum Shin, Mun-Kyu Lee


+ [ Improving Robustness to Model Inversion Attacks via Sparse Coding  Architectures](https://arxiv.org/abs/2403.14772)

	Sayanton V. Dibbo, Adam Breuer, Juston Moore, Michael Teti


+ [ Protected group bias and stereotypes in Large Language Models](https://arxiv.org/abs/2403.14727)

	Hadas Kotek, David Q. Sun, Zidi Xiu, Margit Bowler, Christopher Klein


+ [ Diffusion Attack: Leveraging Stable Diffusion for Naturalistic Image  Attacking](https://arxiv.org/abs/2403.14778)

	Qianyu Guo, Jiaming Fu, Yawen Lu, Dongming Gan


## 2024-03-20
+ [ BadEdit: Backdooring large language models by model editing](https://arxiv.org/abs/2403.13355)

	Yanzhou Li, Tianlin Li, Kangjie Chen, Jian Zhang, Shangqing Liu, Wenhan Wang, Tianwei Zhang, Yang Liu


+ [ Deepfake Detection without Deepfakes: Generalization via Synthetic  Frequency Patterns Injection](https://arxiv.org/abs/2403.13479)

	Davide Alessandro Coccomini, Roberto Caldelli, Claudio Gennaro, Giuseppe Fiameni, Giuseppe Amato, Fabrizio Falchi


+ [ Have You Poisoned My Data? Defending Neural Networks against Data  Poisoning](https://arxiv.org/abs/2403.13523)

	Fabio De Gaspari, Dorjan Hitaj, Luigi V. Mancini


+ [ Adversarial Attacks and Defenses in Automated Control Systems: A  Comprehensive Benchmark](https://arxiv.org/abs/2403.13502)

	Vitaliy Pozdnyakov, Aleksandr Kovalenko, Ilya Makarov, Mikhail Drobyshevskiy, Kirill Lukyanov


+ [ Reducing Large Language Model Bias with Emphasis on 'Restricted  Industries': Automated Dataset Augmentation and Prejudice Quantification](https://arxiv.org/abs/2403.13925)

	Devam Mondal, Carlo Lipizzi



+ [ Multi-Modal Hallucination Control by Visual Information Grounding](https://arxiv.org/abs/2403.14003)

	Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro Achille, Ashwin Swaminathan, Stefano Soatto


+ [ Optimal Transport for Fairness: Archival Data Repair using Small  Research Data Sets](https://arxiv.org/abs/2403.13864)

	Abigail Langbridge, Anthony Quinn, Robert Shorten



## 2024-03-19
+ [ Electioneering the Network: Dynamic Multi-Step Adversarial Attacks for  Community Canvassing](https://arxiv.org/abs/2403.12399)

	Saurabh Sharma, Ambuj SIngh


+ [ FairSIN: Achieving Fairness in Graph Neural Networks through Sensitive  Information Neutralization](https://arxiv.org/abs/2403.12474)

	Cheng Yang, Jixi Liu, Yunhe Yan, Chuan Shi


+ [ RigorLLM: Resilient Guardrails for Large Language Models against  Undesired Content](https://arxiv.org/abs/2403.13031)

	Zhuowen Yuan, Zidi Xiong, Yi Zeng, Ning Yu, Ruoxi Jia, Dawn Song, Bo Li


+ [ Robust NAS under adversarial training: benchmark, theory, and beyond](https://arxiv.org/abs/2403.13134)

	Yongtao Wu, Fanghui Liu, Carl-Johann Simon-Gabriel, Grigorios G Chrysos, Volkan Cevher


+ [ ADAPT to Robustify Prompt Tuning Vision Transformers](https://arxiv.org/abs/2403.13196)

	Masih Eskandar, Tooba Imtiaz, Zifeng Wang, Jennifer Dy


+ [ Analyzing the Impact of Partial Sharing on the Resilience of Online  Federated Learning Against Model Poisoning Attacks](https://arxiv.org/abs/2403.13108)

	Ehsan Lari, Vinay Chakravarthi Gogineni, Reza Arablouei, Stefan Werner


## 2024-03-18
+ [ Problem space structural adversarial attacks for Network Intrusion  Detection Systems based on Graph Neural Networks](https://arxiv.org/abs/2403.11830)

	Andrea Venturi, Dario Stabili, Mirco Marchetti


+ [ A Toolbox for Surfacing Health Equity Harms and Biases in Large Language  Models](https://arxiv.org/abs/2403.12025)

	Stephen R. Pfohl, Heather Cole-Lewis, Rory Sayres, Darlene Neal, Mercy Asiedu, Awa Dieng, Nenad Tomasev, Qazi Mamunur Rashid, Shekoofeh Azizi, Negar Rostamzadeh, Liam G. McCoy, Leo Anthony Celi, Yun Liu, Mike Schaekermann, Alanna Walton, Alicia Parrish, Chirag Nagpal, Preeti Singh, Akeiylah Dewitt, Philip Mansfield, Sushant Prakash, Katherine Heller, Alan Karthikesalingam, Christopher Semturs, Joelle Barral, Greg Corrado, Yossi Matias, Jamila Smith-Loud, Ivor Horn, Karan Singhal


+ [ Defense Against Adversarial Attacks on No-Reference Image Quality Models  with Gradient Norm Regularization](https://arxiv.org/abs/2403.11397)

	Yujia Liu, Chenxi Yang, Dingquan Li, Jianhao Ding, Tingting Jiang


+ [ SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption  of Monocular Depth Estimation in Autonomous Navigation Applications](https://arxiv.org/abs/2403.11515)

	Amira Guesmi, Muhammad Abdullah Hanif, Ihsen Alouani, Bassem Ouni, Muhammad Shafique


+ [ LocalStyleFool: Regional Video Style Transfer Attack Using Segment  Anything Model](https://arxiv.org/abs/2403.11656)

	Yuxin Cao, Jinghao Li, Xi Xiao, Derui Wang, Minhui Xue, Hao Ge, Wei Liu, Guangwu Hu


+ [ Diffusion Denoising as a Certified Defense against Clean-label Poisoning](https://arxiv.org/abs/2403.11981)

	Sanghyun Hong, Nicholas Carlini, Alexey Kurakin


+ [ Diffusion-Reinforcement Learning Hierarchical Motion Planning in  Adversarial Multi-agent Games](https://arxiv.org/abs/2403.10794)

	Zixuan Wu, Sean Ye, Manisha Natarajan, Matthew C. Gombolay


+ [ Improving LoRA in Privacy-preserving Federated Learning](https://arxiv.org/abs/2403.12313)

	Youbang Sun, Zitao Li, Yaliang Li, Bolin Ding


+ [ Impart: An Imperceptible and Effective Label-Specific Backdoor Attack](https://arxiv.org/abs/2403.13017)

	Jingke Zhao, Zan Wang, Yongwei Wang, Lanjun Wang


+ [ Invisible Backdoor Attack Through Singular Value Decomposition](https://arxiv.org/abs/2403.13018)

	Wenmin Chen, Xiaowei Xu



## 2024-03-17
+ [ RobustSentEmbed: Robust Sentence Embeddings Using Adversarial  Self-Supervised Contrastive Learning](https://arxiv.org/abs/2403.11082)

	Javad Rafiei Asl, Prajwal Panzade, Eduardo Blanco, Daniel Takabi, Zhipeng Cai


+ [ COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via  Probabilistic Circuits](https://arxiv.org/abs/2403.11348)

	Mintong Kang, Nezihe Merve Gürel, Linyi Li, Bo Li


+ [ A Modified Word Saliency-Based Adversarial Attack on Text Classification  Models](https://arxiv.org/abs/2403.11297)

	Hetvi Waghela, Sneha Rakshit, Jaydip Sen


## 2024-03-16
+ [ Improving Adversarial Transferability of Visual-Language Pre-training  Models through Collaborative Multimodal Interaction](https://arxiv.org/abs/2403.10883)

	Jiyuan Fu, Zhaoyu Chen, Kaixun Jiang, Haijing Guo, Jiafeng Wang, Shuyong Gao, Wenqiang Zhang


+ [ Understanding Robustness of Visual State Space Models for Image  Classification](https://arxiv.org/abs/2403.10935)

	Chengbin Du, Yanxi Li, Chang Xu


+ [ Adversarial Knapsack and Secondary Effects of Common Information for  Cyber Operations](https://arxiv.org/abs/2403.10789)

	Jon Goohs, Georgel Savin, Lucas Starks, Josiah Dykstra, William Casey




## 2024-03-15
+ [ Global Convergence Guarantees for Federated Policy Gradient Methods with  Adversaries](https://arxiv.org/abs/2403.09940)

	Swetha Ganesh, Jiayu Chen, Gugan Thoppe, Vaneet Aggarwal


+ [ Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A  Pilot Study](https://arxiv.org/abs/2403.10499)

	Chenguang Wang, Ruoxi Jia, Xin Liu, Dawn Song


+ [ Revisiting Adversarial Training under Long-Tailed Distributions](https://arxiv.org/abs/2403.10073)

	Xinli Yue, Ningping Mou, Qian Wang, Lingchen Zhao


+ [ Benchmarking Adversarial Robustness of Image Shadow Removal with  Shadow-adaptive Attacks](https://arxiv.org/abs/2403.10076)

	Chong Wang, Yi Yu, Lanqing Guo, Bihan Wen


+ [ Mitigating Dialogue Hallucination for Large Multi-modal Models via  Adversarial Instruction Tuning](https://arxiv.org/abs/2403.10492)

	Dongmin Park, Zhaofang Qian, Guangxing Han, Ser-Nam Lim


+ [ Towards Adversarially Robust Dataset Distillation by Curvature  Regularization](https://arxiv.org/abs/2403.10045)

	Eric Xue, Yijiang Li, Haoyang Liu, Yifan Shen, Haohan Wang


+ [ Generation is better than Modification: Combating High Class Homophily  Variance in Graph Anomaly Detection](https://arxiv.org/abs/2403.10339)

	Rui Zhang, Dawei Cheng, Xin Liu, Jie Yang, Yi Ouyang, Xian Wu, Yefeng Zheng


+ [ Federated Learning with Anomaly Detection via Gradient and  Reconstruction Analysis](https://arxiv.org/abs/2403.10000)

	Zahir Alsulaimawi


+ [ Securing Federated Learning with Control-Flow Attestation: A Novel  Framework for Enhanced Integrity and Resilience against Adversarial Attacks](https://arxiv.org/abs/2403.10005)

	Zahir Alsulaimawi


+ [ Interactive Trimming against Evasive Online Data Manipulation Attacks: A  Game-Theoretic Approach](https://arxiv.org/abs/2403.10313)

	Yue Fu, Qingqing Ye, Rong Du, Haibo Hu


+ [ Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized  Scaled Prediction Consistency](https://arxiv.org/abs/2403.10717)

	Soumyadeep Pal, Yuguang Yao, Ren Wang, Bingquan Shen, Sijia Liu



## 2024-03-14
+ [ ADEdgeDrop: Adversarial Edge Dropping for Robust Graph Neural Networks](https://arxiv.org/abs/2403.09171)

	Zhaoliang Chen, Zhihao Wu, Ylli Sadikaj, Claudia Plant, Hong-Ning Dai, Shiping Wang, Wenzhong Guo


+ [ Adversarial Training with OCR Modality Perturbation for Scene-Text  Visual Question Answering](https://arxiv.org/abs/2403.09288)

	Zhixuan Shen, Haonan Luo, Sijia Li, Tianrui Li


+ [ AdaShield: Safeguarding Multimodal Large Language Models from  Structure-based Attack via Adaptive Shield Prompting](https://arxiv.org/abs/2403.09513)

	Yu Wang, Xiaogeng Liu, Yu Li, Muhao Chen, Chaowei Xiao


+ [ VDNA-PR: Using General Dataset Representations for Robust Sequential  Visual Place Recognition](https://arxiv.org/abs/2403.09025)

	Benjamin Ramtoula, Daniele De Martini, Matthew Gadd, Paul Newman


+ [ Impact of Synthetic Images on Morphing Attack Detection Using a Siamese  Network](https://arxiv.org/abs/2403.09380)

	Juan Tapia, Christoph Busch


+ [ Anomaly Detection by Adapting a pre-trained Vision Language Model](https://arxiv.org/abs/2403.09493)

	Yuxuan Cai, Xinwei He, Dingkang Liang, Ao Tong, Xiang Bai


+ [ Soften to Defend: Towards Adversarial Robustness via Self-Guided Label  Refinement](https://arxiv.org/abs/2403.09101)

	Daiwei Yu, Zhuorong Li, Lina Wei, Canghong Jin, Yun Zhang, Sixian Chan


+ [ Adversarial Fine-tuning of Compressed Neural Networks for Joint  Improvement of Robustness and Efficiency](https://arxiv.org/abs/2403.09441)

	Hallgrimur Thorsteinsson, Valdemar J Henriksen, Tong Chen, Raghavendra Selvan


+ [ AVIBench: Towards Evaluating the Robustness of Large Vision-Language  Model on Adversarial Visual-Instructions](https://arxiv.org/abs/2403.09346)

	Hao Zhang, Wenqi Shao, Hong Liu, Yongqiang Ma, Ping Luo, Yu Qiao, Kaipeng Zhang


+ [ Counterfactual contrastive learning: robust representations via causal  image synthesis](https://arxiv.org/abs/2403.09605)

	Melanie Roschewitz, Fabio De Sousa Ribeiro, Tian Xia, Galvin Khara, Ben Glocker


+ [ Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text  Transformation](https://arxiv.org/abs/2403.09572)

	Yunhao Gou, Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung, James T. Kwok, Yu Zhang


+ [ Ciphertext-Only Attack on a Secure $k$-NN Computation on Cloud](https://arxiv.org/abs/2403.09080)

	Shyam Murthy, Santosh Kumar Upadhyaya, Srinivas Vivek


+ [ Optimistic Verifiable Training by Controlling Hardware Nondeterminism](https://arxiv.org/abs/2403.09603)

	Megha Srivastava, Simran Arora, Dan Boneh


+ [ Evaluating LLMs for Gender Disparities in Notable Persons](https://arxiv.org/abs/2403.09148)

	Lauren Rhue, Sofie Goethals, Arun Sundararajan


+ [ Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative  Privacy Risk](https://arxiv.org/abs/2403.09450)

	Zhangheng Li, Junyuan Hong, Bo Li, Zhangyang Wang


+ [ An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts  on Vision-Language Models](https://arxiv.org/abs/2403.09766)

	Haochen Luo, Jindong Gu, Fengyuan Liu, Philip Torr


+ [ Robust Subgraph Learning by Monitoring Early Training Representations](https://arxiv.org/abs/2403.09901)

	Sepideh Neshatfar, Salimeh Yasaei Sekeh


+ [ Counter-Samples: A Stateless Strategy to Neutralize Black Box  Adversarial Attacks](https://arxiv.org/abs/2403.10562)

	Roey Bokobza, Yisroel Mirsky



## 2024-03-13
+ [ Robust Decision Aggregation with Adversarial Experts](https://arxiv.org/abs/2403.08222)

	Yongkang Guo, Yuqing Kong


+ [ Versatile Defense Against Adversarial Attacks on Image Recognition](https://arxiv.org/abs/2403.08170)

	Haibo Zhang, Zhihua Yao, Kouichi Sakurai


+ [ RAF-GI: Towards Robust, Accurate and Fast-Convergent Gradient Inversion  Attack in Federated Learning](https://arxiv.org/abs/2403.08383)

	Can Liu, Jin Wang, Dongyang Yu


+ [ AIGCs Confuse AI Too: Investigating and Explaining Synthetic  Image-induced Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2403.08542)

	Yifei Gao, Jiaqi Wang, Zhiyu Lin, Jitao Sang


+ [ Advancing Security in AI Systems: A Novel Approach to Detecting  Backdoors in Deep Neural Networks](https://arxiv.org/abs/2403.08208)

	Khondoker Murad Hossain, Tim Oates


+ [ SoK: Reducing the Vulnerability of Fine-tuned Language Models to  Membership Inference Attacks](https://arxiv.org/abs/2403.08481)

	Guy Amit, Abigail Goldsteen, Ariel Farkash



## 2024-03-12
+ [ Disentangling Policy from Offline Task Representation Learning via  Adversarial Data Augmentation](https://arxiv.org/abs/2403.07261)

	Chengxing Jia, Fuxiang Zhang, Yi-Chen Li, Chen-Xiao Gao, Xu-Hui Liu, Lei Yuan, Zongzhang Zhang, Yang Yu


+ [ A Bayesian Approach to OOD Robustness in Image Classification](https://arxiv.org/abs/2403.07277)

	Prakhar Kaushik, Adam Kortylewski, Alan Yuille


+ [ Exploring Safety Generalization Challenges of Large Language Models via  Code](https://arxiv.org/abs/2403.07865)

	Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Wai Lam, Lizhuang Ma


+ [ Truth-Aware Context Selection: Mitigating the Hallucinations of Large  Language Models Being Misled by Untruthful Contexts](https://arxiv.org/abs/2403.07556)

	Tian Yu, Shaolei Zhang, Yang Feng


+ [ Calibrating Multi-modal Representations: A Pursuit of Group Robustness  without Annotations](https://arxiv.org/abs/2403.07241)

	Chenyu You, Yifei Min, Weicheng Dai, Jasjeet S. Sekhon, Lawrence Staib, James S. Duncan


+ [ Backdoor Attack with Mode Mixture Latent Modification](https://arxiv.org/abs/2403.07463)

	Hongwei Zhang, Xiaoyin Xu, Dongsheng An, Xianfeng Gu, Min Zhang


+ [ FairRR: Pre-Processing for Group Fairness through Randomized Response](https://arxiv.org/abs/2403.07780)

	Xianli Zeng, Joshua Ward, Guang Cheng


+ [ LiveCodeBench: Holistic and Contamination Free Evaluation of Large  Language Models for Code](https://arxiv.org/abs/2403.07974)

	Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, Ion Stoica


## 2024-03-11
+ [ Poisoning Programs by Un-Repairing Code: Security Concerns of  AI-generated Code](https://arxiv.org/abs/2403.06675)

	Cristina Improta


+ [ Data-Independent Operator: A Training-Free Artifact Representation  Extractor for Generalizable Deepfake Detection](https://arxiv.org/abs/2403.06803)

	Chuangchuang Tan, Ping Liu, RenShuai Tao, Huan Liu, Yao Zhao, Baoyuan Wu, Yunchao Wei


+ [ PeerAiD: Improving Adversarial Distillation from a Specialized Peer  Tutor](https://arxiv.org/abs/2403.06668)

	Jaewon Jung, Hongsun Jang, Jaeyong Song, Jinho Lee


+ [ Dynamic Perturbation-Adaptive Adversarial Training on Medical Image  Classification](https://arxiv.org/abs/2403.06798)

	Shuai Li, Xiaoguang Ma, Shancheng Jiang, Lu Meng


+ [ Intra-Section Code Cave Injection for Adversarial Evasion Attacks on  Windows PE Malware File](https://arxiv.org/abs/2403.06428)

	Kshitiz Aryal, Maanak Gupta, Mahmoud Abdelsalam, Moustafa Saleh


+ [ Real is not True: Backdoor Attacks Against Deepfake Detection](https://arxiv.org/abs/2403.06610)

	Hong Sun, Ziqiang Li, Lei Liu, Bin Li


+ [ Stealing Part of a Production Language Model](https://arxiv.org/abs/2403.06634)

	Nicholas Carlini, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A. Feder Cooper, Katherine Lee, Matthew Jagielski, Milad Nasr, Arthur Conmy, Eric Wallace, David Rolnick, Florian Tramèr


+ [ Improving deep learning with prior knowledge and cognitive models: A  survey on enhancing explainability, adversarial robustness and zero-shot  learning](https://arxiv.org/abs/2403.07078)

	Fuseinin Mumuni, Alhassan Mumuni


## 2024-03-10
+ [ In-context Prompt Learning for Test-time Vision Recognition with Frozen  Vision-language Model](https://arxiv.org/abs/2403.06126)

	Junhui Yin, Xinyu Zhang, Lin Wu, Xianghua Xie, Xiaojie Wang


+ [ Federated Learning: Attacks, Defenses, Opportunities, and Challenges](https://arxiv.org/abs/2403.06067)

	Ghazaleh Shirvani, Saeid Ghasemshirazi, Behzad Beigzadeh



+ [ Attacking Transformers with Feature Diversity Adversarial Perturbation](https://arxiv.org/abs/2403.07942)

	Chenxing Gao, Hang Zhou, Junqing Yu, YuTeng Ye, Jiale Cai, Junle Wang, Wei Yang


## 2024-03-09
+ [ Towards Deviation-Robust Agent Navigation via Perturbation-Aware  Contrastive Learning](https://arxiv.org/abs/2403.05770)

	Bingqian Lin, Yanxin Long, Yi Zhu, Fengda Zhu, Xiaodan Liang, Qixiang Ye, Liang Lin


+ [ Hard-label based Small Query Black-box Adversarial Attack](https://arxiv.org/abs/2403.06014)

	Jeonghwan Park, Paul Miller, Niall McLaughlin


## 2024-03-08
+ [ Predicting Single-cell Drug Sensitivity by Adaptive Weighted Feature for  Adversarial Multi-source Domain Adaptation](https://arxiv.org/abs/2403.05260)

	Wei Duan, Hui Liu


+ [ Exploring the Adversarial Frontier: Quantifying Robustness via  Adversarial Hypervolume](https://arxiv.org/abs/2403.05100)

	Ping Guo, Cheng Gong, Xi Lin, Zhiyuan Yang, Qingfu Zhang


+ [ Overcoming Reward Overoptimization via Adversarial Policy Optimization  with Lightweight Uncertainty Estimation](https://arxiv.org/abs/2403.05171)

	Xiaoying Zhang, Jean-Francois Ton, Wei Shen, Hongning Wang, Yang Liu


+ [ Towards Multimodal Sentiment Analysis Debiasing via Bias Purification](https://arxiv.org/abs/2403.05023)

	Dingkang Yang, Mingcheng Li, Dongling Xiao, Yang Liu, Kun Yang, Zhaoyu Chen, Yuzheng Wang, Peng Zhai, Ke Li, Lihua Zhang


+ [ The Impact of Quantization on the Robustness of Transformer-based Text  Classifiers](https://arxiv.org/abs/2403.05365)

	Seyed Parsa Neshaei, Yasaman Boreshban, Gholamreza Ghassem-Sani, Seyed Abolghasem Mirroshandel


+ [ Hide in Thicket: Generating Imperceptible and Rational Adversarial  Perturbations on 3D Point Clouds](https://arxiv.org/abs/2403.05247)

	Tianrui Lou, Xiaojun Jia, Jindong Gu, Li Liu, Siyuan Liang, Bangyan He, Xiaochun Cao


+ [ Federated Learning Method for Preserving Privacy in Face Recognition  System](https://arxiv.org/abs/2403.05344)

	Enoch Solomon, Abraham Woubie


+ [ EVD4UAV: An Altitude-Sensitive Benchmark to Evade Vehicle Detection in  UAV](https://arxiv.org/abs/2403.05422)

	Huiming Sun, Jiacheng Guo, Zibo Meng, Tianyun Zhang, Jianwu Fang, Yuewei Lin, Hongkai Yu


+ [ Adversarial Sparse Teacher: Defense Against Distillation-Based Model  Stealing Attacks Using Adversarial Examples](https://arxiv.org/abs/2403.05181)

	Eda Yilmaz, Hacer Yalim Keles


## 2024-03-07
+ [ Privacy-preserving Fine-tuning of Large Language Models through Flatness](https://arxiv.org/abs/2403.04124)

	Tiejin Chen, Longchao Da, Huixue Zhou, Pingzhi Li, Kaixiong Zhou, Tianlong Chen, Hua Wei


+ [ A Domain Translation Framework with an Adversarial Denoising Diffusion  Model to Generate Synthetic Datasets of Echocardiography Images](https://arxiv.org/abs/2403.04612)

	Cristiana Tiago, Sten Roar Snare, Jurica Sprem, Kristin McLeod


+ [ Membership Inference Attacks and Privacy in Topic Modeling](https://arxiv.org/abs/2403.04451)

	Nico Manzonelli, Wanrong Zhang, Salil Vadhan


+ [ Automatic and Universal Prompt Injection Attacks against Large Language  Models](https://arxiv.org/abs/2403.04957)

	Xiaogeng Liu, Zhiyuan Yu, Yizhe Zhang, Ning Zhang, Chaowei Xiao


## 2024-03-06
+ [ Do You Trust Your Model? Emerging Malware Threats in the Deep Learning  Ecosystem](https://arxiv.org/abs/2403.03593)

	Dorjan Hitaj, Giulio Pagnotta, Fabio De Gaspari, Sediola Ruko, Briland Hitaj, Luigi V. Mancini, Fernando Perez-Cruz


+ [ Adversarial Infrared Geometry: Using Geometry to Perform Adversarial  Attack against Infrared Pedestrian Detectors](https://arxiv.org/abs/2403.03674)

	Kalibinuer Tiliwalidi


+ [ Advancing Out-of-Distribution Detection through Data Purification and  Dynamic Activation Function Design](https://arxiv.org/abs/2403.03412)

	Yingrui Ji, Yao Zhu, Zhigang Li, Jiansheng Chen, Yunlong Kong, Jingbo Chen


+ [ Probing the Robustness of Time-series Forecasting Models with  CounterfacTS](https://arxiv.org/abs/2403.03508)

	Håkon Hanisch Kjærnli, Lluis Mas-Ribas, Aida Ashrafi, Gleb Sizov, Helge Langseth, Odd Erik Gundersen


+ [ Learning Adversarial MDPs with Stochastic Hard Constraints](https://arxiv.org/abs/2403.03672)

	Francesco Emanuele Stradi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti


+ [ Verified Training for Counterfactual Explanation Robustness under Data  Shift](https://arxiv.org/abs/2403.03773)

	Anna P. Meyer, Yuhao Zhang, Aws Albarghouthi, Loris D'Antoni


+ [ DeepEclipse: How to Break White-Box DNN-Watermarking Schemes](https://arxiv.org/abs/2403.03590)

	Alessandro Pegoraro, Carlotta Segna, Kavita Kumari, Ahmad-Reza Sadeghi


+ [ Neural Exec: Learning (and Learning from) Execution Triggers for Prompt  Injection Attacks](https://arxiv.org/abs/2403.03792)

	Dario Pasquini, Martin Strohmeier, Carmela Troncoso


+ [ Unsupervised Contrastive Learning for Robust RF Device Fingerprinting  Under Time-Domain Shift](https://arxiv.org/abs/2403.04036)

	Jun Chen, Weng-Keen Wong, Bechir Hamdaoui


+ [ Improving Adversarial Training using Vulnerability-Aware Perturbation  Budget](https://arxiv.org/abs/2403.04070)

	Olukorede Fakorede, Modeste Atsague, Jin Tian


+ [ Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability](https://arxiv.org/abs/2403.03967)

	Rajdeep Haldar, Yue Xing, Qifan Song


+ [ Belief-Enriched Pessimistic Q-Learning against Adversarial State  Perturbations](https://arxiv.org/abs/2403.04050)

	Xiaolin Sun, Zizhan Zheng


+ [ Fooling Neural Networks for Motion Forecasting via Adversarial Attacks](https://arxiv.org/abs/2403.04954)

	Edgar Medina, Leyong Loh


## 2024-03-05
+ [ FLGuard: Byzantine-Robust Federated Learning via Ensemble of Contrastive  Models](https://arxiv.org/abs/2403.02846)

	Younghan Lee, Yungi Cho, Woorim Han, Ho Bae, Yunheung Paek


+ [ Recall-Oriented Continual Learning with Generative Adversarial  Meta-Model](https://arxiv.org/abs/2403.03082)

	Haneol Kang, Dong-Wan Choi


+ [ Towards Robust Federated Learning via Logits Calibration on Non-IID Data](https://arxiv.org/abs/2403.02803)

	Yu Qiao, Apurba Adhikary, Chaoning Zhang, Choong Seon Hong


+ [ XAI-Based Detection of Adversarial Attacks on Deepfake Detectors](https://arxiv.org/abs/2403.02955)

	Ben Pinhasov, Raz Lapid, Rony Ohayon, Moshe Sipper, Yehudit Aperstein


+ [ Here Comes The AI Worm: Unleashing Zero-click Worms that Target  GenAI-Powered Applications](https://arxiv.org/abs/2403.02817)

	Stav Cohen, Ron Bitton, Ben Nassi


+ [ Enhancing Security in Federated Learning through Adaptive  Consensus-Based Model Update Validation](https://arxiv.org/abs/2403.04803)

	Zahir Alsulaimawi


## 2024-03-03
+ [ GuardT2I: Defending Text-to-Image Models from Adversarial Prompts](https://arxiv.org/abs/2403.01446)

	Yijun Yang, Ruiyuan Gao, Xiao Yang, Jianyuan Zhong, Qiang Xu


+ [ Breaking Down the Defenses: A Comparative Survey of Attacks on Large  Language Models](https://arxiv.org/abs/2403.04786)

	Arijit Ghosh Chowdhury, Md Mofijul Islam, Vaibhav Kumar, Faysal Hossain Shezan, Vaibhav Kumar, Vinija Jain, Aman Chadha


## 2024-03-02
+ [ Query Recovery from Easy to Hard: Jigsaw Attack against SSE](https://arxiv.org/abs/2403.01155)

	Hao Nie, Wei Wang, Peng Xu, Xianglong Zhang, Laurence T. Yang, Kaitai Liang


+ [ AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks](https://arxiv.org/abs/2403.04783)

	Yifan Zeng, Yiran Wu, Xiao Zhang, Huazheng Wang, Qingyun Wu


## 2024-03-01
+ [ AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language  Model Outputs](https://arxiv.org/abs/2403.00198)

	Sana Ebrahimi, Kaiwen Chen, Abolfazl Asudeh, Gautam Das, Nick Koudas


+ [ Robust Deep Reinforcement Learning Through Adversarial Attacks and  Training : A Survey](https://arxiv.org/abs/2403.00420)

	Lucas Schott, Josephine Delas, Hatem Hajri, Elies Gherbi, Reda Yaich, Nora Boulahia-Cuppens, Frederic Cuppens, Sylvain Lamprier


+ [ DPP-Based Adversarial Prompt Searching for Lanugage Models](https://arxiv.org/abs/2403.00292)

	Xu Zhang, Xiaojun Wan


+ [ Robustifying a Policy in Multi-Agent RL with Diverse Cooperative  Behavior and Adversarial Style Sampling for Assistive Tasks](https://arxiv.org/abs/2403.00344)

	Tayuki Osa, Tatsuya Harada


+ [ Attacking Delay-based PUFs with Minimal Adversary Model](https://arxiv.org/abs/2403.00464)

	Hongming Fei, Owen Millwood, Prosanta Gope, Jack Miskelly, Biplab Sikdar



## 2024-02-29
+ [ Differentially Private Worst-group Risk Minimization](https://arxiv.org/abs/2402.19437)

	Xinyu Zhou, Raef Bassily


+ [ Utilizing Local Hierarchy with Adversarial Training for Hierarchical  Text Classification](https://arxiv.org/abs/2402.18825)

	Zihan Wang, Peiyi Wang, Houfeng Wang


+ [ MPAT: Building Robust Deep Neural Networks against Textual Adversarial  Attacks](https://arxiv.org/abs/2402.18792)

	Fangyuan Zhang, Huichi Zhou, Shuangjiao Li, Hongtao Wang


+ [ PRSA: Prompt Reverse Stealing Attacks against Large Language Models](https://arxiv.org/abs/2402.19200)

	Yong Yang, Xuhong Zhang, Yi Jiang, Xi Chen, Haoyu Wang, Shouling Ji, Zonghui Wang


+ [ Unraveling Adversarial Examples against Speaker Identification --  Techniques for Attack Detection and Victim Model Classification](https://arxiv.org/abs/2402.19355)

	Sonal Joshi, Thomas Thebaud, Jesús Villalba, Najim Dehak


+ [ Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on  Pre-trained Language Models](https://arxiv.org/abs/2402.18945)

	Pengzhou Cheng, Wei Du, Zongru Wu, Fengwei Zhang, Libo Chen, Gongshen Liu


+ [ Watermark Stealing in Large Language Models](https://arxiv.org/abs/2402.19361)

	Nikola Jovanović, Robin Staab, Martin Vechev


+ [ PrivatEyes: Appearance-based Gaze Estimation Using Federated Secure  Multi-Party Computation](https://arxiv.org/abs/2402.18970)

	Mayar Elfares, Pascal Reisert, Zhiming Hu, Wenwu Tang, Ralf Küsters, Andreas Bulling


+ [ Typographic Attacks in Large Multimodal Models Can be Alleviated by More  Informative Prompts](https://arxiv.org/abs/2402.19150)

	Hao Cheng, Erjia Xiao, Renjing Xu


+ [ Assessing Visually-Continuous Corruption Robustness of Neural Networks  Relative to Human Performance](https://arxiv.org/abs/2402.19401)

	Huakun Shen, Boyue Caroline Hu, Krzysztof Czarnecki, Lina Marsso, Marsha Chechik


+ [ Verification of Neural Networks' Global Robustness](https://arxiv.org/abs/2402.19322)

	Anan Kabaha, Dana Drachsler-Cohen


+ [ Whispers that Shake Foundations: Analyzing and Mitigating False Premise  Hallucinations in Large Language Models](https://arxiv.org/abs/2402.19103)

	Hongbang Yuan, Pengfei Cao, Zhuoran Jin, Yubo Chen, Daojian Zeng, Kang Liu, Jun Zhao


+ [ Pointing out the Shortcomings of Relation Extraction Models with  Semantically Motivated Adversarials](https://arxiv.org/abs/2402.19076)

	Gennaro Nolano, Moritz Blum, Basil Ell, Philipp Cimiano


+ [ LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario](https://arxiv.org/abs/2403.00108)

	Hongyi Liu, Zirui Liu, Ruixiang Tang, Jiayi Yuan, Shaochen Zhong, Yu-Neng Chuang, Li Li, Rui Chen, Xia Hu


## 2024-02-28
+ [ Making Them Ask and Answer: Jailbreaking Large Language Models in Few  Queries via Disguise and Reconstruction](https://arxiv.org/abs/2402.18104)

	Tong Liu, Yingjie Zhang, Zhe Zhao, Yinpeng Dong, Guozhu Meng, Kai Chen


+ [ Enhancing Tracking Robustness with Auxiliary Adversarial Defense  Networks](https://arxiv.org/abs/2402.17976)

	Zhewei Wu, Ruilong Yu, Qihe Liu, Shuying Cheng, Shilin Qiu, Shijie Zhou


+ [ Catastrophic Overfitting: A Potential Blessing in Disguise](https://arxiv.org/abs/2402.18211)

	Mengnan Zhao, Lihe Zhang, Yuqiu Kong, Baocai Yin


+ [ A New Era in LLM Security: Exploring Security Concerns in Real-World  LLM-based Systems](https://arxiv.org/abs/2402.18649)

	Fangzhou Wu, Ning Zhang, Somesh Jha, Patrick McDaniel, Chaowei Xiao


+ [ Unveiling Privacy, Memorization, and Input Curvature Links](https://arxiv.org/abs/2402.18726)

	Deepak Ravikumar, Efstathia Soufleri, Abolfazl Hashemi, Kaushik Roy


+ [ Model Pairing Using Embedding Translation for Backdoor Attack Detection  on Open-Set Classification Tasks](https://arxiv.org/abs/2402.18718)

	Alexander Unnervik, Hatef Otroshi Shahreza, Anjith George, Sébastien Marcel


+ [ Pre-training Differentially Private Models with Limited Public Data](https://arxiv.org/abs/2402.18752)

	Zhiqi Bu, Xinwei Zhang, Mingyi Hong, Sheng Zha, George Karypis


+ [ Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An  Adversarial Perspective](https://arxiv.org/abs/2402.18607)

	Xinjian Luo, Yangfan Jiang, Fei Wei, Yuncheng Wu, Xiaokui Xiao, Beng Chin Ooi


## 2024-02-27
+ [ Speak Out of Turn: Safety Vulnerability of Large Language Models in  Multi-turn Dialogue](https://arxiv.org/abs/2402.17262)

	Zhenhong Zhou, Jiuyang Xiang, Haopeng Chen, Quan Liu, Zherui Li, Sen Su


+ [ FairBelief - Assessing Harmful Beliefs in Language Models](https://arxiv.org/abs/2402.17389)

	Mattia Setzu, Marta Marchiori Manerba, Pasquale Minervini, Debora Nozza


+ [ Extreme Miscalibration and the Illusion of Adversarial Robustness](https://arxiv.org/abs/2402.17509)

	Vyas Raina, Samson Tan, Volkan Cevher, Aditya Rawal, Sheng Zha, George Karypis


+ [ Enhancing Quality of Compressed Images by Mitigating Enhancement Bias  Towards Compression Domain](https://arxiv.org/abs/2402.17200)

	Qunliang Xing, Mai Xu, Shengxi Li, Xin Deng, Meisong Zheng, Huaida Liu, Ying Chen


+ [ Preserving Fairness Generalization in Deepfake Detection](https://arxiv.org/abs/2402.17229)

	Li Lin, Xinan He, Yan Ju, Xin Wang, Feng Ding, Shu Hu


+ [ Black-box Adversarial Attacks Against Image Quality Assessment Models](https://arxiv.org/abs/2402.17533)

	Yu Ran, Ao-Xiang Zhang, Mingjie Li, Weixuan Tang, Yuan-Gen Wang


+ [ Structure-Guided Adversarial Training of Diffusion Models](https://arxiv.org/abs/2402.17563)

	Ling Yang, Haotian Qian, Zhilong Zhang, Jingwei Liu, Bin Cui


+ [ Towards Fairness-Aware Adversarial Learning](https://arxiv.org/abs/2402.17729)

	Yanghao Zhang, Tianle Zhang, Ronghui Mu, Xiaowei Huang, Wenjie Ruan


+ [ Model X-ray:Detect Backdoored Models via Decision Boundary](https://arxiv.org/abs/2402.17465)

	Yanghao Su, Jie Zhang, Ting Xu, Tianwei Zhang, Weiming Zhang, Nenghai Yu


+ [ Robustness-Congruent Adversarial Training for Secure Machine Learning  Model Updates](https://arxiv.org/abs/2402.17390)

	Daniele Angioni, Luca Demetrio, Maura Pintor, Luca Oneto, Davide Anguita, Battista Biggio, Fabio Roli


+ [ Evaluation of Predictive Reliability to Foster Trust in Artificial  Intelligence. A case study in Multiple Sclerosis](https://arxiv.org/abs/2402.17554)

	Lorenzo Peracchio, Giovanna Nicora, Enea Parimbelli, Tommaso Mario Buonocore, Roberto Bergamaschi, Eleonora Tavazzi, Arianna Dagliati, Riccardo Bellazzi


+ [ LLM-Resistant Math Word Problem Generation via Adversarial Attacks](https://arxiv.org/abs/2402.17916)

	Roy Xie, Chengxuan Huang, Junlin Wang, Bhuwan Dhingra


+ [ Adversarial example soups: averaging multiple adversarial examples  improves transferability without increasing additional generation time](https://arxiv.org/abs/2402.18370)

	Bo Yang, Hengwei Zhang, Chenwei Li, Jindong Wang


## 2024-02-26
+ [ Referee Can Play: An Alternative Approach to Conditional Generation via  Model Inversion](https://arxiv.org/abs/2402.16305)

	Xuantong Liu, Tianyang Hu, Wenjia Wang, Kenji Kawaguchi, Yuan Yao


+ [ Investigating Deep Watermark Security: An Adversarial Transferability  Perspective](https://arxiv.org/abs/2402.16397)

	Biqing Qi, Junqi Gao, Yiang Luo, Jianxing Liu, Ligang Wu, Bowen Zhou


+ [ Training Implicit Generative Models via an Invariant Statistical Loss](https://arxiv.org/abs/2402.16435)

	José Manuel de Frutos, Pablo M. Olmos, Manuel A. Vázquez, Joaquín Míguez


+ [ CodeChameleon: Personalized Encryption Framework for Jailbreaking Large  Language Models](https://arxiv.org/abs/2402.16717)

	Huijie Lv, Xiao Wang, Yuansen Zhang, Caishuang Huang, Shihan Dou, Junjie Ye, Tao Gui, Qi Zhang, Xuanjing Huang


+ [ Immunization against harmful fine-tuning attacks](https://arxiv.org/abs/2402.16382)

	Domenic Rosati, Jan Wehner, Kai Williams, Łukasz Bartoszcze, Jan Batzner, Hassan Sajjad, Frank Rudzicz


+ [ RoCoIns: Enhancing Robustness of Large Language Models through  Code-Style Instructions](https://arxiv.org/abs/2402.16431)

	Yuansen Zhang, Xiao Wang, Zhiheng Xi, Han Xia, Tao Gui, Qi Zhang, Xuanjing Huang


+ [ ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable  Safety Detectors](https://arxiv.org/abs/2402.16444)

	Zhexin Zhang, Yida Lu, Jingyuan Ma, Di Zhang, Rui Li, Pei Ke, Hao Sun, Lei Sha, Zhifang Sui, Hongning Wang, Minlie Huang


+ [ Unveiling Vulnerability of Self-Attention](https://arxiv.org/abs/2402.16470)

	Khai Jiet Liong, Hongqiu Wu, Hai Zhao


+ [ Edge Detectors Can Make Deep Convolutional Neural Networks More Robust](https://arxiv.org/abs/2402.16479)

	Jin Ding, Jie-Chao Zhao, Yong-Zhi Sun, Ping Tan, Jia-Wei Wang, Ji-En Ma, You-Tong Fang


+ [ Improving the JPEG-resistance of Adversarial Attacks on Face Recognition  by Interpolation Smoothing](https://arxiv.org/abs/2402.16586)

	Kefu Guo, Fengfan Zhou, Hefei Ling, Ping Li, Hui Liu


+ [ On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing  Problem](https://arxiv.org/abs/2402.16926)

	Georg Pichler, Marco Romanelli, Divya Prakash Manivannan, Prashanth Krishnamurthy, Farshad Khorrami, Siddharth Garg


+ [ A Curious Case of Remarkable Resilience to Gradient Attacks via Fully  Convolutional and Differentiable Front End with a Skip Connection](https://arxiv.org/abs/2402.17018)

	Leonid Boytsov, Ameya Joshi, Filipe Condessa


## 2024-02-25
+ [ From Noise to Clarity: Unraveling the Adversarial Suffix of Large  Language Model Attacks via Translation of Text Embeddings](https://arxiv.org/abs/2402.16006)

	Hao Wang, Hao Li, Minlie Huang, Lei Sha


+ [ Defending Large Language Models against Jailbreak Attacks via Semantic  Smoothing](https://arxiv.org/abs/2402.16192)

	Jiabao Ji, Bairu Hou, Alexander Robey, George J. Pappas, Hamed Hassani, Yang Zhang, Eric Wong, Shiyu Chang


+ [ Adversarial-Robust Transfer Learning for Medical Imaging via Domain  Assimilation](https://arxiv.org/abs/2402.16005)

	Xiaohui Chen, Tie Luo


+ [ DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM  Jailbreakers](https://arxiv.org/abs/2402.16914)

	Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, Cho-Jui Hsieh


## 2024-02-24
+ [ LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A  Vision Paper](https://arxiv.org/abs/2402.15727)

	Daoyuan Wu, Shuai Wang, Yang Liu, Ning Liu


+ [ RAUCA: A Novel Physical Adversarial Attack on Vehicle Detectors via  Robust and Accurate Camouflage Generation](https://arxiv.org/abs/2402.15853)

	Jiawei Zhou, Linye Lyu, Daojing He, Yu Li


## 2024-02-23
+ [ On the Duality Between Sharpness-Aware Minimization and Adversarial  Training](https://arxiv.org/abs/2402.15152)

	Yihao Zhang, Hangzhou He, Jingyu Zhu, Huanran Chen, Yifei Wang, Zeming Wei


+ [ ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion  Models against Stochastic Perturbation](https://arxiv.org/abs/2402.15429)

	Yi Zhang, Yun Tang, Wenjie Ruan, Xiaowei Huang, Siddartha Khastgir, Paul Jennings, Xingyu Zhao


+ [ A First Look at GPT Apps: Landscape and Vulnerability](https://arxiv.org/abs/2402.15105)

	Zejun Zhang, Li Zhang, Xin Yuan, Anlan Zhang, Mengwei Xu, Feng Qian


+ [ Fast Adversarial Attacks on Language Models In One GPU Minute](https://arxiv.org/abs/2402.15570)

	Vinu Sankar Sadasivan, Shoumik Saha, Gaurang Sriramanan, Priyatham Kattakinda, Atoosa Chegini, Soheil Feizi


+ [ Distilling Adversarial Robustness Using Heterogeneous Teachers](https://arxiv.org/abs/2402.15586)

	Jieren Deng, Aaron Palmer, Rigel Mahmood, Ethan Rathbun, Jinbo Bi, Kaleel Mahmood, Derek Aguiar


+ [ Low-Frequency Black-Box Backdoor Attack via Evolutionary Algorithm](https://arxiv.org/abs/2402.15653)

	Yanqi Qiao, Dazhuang Liu, Rui Wang, Kaitai Liang


+ [ The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented  Generation (RAG)](https://arxiv.org/abs/2402.16893)

	Shenglai Zeng, Jiankun Zhang, Pengfei He, Yue Xing, Yiding Liu, Han Xu, Jie Ren, Shuaiqiang Wang, Dawei Yin, Yi Chang, Jiliang Tang


## 2024-02-22
+ [ Rethinking Invariance Regularization in Adversarial Training to Improve  Robustness-Accuracy Trade-off](https://arxiv.org/abs/2402.14648)

	Futa Waseda, Isao Echizen


+ [ Does the Generator Mind its Contexts? An Analysis of Generative Model  Faithfulness under Context Transfer](https://arxiv.org/abs/2402.14488)

	Xinshuo Hu, Baotian Hu, Dongfang Li, Xiaoguang Li, Lifeng Shang


+ [ Noise-BERT: A Unified Perturbation-Robust Framework with Noise Alignment  Pre-training for Noisy Slot Filling Task](https://arxiv.org/abs/2402.14494)

	Jinxu Zhao, Guanting Dong, Yueyan Qiu, Tingfeng Hui, Xiaoshuai Song, Daichi Guo, Weiran Xu


+ [ Quadruplet Loss For Improving the Robustness to Face Morphing Attacks](https://arxiv.org/abs/2402.14665)

	Iurii Medvedev, Nuno Gonçalves


+ [ BeTAIL: Behavior Transformer Adversarial Imitation Learning from Human  Racing Gameplay](https://arxiv.org/abs/2402.14194)

	Catherine Weaver, Chen Tang, Ce Hao, Kenta Kawamoto, Masayoshi Tomizuka, Wei Zhan


+ [ COBIAS: Contextual Reliability in Bias Assessment](https://arxiv.org/abs/2402.14889)

	Priyanshul Govil, Vamshi Krishna Bonagiri, Manas Gaur, Ponnurangam Kumaraguru, Sanorita Dey


+ [ Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning  Meets Adversarial Images](https://arxiv.org/abs/2402.14899)

	Zefeng Wang, Zhen Han, Shuo Chen, Fan Xue, Zifeng Ding, Xun Xiao, Volker Tresp, Philip Torr, Jindong Gu


+ [ Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment](https://arxiv.org/abs/2402.14968)

	Jiongxiao Wang, Jiazhao Li, Yiquan Li, Xiangyu Qi, Muhao Chen, Junjie Hu, Yixuan Li, Bo Li, Chaowei Xiao


+ [ Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models](https://arxiv.org/abs/2402.14977)

	Hongbin Liu, Michael K. Reiter, Neil Zhenqiang Gong


+ [ SoK: Analyzing Adversarial Examples: A Framework to Study Adversary  Knowledge](https://arxiv.org/abs/2402.14937)

	Lucas Fenaux, Florian Kerschbaum


## 2024-02-21
+ [ Large Language Models are Vulnerable to Bait-and-Switch Attacks for  Generating Harmful Content](https://arxiv.org/abs/2402.13926)

	Federico Bianchi, James Zou


+ [ SDXL-Lightning: Progressive Adversarial Diffusion Distillation](https://arxiv.org/abs/2402.13929)

	Shanchuan Lin, Anran Wang, Xiao Yang


+ [ GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient  Analysis](https://arxiv.org/abs/2402.13494)

	Yueqi Xie, Minghong Fang, Renjie Pi, Neil Gong


+ [ Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on  Zero-shot LLM Assessment](https://arxiv.org/abs/2402.14016)

	Vyas Raina, Adian Liusie, Mark Gales


+ [ Learning to Poison Large Language Models During Instruction Tuning](https://arxiv.org/abs/2402.13459)

	Yao Qiang, Xiangyu Zhou, Saleh Zare Zade, Mohammad Amin Roshani, Douglas Zytko, Dongxiao Zhu


+ [ Coercing LLMs to do and reveal (almost) anything](https://arxiv.org/abs/2402.14020)

	Jonas Geiping, Alex Stein, Manli Shu, Khalid Saifullah, Yuxin Wen, Tom Goldstein


+ [ Robustness of Deep Neural Networks for Micro-Doppler Radar  Classification](https://arxiv.org/abs/2402.13651)

	Mikolaj Czerkawski, Carmine Clemente, Craig MichieCraig Michie, Christos Tachtatzis


+ [ VL-Trojan: Multimodal Instruction Backdoor Attacks against  Autoregressive Visual Language Models](https://arxiv.org/abs/2402.13851)

	Jiawei Liang, Siyuan Liang, Man Luo, Aishan Liu, Dongchen Han, Ee-Chien Chang, Xiaochun Cao


+ [ Stealthy Adversarial Attacks on Stochastic Multi-Armed Bandits](https://arxiv.org/abs/2402.13487)

	Zhiwei Wang, Huazheng Wang, Hongning Wang


+ [ AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement  Learning](https://arxiv.org/abs/2402.13946)

	Vasudev Gohil, Satwik Patnaik, Dileep Kalathil, Jeyavijayan Rajendran


+ [ Uncertainty-driven and Adversarial Calibration Learning for Epicardial  Adipose Tissue Segmentation](https://arxiv.org/abs/2402.14349)

	Kai Zhao, Zhiming Liu, Jiaqi Liu, Jingbiao Zhou, Bihong Liao, Huifang Tang, Qiuyu Wang, Chunquan Li


+ [ Fake Resume Attacks: Data Poisoning on Online Job Platforms](https://arxiv.org/abs/2402.14124)

	Michiharu Yamashita, Thanh Tran, Dongwon Lee


+ [ Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts  Against Open-source LLMs](https://arxiv.org/abs/2402.14872)

	Xiaoxia Li, Siyuan Liang, Jiyi Zhang, Han Fang, Aishan Liu, Ee-Chien Chang



## 2024-02-20
+ [ TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box  Identification](https://arxiv.org/abs/2402.12991)

	Martin Gubri, Dennis Ulmer, Hwaran Lee, Sangdoo Yun, Seong Joon Oh


+ [ VGMShield: Mitigating Misuse of Video Generative Models](https://arxiv.org/abs/2402.13126)

	Yan Pang, Yang Zhang, Tianhao Wang


+ [ Indiscriminate Data Poisoning Attacks on Pre-trained Feature Extractors](https://arxiv.org/abs/2402.12626)

	Yiwei Lu, Matthew Y.R. Yang, Gautam Kamath, Yaoliang Yu


+ [ Beyond Worst-case Attacks: Robust RL with Adaptive Defense via  Non-dominated Policies](https://arxiv.org/abs/2402.12673)

	Xiangyu Liu, Chenghao Deng, Yanchao Sun, Yongyuan Liang, Furong Huang


+ [ Defending Jailbreak Prompts via In-Context Adversarial Game](https://arxiv.org/abs/2402.13148)

	Yujun Zhou, Yufei Han, Haomin Zhuang, Taicheng Guo, Kehan Guo, Zhenwen Liang, Hongyan Bao, Xiangliang Zhang


## 2024-02-19
+ [ ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs](https://arxiv.org/abs/2402.11753)

	Fengqing Jiang, Zhangchen Xu, Luyao Niu, Zhen Xiang, Bhaskar Ramasubramanian, Bo Li, Radha Poovendran


+ [ ChatGPT Based Data Augmentation for Improved Parameter-Efficient  Debiasing of LLMs](https://arxiv.org/abs/2402.11764)

	Pengrui Han, Rafal Kocielnik, Adhithya Saravanan, Roy Jiang, Or Sharir, Anima Anandkumar


+ [ Acquiring Clean Language Models from Backdoor Poisoned Datasets by  Downscaling Frequency Space](https://arxiv.org/abs/2402.12026)

	Zongru Wu, Zhuosheng Zhang, Pengzhou Cheng, Gongshen Liu


+ [ Defending Against Weight-Poisoning Backdoor Attacks for  Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.12168)

	Shuai Zhao, Leilei Gan, Luu Anh Tuan, Jie Fu, Lingjuan Lyu, Meihuizi Jia, Jinming Wen


+ [ Dynamic Environment Responsive Online Meta-Learning with Fairness  Awareness](https://arxiv.org/abs/2402.12319)

	Chen Zhao, Feng Mi, Xintao Wu, Kai Jiang, Latifur Khan, Feng Chen


+ [ Query-Based Adversarial Prompt Generation](https://arxiv.org/abs/2402.12329)

	Jonathan Hayase, Ema Borevkovic, Nicholas Carlini, Florian Tramèr, Milad Nasr


+ [ AICAttack: Adversarial Image Captioning Attack with Attention-Based  Optimization](https://arxiv.org/abs/2402.11940)

	Jiyao Li, Mingze Ni, Yifei Dong, Tianqing Zhu, Wei Liu


+ [ Adversarial Feature Alignment: Balancing Robustness and Accuracy in Deep  Learning via Adversarial Training](https://arxiv.org/abs/2402.12187)

	Leo Hyun Park, Jaeuk Kim, Myung Gyo Oh, Jaewoo Park, Taekyoung Kwon


+ [ Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models](https://arxiv.org/abs/2402.11989)

	Zihao Luo, Xilie Xu, Feng Liu, Yun Sing Koh, Di Wang, Jingfeng Zhang


+ [ Emulated Disalignment: Safety Alignment for Large Language Models May  Backfire!](https://arxiv.org/abs/2402.12343)

	Zhanhui Zhou, Jie Liu, Zhichen Dong, Jiaheng Liu, Chao Yang, Wanli Ouyang, Yu Qiao


+ [ Attacks on Node Attributes in Graph Neural Networks](https://arxiv.org/abs/2402.12426)

	Ying Xu, Michael Lanier, Anindya Sarkar, Yevgeniy Vorobeychik


## 2024-02-18
+ [ How Susceptible are Large Language Models to Ideological Manipulation?](https://arxiv.org/abs/2402.11725)

	Kai Chen, Zihao He, Jun Yan, Taiwei Shi, Kristina Lerman


+ [ Stealthy Attack on Large Language Model based Recommendation](https://arxiv.org/abs/2402.14836)

	Jinghao Zhang, Yuting Liu, Qiang Liu, Shu Wu, Guibing Guo, Liang Wang


## 2024-02-17
+ [ Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated  Text Detection](https://arxiv.org/abs/2402.11167)

	Fan Huang, Haewoon Kwak, Jisun An


+ [ Maintaining Adversarial Robustness in Continuous Learning](https://arxiv.org/abs/2402.11196)

	Xiaolei Ru, Xiaowei Cao, Zijia Liu, Jack Murdoch Moore, Xin-Ya Zhang, Xia Zhu, Wenjia Wei, Gang Yan


+ [ Disclosure and Mitigation of Gender Bias in LLMs](https://arxiv.org/abs/2402.11190)

	Xiangjue Dong, Yibo Wang, Philip S. Yu, James Caverlee


+ [ A White-Box False Positive Adversarial Attack Method on Contrastive Loss  Based Offline Handwritten Signature Verification Models](https://arxiv.org/abs/2308.08925)

	Zhongliang Guo, Weiye Li, Yifei Qian, Ognjen Arandjelović, Lei Fang


## 2024-02-16
+ [ Connect the dots: Dataset Condensation, Differential Privacy, and  Adversarial Uncertainty](https://arxiv.org/abs/2402.10423)

	Kenneth Odoh


+ [ Adversarial Curriculum Graph Contrastive Learning with Pair-wise  Augmentation](https://arxiv.org/abs/2402.10468)

	Xinjian Zhao, Liang Zhang, Yang Liu, Ruocheng Guo, Xiangyu Zhao


+ [ Zero-shot sampling of adversarial entities in biomedical question  answering](https://arxiv.org/abs/2402.10527)

	R. Patrick Xian, Alex J. Lee, Vincent Wang, Qiming Cui, Russell Ro, Reza Abbasi-Asl


+ [ Universal Prompt Optimizer for Safe Text-to-Image Generation](https://arxiv.org/abs/2402.10882)

	Zongyu Wu, Hongcheng Gao, Yueze Wang, Xiang Zhang, Suhang Wang


+ [ Uncertainty, Calibration, and Membership Inference Attacks: An  Information-Theoretic Perspective](https://arxiv.org/abs/2402.10686)

	Meiyi Zhu, Caili Guo, Chunyan Feng, Osvaldo Simeone


+ [ ASGEA: Exploiting Logic Rules from Align-Subgraphs for Entity Alignment](https://arxiv.org/abs/2402.11000)

	Yangyifei Luo, Zhuo Chen, Lingbing Guo, Qian Li, Wenxuan Zeng, Zhixin Cai, Jianxin Li


+ [ The Male CEO and the Female Assistant: Probing Gender Biases in  Text-To-Image Models Through Paired Stereotype Test](https://arxiv.org/abs/2402.11089)

	Yixin Wan, Kai-Wei Chang


+ [ VQAttack: Transferable Adversarial Attacks on Visual Question Answering  via Pre-trained Models](https://arxiv.org/abs/2402.11083)

	Ziyi Yin, Muchao Ye, Tianrong Zhang, Jiaqi Wang, Han Liu, Jinghui Chen, Ting Wang, Fenglong Ma


+ [ DART: A Principled Approach to Adversarially Robust Unsupervised Domain  Adaptation](https://arxiv.org/abs/2402.11120)

	Yunjuan Wang, Hussein Hazimeh, Natalia Ponomareva, Alexey Kurakin, Ibrahim Hammoud, Raman Arora


## 2024-02-15
+ [ Generating Visual Stimuli from EEG Recordings using Transformer-encoder  based EEG encoder and GAN](https://arxiv.org/abs/2402.10115)

	Rahul Mishra, Arnav Bhavsar


+ [ Improving EEG Signal Classification Accuracy Using Wasserstein  Generative Adversarial Networks](https://arxiv.org/abs/2402.09453)

	Joshua Park, Priyanshu Mahey, Ore Adeniyi


+ [ Examining Pathological Bias in a Generative Adversarial Network  Discriminator: A Case Study on a StyleGAN3 Model](https://arxiv.org/abs/2402.09786)

	Alvin Grissom II, Ryan F. Lei, Jeova Farias Sales Rocha Neto, Bailey Lin, Ryan Trotter


+ [ A Trembling House of Cards? Mapping Adversarial Attacks against Language  Agents](https://arxiv.org/abs/2402.10196)

	Lingbo Mo, Zeyi Liao, Boyuan Zheng, Yu Su, Chaowei Xiao, Huan Sun


+ [ Align before Attend: Aligning Visual and Textual Features for Multimodal  Hateful Content Detection](https://arxiv.org/abs/2402.09738)

	Eftekhar Hossain, Omar Sharif, Mohammed Moshiul Hoque, Sarah M. Preum


+ [ Camouflage is all you need: Evaluating and Enhancing Language Model  Robustness Against Camouflage Adversarial Attacks](https://arxiv.org/abs/2402.09874)

	Álvaro Huertas-García, Alejandro Martín, Javier Huertas-Tato, David Camacho


+ [ FedRDF: A Robust and Dynamic Aggregation Function against Poisoning  Attacks in Federated Learning](https://arxiv.org/abs/2402.10082)

	Enrique Mármol Campos, Aurora González Vidal, José Luis Hernández Ramos, Antonio Skarmeta


+ [ Unlocking the Potential of Transformers in Time Series Forecasting with  Sharpness-Aware Minimization and Channel-Wise Attention](https://arxiv.org/abs/2402.10198)

	Romain Ilbert, Ambroise Odonnat, Vasilii Feofanov, Aladin Virmaux, Giuseppe Paolo, Themis Palpanas, Ievgen Redko


+ [ PAL: Proxy-Guided Black-Box Attack on Large Language Models](https://arxiv.org/abs/2402.09674)

	Chawin Sitawarin, Norman Mu, David Wagner, Alexandre Araujo


+ [ Reward Poisoning Attack Against Offline Reinforcement Learning](https://arxiv.org/abs/2402.09695)

	Yinglun Xu, Rohan Gumaste, Gagandeep Singh


+ [ AbuseGPT: Abuse of Generative AI ChatBots to Create Smishing Campaigns](https://arxiv.org/abs/2402.09728)

	Ashfak Md Shibli, Mir Mehedi A. Pritom, Maanak Gupta


+ [ Privacy Attacks in Decentralized Learning](https://arxiv.org/abs/2402.10001)

	Abdellah El Mrini, Edwige Cyffers, Aurélien Bellet


+ [ How Much Does Each Datapoint Leak Your Privacy? Quantifying the  Per-datum Membership Leakage](https://arxiv.org/abs/2402.10065)

	Achraf Azize, Debabrota Basu


+ [ A Trembling House of Cards? Mapping Adversarial Attacks against Language  Agents](https://arxiv.org/abs/2402.10196)

	Lingbo Mo, Zeyi Liao, Boyuan Zheng, Yu Su, Chaowei Xiao, Huan Sun


+ [ Camouflage is all you need: Evaluating and Enhancing Language Model  Robustness Against Camouflage Adversarial Attacks](https://arxiv.org/abs/2402.09874)

	Álvaro Huertas-García, Alejandro Martín, Javier Huertas-Tato, David Camacho


+ [ Risk-Sensitive Soft Actor-Critic for Robust Deep Reinforcement Learning  under Distribution Shifts](https://arxiv.org/abs/2402.09992)

	Tobias Enders, James Harrison, Maximilian Schiffer


+ [ How Much Does Each Datapoint Leak Your Privacy? Quantifying the  Per-datum Membership Leakage](https://arxiv.org/abs/2402.10065)

	Achraf Azize, Debabrota Basu


+ [ Backdoor Attack against One-Class Sequential Anomaly Detection Models](https://arxiv.org/abs/2402.10283)

	He Cheng, Shuhan Yuan


## 2024-02-14
+ [Exploring the Adversarial Capabilities of Large Language Models](https://arxiv.org/abs/2402.09132)

	Lukas Struppek, Minh Hieu Le, Dominik Hintersdorf, Kristian Kersting


+ [Only My Model On My Data: A Privacy Preserving Approach Protecting one Model and Deceiving Unauthorized Black-Box Models](https://arxiv.org/abs/2402.09316)

	Weiheng Chai, Brian Testa, Huantao Ren, Asif Salekin, Senem Velipasalar


+ [Review-Incorporated Model-Agnostic Profile Injection Attacks on Recommender Systems](https://arxiv.org/abs/2402.09023)

	Shiyi Yang, Lina Yao, Chen Wang, Xiwei Xu, Liming Zhu


+ [Attacking Large Language Models with Projected Gradient Descent](https://arxiv.org/abs/2402.09154)

	Simon Geisler, Tom Wollschläger, M. H. I. Abdalla, Johannes Gasteiger, Stephan Günnemann


+ [Detecting Adversarial Spectrum Attacks via Distance to Decision Boundary Statistics](https://arxiv.org/abs/2402.08986)

	Wenwei Zhao, Xiaowen Li, Shangqing Zhao, Jie Xu, Yao Liu, Zhuo Lu


+ [SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding](https://arxiv.org/abs/2402.08983)

	Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Bill Yuchen Lin, Radha Poovendran


+ [Ten Words Only Still Help: Improving Black-Box AI-Generated Text Detection via Proxy-Guided Efficient Re-Sampling](https://arxiv.org/abs/2402.09199)

	Yuhui Shi, Qiang Sheng, Juan Cao, Hao Mi, Beizhe Hu, Danding Wang


+ [Towards Robust Model-Based Reinforcement Learning Against Adversarial Corruption](https://arxiv.org/abs/2402.08991)

	Chenlu Ye, Jiafan He, Quanquan Gu, Tong Zhang


+ [Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues](https://arxiv.org/abs/2402.09091)

	Zhiyuan Chang, Mingyang Li, Yi Liu, Junjie Wang, Qing Wang, Yang Liu


+ [ Why Does Differential Privacy with Large Epsilon Defend Against  Practical Membership Inference Attacks?](https://arxiv.org/abs/2402.09540)

	Andrew Lowy, Zhuohang Li, Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang


## 2024-02-13
+ [ Data Reconstruction Attacks and Defenses: A Systematic Evaluation](https://arxiv.org/abs/2402.09478)

	Sheng Liu, Zihan Wang, Qi Lei


+ [Faster Repeated Evasion Attacks in Tree Ensembles](https://arxiv.org/abs/2402.08586)

	Lorenzo Cascioli, Laurens Devos, Ondřej Kuželka, Jesse Davis


+ [Generating Universal Adversarial Perturbations for Quantum Classifiers](https://arxiv.org/abs/2402.08648)

	Gautham Anil, Vishnu Vinod, Apurva Narayan


+ [Enhancing Robustness of Indoor Robotic Navigation with Free-Space Segmentation Models Against Adversarial Attacks](https://arxiv.org/abs/2402.08763)

	Qiyuan An, Christos Sevastopoulos, Fillia Makedon


+ [COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability](https://arxiv.org/abs/2402.08679)

	Xingang Guo, Fangxu Yu, Huan Zhang, Lianhui Qin, Bin Hu


+ [Test-Time Backdoor Attacks on Multimodal Large Language Models](https://arxiv.org/abs/2402.08577)

	Dong Lu, Tianyu Pang, Chao Du, Qian Liu, Xianjun Yang, Min Lin


+ [Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast](https://arxiv.org/abs/2402.08567)

	Xiangming Gu, Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Ye Wang, Jing Jiang, Min Lin


+ [ Oracle-Efficient Differentially Private Learning with Public Data](https://arxiv.org/abs/2402.09483)

	Adam Block, Mark Bun, Rathin Desai, Abhishek Shetty, Steven Wu


## 2024-02-12
+ [ PANORAMIA: Privacy Auditing of Machine Learning Models without  Retraining](https://arxiv.org/abs/2402.09477)

	Mishaal Kazmi, Hadrien Lautraite, Alireza Akbari, Mauricio Soroco, Qiaoyue Tang, Tao Wang, Sébastien Gambs, Mathias Lécuyer

+ [Understanding Deep Learning defenses Against Adversarial Examples Through Visualizations for Dynamic Risk Assessment](https://arxiv.org/abs/2402.07496)

	Xabier Echeberria-Barrio, Amaia Gil-Lerchundi, Jon Egana-Zubia, Raul Orduna-Urrutia


+ [Topological safeguard for evasion attack interpreting the neural networks' behavior](https://arxiv.org/abs/2402.07480)

	Xabier Echeberria-Barrio, Amaia Gil-Lerchundi, Iñigo Mendialdua, Raul Orduna-Urrutia


+ [PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2402.07867)

	Wei Zou, Runpeng Geng, Binghui Wang, Jinyuan Jia


+ [OrderBkd: Textual backdoor attack through repositioning](https://arxiv.org/abs/2402.07689)

	Irina Alekseevskaia, Konstantin Arkhipenko


+ [Customizable Perturbation Synthesis for Robust SLAM Benchmarking](https://arxiv.org/abs/2402.08125)

	Xiaohao Xu, Tianyi Zhang, Sibo Wang, Xiang Li, Yongqi Chen, Ye Li, Bhiksha Raj, Matthew Johnson-Roberson, Xiaonan Huang


+ [Multi-Attribute Vision Transformers are Efficient and Robust Learners](https://arxiv.org/abs/2402.08070)

	Hanan Gani, Nada Saadi, Noor Hussein, Karthik Nandakumar


+ [Do Membership Inference Attacks Work on Large Language Models?](https://arxiv.org/abs/2402.07841)

	Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, Hannaneh Hajishirzi


+ [NeuralSentinel: Safeguarding Neural Network Reliability and Trustworthiness](https://arxiv.org/abs/2402.07506)

	Xabier Echeberria-Barrio, Mikel Gorricho, Selene Valencia, Francesco Zola


## 2024-02-11
+ [Accuracy of TextFooler black box adversarial attacks on 01 loss sign activation neural network ensemble](https://arxiv.org/abs/2402.07347)

	Yunzhe Xue, Usman Roshan

+ [A Random Ensemble of Encrypted Vision Transformers for Adversarially Robust Defense](https://arxiv.org/abs/2402.07183)

	Ryota Iijima, Sayaka Shiota, Hitoshi Kiya


## 2024-02-10
+ [Architectural Neural Backdoors from First Principles](https://arxiv.org/abs/2402.06957)

	Harry Langford, Ilia Shumailov, Yiren Zhao, Robert Mullins, Nicolas Papernot


## 2024-02-09
+ [TETRIS: Towards Exploring the Robustness of Interactive Segmentation](https://arxiv.org/abs/2402.06132)

	Andrey Moskalenko, Vlad Shakhuro, Anna Vorontsova, Anton Konushin, Anton Antonov, Alexander Krapukhin, Denis Shepelev, Konstantin Soshin

+ [RAMP: Boosting Adversarial Robustness Against Multiple lp Perturbations](https://arxiv.org/abs/2402.06827)

	Enyi Jiang, Gagandeep Singh


+ [Anomaly Unveiled: Securing Image Classification against Adversarial Patch Attacks](https://arxiv.org/abs/2402.06249)

	Nandish Chattopadhyay, Amira Guesmi, Muhammad Shafique


+ [Evaluating Membership Inference Attacks and Defenses in Federated Learning](https://arxiv.org/abs/2402.06289)

	Gongxi Zhu, Donghao Li, Hanlin Gu, Yuxing Han, Yuan Yao, Lixin Fan, Qiang Yang


+ [Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning](https://arxiv.org/abs/2402.06255)

	Yichuan Mo, Yuji Wang, Zeming Wei, Yisen Wang

+ [StruQ: Defending Against Prompt Injection with Structured Queries](https://arxiv.org/abs/2402.06363)

	Sizhe Chen, Julien Piet, Chawin Sitawarin, David Wagner


+ [Quantifying and Enhancing Multi-modal Robustness with Modality Preference](https://arxiv.org/abs/2402.06244)

	Zequn Yang, Yake Wei, Ce Liang, Di Hu


## 2024-02-08
+ [Reinforcement Learning as a Catalyst for Robust and Fair Federated Learning: Deciphering the Dynamics of Client Contributions](https://arxiv.org/abs/2402.05541)

	Jialuo He, Wei Chen, Xiaojin Zhang


+ [Is Adversarial Training with Compressed Datasets Effective?](https://arxiv.org/abs/2402.05675)

	Tong Chen, Raghavendra Selvan

+ [Investigating White-Box Attacks for On-Device Models](https://arxiv.org/abs/2402.05493)

	Mingyi Zhou, Xiang Gao, Jing Wu, Kui Liu, Hailong Sun, Li Li


+ [Comprehensive Assessment of Jailbreak Attacks Against LLMs](https://arxiv.org/abs/2402.05668)

	Junjie Chu, Yugeng Liu, Ziqing Yang, Xinyue Shen, Michael Backes, Yang Zhang


+ [ Savvy: Trustworthy Autonomous Vehicles Architecture](https://arxiv.org/abs/2402.14580)

	Ali Shoker, Rehana Yasmin, Paulo Esteves-Verissimo


## 2024-02-07
+ [ InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding  Extremely Long Sequences with Training-Free Memory](https://arxiv.org/abs/2402.04617)

	Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, Maosong Sun


+ [ Adversarial Robustness Through Artifact Design](https://arxiv.org/abs/2402.04660)

	Tsufit Shua, Mahmood Sharif


+ [ Group Distributionally Robust Dataset Distillation with Risk  Minimization](https://arxiv.org/abs/2402.04676)

	Saeed Vahidian, Mingyu Wang, Jianyang Gu, Vyacheslav Kungurtsev, Wei Jiang, Yiran Chen


+ [ EvoSeed: Unveiling the Threat on Deep Neural Networks with Real-World  Illusions](https://arxiv.org/abs/2402.04699)

	Shashank Kotyan, PoYuan Mao, Danilo Vasconcellos Vargas


+ [ SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large  Language Models](https://arxiv.org/abs/2402.05044)

	Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu Qiao, Jing Shao


+ [ Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations  from Large Language Models](https://arxiv.org/abs/2402.04614)

	Chirag Agarwal, Sree Harsha Tanneru, Himabindu Lakkaraju


+ [ Channel-Selective Normalization for Label-Shift Robust Test-Time  Adaptation](https://arxiv.org/abs/2402.04958)

	Pedro Vianna, Muawiz Chaudhary, Paria Mehrbod, An Tang, Guy Cloutier, Guy Wolf, Michael Eickenberg, Eugene Belilovsky


+ [ De-amplifying Bias from Differential Privacy in Language Model  Fine-tuning](https://arxiv.org/abs/2402.04489)

	Sanjari Srivastava, Piotr Mardziel, Zhikhun Zhang, Archana Ahlawat, Anupam Datta, John C Mitchell


## 2024-02-06
+ [ Partially Recentralization Softmax Loss for Vision-Language Models  Robustness](https://arxiv.org/abs/2402.03627)

	Hao Wang, Xin Zhang, Jinzhe Jiang, Yaqian Zhao, Chen Li


+ [ A Survey of Privacy Threats and Defense in Vertical Federated Learning:  From Model Life Cycle Perspective](https://arxiv.org/abs/2402.03688)

	Lei Yu, Meng Han, Yiming Li, Changting Lin, Yao Zhang, Mingyang Zhang, Yan Liu, Haiqin Weng, Yuseok Jeon, Ka-Ho Chow, Stacy Patterson


+ [ Boosting Adversarial Transferability across Model Genus by  Deformation-Constrained Warping](https://arxiv.org/abs/2402.03951)

	Qinliang Lin, Cheng Luo, Zenghao Niu, Xilin He, Weicheng Xie, Yuanbo Hou, Linlin Shen, Siyang Song


+ [ Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science](https://arxiv.org/abs/2402.04247)

	Xiangru Tang, Qiao Jin, Kunlun Zhu, Tongxin Yuan, Yichi Zhang, Wangchunshu Zhou, Meng Qu, Yilun Zhao, Jian Tang, Zhuosheng Zhang, Arman Cohan, Zhiyong Lu, Mark Gerstein


+ [ HarmBench: A Standardized Evaluation Framework for Automated Red Teaming  and Robust Refusal](https://arxiv.org/abs/2402.04249)

	Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, David Forsyth, Dan Hendrycks


+ [ Measuring Implicit Bias in Explicitly Unbiased Large Language Models](https://arxiv.org/abs/2402.04105)

	Xuechunzi Bai, Angelina Wang, Ilia Sucholutsky, Thomas L. Griffiths


+ [ Privacy Leakage on DNNs: A Survey of Model Inversion Attacks and  Defenses](https://arxiv.org/abs/2402.04013)

	Hao Fang, Yixiang Qiu, Hongyao Yu, Wenbo Yu, Jiawei Kong, Baoli Chong, Bin Chen, Xuan Wang, Shu-Tao Xia


+ [ Fairness and Privacy Guarantees in Federated Contextual Bandits](https://arxiv.org/abs/2402.03531)

	Sambhav Solanki, Shweta Jain, Sujit Gujar


+ [ PAC-Bayesian Adversarially Robust Generalization Bounds for Graph Neural  Network](https://arxiv.org/abs/2402.04038)

	Tan Sun, Junhong Lin


+ [ Towards Fair, Robust and Efficient Client Contribution Evaluation in  Federated Learning](https://arxiv.org/abs/2402.04409)

	Meiying Zhang, Huan Zhao, Sheldon Ebron, Kan Yang


+ [ PreGIP: Watermarking the Pretraining of Graph Neural Networks for Deep  Intellectual Property Protection](https://arxiv.org/abs/2402.04435)

	Enyan Dai, Minhua Lin, Suhang Wang



+ [ Adversarially Robust Deepfake Detection via Adversarial Feature  Similarity Learning](https://arxiv.org/abs/2403.08806)

	Sarwar Khan


## 2024-02-05
+ [ Exploiting Class Probabilities for Black-box Sentence-level Attacks](https://arxiv.org/abs/2402.02695)

	Raha Moraffah, Huan Liu


+ [ A Generative Approach to Surrogate-based Black-box Attacks](https://arxiv.org/abs/2402.02732)

	Raha Moraffah, Huan Liu


+ [ Evading Data Contamination Detection for Language Models is (too) Easy](https://arxiv.org/abs/2402.02823)

	Jasper Dekoninck, Mark Niklas Müller, Maximilian Baader, Marc Fischer, Martin Vechev


+ [ Homograph Attacks on Maghreb Sentiment Analyzers](https://arxiv.org/abs/2402.03171)

	Fatima Zahra Qachfar, Rakesh M. Verma


+ [ Conversation Reconstruction Attack Against GPT Models](https://arxiv.org/abs/2402.02987)

	Junjie Chu, Zeyang Sha, Michael Backes, Yang Zhang


+ [ GUARD: Role-playing to Generate Natural-language Jailbreakings to Test  Guideline Adherence of Large Language Models](https://arxiv.org/abs/2402.03299)

	Haibo Jin, Ruoxi Chen, Andy Zhou, Jinyin Chen, Yang Zhang, Haohan Wang


+ [ Transcending Adversarial Perturbations: Manifold-Aided Adversarial  Examples with Legitimate Semantics](https://arxiv.org/abs/2402.03095)

	Shuai Li, Xiaoyu Jiang, Xiaoguang Ma


+ [ DisDet: Exploring Detectability of Backdoor Attack on Diffusion Models](https://arxiv.org/abs/2402.02739)

	Yang Sui, Huy Phan, Jinqi Xiao, Tianfang Zhang, Zijie Tang, Cong Shi, Yan Wang, Yingying Chen, Bo Yuan


+ [ Time-Distributed Backdoor Attacks on Federated Spiking Learning](https://arxiv.org/abs/2402.02886)

	Gorka Abad, Stjepan Picek, Aitor Urbieta


+ [ Adversarial Data Augmentation for Robust Speaker Verification](https://arxiv.org/abs/2402.02699)

	Zhenyu Zhou, Junhui Chen, Namin Wang, Lantian Li, Dong Wang


+ [ Arabic Synonym BERT-based Adversarial Examples for Text Classification](https://arxiv.org/abs/2402.03477)

	Norah Alshahrani, Saied Alshahrani, Esma Wali, Jeanna Matthews


+ [ Generalization Properties of Adversarial Training for $\ell_0$-Bounded  Adversarial Attacks](https://arxiv.org/abs/2402.03576)

	Payam Delgosha, Hamed Hassani, Ramtin Pedarsani


## 2024-02-04
+ [ DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms  in Vision Transformers](https://arxiv.org/abs/2402.02554)

	Oryan Yehezkel, Alon Zolfi, Amit Baras, Yuval Elovici, Asaf Shabtai


+ [ PROSAC: Provably Safe Certification for Machine Learning Models under  Adversarial Attacks](https://arxiv.org/abs/2402.02629)

	Ziquan Liu, Zhuo Zhi, Ilija Bogunovic, Carsten Gerner-Beuerle, Miguel Rodrigues

## 2024-02-03
+ [ Seeing is not always believing: The Space of Harmless Perturbations](https://arxiv.org/abs/2402.02095)

	Lu Chen, Shaofeng Li, Benhao Huang, Fan Yang, Zheng Li, Jie Li, Yuan Luo


+ [ Towards Optimal Adversarial Robust Q-learning with Bellman  Infinity-error](https://arxiv.org/abs/2402.02165)

	Haoran Li, Zicheng Zhang, Wang Luo, Congying Han, Yudong Hu, Tiande Guo, Shichen Liao


+ [ Rethinking the Starting Point: Enhancing Performance and Fairness of  Federated Learning via Collaborative Pre-Training](https://arxiv.org/abs/2402.02225)

	Yun-Wei Chu, Dong-Jun Han, Seyyedali Hosseinalipour, Christopher G. Brinton


+ [ Data Poisoning for In-context Learning](https://arxiv.org/abs/2402.02160)

	Pengfei He, Han Xu, Yue Xing, Hui Liu, Makoto Yamada, Jiliang Tang


+ [ Universal Post-Training Reverse-Engineering Defense Against Backdoors in  Deep Neural Networks](https://arxiv.org/abs/2402.02034)

	Xi Li, Hang Wang, David J. Miller, George Kesidis


+ [ MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly  Mixed Classifiers](https://arxiv.org/abs/2402.02263)

	Yatong Bai, Mo Zhou, Vishal M. Patel, Somayeh Sojoudi


## 2024-02-02
+ [ Trustworthy Distributed AI Systems: Robustness, Privacy, and Governance](https://arxiv.org/abs/2402.01096)

	Wenqi Wei, Ling Liu


+ [ STAA-Net: A Sparse and Transferable Adversarial Attack for Speech  Emotion Recognition](https://arxiv.org/abs/2402.01227)

	Yi Chang, Zhao Ren, Zixing Zhang, Xin Jing, Kun Qian, Xi Shao, Bin Hu, Tanja Schultz, Björn W. Schuller


+ [ Delving into Decision-based Black-box Attacks on Semantic Segmentation](https://arxiv.org/abs/2402.01220)

	Zhaoyu Chen, Zhengyang Shan, Jingwen Chang, Kaixun Jiang, Dingkang Yang, Yiting Cheng, Wenqiang Zhang


+ [ Synthetic Data for the Mitigation of Demographic Biases in Face  Recognition](https://arxiv.org/abs/2402.01472)

	Pietro Melzi, Christian Rathgeb, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Dominik Lawatsch, Florian Domin, Maxim Schaubert


+ [ Cheating Suffix: Targeted Attack to Text-To-Image Diffusion Models with  Multi-Modal Priors](https://arxiv.org/abs/2402.01369)

	Dingcheng Yang, Yang Bai, Xiaojun Jia, Yang Liu, Xiaochun Cao, Wenjian Yu


+ [ SignSGD with Federated Defense: Harnessing Adversarial Attacks through  Gradient Sign Decoding](https://arxiv.org/abs/2402.01340)

	Chanho Park, Namyoon Lee


+ [ On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio  Classification](https://arxiv.org/abs/2402.01274)

	Calum Heggan, Sam Budgett, Timothy Hosepedales, Mehrdad Yeghoobi


+ [ $σ$-zero: Gradient-based Optimization of $\ell_0$-norm Adversarial  Examples](https://arxiv.org/abs/2402.01879)

	Antonio Emanuele Cinà, Francesco Villani, Maura Pintor, Lea Schönherr, Battista Biggio, Marcello Pelillo


+ [ Position Paper: Assessing Robustness, Privacy, and Fairness in Federated  Learning Integrated with Foundation Models](https://arxiv.org/abs/2402.01857)

	Xi Li, Jiaqi Wang


+ [ HQA-Attack: Toward High Quality Black-Box Hard-Label Adversarial Attack  on Text](https://arxiv.org/abs/2402.01806)

	Han Liu, Zhi Xu, Xiaotong Zhang, Feng Zhang, Fenglong Ma, Hongyang Chen, Hong Yu, Xianchao Zhang


+ [ Preference Poisoning Attacks on Reward Model Learning](https://arxiv.org/abs/2402.01920)

	Junlin Wu, Jiongxiao Wang, Chaowei Xiao, Chenguang Wang, Ning Zhang, Yevgeniy Vorobeychik


+ [ SpecFormer: Guarding Vision Transformer Robustness via Maximum Singular  Value Penalization](https://arxiv.org/abs/2402.03317)

	Xixu Hu, Runkai Zheng, Jindong Wang, Cheuk Hang Leung, Qi Wu, Xing Xie


## 2024-02-01
+ [ Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated  Student Essay Detection](https://arxiv.org/abs/2402.00412)

	Xinlin Peng, Ying Zhou, Ben He, Le Sun, Yingfei Sun


+ [ Safety of Multimodal Large Language Models on Images and Text](https://arxiv.org/abs/2402.00357)

	Xin Liu, Yichen Zhu, Yunshi Lan, Chao Yang, Yu Qiao


+ [ Masked Conditional Diffusion Model for Enhancing Deepfake Detection](https://arxiv.org/abs/2402.00541)

	Tiewen Chen, Shanmin Yang, Shu Hu, Zhenghan Fang, Ying Fu, Xi Wu, Xin Wang


+ [ Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks](https://arxiv.org/abs/2402.00626)

	Maan Qraitem, Nazia Tasnim, Kate Saenko, Bryan A. Plummer


+ [ Approximating Optimal Morphing Attacks using Template Inversion](https://arxiv.org/abs/2402.00695)

	Laurent Colbois, Hatef Otroshi Shahreza, Sébastien Marcel


+ [ Tropical Decision Boundaries for Neural Networks Are Robust Against  Adversarial Attacks](https://arxiv.org/abs/2402.00576)

	Kurt Pasque, Christopher Teska, Ruriko Yoshida, Keiji Miura, Jefferson Huang


+ [Short: Benchmarking Transferable Adversarial Attacks](https://arxiv.org/abs/2402.00418)

	Zhibo Jin, Jiayu Zhang, Zhiyu Zhu, Huaming Chen


+ [ Large Language Models Based Fuzzing Techniques: A Survey](https://arxiv.org/abs/2402.00350)

	Linghan Huang, Peizhou Zhao, Huaming Chen, Lei Ma


+ [ Investigating Bias Representations in Llama 2 Chat via Activation  Steering](https://arxiv.org/abs/2402.00402)

	Dawn Lu, Nina Rimsky


+ [ Invariance-powered Trustworthy Defense via Remove Then Restore](https://arxiv.org/abs/2402.00304)

	Xiaowei Fu, Yuhang Zhou, Lina Ma, Lei Zhang


+ [ Safety of Multimodal Large Language Models on Images and Text](https://arxiv.org/abs/2402.00357)

	Xin Liu, Yichen Zhu, Yunshi Lan, Chao Yang, Yu Qiao


+ [ Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks](https://arxiv.org/abs/2402.00626)

	Maan Qraitem, Nazia Tasnim, Kate Saenko, Bryan A. Plummer


+ [ Survey of Privacy Threats and Countermeasures in Federated Learning](https://arxiv.org/abs/2402.00342)

	Masahiro Hayashitani, Junki Mori, Isamu Teranishi


+ [ FairEHR-CLP: Towards Fairness-Aware Clinical Predictions with  Contrastive Learning in Multimodal Electronic Health Records](https://arxiv.org/abs/2402.00955)

	Yuqing Wang, Malvika Pillai, Yun Zhao, Catherine Curtin, Tina Hernandez-Boussard


+ [ BrainLeaks: On the Privacy-Preserving Properties of Neuromorphic  Architectures against Model Inversion Attacks](https://arxiv.org/abs/2402.00906)

	Hamed Poursiami, Ihsen Alouani, Maryam Parsa


## 2024-01-31
+ [ Manipulating Predictions over Discrete Inputs in Machine Teaching](https://arxiv.org/abs/2401.17865)

	Xiaodong Wu, Yufei Han, Hayssam Dahrouj, Jianbing Ni, Zhenwen Liang, Xiangliang Zhang


+ [ Unified Physical-Digital Face Attack Detection](https://arxiv.org/abs/2401.17699)

	Hao Fang, Ajian Liu, Haocheng Yuan, Junze Zheng, Dingheng Zeng, Yanhong Liu, Jiankang Deng, Sergio Escalera, Xiaoming Liu, Jun Wan, Zhen Lei


+ [ Logit Poisoning Attack in Distillation-based Federated Learning and its  Countermeasures](https://arxiv.org/abs/2401.17746)

	Yonghao Yu, Shunan Zhu, Jinglu Hu


+ [ Adversarial Quantum Machine Learning: An Information-Theoretic  Generalization Analysis](https://arxiv.org/abs/2402.00176)

	Petros Georgiou, Sharu Theresa Jose, Osvaldo Simeone


+ [ Common Sense Reasoning for Deep Fake Detection](https://arxiv.org/abs/2402.00126)

	Yue Zhang, Ben Colman, Ali Shahriyari, Gaurav Bharaj


+ [ Privacy and Security Implications of Cloud-Based AI Services : A Survey](https://arxiv.org/abs/2402.00896)

	Alka Luqman, Riya Mahesh, Anupam Chattopadhyay


+ [ An Early Categorization of Prompt Injection Attacks on Large Language  Models](https://arxiv.org/abs/2402.00898)

	Sippo Rossi, Alisia Marianne Michel, Raghava Rao Mukkamala, Jason Bennett Thatcher


## 2024-01-30
+ [ Detection and Recovery Against Deep Neural Network Fault Injection  Attacks Based on Contrastive Learning](https://arxiv.org/abs/2401.16766)

	Chenan Wang, Pu Zhao, Siyue Wang, Xue Lin


+ [ Can Large Language Models be Trusted for Evaluation? Scalable  Meta-Evaluation of LLMs as Evaluators via Agent Debate](https://arxiv.org/abs/2401.16788)

	Steffi Chern, Ethan Chern, Graham Neubig, Pengfei Liu


+ [ Finetuning Large Language Models for Vulnerability Detection](https://arxiv.org/abs/2401.17010)

	Alexey Shestov, Anton Cheshkov, Rodion Levichev, Ravil Mussabayev, Pavel Zadorozhny, Evgeny Maslov, Chibirev Vadim, Egor Bulychev


+ [ Robust Prompt Optimization for Defending Language Models Against  Jailbreaking Attacks](https://arxiv.org/abs/2401.17263)

	Andy Zhou, Bo Li, Haohan Wang


+ [ Gradient-Based Language Model Red Teaming](https://arxiv.org/abs/2401.16656)

	Nevan Wichers, Carson Denison, Ahmad Beirami


+ [ Single Word Change is All You Need: Designing Attacks and Defenses for  Text Classifiers](https://arxiv.org/abs/2401.17196)

	Lei Xu, Sarah Alnegheimish, Laure Berti-Equille, Alfredo Cuesta-Infante, Kalyan Veeramachaneni


+ [ Weak-to-Strong Jailbreaking on Large Language Models](https://arxiv.org/abs/2401.17256)

	Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, William Yang Wang


+ [ Optimal-Landmark-Guided Image Blending for Face Morphing Attacks](https://arxiv.org/abs/2401.16722)

	Qiaoyun He, Zongyong Deng, Zuyuan He, Qijun Zhao


+ [ Towards Assessing the Synthetic-to-Measured Adversarial Vulnerability of  SAR ATR](https://arxiv.org/abs/2401.17038)

	Bowen Peng, Bo Peng, Jingyuan Xia, Tianpeng Liu, Yongxiang Liu, Li Liu


+ [ Revisiting Gradient Pruning: A Dual Realization for Defending against  Gradient Attacks](https://arxiv.org/abs/2401.16687)

	Lulu Xue, Shengshan Hu, Ruizhi Zhao, Leo Yu Zhang, Shengqing Hu, Lichao Sun, Dezhong Yao


+ [ Systematically Assessing the Security Risks of AI/ML-enabled Connected  Healthcare Systems](https://arxiv.org/abs/2401.17136)

	Mohammed Elnawawy, Mohammadreza Hallajiyan, Gargi Mitra, Shahrear Iqbal, Karthik Pattabiraman


+ [ Provably Robust Multi-bit Watermarking for AI-generated Text via Error  Correction Code](https://arxiv.org/abs/2401.16820)

	Wenjie Qu, Dong Yin, Zixin He, Wei Zou, Tianyang Tao, Jinyuan Jia, Jiaheng Zhang


+ [ AdvGPS: Adversarial GPS for Multi-Agent Perception Attack](https://arxiv.org/abs/2401.17499)

	Jinlong Li, Baolu Li, Xinyu Liu, Jianwu Fang, Felix Juefei-Xu, Qing Guo, Hongkai Yu


+ [ Security and Privacy Challenges of Large Language Models: A Survey](https://arxiv.org/abs/2402.00888)

	Badhan Chandra Das, M. Hadi Amini, Yanzhao Wu


+ [ Large Language Models in Cybersecurity: State-of-the-Art](https://arxiv.org/abs/2402.00891)

	Farzad Nourmohammadzadeh Motlagh, Mehrdad Hajizadeh, Mehryar Majd, Pejman Najafi, Feng Cheng, Christoph Meinel


## 2024-01-29
+ [ Adversarial Training on Purification (AToP): Advancing Both Robustness  and Generalization](https://arxiv.org/abs/2401.16352)

	Guang Lin, Chao Li, Jianhai Zhang, Toshihisa Tanaka, Qibin Zhao


+ [ Finding Challenging Metaphors that Confuse Pretrained Language Models](https://arxiv.org/abs/2401.16012)

	Yucheng Li, Frank Guerin, Chenghua Lin


+ [ Transparency Attacks: How Imperceptible Image Layers Can Fool AI  Perception](https://arxiv.org/abs/2401.15817)

	Forrest McKee, David Noever


+ [ TransTroj: Transferable Backdoor Attacks to Pre-trained Models via  Embedding Indistinguishability](https://arxiv.org/abs/2401.15883)

	Hao Wang, Tao Xiang, Shangwei Guo, Jialing He, Hangcheng Liu, Tianwei Zhang


+ [ AdvNF: Reducing Mode Collapse in Conditional Normalising Flows using  Adversarial Learning](https://arxiv.org/abs/2401.15948)

	Vikas Kanaujia, Mathias S. Scheurer, Vipul Arora


+ [ Red-Teaming for Generative AI: Silver Bullet or Security Theater?](https://arxiv.org/abs/2401.15897)

	Michael Feffer, Anusha Sinha, Zachary C. Lipton, Hoda Heidari


+ [ LESSON: Multi-Label Adversarial False Data Injection Attack for Deep  Learning Locational Detection](https://arxiv.org/abs/2401.16001)

	Jiwei Tian, Chao Shen, Buhong Wang, Xiaofang Xia, Meng Zhang, Chenhao Lin, Qian Li


+ [ Effective Controllable Bias Mitigation for Classification and Retrieval  using Gate Adapters](https://arxiv.org/abs/2401.16457)

	Shahed Masoudian, Cornelia Volaucnik, Markus Schedl, Shahed Masoudian


## 2024-01-28
+ [ Lips Are Lying: Spotting the Temporal Inconsistency between Audio and  Visual in Lip-Syncing DeepFakes](https://arxiv.org/abs/2401.15668)

	Weifeng Liu, Tianyi She, Jiawei Liu, Run Wang, Dongyu Yao, Ziyou Liang


+ [ Addressing Noise and Efficiency Issues in Graph-Based Machine Learning  Models From the Perspective of Adversarial Attack](https://arxiv.org/abs/2401.15615)

	Yongyu Wang


+ [ Integrating Differential Privacy and Contextual Integrity](https://arxiv.org/abs/2401.15774)

	Sebastian Benthall, Rachel Cummings


## 2024-01-27
+ [ L-AutoDA: Leveraging Large Language Models for Automated Decision-based  Adversarial Attacks](https://arxiv.org/abs/2401.15335)

	Ping Guo, Fei Liu, Xi Lin, Qingchuan Zhao, Qingfu Zhang


+ [ Style-News: Incorporating Stylized News Generation and Adversarial  Verification for Neural Fake News Detection](https://arxiv.org/abs/2401.15509)

	Wei-Yao Wang, Yu-Chieh Chang, Wen-Chih Peng


+ [ Multi-Trigger Backdoor Attacks: More Triggers, More Threats](https://arxiv.org/abs/2401.15295)

	Yige Li, Xingjun Ma, Jiabo He, Hanxun Huang, Yu-Gang Jiang


+ [ Asymptotic Behavior of Adversarial Training Estimator under  $\ell_\infty$-Perturbation](https://arxiv.org/abs/2401.15262)

	Yiling Xie, Xiaoming Huo


## 2024-01-26
+ [ Mitigating Feature Gap for Adversarial Robustness by Feature  Disentanglement](https://arxiv.org/abs/2401.14707)

	Nuoyan Zhou, Dawei Zhou, Decheng Liu, Xinbo Gao, Nannan Wang


+ [ Conserve-Update-Revise to Cure Generalization and Robustness Trade-off  in Adversarial Training](https://arxiv.org/abs/2401.14948)

	Shruthi Gowda, Bahram Zonooz, Elahe Arani


+ [ BackdoorBench: A Comprehensive Benchmark and Analysis of Backdoor  Learning](https://arxiv.org/abs/2401.15002)

	Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, Mingli Zhu, Ruotong Wang, Li Liu, Chao Shen


+ [ Unrecognizable Yet Identifiable: Image Distortion with Preserved  Embeddings](https://arxiv.org/abs/2401.15048)

	Dmytro Zakharov, Oleksandr Kuznetsov, Emanuele Frontoni


+ [ GuardML: Efficient Privacy-Preserving Machine Learning Services Through  Hybrid Homomorphic Encryption](https://arxiv.org/abs/2401.14840)

	Eugene Frimpong, Khoa Nguyen, Mindaugas Budzys, Tanveer Khan, Antonis Michalas


+ [ PrivStream: An Algorithm for Streaming Differentially Private Data](https://arxiv.org/abs/2401.14577)

	Girish Kumar, Thomas Strohmer, Roman Vershynin


+ [ Coca: Improving and Explaining Graph Neural Network-Based Vulnerability  Detection Systems](https://arxiv.org/abs/2401.14886)

	Sicong Cao, Xiaobing Sun, Xiaoxue Wu, David Lo, Lili Bo, Bin Li, Wei Liu


+ [ Better Representations via Adversarial Training in Pre-Training: A  Theoretical Perspective](https://arxiv.org/abs/2401.15248)

	Yue Xing, Xiaofeng Lin, Qifan Song, Yi Xu, Belinda Zeng, Guang Cheng


+ [ MEA-Defender: A Robust Watermark against Model Extraction Attack](https://arxiv.org/abs/2401.15239)

	Peizhuo Lv, Hualong Ma, Kai Chen, Jiachen Zhou, Shengzhi Zhang, Ruigang Liang, Shenchen Zhu, Pan Li, Yingjun Zhang


## 2024-01-25
+ [ Unmasking and Quantifying Racial Bias of Large Language Models in  Medical Report Generation](https://arxiv.org/abs/2401.13867)

	Yifan Yang, Xiaoyu Liu, Qiao Jin, Furong Huang, Zhiyong Lu


+ [ Adaptive Text Watermark for Large Language Models](https://arxiv.org/abs/2401.13927)

	Yepeng Liu, Yuheng Bu



+ [ Sparse and Transferable Universal Singular Vectors Attack](https://arxiv.org/abs/2401.14031)

	Kseniia Kuvshinova, Olga Tsymboi, Ivan Oseledets


+ [ The Risk of Federated Learning to Skew Fine-Tuning Features and  Underperform Out-of-Distribution Robustness](https://arxiv.org/abs/2401.14027)

	Mengyao Du, Miao Zhang, Yuwen Pu, Kai Xu, Shouling Ji, Quanjun Yin


+ [ Information Leakage Detection through Approximate Bayes-optimal  Prediction](https://arxiv.org/abs/2401.14283)

	Pritha Gupta, Marcel Wever, Eyke Hüllermeier


+ [ Improving Pseudo-labelling and Enhancing Robustness for Semi-Supervised  Domain Generalization](https://arxiv.org/abs/2401.13965)

	Adnan Khan, Mai A. Shaaban, Muhammad Haris Khan


+ [ Producing Plankton Classifiers that are Robust to Dataset Shift](https://arxiv.org/abs/2401.14256)

	Cheng Chen, Sreenath Kyathanahally, Marta Reyes, Stefanie Merkli, Ewa Merz, Emanuele Francazi, Marvin Hoege, Francesco Pomati, Marco Baity-Jesi


+ [ Decentralized Federated Learning: A Survey on Security and Privacy](https://arxiv.org/abs/2401.17319)

	Ehsan Hallaji, Roozbeh Razavi-Far, Mehrdad Saif, Boyu Wang, Qiang Yang


## 2024-01-24
+ [ Boosting the Transferability of Adversarial Examples via Local Mixup and  Adaptive Step Size](https://arxiv.org/abs/2401.13205)

	Junlin Liu, Xinchen Lyu


+ [ AdCorDA: Classifier Refinement via Adversarial Correction and Domain  Adaptation](https://arxiv.org/abs/2401.13212)

	Lulan Shen, Ali Edalati, Brett Meyer, Warren Gross, James J. Clark


+ [ Multi-Agent Diagnostics for Robustness via Illuminated Diversity](https://arxiv.org/abs/2401.13460)

	Mikayel Samvelyan, Davide Paglieri, Minqi Jiang, Jack Parker-Holder, Tim Rocktäschel


+ [ Can overfitted deep neural networks in adversarial training generalize?  -- An approximation viewpoint](https://arxiv.org/abs/2401.13624)

	Zhongjie Shi, Fanghui Liu, Yuan Cao, Johan A.K. Suykens


+ [ A Systematic Approach to Robustness Modelling for Deep Convolutional  Neural Networks](https://arxiv.org/abs/2401.13751)

	Charles Meyers, Mohammad Reza Saleh Sedghpour, Tommy Löfstedt, Erik Elmroth


+ [ Don't Push the Button! Exploring Data Leakage Risks in Machine Learning  and Transfer Learning](https://arxiv.org/abs/2401.13796)

	Andrea Apicella, Francesco Isgrò, Roberto Prevete


+ [ LAA-Net: Localized Artifact Attention Network for High-Quality Deepfakes  Detection](https://arxiv.org/abs/2401.13856)

	Dat Nguyen, Nesryne Mejri, Inder Pal Singh, Polina Kuleshova, Marcella Astrid, Anis Kacem, Enjie Ghorbel, Djamila Aouada


+ [ Inference Attacks Against Face Recognition Model without Classification  Layers](https://arxiv.org/abs/2401.13719)

	Yuanqing Huang, Huilong Chen, Yinggui Wang, Lei Wang


+ [ A Systematic Approach to Robustness Modelling for Deep Convolutional  Neural Networks](https://arxiv.org/abs/2401.13751)

	Charles Meyers, Mohammad Reza Saleh Sedghpour, Tommy Löfstedt, Erik Elmroth


## 2024-01-23
+ [ Securing Recommender System via Cooperative Training](https://arxiv.org/abs/2401.12700)

	Qingyang Wang, Chenwang Wu, Defu Lian, Enhong Chen


+ [ DAFA: Distance-Aware Fair Adversarial Training](https://arxiv.org/abs/2401.12532)

	Hyungyu Lee, Saehyung Lee, Hyemi Jang, Junsung Park, Ho Bae, Sungroh Yoon


+ [ Fast Adversarial Training against Textual Adversarial Attacks](https://arxiv.org/abs/2401.12461)

	Yichen Yang, Xin Liu, Kun He


+ [ MAPPING: Debiasing Graph Neural Networks for Fair Node Classification  with Limited Sensitive Information Leakage](https://arxiv.org/abs/2401.12824)

	Ying Song, Balaji Palanisamy


+ [ ToDA: Target-oriented Diffusion Attacker against Recommendation System](https://arxiv.org/abs/2401.12578)

	Xiaohao Liu, Zhulin Tao, Ting Jiang, He Chang, Yunshan Ma, Xianglin Huang


+ [ Digital Divides in Scene Recognition: Uncovering Socioeconomic Biases in  Deep Learning Systems](https://arxiv.org/abs/2401.13097)

	Michelle R. Greene, Mariam Josyula, Wentao Si, Jennifer A. Hart


+ [ The Language Barrier: Dissecting Safety Challenges of LLMs in  Multilingual Contexts](https://arxiv.org/abs/2401.13136)

	Lingfeng Shen, Weiting Tan, Sihao Chen, Yunmo Chen, Jingyu Zhang, Haoran Xu, Boyuan Zheng, Philipp Koehn, Daniel Khashabi


## 2024-01-22
+ [ GI-PIP: Do We Require Impractical Auxiliary Dataset for Gradient  Inversion Attacks?](https://arxiv.org/abs/2401.11748)

	Yu sun, Gaojian Xiong, Xianxun Yao, Kailang Ma, Jian Cui


+ [ Safe and Generalized end-to-end Autonomous Driving System with  Reinforcement Learning and Demonstrations](https://arxiv.org/abs/2401.11792)

	Zuojin Tang, Xiaoyu Chen, YongQiang Li, Jianyu Chen


+ [ Robustness to distribution shifts of compressed networks for edge  devices](https://arxiv.org/abs/2401.12014)

	Lulan Shen, Ali Edalati, Brett Meyer, Warren Gross, James J. Clark


+ [ Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated  Text](https://arxiv.org/abs/2401.12070)

	Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein


+ [ Text Embedding Inversion Attacks on Multilingual Language Models](https://arxiv.org/abs/2401.12192)

	Yiyi Chen, Heather Lent, Johannes Bjerva


+ [ A Training-Free Defense Framework for Robust Learned Image Compression](https://arxiv.org/abs/2401.11902)

	Myungseo Song, Jinyoung Choi, Bohyung Han


+ [ Privacy-Preserving Data Fusion for Traffic State Estimation: A Vertical  Federated Learning Approach](https://arxiv.org/abs/2401.11836)

	Qiqing Wang, Kaidi Yang


+ [ Analyzing the Quality Attributes of AI Vision Models in Open  Repositories Under Adversarial Attacks](https://arxiv.org/abs/2401.12261)

	Zerui Wang, Yan Liu


+ [ GRATH: Gradual Self-Truthifying for Large Language Models](https://arxiv.org/abs/2401.12292)

	Weixin Chen, Bo Li


## 2024-01-21
+ [ Finding a Needle in the Adversarial Haystack: A Targeted Paraphrasing  Approach For Uncovering Edge Cases with Minimal Distribution Distortion](https://arxiv.org/abs/2401.11373)

	Aly M. Kassem, Sherif Saad


+ [ Adversarial Augmentation Training Makes Action Recognition Models More  Robust to Realistic Video Distribution Shifts](https://arxiv.org/abs/2401.11406)

	Kiyoon Kim, Shreyank N Gowda, Panagiotis Eustratiadis, Antreas Antoniou, Robert B Fisher


+ [ TetraLoss: Improving the Robustness of Face Recognition against Morphing  Attacks](https://arxiv.org/abs/2401.11598)

	Mathias Ibsen, Lázaro J. González-Soler, Christian Rathgeb, Christoph Busch


+ [ How Robust Are Energy-Based Models Trained With Equilibrium Propagation?](https://arxiv.org/abs/2401.11543)

	Siddharth Mansingh, Michal Kucer, Garrett Kenyon, Juston Moore, Michael Teti


## 2024-01-20
+ [ CARE: Ensemble Adversarial Robustness Evaluation Against Adaptive  Attackers for Security Applications](https://arxiv.org/abs/2401.11126)

	Hangsheng Zhang, Jiqiang Liu, Jinsong Dong


+ [ BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models](https://arxiv.org/abs/2401.12242)

	Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, Bo Li


## 2024-01-19
+ [ PuriDefense: Randomized Local Implicit Adversarial Purification for  Defending Black-box Query-based Attacks](https://arxiv.org/abs/2401.10586)

	Ping Guo, Zhiyuan Yang, Xi Lin, Qingchuan Zhao, Qingfu Zhang


+ [ Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs  Without Fine-Tuning](https://arxiv.org/abs/2401.10862)

	Adib Hasan, Ileana Rugina, Alex Wang


+ [ Mitigating Hallucinations of Large Language Models via Knowledge  Consistent Alignment](https://arxiv.org/abs/2401.10768)

	Fanqi Wan, Xinting Huang, Leyang Cui, Xiaojun Quan, Wei Bi, Shuming Shi


+ [ Differentially Private and Adversarially Robust Machine Learning: An  Empirical Evaluation](https://arxiv.org/abs/2401.10405)

	Janvi Thakkar, Giulio Zizzo, Sergio Maffeis


+ [ Adversarially Robust Signed Graph Contrastive Learning from Balance  Augmentation](https://arxiv.org/abs/2401.10590)

	Jialong Zhou, Xing Ai, Yuni Lai, Kai Zhou


+ [ Hacking Predictors Means Hacking Cars: Using Sensitivity Analysis to  Identify Trajectory Prediction Vulnerabilities for Autonomous Driving  Security](https://arxiv.org/abs/2401.10313)

	Marsalis Gibson, David Babazadeh, Claire Tomlin, Shankar Sastry


+ [ A Lightweight Multi-Attack CAN Intrusion Detection System on Hybrid  FPGAs](https://arxiv.org/abs/2401.10689)

	Shashwat Khandelwal, Shreejith Shanker


+ [ Real-Time Zero-Day Intrusion Detection System for Automotive Controller  Area Network on FPGAs](https://arxiv.org/abs/2401.10724)

	Shashwat Khandelwal, Shreejith Shanker


+ [ The Surprising Harmfulness of Benign Overfitting for Adversarial  Robustness](https://arxiv.org/abs/2401.12236)

	Yifan Hao, Tong Zhang

	
## 2024-01-18
+ [ Uncertainty-Aware Hardware Trojan Detection Using Multimodal Deep  Learning](https://arxiv.org/abs/2401.09479)

	Rahul Vishwakarma, Amin Rezaei


+ [ Power in Numbers: Robust reading comprehension by finetuning with four  adversarial sentences per example](https://arxiv.org/abs/2401.10091)

	Ariel Marcus


+ [ Marrying Adapters and Mixup to Efficiently Enhance the Adversarial  Robustness of Pre-Trained Language Models for Text Classification](https://arxiv.org/abs/2401.10111)

	Tuc Nguyen, Thai Le


+ [ Artwork Protection Against Neural Style Transfer Using Locally Adaptive  Adversarial Color Attack](https://arxiv.org/abs/2401.09673)

	Zhongliang Guo, Kaixuan Wang, Weiye Li, Yifei Qian, Ognjen Arandjelović, Lei Fang


+ [ Cross-Modality Perturbation Synergy Attack for Person Re-identification](https://arxiv.org/abs/2401.10090)

	Yunpeng Gong, others


+ [ MedBlindTuner: Towards Privacy-preserving Fine-tuning on Biomedical  Images with Transformers and Fully Homomorphic Encryption](https://arxiv.org/abs/2401.09604)

	Prajwal Panzade, Daniel Takabi, Zhipeng Cai


+ [ MITS-GAN: Safeguarding Medical Imaging from Tampering with Generative  Adversarial Networks](https://arxiv.org/abs/2401.09624)

	Giovanni Pasqualino, Luca Guarnera, Alessandro Ortis, Sebastiano Battiato


+ [ Universally Robust Graph Neural Networks by Preserving Neighbor  Similarity](https://arxiv.org/abs/2401.09754)

	Yulin Zhu, Yuni Lai, Xing Ai, Kai Zhou


+ [ HGAttack: Transferable Heterogeneous Graph Adversarial Attack](https://arxiv.org/abs/2401.09945)

	He Zhao, Zhiwei Zeng, Yongwei Wang, Deheng Ye, Chunyan Miao


+ [ Hijacking Attacks against Neural Networks by Analyzing Training Data](https://arxiv.org/abs/2401.09740)

	Yunjie Ge, Qian Wang, Huayang Huang, Qi Li, Cong Wang, Chao Shen, Lingchen Zhao, Peipei Jiang, Zheng Fang, Shenyi Zhang


## 2024-01-17
+ [MITS-GAN: Safeguarding Medical Imaging from Tampering with Generative Adversarial Networks](https://arxiv.org/abs/2401.09624)

	Giovanni Pasqualino, Luca Guarnera, Alessandro Ortis, Sebastiano Battiato


+ [An Optimal Transport Approach for Computing Adversarial Training Lower Bounds in Multiclass Classification](https://arxiv.org/abs/2401.09191)

	Nicolas Garcia Trillos, Matt Jacobs, Jakwang Kim, Matthew Werenski


+ [Artwork Protection Against Neural Style Transfer Using Locally Adaptive Adversarial Color Attack](https://arxiv.org/abs/2401.09673)

	Zhongliang Guo, Kaixuan Wang, Weiye Li, Yifei Qian, Ognjen Arandjelović, Lei Fang


## 2024-01-16
+ [ Bag of Tricks to Boost Adversarial Transferability](https://arxiv.org/abs/2401.08734)

	Zeliang Zhang, Rongyi Zhu, Wei Yao, Xiaosen Wang, Chenliang Xu


+ [ Towards Efficient and Certified Recovery from Poisoning Attacks in Federated Learning](https://arxiv.org/abs/2401.08216)

	Yu Jiang, Jiyuan Shen, Ziyao Liu, Chee Wei Tan, Kwok-Yan Lam


+ [ PPR: Enhancing Dodging Attacks while Maintaining Impersonation Attacks on Face Recognition Systems](https://arxiv.org/abs/2401.08903)

	Fengfan Zhou, Heifei Ling


+ [ A Generative Adversarial Attack for Multilingual Text Classifiers](https://arxiv.org/abs/2401.08255)

	Tom Roth, Inigo Jauregi Unanue, Alsharif Abuadbba, Massimo Piccardi



## 2024-01-15
+ [ Left-right Discrepancy for Adversarial Attack on Stereo Networks](https://arxiv.org/abs/2401.07188)

	Pengfei Wang, Xiaofei Hui, Beijia Lu, Nimrod Lilith, Jun Liu, Sameer Alam


+ [ Exploring Adversarial Attacks against Latent Diffusion Model from the Perspective of Adversarial Transferability](https://arxiv.org/abs/2401.07087)

	Junxi Chen, Junhao Dong, Xiaohua Xie


+ [ Crafter: Facial Feature Crafting against Inversion-based Identity Theft on Deep Models](https://arxiv.org/abs/2401.07205)

	Shiming Wang, Zhe Ji, Liyao Xiang, Hao Zhang, Xinbing Wang, Chenghu Zhou, Bo Li


+ [ Robustness Against Adversarial Attacks via Learning Confined Adversarial Polytopes](https://arxiv.org/abs/2401.06373)

	Shayan Mohajer Hamidi; Linfeng Ye


## 2024-01-12
+ [ An Analytical Framework for Modeling and Synthesizing Malicious Attacks on ACC Vehicles](https://arxiv.org/abs/2401.06916)

	Shian Wang


+ [ Adversarial Examples are Misaligned in Diffusion Model Manifolds](https://arxiv.org/abs/2401.06637)

	Peter Lorenz, Ricard Durall, Janis Keuper


+ [ How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs](https://arxiv.org/abs/2401.06373)

	Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, Weiyan Shi


## 2024-01-11
+ [ Combating Adversarial Attacks with Multi-Agent Debate](https://arxiv.org/abs/2401.05998)

	Steffi Chern, Zhen Fan, Andy Liu


+ [ GE-AdvGAN: Improving the transferability of adversarial samples by gradient editing-based adversarial generative model](https://arxiv.org/abs/2401.06031)

	Zhiyu Zhu, Huaming Chen, Xinyi Wang, Jiayu Zhang, Zhibo Jin, Kim-Kwang Raymond Choo


+ [ Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks](https://arxiv.org/abs/2401.05949)

	Shuai Zhao, Meihuizi Jia, Luu Anh Tuan, Jinming Wen


+ [ Can We Trust the Unlabeled Target Data? Towards Backdoor Attack and Defense on Model Adaptation](https://arxiv.org/abs/2401.06030)

	Lijun Sheng, Jian Liang, Ran He, Zilei Wang, Tieniu Tan


## 2024-01-10
+ [ Revisiting Adversarial Training at Scale](https://arxiv.org/abs/2401.04727)

	Zeyu Wang, Xianhang Li, Hongru Zhu, Cihang Xie


+ [ SoK: Facial Deepfake Detectors](https://arxiv.org/abs/2401.04364)

	Binh M. Le, Jiwon Kim, Shahroz Tariq, Kristen Moore, Alsharif Abuadbba, Simon S. Woo


+ [ Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training](https://arxiv.org/abs/2401.05566)

	Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M. Ziegler, Tim Maxwell, Newton Cheng, Adam Jermyn, Amanda Askell, Ansh Radhakrishnan, Cem Anil, David Duvenaud, Deep Ganguli, Fazl Barez, Jack Clark, Kamal Ndousse, Kshitij Sachan, Michael Sellitto, Mrinank Sharma, Nova DasSarma, Roger Grosse, Shauna Kravec, Yuntao Bai, Zachary Witten, Marina Favaro, Jan Brauner, Holden Karnofsky, Paul Christiano, Samuel R. Bowman, Logan Graham, Jared Kaplan, Sören Mindermann, Ryan Greenblatt, Buck Shlegeris, Nicholas Schiefer, Ethan Perez


+ [ TrustLLM: Trustworthiness in Large Language Models](https://arxiv.org/abs/2401.05561)

	Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zhengliang Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bhavya Kailkhura, Caiming Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes, Neil Zhenqiang Gong, Philip S. Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong Chen, Tianming Liu, Tianyi Zhou, Willian Wang, Xiang Li, Xiangliang Zhang, Xiao Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen, Yue Zhao


+ [ Exploring Vulnerabilities of No-Reference Image Quality Assessment Models: A Query-Based Black-Box Method](https://arxiv.org/abs/2401.05217)

	Chenxi Yang, Yujia Liu, Dingquan Li, Tingting Jiang


## 2024-01-09
+ [ Deep Anomaly Detection in Text](https://arxiv.org/abs/2401.02971)

	Andrei Manolache


+ [ Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning  for Safe and Efficient Autonomous Driving](https://arxiv.org/abs/2401.03160)

	Zilin Huang, Zihao Sheng, Chengyuan Ma, Sikai Chen


+ [ An Investigation of Large Language Models for Real-World Hate Speech  Detection](https://arxiv.org/abs/2401.03346)

	Keyan Guo, Alexander Hu, Jaden Mu, Ziheng Shi, Ziming Zhao, Nishant Vishwamitra, Hongxin Hu


+ [ LLM-Powered Code Vulnerability Repair with Reinforcement Learning and  Semantic Reward](https://arxiv.org/abs/2401.03374)

	Nafis Tanveer Islam, Joseph Khoury, Andrew Seong, Gonzalo De La Torre Parra, Elias Bou-Harb, Peyman Najafirad


+ [ GLOCALFAIR: Jointly Improving Global and Local Group Fairness in  Federated Learning](https://arxiv.org/abs/2401.03562)

	Syed Irfan Ali Meerza, Luyang Liu, Jiaxin Zhang, Jian Liu


+ [ Few-Shot Causal Representation Learning for Out-of-Distribution  Generalization on Heterogeneous Graphs](https://arxiv.org/abs/2401.03597)

	Pengfei Ding, Yan Wang, Guanfeng Liu, Nan Wang


+ [ A Large-scale Empirical Study on Improving the Fairness of Deep Learning  Models](https://arxiv.org/abs/2401.03695)

	Junjie Yang, Jiajun Jiang, Zeyu Sun, Junjie Chen


+ [ The Butterfly Effect of Altering Prompts: How Small Changes and  Jailbreaks Affect Large Language Model Performance](https://arxiv.org/abs/2401.03729)

	Abel Salinas, Fred Morstatter


+ [ Transferable Learned Image Compression-Resistant Adversarial  Perturbations](https://arxiv.org/abs/2401.03115)

	Yang Sui, Zhuohang Li, Ding Ding, Xiang Pan, Xiaozhong Xu, Shan Liu, Zhenzhong Chen


+ [ Adaptive Boosting with Fairness-aware Reweighting Technique for Fair  Classification](https://arxiv.org/abs/2401.03097)

	Xiaobin Song, Zeyuan Liu, Benben Jiang


+ [ Accurate and Scalable Estimation of Epistemic Uncertainty for Graph  Neural Networks](https://arxiv.org/abs/2401.03350)

	Puja Trivedi, Mark Heimann, Rushil Anirudh, Danai Koutra, Jayaraman J. Thiagarajan


## 2024-01-08
+ [ Logits Poisoning Attack in Federated Distillation](http://arxiv.org/abs/2401.03685)

    Yuhan Tang, Zhiyuan Wu, Bo Gao, Tian Wen, Yuwei Wang, Sheng Sun


## 2024-01-07
+ [ Invisible Reflections: Leveraging Infrared Laser Reflections to Target Traffic Sign Perception](http://arxiv.org/abs/2401.03582)

    Takami Sato, Sri Hrushikesh Varma Bhupathiraju, Michael Clifford, Takeshi Sugawara, Qi Alfred Chen, Sara Rampazzi


+ [ Data-Driven Subsampling in the Presence of an Adversarial Actor](http://arxiv.org/abs/2401.03488)

    Abu Shafin Mohammad Mahdee Jameel, Ahmed P. Mohamed, Jinho Yi, Aly El Gamal, Akshay Malhotra


+ [ ROIC-DM: Robust Text Inference and Classification via Diffusion Model](http://arxiv.org/abs/2401.03514)

    Shilong Yuan, Wei Yuan, Tieke HE


## 2024-01-06
+ [ Data-Dependent Stability Analysis of Adversarial Training](http://arxiv.org/abs/2401.03156)

    Yihan Wang, Shuang Liu, Xiao-Shan Gao


+ [ End-to-End Anti-Backdoor Learning on Images and Time Series](http://arxiv.org/abs/2401.03215)

    Yujing Jiang, Xingjun Ma, Sarah Monazam Erfani, Yige Li, James Bailey


## 2024-01-05
+ [ Transferable Learned Image Compression-Resistant Adversarial Perturbations](http://arxiv.org/abs/2401.03115)

    Yang Sui, Zhuohang Li, Ding Ding, Xiang Pan, Xiaozhong Xu, Shan Liu, Zhenzhong Chen


+ [ Enhancing targeted transferability via feature space fine-tuning](http://arxiv.org/abs/2401.02727)

    Hui Zeng, Biwei Chen, Anjie Peng


+ [ Calibration Attack: A Framework For Adversarial Attacks Targeting Calibration](http://arxiv.org/abs/2401.02718)

    Stephen Obadinma, Xiaodan Zhu, Hongyu Guo


+ [ A backdoor attack against link prediction tasks with graph neural networks](http://arxiv.org/abs/2401.02663)

    Jiazhu Dai, Haoyu Sun


+ [ MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance](http://arxiv.org/abs/2401.02906)

    Renjie Pi, Tianyang Han, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong, Jipeng Zhang, Tong Zhang


## 2024-01-04
+ [ Vulnerabilities Unveiled: Adversarially Attacking a Multimodal Vision Langauge Model for Pathology Imaging](http://arxiv.org/abs/2401.02565)   

    Jai Prakash Veerla, Poojitha Thota, Partha Sai Guttikonda, Shirin Nilizadeh, Jacob M. Luber


+ [ A Random Ensemble of Encrypted models for Enhancing Robustness against Adversarial Examples](http://arxiv.org/abs/2401.02633)

    Ryota Iijima, Sayaka Shiota, Hitoshi Kiya


+ [ AdvSQLi: Generating Adversarial SQL Injections against Real-world WAF-as-a-service](http://arxiv.org/abs/2401.02615)

    Zhenqing Qu, Xiang Ling, Ting Wang, Xiang Chen, Shouling Ji, Chunming Wu


+ [ Evasive Hardware Trojan through Adversarial Power Trace](http://arxiv.org/abs/2401.02342)

    Behnam Omidi, Khaled N. Khasawneh, Ihsen Alouani


+ [ Object-oriented backdoor attack against image captioning](http://arxiv.org/abs/2401.02600)

    Meiling Li, Nan Zhong, Xinpeng Zhang, Zhenxing Qian, Sheng Li


+ [ DEM: A Method for Certifying Deep Neural Network Classifier Outputs in Aerospace](http://arxiv.org/abs/2401.02283)

    Guy Katz, Natan Levy, Idan Refaeli, Raz Yerushalmi


+ [ Secure Control of Connected and Automated Vehicles Using Trust-Aware Robust Event-Triggered Control Barrier Functions](http://arxiv.org/abs/2401.02306)

    H M Sabbir Ahmad, Ehsan Sabouni, Akua Dickson, Wei Xiao, Christos G. Cassandras, Wenchao Li


## 2024-01-03
+ [ Towards Robust Semantic Segmentation against Patch-based Attack via Attention Refinement](http://arxiv.org/abs/2401.01750)

    Zheng Yuan, Jie Zhang, Yude Wang, Shiguang Shan, Xilin Chen


+ [ Spy-Watermark: Robust Invisible Watermarking for Backdoor Attack](http://arxiv.org/abs/2401.02031)

    Ruofei Wang, Renjie Wan, Zongyu Guo, Qing Guo, Rui Huang


+ [ FullLoRA-AT: Efficiently Boosting the Robustness of Pretrained Vision Transformers](http://arxiv.org/abs/2401.01752)

    Zheng Yuan, Jie Zhang, Shiguang Shan


+ [ Integrated Cyber-Physical Resiliency for Power Grids under IoT-Enabled Dynamic Botnet Attacks](http://arxiv.org/abs/2401.01963)

    Yuhan Zhao, Juntao Chen, Quanyan Zhu


+ [ Enhancing Generalization of Invisible Facial Privacy Cloak via Gradient Accumulation](http://arxiv.org/abs/2401.01575)

    Xuannan Liu, Yaoyao Zhong, Weihong Deng, Hongzhi Shi, Xingchen Cui, Yunfeng Yin, Dongchao Wen


## 2024-01-02
+ [ Safety and Performance, Why Not Both? Bi-Objective Optimized Model  Compression against Heterogeneous Attacks Toward AI Software Deployment](https://arxiv.org/abs/2401.00996)

	Jie Zhu, Leye Wang, Xiao Han, Anmin Liu, Tao Xie


+ [ Teach Large Language Models to Forget Privacy](https://arxiv.org/abs/2401.00870)

	Ran Yan, Yujun Li, Wenqian Li, Peihua Mai, Yan Pang, Yinchuan Li


+ [ PPBFL: A Privacy Protected Blockchain-based Federated Learning Model](https://arxiv.org/abs/2401.01204)

	Yang Li, Chunhe Xia, Wanshuang Lin, Tianbo Wang


+ [ LLbezpeky: Leveraging Large Language Models for Vulnerability Detection](https://arxiv.org/abs/2401.01269)

	Noble Saji Mathews, Yelizaveta Brus, Yousra Aafer, Mei Nagappan, Shane McIntosh


+ [ Noise-NeRF: Hide Information in Neural Radiance Fields using Trainable  Noise](https://arxiv.org/abs/2401.01216)

	Qinglong Huang, Yong Liao, Yanbin Hao, Pengyuan Zhou


+ [ Imperio: Language-Guided Backdoor Attacks for Arbitrary Model Control](https://arxiv.org/abs/2401.01085)

	Ka-Ho Chow, Wenqi Wei, Lei Yu


+ [ Efficient Sparse Least Absolute Deviation Regression with Differential  Privacy](https://arxiv.org/abs/2401.01294)

	Weidong Liu, Xiaojun Mao, Xiaofei Zhang, Xin Zhang


+ [ Opening A Pandora's Box: Things You Should Know in the Era of Custom  GPTs](https://arxiv.org/abs/2401.00905)

	Guanhong Tao, Siyuan Cheng, Zhuo Zhang, Junmin Zhu, Guangyu Shen, Xiangyu Zhang


+ [ A Novel Evaluation Framework for Assessing Resilience Against Prompt  Injection Attacks in Large Language Models](https://arxiv.org/abs/2401.00991)

	Daniel Wankit Yip, Aysan Esmradi, Chun Fai Chan


+ [ Detection and Defense Against Prominent Attacks on Preconditioned  LLM-Integrated Virtual Assistants](https://arxiv.org/abs/2401.00994)

	Chun Fai Chan, Daniel Wankit Yip, Aysan Esmradi


## 2024-01-01
+ [ Red Teaming for Large Language Models At Scale: Tackling Hallucinations  on Mathematics Tasks](https://arxiv.org/abs/2401.00290)

	Aleksander Buszydlik, Karol Dobiczek, Michał Teodor Okoń, Konrad Skublicki, Philip Lippmann, Jie Yang


+ [ A & B == B & A: Triggering Logical Reasoning Failures in Large Language  Models](https://arxiv.org/abs/2401.00757)

	Yuxuan Wan, Wenxuan Wang, Yiliu Yang, Youliang Yuan, Jen-tse Huang, Pinjia He, Wenxiang Jiao, Michael R. Lyu


+ [ SHARE: Single-view Human Adversarial REconstruction](https://arxiv.org/abs/2401.00343)

	Shreelekha Revankar, Shijia Liao, Yu Shen, Junbang Liang, Huaishu Peng, Ming Lin


+ [ SSL-OTA: Unveiling Backdoor Threats in Self-Supervised Learning for  Object Detection](https://arxiv.org/abs/2401.00137)

	Qiannan Wang, Changchun Yin, Liming Fang, Lu Zhou, Zhe Liu, Run Wang, Chenhao Lin


+ [ Adversarially Trained Actor Critic for offline CMDPs](https://arxiv.org/abs/2401.00629)

	Honghao Wei, Xiyue Peng, Xin Liu, Arnob Ghosh


## 2023-12-31
+ [ Is It Possible to Backdoor Face Forgery Detection with Natural Triggers](http://arxiv.org/abs/2401.00414)

    Xiaoxuan Han, Songlin Yang, Wei Wang, Ziwen He, Jing Dong


## 2023-12-30
+ [ Explainability-Driven Leaf Disease Classification using Adversarial Training and Knowledge Distillation](http://arxiv.org/abs/2401.00334)

    Sebastian-Vasile Echim, Iulian-Marius Tăiatu, Dumitru-Clementin Cercel, Florin Pop       


+ [ CamPro: Camera-based Anti-Facial Recognition](http://arxiv.org/abs/2401.00151)  

    Wenjun Zhu, Yuan Sun, Jiani Liu, Yushi Cheng, Xiaoyu Ji, Wenyuan Xu


+ [ TPatch: A Triggered Physical Adversarial Patch](http://arxiv.org/abs/2401.00148)

    Wenjun Zhu, Xiaoyu Ji, Yushi Cheng, Shibo Zhang, Wenyuan Xu


+ [ A clean-label graph backdoor attack method in node classification task](http://arxiv.org/abs/2401.00163)

    Xiaogang Xing, Ming Xu, Yujing Bai, Dongdong Yang


## 2023-12-29
+ [ Jatmo: Prompt Injection Defense by Task-Specific Finetuning](http://arxiv.org/abs/2312.17673)

    Julien Piet, Maha Alrashed, Chawin Sitawarin, Sizhe Chen, Zeming Wei, Elizabeth Sun, Basel Alomair, David Wagner


+ [ Towards Faithful Explanations for Text Classification with Robustness Improvement and Explanation Guided Training](http://arxiv.org/abs/2312.17591)

    Dongfang Li, Baotian Hu, Qingcai Chen, Shan He


## 2023-12-28
+ [ Layer Attack Unlearning: Fast and Accurate Machine Unlearning via Layer  Level Attack and Knowledge Distillation](https://arxiv.org/abs/2312.16823)

	Hyunjune Kim, Sangyong Lee, Simon S. Woo


+ [ Adversarial Representation with Intra-Modal and Inter-Modal Graph  Contrastive Learning for Multimodal Emotion Recognition](https://arxiv.org/abs/2312.16778)

	Yuntao Shou, Tao Meng, Wei Ai, Keqin Li


+ [ SoK: Taming the Triangle -- On the Interplays between Fairness,  Interpretability and Privacy in Machine Learning](https://arxiv.org/abs/2312.16191)

	Julien Ferry, Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Mohamed Siala


+ [ Adversarial Attacks on Image Classification Models: Analysis and Defense](http://arxiv.org/abs/2312.16880)

    Jaydip Sen, Abhiraj Sen, Ananda Chatterjee


+ [ BlackboxBench: A Comprehensive Benchmark of Black-box Adversarial Attacks](http://arxiv.org/abs/2312.16979)

    Meixi Zheng, Xuanchen Yan, Zihao Zhu, Hongrui Chen, Baoyuan Wu


+ [ Attack Tree Analysis for Adversarial Evasion Attacks](http://arxiv.org/abs/2312.16957)

    Yuki Yamaguchi, Toshiaki Aoki


+ [ DOEPatch: Dynamically Optimized Ensemble Model for Adversarial Patches Generation](http://arxiv.org/abs/2312.16907)

    Wenyi Tan, Yang Li, Chenxing Zhao, Zhunga Liu, Quan Pan


+ [ Securing NextG Systems against Poisoning Attacks on Federated Learning: A Game-Theoretic Solution](http://arxiv.org/abs/2312.17164)

    Yalin E. Sagduyu, Tugba Erpek, Yi Shi


+ [ Timeliness: A New Design Metric and a New Attack Surface](http://arxiv.org/abs/2312.17220)

    Priyanka Kaswan, Sennur Ulukus


## 2023-12-27
+ [ Adversarial Data Poisoning for Fake News Detection: How to Make a Model  Misclassify a Target News without Modifying It](https://arxiv.org/abs/2312.15228)

	Federico Siciliano, Luca Maiano, Lorenzo Papa, Federica Baccin, Irene Amerini, Fabrizio Silvestri


+ [ Adversarial Attacks on LoRa Device Identification and Rogue Signal Detection with Deep Learning](http://arxiv.org/abs/2312.16715)

    Yalin E. Sagduyu, Tugba Erpek


+ [ Domain Generalization with Vital Phase Augmentation](http://arxiv.org/abs/2312.16451)

    Ingyun Lee, Wooju Lee, Hyun Myung


## 2023-12-26
+ [ From Text to Multimodal: A Comprehensive Survey of Adversarial Example Generation in Question Answering Systems](http://arxiv.org/abs/2312.16156) 

    Gulsum Yigit, Mehmet Fatih Amasyali


+ [ Natural Adversarial Patch Generation Method Based on Latent Diffusion Model](http://arxiv.org/abs/2312.16401)

    Xianyi Chen, Fazhan Liu, Dong Jiang, Kai Yan


+ [ Universal Pyramid Adversarial Training for Improved ViT Performance](http://arxiv.org/abs/2312.16339)

    Ping-yeh Chiang, Yipin Zhou, Omid Poursaeed, Satya Narayan Shukla, Ashish Shah, Tom Goldstein, Ser-Nam Lim


+ [ Robust Survival Analysis with Adversarial Regularization](http://arxiv.org/abs/2312.16019)

    Michael Potter, Stefano Maxenti, Michael Everett


## 2023-12-25
+ [ GanFinger: GAN-Based Fingerprint Generation for Deep Neural Network Ownership Verification](http://arxiv.org/abs/2312.15617)

    Huali Ren, Anli Yan, Xiaojun Ren, Pei-Gen Ye, Chong-zhi Gao, Zhili Zhou, Jin Li


+ [ Adversarial Item Promotion on Visually-Aware Recommender Systems by Guided Diffusion](http://arxiv.org/abs/2312.15826)

    Lijian Chen, Wei Yuan, Tong Chen, Quoc Viet Hung Nguyen, Lizhen Cui, Hongzhi Yin


+ [ Punctuation Matters! Stealthy Backdoor Attack for Language Models](http://arxiv.org/abs/2312.15867)

    Xuan Sheng, Zhicheng Li, Zhaoyang Han, Xiangmao Chang, Piji Li


## 2023-12-24
+ [ Benchmarking and Defending Against Indirect Prompt Injection Attacks on  Large Language Models](https://arxiv.org/abs/2312.14197)

	Jingwei Yi, Yueqi Xie, Bin Zhu, Keegan Hines, Emre Kiciman, Guangzhong Sun, Xing Xie, Fangzhao Wu


+ [ Hierarchical Multi-Agent Reinforcement Learning for Assessing False-Data  Injection Attacks on Transportation Networks](https://arxiv.org/abs/2312.14625)

	Taha Eghtesad, Sirui Li, Yevgeniy Vorobeychik, Aron Laszka


+ [ AutoAugment Input Transformation for Highly Transferable Targeted  Attacks](https://arxiv.org/abs/2312.14218)

	Haobo Lu, Xin Liu, Kun He


+ [ Can Machines Learn Robustly, Privately, and Efficiently?](https://arxiv.org/abs/2312.14712)

	Youssef Allouah, Rachid Guerraoui, John Stephan


## 2023-12-23
+ [ Adversarial Data Poisoning for Fake News Detection: How to Make a Model Misclassify a Target News without Modifying It](http://arxiv.org/abs/2312.15228)

    Federico Siciliano, Luca Maiano, Lorenzo Papa, Federica Baccin, Irene Amerini, Fabrizio Silvestri


+ [ Pre-trained Trojan Attacks for Visual Recognition](http://arxiv.org/abs/2312.15172)

    Aishan Liu, Xinwei Zhang, Yisong Xiao, Yuguang Zhou, Siyuan Liang, Jiakai Wang, Xianglong Liu, Xiaochun Cao, Dacheng Tao


## 2023-12-22
+ [ MEAOD: Model Extraction Attack against Object Detectors](http://arxiv.org/abs/2312.14677)

    Zeyu Li, Chenghui Shi, Yuwen Pu, Xuhong Zhang, Yu Li, Jinbao Li, Shouling Ji


+ [ Asymmetric Bias in Text-to-Image Generation with Adversarial Attacks](http://arxiv.org/abs/2312.14440)

    Haz Sameen Shahgir, Xianghao Kong, Greg Ver Steeg, Yue Dong


+ [ Understanding the Regularity of Self-Attention with Optimal Transport](http://arxiv.org/abs/2312.14820)

    Valérie Castin, Pierre Ablin, Gabriel Peyré


+ [ Attacking Byzantine Robust Aggregation in High Dimensions](http://arxiv.org/abs/2312.14461)

    Sarthak Choudhary, Aashish Kolluri, Prateek Saxena


+ [ SODA: Protecting Proprietary Information in On-Device Machine Learning Models](http://arxiv.org/abs/2312.15036)

    Akanksha Atrey, Ritwik Sinha, Saayan Mitra, Prashant Shenoy


+ [ Energy-based learning algorithms for analog computing: a comparative study](http://arxiv.org/abs/2312.15103)

    Benjamin Scellier, Maxence Ernoult, Jack Kendall, Suhas Kumar


+ [ Adaptive Domain Inference Attack](http://arxiv.org/abs/2312.15088)

    Yuechun Gu, Keke Chen


## 2023-12-21
+ [ Adversarial Markov Games: On Adaptive Decision-Based Attacks and  Defenses](https://arxiv.org/abs/2312.13435)

	Ilias Tsingenopoulos, Vera Rimmer, Davy Preuveneers, Fabio Pierazzi, Lorenzo Cavallaro, Wouter Joosen


+ [ Using GPT-4 Prompts to Determine Whether Articles Contain Functional  Evidence Supporting or Refuting Variant Pathogenicity](https://arxiv.org/abs/2312.13521)

	Samuel J. Aronson, Kalotina Machini, Pranav Sriraman, Jiyeon Shin, Emma R. Henricks, Charlotte Mailly, Angie J. Nottage, Michael Oates, Matthew S. Lebo


+ [ SADA: Semantic adversarial unsupervised domain adaptation for Temporal  Action Localization](https://arxiv.org/abs/2312.13377)

	David Pujol-Perich, Albert Clapés, Sergio Escalera


+ [ ARBiBench: Benchmarking Adversarial Robustness of Binarized Neural  Networks](https://arxiv.org/abs/2312.13575)

	Peng Zhao, Jiehua Zhang, Bowen Peng, Longguang Wang, YingMei Wei, Yu Liu, Li Liu


+ [ Where and How to Attack? A Causality-Inspired Recipe for Generating  Counterfactual Adversarial Examples](https://arxiv.org/abs/2312.13628)

	Ruichu Cai, Yuxuan Zhu, Jie Qiao, Zefeng Liang, Furui Liu, Zhifeng Hao


+ [ Manipulating Trajectory Prediction with Backdoors](https://arxiv.org/abs/2312.13863)

	Kaouther Massoud, Kathrin Grosse, Mickael Chen, Matthieu Cord, Patrick Pérez, Alexandre Alahi


## 2023-12-20
+ [ Progressive Poisoned Data Isolation for Training-time Backdoor Defense](https://arxiv.org/abs/2312.12724)

	Yiming Chen, Haiwei Wu, Jiantao Zhou


+ [ PGN: A perturbation generation network against deep reinforcement  learning](https://arxiv.org/abs/2312.12904)

	Xiangjuan Li, Feifan Li, Yang Li, Quan Pan


+ [ Learning and Forgetting Unsafe Examples in Large Language Models](https://arxiv.org/abs/2312.12736)

	Jiachen Zhao, Zhun Deng, David Madras, James Zou, Mengye Ren


+ [ Adaptive Distribution Masked Autoencoders for Continual Test-Time  Adaptation](https://arxiv.org/abs/2312.12480)

	Jiaming Liu, Ran Xu, Senqiao Yang, Renrui Zhang, Qizhe Zhang, Zehui Chen, Yandong Guo, Shanghang Zhang


+ [ Misalign, Contrast then Distill: Rethinking Misalignments in  Language-Image Pretraining](https://arxiv.org/abs/2312.12661)

	Bumsoo Kim, Yeonsik Jo, Jinhyung Kim, Seung Hwan Kim


+ [ Mutual-modality Adversarial Attack with Semantic Perturbation](https://arxiv.org/abs/2312.12768)

	Jingwen Ye, Ruonan Yu, Songhua Liu, Xinchao Wang


+ [ Trust, But Verify: A Survey of Randomized Smoothing Techniques](https://arxiv.org/abs/2312.12608)

	Anupriya Kumari, Devansh Bhardwaj, Sukrit Jindal, Sarthak Gupta


+ [ Stability of Graph Convolutional Neural Networks through the lens of  small perturbation analysis](https://arxiv.org/abs/2312.12934)

	Lucia Testa, Claudio Battiloro, Stefania Sardellitti, Sergio Barbarossa


+ [ Neural Stochastic Differential Equations with Change Points: A  Generative Adversarial Approach](https://arxiv.org/abs/2312.13152)

	Zhongchang Sun, Yousef El-Laham, Svitlana Vyetrenko


+ [ SkyMask: Attack-agnostic Robust Federated Learning with Fine-grained  Learnable Masks](https://arxiv.org/abs/2312.12484)

	Peishen Yan, Hao Wang, Tao Song, Yang Hua, Ruhui Ma, Ningxin Hu, Mohammad R. Haghighat, Haibing Guan


+ [ Can Large Language Models Identify And Reason About Security  Vulnerabilities? Not Yet](https://arxiv.org/abs/2312.12575)

	Saad Ullah, Mingji Han, Saurabh Pujar, Hammond Pearce, Ayse Coskun, Gianluca Stringhini


## 2023-12-19
+ [ A Red Teaming Framework for Securing AI in Maritime Autonomous Systems](https://arxiv.org/abs/2312.11500)

	Mathew J. Walter, Aaron Barrett, Kimberly Tam


+ [ Maatphor: Automated Variant Analysis for Prompt Injection Attacks](https://arxiv.org/abs/2312.11513)

	Ahmed Salem, Andrew Paverd, Boris Köpf


+ [ Robust Communicative Multi-Agent Reinforcement Learning with Active  Defense](https://arxiv.org/abs/2312.11545)

	Lebin Yu, Yunbo Qiu, Quanming Yao, Yuan Shen, Xudong Zhang, Jian Wang


+ [ Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation  in ultra low-data regimes](https://arxiv.org/abs/2312.12112)

	Nabeel Seedat, Nicolas Huynh, Boris van Breugel, Mihaela van der Schaar


+ [ Bypassing the Safety Training of Open-Source LLMs with Priming Attacks](https://arxiv.org/abs/2312.12321)

	Jason Vega, Isha Chaudhary, Changming Xu, Gagandeep Singh


+ [ Chasing Fairness in Graphs: A GNN Architecture Perspective](https://arxiv.org/abs/2312.12369)

	Zhimeng Jiang, Xiaotian Han, Chao Fan, Zirui Liu, Na Zou, Ali Mostafavi, Xia Hu


+ [ Adversarial AutoMixup](https://arxiv.org/abs/2312.11954)

	Huafeng Qin, Xin Jin, Yun Jiang, Mounim A. El-Yacoubi, Xinbo Gao


+ [ Prompting Hard or Hardly Prompting: Prompt Inversion for Text-to-Image  Diffusion Models](https://arxiv.org/abs/2312.12416)

	Shweta Mahajan, Tanzila Rahman, Kwang Moo Yi, Leonid Sigal


+ [ A Study on Transferability of Deep Learning Models for Network Intrusion  Detection](https://arxiv.org/abs/2312.11550)

	Shreya Ghosh, Abu Shafin Mohammad Mahdee Jameel, Aly El Gamal


+ [ Android Malware Detection with Unbiased Confidence Guarantees](https://arxiv.org/abs/2312.11559)

	Harris Papadopoulos, Nestoras Georgiou, Charalambos Eliades, Andreas Konstantinidis


+ [ Terrapin Attack: Breaking SSH Channel Integrity By Sequence Number  Manipulation](https://arxiv.org/abs/2312.12422)

	Fabian Bäumer, Marcus Brinkmann, Jörg Schwenk


## 2023-12-18
+ [ Annotation-Free Automatic Music Transcription with Scalable Synthetic  Data and Adversarial Domain Confusion](https://arxiv.org/abs/2312.10402)

	Gakusei Sato, Taketo Akama


+ [ SAME: Sample Reconstruction Against Model Extraction Attacks](https://arxiv.org/abs/2312.10578)

	Yi Xie, Jie Zhang, Shiqian Zhao, Tianwei Zhang, Xiaofeng Chen


+ [ Synthesizing Black-box Anti-forensics DeepFakes with High Visual Quality](https://arxiv.org/abs/2312.10713)

	Bing Fan, Shu Hu, Feng Ding


+ [ Unmasking Deepfake Faces from Videos Using An Explainable Cost-Sensitive  Deep Learning Approach](https://arxiv.org/abs/2312.10740)

	Faysal Mahmud, Yusha Abdullah, Minhajul Islam, Tahsin Aziz


+ [ DataElixir: Purifying Poisoned Dataset to Mitigate Backdoor Attacks via  Diffusion Models](https://arxiv.org/abs/2312.11057)

	Jiachen Zhou, Peizhuo Lv, Yibing Lan, Guozhu Meng, Kai Chen, Hualong Ma


+ [ Adv-Diffusion: Imperceptible Adversarial Face Identity Attack via Latent  Diffusion Model](https://arxiv.org/abs/2312.11285)

	Decheng Liu, Xijun Wang, Chunlei Peng, Nannan Wang, Ruiming Hu, Xinbo Gao


+ [ Bengali Intent Classification with Generative Adversarial BERT](https://arxiv.org/abs/2312.10679)

	Mehedi Hasan, Mohammad Jahid Ibna Basher, Md. Tanvir Rouf Shawon


+ [ Perturbation-Invariant Adversarial Training for Neural Ranking Models:  Improving the Effectiveness-Robustness Trade-Off](https://arxiv.org/abs/2312.10329)

	Yu-An Liu, Ruqing Zhang, Mingkun Zhang, Wei Chen, Maarten de Rijke, Jiafeng Guo, Xueqi Cheng


+ [ Closing the Gap: Achieving Better Accuracy-Robustness Tradeoffs Against  Query-Based Attacks](https://arxiv.org/abs/2312.10132)

	Pascal Zimmer, Sébastien Andreina, Giorgia Azzurra Marson, Ghassan Karame


+ [ Anomaly Score: Evaluating Generative Models and Individual Generated  Images based on Complexity and Vulnerability](https://arxiv.org/abs/2312.10634)

	Jaehui Hwang, Junghyuk Lee, Jong-Seok Lee


+ [ The Ultimate Combo: Boosting Adversarial Example Transferability by  Composing Data Augmentations](https://arxiv.org/abs/2312.11309)

	Zebin Yun, Achi-Or Weingarten, Eyal Ronen, Mahmood Sharif


+ [ On Robustness to Missing Video for Audiovisual Speech Recognition](https://arxiv.org/abs/2312.10088)

	Oscar Chang, Otavio Braga, Hank Liao, Dmitriy Serdyuk, Olivier Siohan


+ [ Rethinking Robustness of Model Attributions](https://arxiv.org/abs/2312.10534)

	Sandesh Kamath, Sankalp Mittal, Amit Deshpande, Vineeth N Balasubramanian


+ [ The Pros and Cons of Adversarial Robustness](https://arxiv.org/abs/2312.10911)

	Yacine Izza, Joao Marques-Silva


+ [ PPIDSG: A Privacy-Preserving Image Distribution Sharing Scheme with GAN  in Federated Learning](https://arxiv.org/abs/2312.10380)

	Yuting Ma, Yuanzhi Yao, Xiaohua Xu


+ [ TrojFSP: Trojan Insertion in Few-shot Prompt Tuning](https://arxiv.org/abs/2312.10467)

	Mengxin Zheng, Jiaqi Xue, Xun Chen, YanShan Wang, Qian Lou, Lei Jiang


+ [ TrojFair: Trojan Fairness Attacks](https://arxiv.org/abs/2312.10508)

	Mengxin Zheng, Jiaqi Xue, Yi Sheng, Lei Yang, Qian Lou, Lei Jiang


+ [ Adversarially Balanced Representation for Continuous Treatment Effect  Estimation](https://arxiv.org/abs/2312.10570)

	Amirreza Kazemi, Martin Ester


+ [ Model Stealing Attack against Graph Classification with Authenticity,  Uncertainty and Diversity](https://arxiv.org/abs/2312.10943)

	Zhihao Zhu, Chenwang Wu, Rui Fan, Yi Yang, Defu Lian, Enhong Chen


+ [ MISA: Unveiling the Vulnerabilities in Split Federated Learning](https://arxiv.org/abs/2312.11026)

	Wei Wan, Yuxuan Ning, Shengshan Hu1, Lulu Xue, Minghui Li, Leo Yu Zhang, Hai Jin


+ [ Uncertainty-based Fairness Measures](https://arxiv.org/abs/2312.11299)

	Selim Kuzucu, Jiaee Cheong, Hatice Gunes, Sinan Kalkan


+ [ Improving Environment Robustness of Deep Reinforcement Learning  Approaches for Autonomous Racing Using Bayesian Optimization-based Curriculum  Learning](https://arxiv.org/abs/2312.10557)

	Rohan Banerjee, Prishita Ray, Mark Campbell


+ [ Harnessing Inherent Noises for Privacy Preservation in Quantum Machine  Learning](https://arxiv.org/abs/2312.11126)

	Keyi Ju, Xiaoqi Qin, Hui Zhong, Xinyue Zhang, Miao Pan, Baoling Liu


+ [ UltraClean: A Simple Framework to Train Robust Neural Networks against  Backdoor Attacks](https://arxiv.org/abs/2312.10657)

	Bingyin Zhao, Yingjie Lao


+ [ A Mutation-Based Method for Multi-Modal Jailbreaking Attack Detection](https://arxiv.org/abs/2312.10766)

	Xiaoyu Zhang, Cen Zhang, Tianlin Li, Yihao Huang, Xiaojun Jia, Xiaofei Xie, Yang Liu, Chao Shen


+ [ Federated learning with differential privacy and an untrusted aggregator](https://arxiv.org/abs/2312.10789)

	Kunlong Liu, Trinabh Gupta


+ [ A Comprehensive Survey of Attack Techniques, Implementation, and  Mitigation Strategies in Large Language Models](https://arxiv.org/abs/2312.10982)

	Aysan Esmradi, Daniel Wankit Yip, Chun Fai Chan


## 2023-12-17
+ [ Investigating Responsible AI for Scientific Research: An Empirical Study](https://arxiv.org/abs/2312.09561)

	Muneera Bano, Didar Zowghi, Pip Shea, Georgina Ibarra


+ [ Continual Adversarial Defense](https://arxiv.org/abs/2312.09481)

	Qian Wang, Yaoyao Liu, Hefei Ling, Yingwei Li, Qihao Liu, Ping Li, Jiazhong Chen, Alan Yuille, Ning Yu


+ [ SlowTrack: Increasing the Latency of Camera-based Perception in  Autonomous Driving Using Adversarial Examples](https://arxiv.org/abs/2312.09520)

	Chen Ma, Ningfei Wang, Qi Alfred Chen, Chao Shen


+ [ Embodied Adversarial Attack: A Dynamic Robust Physical Attack in  Autonomous Driving](https://arxiv.org/abs/2312.09554)

	Yitong Sun, Yao Huang, Xingxing Wei


+ [ Adversarial Robustness on Image Classification with $k$-means](https://arxiv.org/abs/2312.09533)

	Rollin Omari, Junae Kim, Paul Montague


+ [ Fragility, Robustness and Antifragility in Deep Learning](https://arxiv.org/abs/2312.09821)

	Chandresh Pravin, Ivan Martino, Giuseppe Nicosia, Varun Ojha


+ [ Reliable Probabilistic Classification with Neural Networks](https://arxiv.org/abs/2312.09912)

	Harris Papadopoulos


+ [ A Malware Classification Survey on Adversarial Attacks and Defences](https://arxiv.org/abs/2312.09636)

	Mahesh Datta Sai Ponnuru, Likhitha Amasala, Tanu Sree Bhimavarapu, Guna Chaitanya Garikipati


+ [ Silent Guardian: Protecting Text from Malicious Exploitation by Large  Language Models](https://arxiv.org/abs/2312.09669)

	Jiawei Zhao, Kejiang Chen, Xiaojian Yuan, Yuang Qi, Weiming Zhang, Nenghai Yu


## 2023-12-16
+ [ Towards Inductive Robustness: Distilling and Fostering Wave-induced  Resonance in Transductive GCNs Against Graph Adversarial Attacks](https://arxiv.org/abs/2312.08651)

	Ao Liu, Wenshan Li, Tao Li, Beibei Li, Hanyuan Huang, Pan Zhou


## 2023-12-15
+ [ PhasePerturbation: Speech Data Augmentation via Phase Perturbation for  Automatic Speech Recognition](https://arxiv.org/abs/2312.08571)

	Chengxi Lei, Satwinder Singh, Feng Hou, Xiaoyun Jia, Ruili Wang


+ [ Data and Model Poisoning Backdoor Attacks on Wireless Federated  Learning, and the Defense Mechanisms: A Comprehensive Survey](https://arxiv.org/abs/2312.08667)

	Yichen Wan, Youyang Qu, Wei Ni, Yong Xiang, Longxiang Gao, Ekram Hossain


+ [ The Earth is Flat because...: Investigating LLMs' Belief towards  Misinformation via Persuasive Conversation](https://arxiv.org/abs/2312.09085)

	Rongwu Xu, Brian S. Lin, Shujian Yang, Tianqi Zhang, Weiyan Shi, Tianwei Zhang, Zhixuan Fang, Wei Xu, Han Qiu


+ [ Privacy Constrained Fairness Estimation for Decision Trees](https://arxiv.org/abs/2312.08413)

	Florian van der Steen, Fré Vink, Heysem Kaya


+ [ Scalable Ensemble-based Detection Method against Adversarial Attacks for  speaker verification](https://arxiv.org/abs/2312.08622)

	Haibin Wu, Heng-Cheng Kuo, Yu Tsao, Hung-yi Lee

	
## 2023-12-14
+ [ AVA: Inconspicuous Attribute Variation-based Adversarial Attack bypassing DeepFake Detection](http://arxiv.org/abs/2312.08675)

    Xiangtao Meng, Li Wang, Shanqing Guo, Lei Ju, Qingchuan Zhao


+ [ On the Difficulty of Defending Contrastive Learning against Backdoor Attacks](http://arxiv.org/abs/2312.09057)

    Changjiang Li, Ren Pang, Bochuan Cao, Zhaohan Xi, Jinghui Chen, Shouling Ji, Ting Wang


+ [ Detection and Defense of Unlearnable Examples](http://arxiv.org/abs/2312.08898)

    Yifan Zhu, Lijia Yu, Xiao-Shan Gao


+ [ Improve Robustness of Reinforcement Learning against Observation Perturbations via $l_\infty$ Lipschitz Policy Networks](http://arxiv.org/abs/2312.08751)

    Buqing Nie, Jingtian Ji, Yangqing Fu, Yue Gao


+ [ Data and Model Poisoning Backdoor Attacks on Wireless Federated Learning, and the Defense Mechanisms: A Comprehensive Survey](http://arxiv.org/abs/2312.08667)     

    Yichen Wan, Youyang Qu, Wei Ni, Yong Xiang, Longxiang Gao, Ekram Hossain


+ [ DRAM-Locker: A General-Purpose DRAM Protection Mechanism against Adversarial DNN Weight Attacks](http://arxiv.org/abs/2312.09027)

    Ranyang Zhou, Sabbir Ahmed, Arman Roohi, Adnan Siraj Rakin, Shaahin Angizi


+ [ Forbidden Facts: An Investigation of Competing Objectives in Llama-2](http://arxiv.org/abs/2312.08793)

    Tony T. Wang, Miles Wang, Kaivu Hariharan, Nir Shavit


+ [ Coevolutionary Algorithm for Building Robust Decision Trees under Minimax Regret](http://arxiv.org/abs/2312.09078)

    Adam Żychowski, Andrew Perrault, Jacek Mańdziuk


+ [ Exploring Transferability for Randomized Smoothing](http://arxiv.org/abs/2312.09020)

    Kai Qiu, Huishuai Zhang, Zhirong Wu, Stephen Lin


+ [ Split-Ensemble: Efficient OOD-aware Ensemble via Task and Model Splitting](http://arxiv.org/abs/2312.09148)

    Anthony Chen, Huanrui Yang, Yulu Gan, Denis A Gudovskiy, Zhen Dong, Haofan Wang, Tomoyuki Okuno, Yohei Nakata, Shanghang Zhang, Kurt Keutzer


## 2023-12-13
+ [ Defenses in Adversarial Machine Learning: A Survey](http://arxiv.org/abs/2312.08890)

    Baoyuan Wu, Shaokui Wei, Mingli Zhu, Meixi Zheng, Zihao Zhu, Mingda Zhang, Hongrui Chen, Danni Yuan, Li Liu, Qingshan Liu


+ [ Robust Few-Shot Named Entity Recognition with Boundary Discrimination and Correlation Purification](http://arxiv.org/abs/2312.07961)

    Xiaojun Xue, Chunxia Zhang, Tianxiang Xu, Zhendong Niu


+ [ Universal Adversarial Framework to Improve Adversarial Robustness for Diabetic Retinopathy Detection](http://arxiv.org/abs/2312.08193)

    Samrat Mukherjee, Dibyanayan Bandyopadhyay, Baban Gain, Asif Ekbal


+ [ Towards Inductive Robustness: Distilling and Fostering Wave-induced Resonance in Transductive GCNs Against Graph Adversarial Attacks](http://arxiv.org/abs/2312.08651)

    Ao Liu, Wenshan Li, Tao Li, Beibei Li, Hanyuan Huang, Pan Zhou


+ [ Scalable Ensemble-based Detection Method against Adversarial Attacks for speaker verification](http://arxiv.org/abs/2312.08622)

    Haibin Wu, Heng-Cheng Kuo, Yu Tsao, Hung-yi Lee


+ [ Accelerating the Global Aggregation of Local Explanations](http://arxiv.org/abs/2312.07991)

    Alon Mor, Yonatan Belinkov, Benny Kimelfeld


+ [ Erasing Self-Supervised Learning Backdoor by Cluster Activation Masking](http://arxiv.org/abs/2312.07955)

    Shengsheng Qian, Yifei Wang, Dizhan Xue, Shengjie Zhang, Huaiwen Zhang, Changsheng Xu


+ [ Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models](http://arxiv.org/abs/2312.08303)

    Jiang Zhang, Qiong Wu, Yiming Xu, Cheng Cao, Zheng Du, Konstantinos Psounis


## 2023-12-12
+ [ Patch-MI: Enhancing Model Inversion Attacks via Patch-Based  Reconstruction](https://arxiv.org/abs/2312.07040)

	Jonggyu Jang, Hyeonsu Lyu, Hyun Jong Yang


+ [ Radio Signal Classification by Adversarially Robust Quantum Machine Learning](http://arxiv.org/abs/2312.07821)

    Yanqiu Wu, Eromanga Adermann, Chandra Thapa, Seyit Camtepe, Hajime Suzuki, Muhammad 
Usman


+ [ SSTA: Salient Spatially Transformed Attack](http://arxiv.org/abs/2312.07258)

    Renyang Liu, Wei Zhou, Sixin Wu, Jun Zhao, Kwok-Yan Lam


+ [ DTA: Distribution Transform-based Attack for Query-Limited Scenario](http://arxiv.org/abs/2312.07245)

    Renyang Liu, Wei Zhou, Xin Jin, Song Gao, Yuanyu Wang, Ruxin Wang


+ [ May the Noise be with you: Adversarial Training without Adversarial Examples](http://arxiv.org/abs/2312.08877)

    Ayoub Arous, Andres F Lopez-Lopera, Nael Abu-Ghazaleh, Ihsen Alouani


+ [ Focus on Hiders: Exploring Hidden Threats for Enhancing Adversarial Training](http://arxiv.org/abs/2312.07067)

    Qian Li, Yuxiao Hu, Yinpeng Dong, Dongxiao Zhang, Yuntian Chen


+ [ Attacking the Loop: Adversarial Attacks on Graph-based Loop Closure Detection](http://arxiv.org/abs/2312.06991)

    Jonathan J. Y. Kim, Martin Urschler, Patricia J. Riddle, Jorg S. Wicker


+ [ Collapse-Oriented Adversarial Training with Triplet Decoupling for Robust Image Retrieval](http://arxiv.org/abs/2312.07364)

    Qiwei Tian, Chenhao Lin, Qian Li, Zhengyu Zhao, Chao Shen


+ [ ReRoGCRL: Representation-based Robustness in Goal-Conditioned Reinforcement Learning](http://arxiv.org/abs/2312.07392)

    Xiangyu Yin, Sihao Wu, Jiaxu Liu, Meng Fang, Xingyu Zhao, Xiaowei Huang, Wenjie Ruan

+ [ Robust MRI Reconstruction by Smoothed Unrollin](http://arxiv.org/abs/2312.07784)

    Shijun Liang, Van Hoang Minh Nguyen, Jinghan Jia, Ismail Alkhouri, Sijia Liu, Saiprasad Ravishankar


+ [ Cost Aware Untargeted Poisoning Attack against Graph Neural Networks,](http://arxiv.org/abs/2312.07158)

    Yuwei Han, Yuni Lai, Yulin Zhu, Kai Zhou


+ [ EdgePruner: Poisoned Edge Pruning in Graph Contrastive Learning](http://arxiv.org/abs/2312.07022)

    Hiroya Kato, Kento Hasegawa, Seira Hidano, Kazuhide Fukushima


+ [ Causality Analysis for Evaluating the Security of Large Language Models](http://arxiv.org/abs/2312.07876)

    Wei Zhao, Zhe Li, Jun Sun


+ [ SimAC: A Simple Anti-Customization Method against Text-to-Image Synthesis of Diffusion Models](http://arxiv.org/abs/2312.07865)

    Feifei Wang, Zhentao Tan, Tianyi Wei, Yue Wu, Qidong Huang


+ [ Eroding Trust In Aerial Imagery: Comprehensive Analysis and Evaluation Of Adversarial Attacks In Geospatial Systems](http://arxiv.org/abs/2312.07389)

    Michael Lanier, Aayush Dhakal, Zhexiao Xiong, Arthur Li, Nathan Jacobs, Yevgeniy Vorobeychik


+ [ Securing Graph Neural Networks in MLaaS: A Comprehensive Realization of Query-based 
Integrity Verification](http://arxiv.org/abs/2312.07870)

    Bang Wu, Xingliang Yuan, Shuo Wang, Qi Li, Minhui Xue, Shirui Pan


+ [ Majority is Not Required: A Rational Analysis of the Private Double-Spend Attack from a Sub-Majority Adversary](http://arxiv.org/abs/2312.07709)

    Yanni Georghiades, Rajesh Mishra, Karl Kreder, Sriram Vishwanath


+ [ Divide-and-Conquer Attack: Harnessing the Power of LLM to Bypass the  Censorship of Text-to-Image Generation Model](https://arxiv.org/abs/2312.07130)

	Yimo Deng, Huangxun Chen


+ [ Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an  In-Context Attack](https://arxiv.org/abs/2312.06924)

	Yu Fu, Yufei Li, Wen Xiao, Cong Liu, Yue Dong


+ [ Adversarial Semi-Supervised Domain Adaptation for Semantic Segmentation:  A New Role for Labeled Target Samples](https://arxiv.org/abs/2312.07370)

	Marwa Kechaou, Mokhtar Z. Alaya, Romain Hérault, Gilles Gasso


+ [ Dynamic Adversarial Attacks on Autonomous Driving Systems](https://arxiv.org/abs/2312.06701)

	Amirhosein Chahe, Chenan Wang, Abhishek Jeyapratap, Kaidi Xu, Lifeng Zhou


+ [ AI Control: Improving Safety Despite Intentional Subversion](https://arxiv.org/abs/2312.06942)

	Ryan Greenblatt, Buck Shlegeris, Kshitij Sachan, Fabien Roger


+ [ Task-Agnostic Privacy-Preserving Representation Learning for Federated  Learning Against Attribute Inference Attacks](https://arxiv.org/abs/2312.06989)

	Caridad Arroyo Arevalo, Sayedeh Leila Noorbakhsh, Yun Dong, Yuan Hong, Binghui Wang


## 2023-12-11
+ [ Towards Transferable Adversarial Attacks with Centralized Perturbation](http://arxiv.org/abs/2312.06199)

    Shangbo Wu, Yu-an Tan, Yajie Wang, Ruinan Ma, Wencong Ma, Yuanzhang Li


+ [ MalPurifier: Enhancing Android Malware Detection with Adversarial Purification against Evasion Attacks](http://arxiv.org/abs/2312.06423)

    Yuyang Zhou, Guang Cheng, Zongyao Chen, Shui Yu


+ [ Sparse but Strong: Crafting Adversarially Robust Graph Lottery Tickets](http://arxiv.org/abs/2312.06568)

    Subhajit Dutta Chowdhury, Zhiyu Ni, Qingyuan Peng, Souvik Kundu, Pierluigi Nuzzo    


+ [ Reward Certification for Policy Smoothed Reinforcement Learning](http://arxiv.org/abs/2312.06436)

    Ronghui Mu, Leandro Soriano Marcolino, Tianle Zhang, Yanghao Zhang, Xiaowei Huang, Wenjie Ruan


+ [ Activation Gradient based Poisoned Sample Detection Against Backdoor Attacks](http://arxiv.org/abs/2312.06230)

    Danni Yuan, Shaokui Wei, Mingda Zhang, Li Liu, Baoyuan Wu


+ [ Poisoned ChatGPT Finds Work for Idle Hands: Exploring Developers' Coding Practices with Insecure Suggestions from Poisoned AI Models](http://arxiv.org/abs/2312.06227)

    Sanghak Oh, Kiho Lee, Seonhye Park, Doowon Kim, Hyoungshick Kim


+ [ Promoting Counterfactual Robustness through Diversity](http://arxiv.org/abs/2312.06564)

    Francesco Leofante, Nico Potyka


+ [ Robust Graph Neural Network based on Graph Denoising](http://arxiv.org/abs/2312.06557)

	Victor M. Tenorio, Samuel Rey, Antonio G. Marques


+ [ Privacy Preserving Multi-Agent Reinforcement Learning in Supply Chains](https://arxiv.org/abs/2312.05686)

	Ananta Mukherjee, Peeyush Kumar, Boling Yang, Nishanth Chandran, Divya Gupta


+ [ Exploring the Limits of ChatGPT in Software Security Applications](https://arxiv.org/abs/2312.05275)

	Fangzhou Wu, Qingzhao Zhang, Ati Priya Bajaj, Tiffany Bao, Ning Zhang, Ruoyu "Fish" Wang, Chaowei Xiao


+ [ Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer  Inputs of Language Models in Federated Learning](https://arxiv.org/abs/2312.05720)

	Jianwei Li, Sheng Liu, Qi Lei


+ [ MalPurifier: Enhancing Android Malware Detection with Adversarial  Purification against Evasion Attacks](https://arxiv.org/abs/2312.06423)

	Yuyang Zhou, Guang Cheng, Zongyao Chen, Shui Yu


+ [ GTA: Gated Toxicity Avoidance for LM Performance Preservation](https://arxiv.org/abs/2312.06122)

	Heegyu Kim, Hyunsouk Cho


+ [ Initialization Matters for Adversarial Transfer Learning](https://arxiv.org/abs/2312.05716)

	Andong Hua, Jindong Gu, Zhiyu Xue, Nicholas Carlini, Eric Wong, Yao Qin


+ [ Adversarial Camera Patch: An Effective and Robust Physical-World Attack  on Object Detectors](https://arxiv.org/abs/2312.06163)

	Kalibinuer Tiliwalidi


+ [ CAD: Photorealistic 3D Generation via Adversarial Distillation](https://arxiv.org/abs/2312.06663)

	Ziyu Wan, Despoina Paschalidou, Ian Huang, Hongyu Liu, Bokui Shen, Xiaoyu Xiang, Jing Liao, Leonidas Guibas


+ [ Improving Adversarial Robust Fairness via Anti-Bias Soft Label  Distillation](https://arxiv.org/abs/2312.05508)

	Shiji Zhao, Xizhe Wang, Xingxing Wei


+ [ Model Extraction Attacks Revisited](https://arxiv.org/abs/2312.05386)

	Jiacheng Liang, Ren Pang, Changjiang Li, Ting Wang


+ [ Poisoning $\times$ Evasion: Symbiotic Adversarial Robustness for Graph  Neural Networks](https://arxiv.org/abs/2312.05502)

	Ege Erdogan, Simon Geisler, Stephan Günnemann


+ [ A Practical Survey on Emerging Threats from AI-driven Voice Attacks: How  Vulnerable are Commercial Voice Control Systems?](https://arxiv.org/abs/2312.06010)

	Yuanda Wang, Qiben Yan, Nikolay Ivanov, Xun Chen


+ [ DiffAIL: Diffusion Adversarial Imitation Learning](https://arxiv.org/abs/2312.06348)

	Bingzheng Wang, Yan Zhang, Teng Pang, Guoqiang Wu, Yilong Yin


+ [ Security and Reliability Evaluation of Countermeasures implemented using  High-Level Synthesis](https://arxiv.org/abs/2312.06268)

	Amalia Artemis Koufopoulou, Kalliopi Xevgeni, Athanasios Papadimitriou, Mihalis Psarakis, David Hely

## 2023-12-10
+ [ zkFDL: An efficient and privacy-preserving decentralized federated  learning with zero knowledge proof](https://arxiv.org/abs/2312.04579)

	Mojtaba Ahmadi, Reza Nourmohammadi


+ [ Towards Sample-specific Backdoor Attack with Clean Labels via Attribute  Trigger](https://arxiv.org/abs/2312.04584)

	Yiming Li, Mingyan Zhu, Junfeng Guo, Tao Wei, Shu-Tao Xia, Zhan Qin



+ [ DeceptPrompt: Exploiting LLM-driven Code Generation via Adversarial  Natural Language Instructions](https://arxiv.org/abs/2312.04730)

	Fangzhou Wu, Xiaogeng Liu, Chaowei Xiao


+ [ Forcing Generative Models to Degenerate Ones: The Power of Data  Poisoning Attacks](https://arxiv.org/abs/2312.04748)

	Shuli Jiang, Swanand Ravindra Kadhe, Yi Zhou, Ling Cai, Nathalie Baracaldo


+ [ BELT: Old-School Backdoor Attacks can Evade the State-of-the-Art Defense  with Backdoor Exclusivity Lifting](https://arxiv.org/abs/2312.04902)

	Huming Qiu, Junjie Sun, Mi Zhang, Xudong Pan, Min Yang


+ [ SA-Attack: Improving Adversarial Transferability of Vision-Language  Pre-training Models via Self-Augmentation](https://arxiv.org/abs/2312.04913)

	Bangyan He, Xiaojun Jia, Siyuan Liang, Tianrui Lou, Yang Liu, Xiaochun Cao


+ [ MIMIR: Masked Image Modeling for Mutual Information-based Adversarial  Robustness](https://arxiv.org/abs/2312.04960)

	Xiaoyun Xu, Shujian Yu, Jingzheng Wu, Stjepan Picek


+ [ On the Inadequacy of Similarity-based Privacy Metrics: Reconstruction  Attacks against "Truly Anonymous Synthetic Data''](https://arxiv.org/abs/2312.05114)

	Georgi Ganev, Emiliano De Cristofaro


+ [ MimicDiffusion: Purifying Adversarial Perturbation via Mimicking Clean  Diffusion Model](https://arxiv.org/abs/2312.04802)

	Kaiyu Song, Hanjiang Lai


+ [ Annotation-Free Group Robustness via Loss-Based Resampling](https://arxiv.org/abs/2312.04893)

	Mahdi Ghaznavi, Hesam Asadollahzadeh, HamidReza Yaghoubi Araghi, Fahimeh Hosseini Noohdani, Mohammad Hossein Rohban, Mahdieh Soleymani Baghshah


+ [ Diffence: Fencing Membership Privacy With Diffusion Models](https://arxiv.org/abs/2312.04692)

	Yuefeng Peng, Ali Naseh, Amir Houmansadr


+ [ HC-Ref: Hierarchical Constrained Refinement for Robust Adversarial  Training of GNNs](https://arxiv.org/abs/2312.04879)

	Xiaobing Pei, Haoran Yang, Gang Shen


+ [ FedBayes: A Zero-Trust Federated Learning Aggregation to Defend Against  Adversarial Attacks](https://arxiv.org/abs/2312.04587)

	Marc Vucovich, Devin Quinn, Kevin Choi, Christopher Redino, Abdul Rahman, Edward Bowen


+ [ TrustFed: A Reliable Federated Learning Framework with Malicious-Attack  Resistance](https://arxiv.org/abs/2312.04597)

	Hangn Su, Jianhong Zhou, Xianhua Niu, Gang Feng


+ [ Topology-Based Reconstruction Prevention for Decentralised Learning](https://arxiv.org/abs/2312.05248)

	Florine W. Dekker, Zekeriya Erkin, Mauro Conti


## 2023-12-09
+ [ Cognitive Dissonance: Why Do Language Model Outputs Disagree with  Internal Representations of Truthfulness?](https://arxiv.org/abs/2312.03729)

	Kevin Liu, Stephen Casper, Dylan Hadfield-Menell, Jacob Andreas


+ [ On the Learnability of Watermarks for Language Models](https://arxiv.org/abs/2312.04469)

	Chenchen Gu, Xiang Lisa Li, Percy Liang, Tatsunori Hashimoto


+ [ Defense against ML-based Power Side-channel Attacks on DNN Accelerators  with Adversarial Attacks](https://arxiv.org/abs/2312.04035)

	Xiaobei Yan, Chip Hong Chang, Tianwei Zhang


+ [ GaitGuard: Towards Private Gait in Mixed Reality](https://arxiv.org/abs/2312.04470)

	Diana Romero, Ruchi Jagdish Patel, Athina Markopolou, Salma Elmalaki


## 2023-12-08
+ [ An Evaluation of State-of-the-Art Large Language Models for Sarcasm  Detection](https://arxiv.org/abs/2312.03706)

	Juliann Zhou


+ [ Negotiating with LLMS: Prompt Hacks, Skill Gaps, and Reasoning Deficits](https://arxiv.org/abs/2312.03720)

	Johannes Schneider, Steffi Haag, Leona Chandra Kruse


+ [ DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt  Engineer](https://arxiv.org/abs/2312.03724)

	Junyuan Hong, Jiachen T. Wang, Chenhui Zhang, Zhangheng Li, Bo Li, Zhangyang Wang


+ [ Making Translators Privacy-aware on the User's Side](https://arxiv.org/abs/2312.04068)

	Ryoma Sato


+ [ Adversarial Denoising Diffusion Model for Unsupervised Anomaly Detection](https://arxiv.org/abs/2312.04382)

	Jongmin Yu, Hyeontaek Oh, Jinhong Yang


+ [ RoAST: Robustifying Language Models via Adversarial Perturbation with  Selective Training](https://arxiv.org/abs/2312.04032)

	Jaehyung Kim, Yuning Mao, Rui Hou, Hanchao Yu, Davis Liang, Pascale Fung, Qifan Wang, Fuli Feng, Lifu Huang, Madian Khabsa


+ [ The Potential of Vision-Language Models for Content Moderation of  Children's Videos](https://arxiv.org/abs/2312.03936)

	Syed Hammad Ahmed, Shengnan Hu, Gita Sukthankar


+ [ On the Impact of Multi-dimensional Local Differential Privacy on  Fairness](https://arxiv.org/abs/2312.04404)

	karima Makhlouf, Heber H. Arcolezi, Sami Zhioua, Ghassen Ben Brahim, Catuscia Palamidessi


+ [ FreqFed: A Frequency Analysis-Based Approach for Mitigating Poisoning  Attacks in Federated Learning](https://arxiv.org/abs/2312.04432)

	Hossein Fereidooni, Alessandro Pegoraro, Phillip Rieger, Alexandra Dmitrienko, Ahmad-Reza Sadeghi


+ [ SoK: Unintended Interactions among Machine Learning Defenses and Risks](https://arxiv.org/abs/2312.04542)

	Vasisht Duddu, Sebastian Szyller, N. Asokan


## 2023-12-07
+ [ GaitGuard: Towards Private Gait in Mixed Reality](https://arxiv.org/abs/2312.04470)

	Diana Romero, Ruchi Jagdish Patel, Athina Markopolou, Salma Elmalaki


+ [ Exploring the Robustness of Model-Graded Evaluations and Automated  Interpretability](https://arxiv.org/abs/2312.03721)

	Simon Lermen, Ondřej Kvapil


+ [ On The Fairness Impacts of Hardware Selection in Machine Learning](https://arxiv.org/abs/2312.03886)

	Sree Harsha Nelaturu, Nishaanth Kanna Ravichandran, Cuong Tran, Sara Hooker, Ferdinando Fioretto


+ [ Detecting and Restoring Non-Standard Hands in Stable Diffusion Generated  Images](https://arxiv.org/abs/2312.04236)

	Yiqun Zhang, Zhenyue Qin, Yang Liu, Dylan Campbell


+ [ Adversarial Learning for Feature Shift Detection and Correction](https://arxiv.org/abs/2312.04546)

	Miriam Barrabes, Daniel Mas Montserrat, Margarita Geleta, Xavier Giro-i-Nieto, Alexander G. Ioannidis


+ [ OT-Attack: Enhancing Adversarial Transferability of Vision-Language Models via Optimal Transport Optimization](http://arxiv.org/abs/2312.04403)

    Dongchen Han, Xiaojun Jia, Yang Bai, Jindong Gu, Yang Liu, Xiaochun Cao


+ [ FreqFed: A Frequency Analysis-Based Approach for Mitigating Poisoning Attacks in Federated Learning](http://arxiv.org/abs/2312.04432)

    Hossein Fereidooni, Alessandro Pegoraro, Phillip Rieger, Alexandra Dmitrienko, Ahmad-Reza Sadeghi


## 2023-12-06
+ [ Defense against ML-based Power Side-channel Attacks on DNN Accelerators with Adversarial Attacks](http://arxiv.org/abs/2312.04035)

    Xiaobei Yan, Chip Hong Chang, Tianwei Zhang


+ [ Defense Against Adversarial Attacks using Convolutional Auto-Encoders](http://arxiv.org/abs/2312.03520)

    Shreyasi Mandal


+ [ Node-aware Bi-smoothing: Certified Robustness against Graph Injection Attacks](http://arxiv.org/abs/2312.03979)

    Yuni Lai, Yulin Zhu, Bailin Pan, Kai Zhou


+ [ Privacy-preserving quantum federated learning via gradient hiding](https://arxiv.org/abs/2312.04447)

	Changhao Li, Niraj Kumar, Zhixin Song, Shouvanik Chakrabarti, Marco Pistoia


+ [ RoAST: Robustifying Language Models via Adversarial Perturbation with Selective Training](http://arxiv.org/abs/2312.04032)

    Jaehyung Kim, Yuning Mao, Rui Hou, Hanchao Yu, Davis Liang, Pascale Fung, Qifan Wang, 
Fuli Feng, Lifu Huang, Madian Khabsa


+ [ Analyzing the Inherent Response Tendency of LLMs: Real-World  Instructions-Driven Jailbreak](https://arxiv.org/abs/2312.04127)

	Yanrui Du, Sendong Zhao, Ming Ma, Yuhan Chen, Bing Qin


+ [ Detecting Voice Cloning Attacks via Timbre Watermarking](http://arxiv.org/abs/2312.03410)

    Chang Liu, Jie Zhang, Tianwei Zhang, Xi Yang, Weiming Zhang, Nenghai Yu


+ [ Identity-Obscured Neural Radiance Fields: Privacy-Preserving 3D Facial  Reconstruction](https://arxiv.org/abs/2312.04106)

	Jiayi Kong, Baixin Xu, Xurui Song, Chen Qian, Jun Luo, Ying He


+ [ Learn to Unlearn for Deep Neural Networks: Minimizing Unlearning  Interference with Gradient Projection](https://arxiv.org/abs/2312.04095)

	Tuan Hoang, Santu Rana, Sunil Gupta, Svetha Venkatesh


+ [ Synthesizing Physical Backdoor Datasets: An Automated Framework Leveraging Deep Generative Models](http://arxiv.org/abs/2312.03419)

    Sze Jue Yang, Chinh D. La, Quang H. Nguyen, Eugene Bagdasaryan, Kok-Seng Wong, Anh Tuan Tran, Chee Seng Chan, Khoa D. Doan


+ [ MICRO: Model-Based Offline Reinforcement Learning with a Conservative Bellman Operator](http://arxiv.org/abs/2312.03991)

    Xiao-Yin Liu, Xiao-Hu Zhou, Guo-Tao Li, Hao Li, Mei-Jiang Gui, Tian-Yu Xiang, De-Xing 
Huang, Zeng-Guang Hou


## 2023-12-05
+ [ Generating Visually Realistic Adversarial Patch](http://arxiv.org/abs/2312.03030)

    Xiaosen Wang, Kunyu Wang


+ [ ScAR: Scaling Adversarial Robustness for LiDAR Object Detection](http://arxiv.org/abs/2312.03085)

    Xiaohu Lu, Hayder Radha


+ [ A Simple Framework to Enhance the Adversarial Robustness of Deep Learning-based Intrusion Detection System](http://arxiv.org/abs/2312.03245)

    Xinwei Yuan, Shu Han, Wei Huang, Hongliang Ye, Xianglong Kong, Fan Zhang


+ [ Realistic Scatterer Based Adversarial Attacks on SAR Image Classifiers](http://arxiv.org/abs/2312.02912)

    Tian Ye, Rajgopal Kannan, Viktor Prasanna, Carl Busart, Lance Kaplan


+ [ Class Incremental Learning for Adversarial Robustness](http://arxiv.org/abs/2312.03289)

    Seungju Cho, Hongshin Lee, Changick Kim


+ [ (Provable) Adversarial Robustness for Group Equivariant Tasks: Graphs, Point Clouds, Molecules, and More](http://arxiv.org/abs/2312.02708)

    Jan Schuchardt, Yan Scholten, Stephan Günnemann


+ [ On the Robustness of Large Multimodal Models Against Image Adversarial Attacks](http://arxiv.org/abs/2312.03777)

    Xuanimng Cui, Alejandro Aparcedo, Young Kyun Jang, Ser-Nam Lim


+ [ Scaling Laws for Adversarial Attacks on Language Model Activations](http://arxiv.org/abs/2312.02780)

    Stanislav Fort


+ [ Indirect Gradient Matching for Adversarial Robust Distillation](http://arxiv.org/abs/2312.03286)

    Hongsin Lee, Seungju Cho, Changick Kim


+ [ Robust Backdoor Detection for Deep Learning via Topological Evolution Dynamics](http://arxiv.org/abs/2312.02673)

    Xiaoxing Mo, Yechao Zhang, Leo Yu Zhang, Wei Luo, Nan Sun, Shengshan Hu, Shang Gao, Yang Xiang


+ [ Prompt Optimization via Adversarial In-Context Learning](http://arxiv.org/abs/2312.02614)

    Xuan Long Do, Yiran Zhao, Hannah Brown, Yuxi Xie, James Xu Zhao, Nancy F. Chen, Kenji 
Kawaguchi, Michael Qizhe Xie, Junxian He


+ [ Privacy-Preserving Task-Oriented Semantic Communications Against Model Inversion Attacks](http://arxiv.org/abs/2312.03252)

    Yanhu Wang, Shuaishuai Guo, Yiqin Deng, Haixia Zhang, Yuguang Fang


+ [ Machine Vision Therapy: Multimodal Large Language Models Can Enhance Visual Robustness via Denoising In-Context Learning](http://arxiv.org/abs/2312.02546)

    Zhuo Huang, Chang Liu, Yinpeng Dong, Hang Su, Shibao Zheng, Tongliang Liu


## 2023-12-04
+ [ Adversarial Medical Image with Hierarchical Feature Hiding](http://arxiv.org/abs/2312.01679)

    Qingsong Yao, Zecheng He, Yuexiang Li, Yi Lin, Kai Ma, Yefeng Zheng, S. Kevin Zhou    


+ [ InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models](http://arxiv.org/abs/2312.01886)

    Xunguang Wang, Zhenlan Ji, Pingchuan Ma, Zongjie Li, Shuai Wang


+ [ Singular Regularization with Information Bottleneck Improves Model's Adversarial Robustness](http://arxiv.org/abs/2312.02237)

    Guanlin Li, Naishan Zheng, Man Zhou, Jie Zhang, Tianwei Zhang


+ [ Two-stage optimized unified adversarial patch for attacking visible-infrared cross-modal detectors in the physical world](http://arxiv.org/abs/2312.01789)

    Chengyin Hu, Weiwen Shi


+ [ Auto DP-SGD: Dual Improvements of Privacy and Accuracy via Automatic Clipping Threshold and Noise Multiplier Estimation](http://arxiv.org/abs/2312.02400)

    Sai Venkatesh Chilukoti, Md Imran Hossen, Liqun Shan, Vijay Srinivas Tida, Xiai Hei   


+ [ Rejuvenating image-GPT as Strong Visual Representation Learners](http://arxiv.org/abs/2312.02147)

    Sucheng Ren, Zeyu Wang, Hongru Zhu, Junfei Xiao, Alan Yuille, Cihang Xie


## 2023-12-03
+ [ QuantAttack: Exploiting Dynamic Quantization to Attack Vision Transformers](http://arxiv.org/abs/2312.02220)

    Amit Baras, Alon Zolfi, Yuval Elovici, Asaf Shabtai


+ [ OCGEC: One-class Graph Embedding Classification for DNN Backdoor Detection](http://arxiv.org/abs/2312.01585)

    Haoyu Jiang, Haiyang Yu, Nan Li, Ping Yi


+ [ Evaluating the Security of Satellite Systems](http://arxiv.org/abs/2312.01330)

    Roy Peled, Eran Aizikovich, Edan Habler, Yuval Elovici, Asaf Shabtai


+ [ Exploring Adversarial Robustness of LiDAR-Camera Fusion Model in Autonomous Driving](http://arxiv.org/abs/2312.01468)

    Bo Yang, Xiaoyu Ji, Xiaoyu Ji, Xiaoyu Ji, Xiaoyu Ji


## 2023-12-02
+ [ TranSegPGD: Improving Transferability of Adversarial Examples on Semantic Segmentation](http://arxiv.org/abs/2312.02207)

    Xiaojun Jia, Jindong Gu, Yihao Huang, Simeng Qin, Qing Guo, Yang Liu, Xiaochun Cao    


+ [ Rethinking PGD Attack: Is Sign Function Necessary? (98%](http://arxiv.org/abs/2312.01260)

    Junjie Yang, Tianlong Chen, Xuxi Chen, Zhangyang Wang, Yingbin Liang


+ [ PROFL: A Privacy-Preserving Federated Learning Method with Stringent Defense Against Poisoning Attacks](http://arxiv.org/abs/2312.01045)

    Yisheng Zhong, Li-Ping Wang


+ [ Mendata: A Framework to Purify Manipulated Training Data](http://arxiv.org/abs/2312.01281)

    Zonghao Huang, Neil Gong, Michael K. Reiter


## 2023-12-01
+ [ PyraTrans: Learning Attention-Enriched Multi-Scale Pyramid Network from Pre-Trained Transformers for Effective Malicious URL Detection](http://arxiv.org/abs/2312.00508)

    Ruitong Liu, Yanbin Wang, Zhenhao Guo, Haitao Xu, Zhan Qin, Wenrui Ma, Fan Zhang      


+ [ Survey of Security Issues in Memristor-based Machine Learning Accelerators for RF Analysis](http://arxiv.org/abs/2312.00942)

    William Lillis, Max Cohen Hoffing, Wayne Burleson


+ [ Deep Generative Attacks and Countermeasures for Data-Driven Offline Signature Verification](http://arxiv.org/abs/2312.00987)

    An Ngo, MinhPhuong Cao, Rajesh Kumar


+ [ Temperature Balancing, Layer-wise Weight Analysis, and Neural Network Training](http://arxiv.org/abs/2312.00359)

    Yefan Zhou, Tianyu Pang, Keqin Liu, Charles H. Martin, Michael W. Mahoney, Yaoqing Yang


## 2023-11-30
+ [ Improving Faithfulness for Vision Transformers](https://arxiv.org/abs/2311.17983)

	Lijie Hu, Yixin Liu, Ninghao Liu, Mengdi Huai, Lichao Sun, Di Wang


+ [ TrustMark: Universal Watermarking for Arbitrary Resolution Images](https://arxiv.org/abs/2311.18297)

	Tu Bui, Shruti Agarwal, John Collomosse


+ [ Unnatural Error Correction: GPT-4 Can Almost Perfectly Handle Unnatural  Scrambled Text](https://arxiv.org/abs/2311.18805)

	Qi Cao, Takeshi Kojima, Yutaka Matsuo, Yusuke Iwasawa


+ [ ROBBIE: Robust Bias Evaluation of Large Generative Language Models](https://arxiv.org/abs/2311.18140)

	David Esiobu, Xiaoqing Tan, Saghar Hosseini, Megan Ung, Yuchen Zhang, Jude Fernandes, Jane Dwivedi-Yu, Eleonora Presani, Adina Williams, Eric Michael Smith


+ [ What Do Llamas Really Think? Revealing Preference Biases in Language  Model Representations](https://arxiv.org/abs/2311.18812)

	Raphael Tang, Xinyu Zhang, Jimmy Lin, Ferhan Ture


+ [ Improving Adversarial Transferability via Model Alignment](https://arxiv.org/abs/2311.18495)

	Avery Ma, Amir-massoud Farahmand, Yangchen Pan, Philip Torr, Jindong Gu


+ [ Poisoning Attacks Against Contrastive Recommender Systems](https://arxiv.org/abs/2311.18244)

	Zongwei Wang, Junliang Yu, Min Gao, Hongzhi Yin, Bin Cui, Shazia Sadiq


## 2023-11-29
+ [ Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention](http://arxiv.org/abs/2311.17400)

 Lujia Shen, Yuwen Pu, Shouling Ji, Changjiang Li, Xuhong Zhang, Chunpeng Ge, Ting Wang  


+ [ Group-wise Sparse and Explainable Adversarial Attacks](http://arxiv.org/abs/2311.17434)

 Shpresim Sadiku, Moritz Wagner, Sebastian Pokutta


+ [ Quantum Neural Networks under Depolarization Noise: Exploring White-Box Attacks and Defenses](http://arxiv.org/abs/2311.17458)

 David Winderl, Nicola Franco, Jeanette Miriam Lorenz


+ [ On the Adversarial Robustness of Graph Contrastive Learning Methods](http://arxiv.org/abs/2311.17853)

 Filippo Guerranti, Zinuo Yi, Anna Starovoit, Rafiq Kamel, Simon Geisler, Stephan Günnemann


+ [ Adversarial Robust Memory-Based Continual Learner](http://arxiv.org/abs/2311.17608)

 Xiaoyue Mi, Fan Tang, Zonghan Yang, Danding Wang, Juan Cao, Peng Li, Yang Liu


+ [ TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4](http://arxiv.org/abs/2311.17429)

 Zihao Tan, Qingliang Chen, Yongjian Huang, Chen Liang


+ [ Topology-Preserving Adversarial Training](http://arxiv.org/abs/2311.17607)  

 Xiaoyue Mi, Fan Tang, Yepeng Weng, Danding Wang, Juan Cao, Sheng Tang, Peng Li, Yang Liu

+ [ Query-Relevant Images Jailbreak Large Multi-Modal Models](http://arxiv.org/abs/2311.17600)

 Xin Liu, Yichen Zhu, Yunshi Lan, Chao Yang, Yu Qiao


+ [ Analyzing and Explaining Image Classifiers via Diffusion Guidance](http://arxiv.org/abs/2311.17833)

 Maximilian Augustin, Yannic Neuhaus, Matthias Hein


+ [ SenTest: Evaluating Robustness of Sentence Encoders](http://arxiv.org/abs/2311.17722)

 Tanmay Chavan, Shantanu Patankar, Aditya Kane, Omkar Gokhale, Geetanjali Kale, Raviraj Joshi


+ [ CLIPC8: Face liveness detection algorithm based on image-text pairs and contrastive learning](http://arxiv.org/abs/2311.17583)

 Xu Liu, Shu Zhou, Yurong Song, Wenzhe Luo, Xin Zhang


## 2023-11-28
+ [ Unveiling the Implicit Toxicity in Large Language Models](http://arxiv.org/abs/2311.17391)

 Jiaxin Wen, Pei Ke, Hao Sun, Zhexin Zhang, Chengfei Li, Jinfeng Bai, Minlie Huang       


+ [ Vulnerability Analysis of Transformer-based Optical Character Recognition to Adversarial Attacks](http://arxiv.org/abs/2311.17128)

 Lucas Beerens, Desmond J. Higham


+ [ NeRFTAP: Enhancing Transferability of Adversarial Patches on Face Recognition using Neural Radiance Fields](http://arxiv.org/abs/2311.17332)

 Xiaoliang Liu, Furao Shen, Feng Han, Jian Zhao, Changhai Nie


+ [ Efficient Key-Based Adversarial Defense for ImageNet by Using Pre-trained Model](http://arxiv.org/abs/2311.16577)

 AprilPyone MaungMaung, Isao Echizen, Hitoshi Kiya


+ [ RADAP: A Robust and Adaptive Defense Against Diverse Adversarial Patches on Face Recognition](http://arxiv.org/abs/2311.17339)

 Xiaoliang Liu, Furao Shen, Jian Zhao, Changhai Nie


+ [ 1-Lipschitz Layers Compared: Memory, Speed, and Certifiable Robustness](http://arxiv.org/abs/2311.16833)

 Bernd Prach, Fabio Brau, Giorgio Buttazzo, Christoph H. Lampert


+ [ Scalable Extraction of Training Data from (Production) Language Models](http://arxiv.org/abs/2311.17035)

 Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace, Florian Tramèr, Katherine Lee   


+ [ Cooperative Abnormal Node Detection with Adversary Resistance: A Probabilistic Approach](http://arxiv.org/abs/2311.16661)

 Yingying Huangfu, Tian Bai


+ [ Shadows Don't Lie and Lines Can't Bend! Generative Models don't know Projective Geometry](http://arxiv.org/abs/2311.17138)

 Ayush Sarkar, Hanlin Mai, Amitabh Mahapatra, Svetlana Lazebnik, D. A. Forsyth, Anand Bhattad


+ [ On robust overfitting: adversarial training induced distribution matters](http://arxiv.org/abs/2311.16526)

 Runzhi Tian, Yongyi Mao


+ [ Understanding the (Extra-)Ordinary: Validating Deep Model Decisions with Prototypical Concept-based Explanations](http://arxiv.org/abs/2311.16681)

 Maximilian Dreyer, Reduan Achtibat, Wojciech Samek, Sebastian Lapuschkin


## 2023-11-27
+ [Rethinking Mixup for Improving the Adversarial Transferability](https://arxiv.org/abs/2311.17087)

Xiaosen Wang, Zeyuan Yin

  
+ [ Microarchitectural Security of AWS Firecracker VMM for Serverless Cloud Platforms](http://arxiv.org/abs/2311.15999)

	Zane Worcester Polytechnic Institute Weissman, Thore University of Lübeck Tiemann, Thomas University of Lübeck Eisenbarth, Berk Worcester Polytechnic Institute Sunar


+ [ Scale-Dropout: Estimating Uncertainty in Deep Neural Networks Using  Stochastic Scale](https://arxiv.org/abs/2311.15816)

	Soyed Tuhin Ahmed, Kamal Danouchi, Michael Hefenbrock, Guillaume Prenat, Lorena Anghel, Mehdi B. Tahoori


+ [ Instruct2Attack: Language-Guided Semantic Adversarial Attacks](http://arxiv.org/abs/2311.15551)

	Jiang Liu, Chen Wei, Yuxiang Guo, Heng Yu, Alan Yuille, Soheil Feizi, Chun Pong Lau, Rama Chellappa


+ [ A Survey on Vulnerability of Federated Learning: A Learning Algorithm  Perspective](https://arxiv.org/abs/2311.16065)

	Xianghua Xie, Chen Hu, Hanchi Ren, Jingjing Deng


+ [ Distributed Attacks over Federated Reinforcement Learning-enabled Cell Sleep Control](http://arxiv.org/abs/2311.15894)

	Han Zhang, Hao Zhou, Medhat Elsayed, Majid Bavand, Raimundas Gaigalas, Yigit Ozcan, Melike Erol-Kantarci


+ [ How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs](http://arxiv.org/abs/2311.16101)

	Haoqin Tu, Chenhang Cui, Zijun Wang, Yiyang Zhou, Bingchen Zhao, Junlin Han, Wangchunshu Zhou, Huaxiu Yao, Cihang Xie


+ [ Trainwreck: A damaging adversarial attack on image classifiers](https://arxiv.org/abs/2311.14772)

	Jan Zahálka


+ [ Adversaral Doodles: Interpretable and Human-drawable Attacks Provide  Describable Insights](https://arxiv.org/abs/2311.15994)

	Ryoya Nara, Yusuke Matsui


+ [ Automated discovery of trade-off between utility, privacy and fairness  in machine learning models](https://arxiv.org/abs/2311.15691)

	Bogdan Ficiu, Neil D. Lawrence, Andrei Paleyes


+ [ Attend Who is Weak: Enhancing Graph Condensation via Cross-Free  Adversarial Training](https://arxiv.org/abs/2311.15772)

	Xinglin Li, Kun Wang, Hanhui Deng, Yuxuan Liang, Di Wu


+ [ Rethinking Privacy in Machine Learning Pipelines from an Information  Flow Control Perspective](https://arxiv.org/abs/2311.15792)

	Lukas Wutschitz, Boris Köpf, Andrew Paverd, Saravan Rajmohan, Ahmed Salem, Shruti Tople, Santiago Zanella-Béguelin, Menglin Xia, Victor Rühle


## 2023-11-26
+ [ Confidence Is All You Need for MI Attacks](https://arxiv.org/abs/2311.15373)

	Abhishek Sinha, Himanshi Tibrewal, Mansi Gupta, Nikhar Waghela, Shivank Garg


+ [ Mixing Classifiers to Alleviate the Accuracy-Robustness Trade-Off](https://arxiv.org/abs/2311.15165)

	Yatong Bai, Brendon G. Anderson, Somayeh Sojoudi


+ [ Adversarial Purification of Information Masking](http://arxiv.org/abs/2311.15339)

	Sitong Liu, Zhichao Lian, Shuangquan Zhang, Liang Xiao


+ [ Having Second Thoughts? Let's hear it](http://arxiv.org/abs/2311.15356)

	Jung H. Lee, Sujith Vijayan


## 2023-11-25
+ [ Effective Backdoor Mitigation Depends on the Pre-training Objective](https://arxiv.org/abs/2311.14948)

	Sahil Verma, Gantavya Bhatt, Avi Schwarzschild, Soumye Singhal, Arnav Mohanty Das, Chirag Shah, John P Dickerson, Jeff Bilmes


+ [ Robust Graph Neural Networks via Unbiased Aggregation](https://arxiv.org/abs/2311.14934)

	Ruiqi Feng, Zhichao Hou, Tyler Derr, Xiaorui Liu


+ [ Mixing Classifiers to Alleviate the Accuracy-Robustness Trade-Off](http://arxiv.org/abs/2311.15165)

	Yatong Bai, Brendon G. Anderson, Somayeh Sojoudi


+ [ Robust Graph Neural Networks via Unbiased Aggregation](http://arxiv.org/abs/2311.14934)

	Ruiqi Feng, Zhichao Hou, Tyler Derr, Xiaorui Liu


+ [ Effective Backdoor Mitigation Depends on the Pre-training Objective](http://arxiv.org/abs/2311.14948)

	Sahil Verma, Gantavya Bhatt, Avi Schwarzschild, Soumye Singhal, Arnav Mohanty Das, Chirag Shah, John P Dickerson, Jeff Bilmes


## 2023-11-24
+ [ Universal Jailbreak Backdoors from Poisoned Human Feedback](https://arxiv.org/abs/2311.14455)

	Javier Rando, Florian Tramèr


+ [ How to ensure a safe control strategy? Towards a SRL for urban transit  autonomous operation](https://arxiv.org/abs/2311.14457)

	Zicong Zhao


+ [ Potential Societal Biases of ChatGPT in Higher Education: A Scoping  Review](https://arxiv.org/abs/2311.14381)

	Ming Li, Ariunaa Enkhtur, Beverley Anne Yamamoto, Fei Cheng


+ [ AI-based Attack Graph Generation](https://arxiv.org/abs/2311.14342)

	Sangbeom Park, Jaesung Lee, Jeongdo Yoo, Min Geun Song, Hyosun Lee, Jaewoong Choi, Chaeyeon Sagong, Huy Kang Kim


+ [ Segment (Almost) Nothing: Prompt-Agnostic Adversarial Attacks on Segmentation Models](http://arxiv.org/abs/2311.14450)

	Francesco Croce, Matthias Hein


+ [ Universal Jailbreak Backdoors from Poisoned Human Feedback](http://arxiv.org/abs/2311.14455)

	Javier Rando, Florian Tramèr


## 2023-11-23
+ [ Exploring Methods for Cross-lingual Text Style Transfer: The Case of  Text Detoxification](https://arxiv.org/abs/2311.13937)

	Daryna Dementieva, Daniil Moskovskiy, David Dale, Alexander Panchenko


+ [ Efficient Trigger Word Insertion](https://arxiv.org/abs/2311.13957)

	Yueqi Zeng, Ziqiang Li, Pengfei Xia, Lei Liu, Bin Li


+ [ ACT: Adversarial Consistency Models](https://arxiv.org/abs/2311.14097)

	Fei Kong, Jinhao Duan, Lichao Sun, Hao Cheng, Renjing Xu, Hengtao Shen, Xiaofeng Zhu, Xiaoshuang Shi, Kaidi Xu


+ [ Robust and Interpretable COVID-19 Diagnosis on Chest X-ray Images using  Adversarial Training](https://arxiv.org/abs/2311.14227)

	Karina Yang, Alexis Bennett, Dominique Duncan


+ [ When Side-Channel Attacks Break the Black-Box Property of Embedded Artificial Intelligence](http://arxiv.org/abs/2311.14005)

	Benoit Coqueret, Mathieu Carbone, Olivier Sentieys, Gabriel Zaid


+ [ Adversarial defense based on distribution transfer](http://arxiv.org/abs/2311.13841)

	Jiahao Chen, Diqun Yan, Li Dong


+ [ Robust and Interpretable COVID-19 Diagnosis on Chest X-ray Images using Adversarial Training](http://arxiv.org/abs/2311.14227)

	Karina Yang, Alexis Bennett, Dominique Duncan


## 2023-11-22
+ [ Prompt Risk Control: A Rigorous Framework for Responsible Deployment of  Large Language Models](https://arxiv.org/abs/2311.13628)

	Thomas P. Zollo, Todd Morrill, Zhun Deng, Jake C. Snell, Toniann Pitassi, Richard Zemel


+ [ A Theoretical Insight into Attack and Defense of Gradient Leakage in  Transformer](https://arxiv.org/abs/2311.13624)

	Chenyang Li, Zhao Song, Weixin Wang, Chiwun Yang


+ [ OASIS: Offsetting Active Reconstruction Attacks in Federated Learning](https://arxiv.org/abs/2311.13739)

	Tre' R. Jeter, Truc Nguyen, Raed Alharbi, My T. Thai


+ [ RAEDiff: Denoising Diffusion Probabilistic Models Based Reversible  Adversarial Examples Self-Generation and Self-Recovery](https://arxiv.org/abs/2311.12858)

	Fan Xing, Xiaoyi Zhou, Xuefeng Fan, Zhuo Tian, Yan Zhao


+ [ A Survey of Adversarial CAPTCHAs on its History, Classification and  Generation](https://arxiv.org/abs/2311.13233)

	Zisheng Xu, Qiao Yan, F. Richard Yu, Victor C. M. Leung


+ [ Panda or not Panda? Understanding Adversarial Attacks with Interactive Visualization](http://arxiv.org/abs/2311.13656)

	Yuzhe You, Jarvis Tse, Jian Zhao


+ [ Security and Privacy Challenges in Deep Learning Models](http://arxiv.org/abs/2311.13744)

	Gopichandh Golla


+ [ A Somewhat Robust Image Watermark against Diffusion-based Editing Models](http://arxiv.org/abs/2311.13713)

	Mingtian Tan, Tianhao Wang, Somesh Jha


+ [ Adversarial sample generation and training using geometric masks for  accurate and resilient license plate character recognition](https://arxiv.org/abs/2311.12857)

	Bishal Shrestha, Griwan Khakurel, Kritika Simkhada, Badri Adhikari


+ [ Attention Deficit is Ordered! Fooling Deformable Vision Transformers  with Collaborative Adversarial Patches](https://arxiv.org/abs/2311.12914)

	Quazi Mishkatul Alam, Bilel Tarchoun, Ihsen Alouani, Nael Abu-Ghazaleh


+ [ SD-NAE: Generating Natural Adversarial Examples with Stable Diffusion](https://arxiv.org/abs/2311.12981)

	Yueqian Lin, Jingyang Zhang, Yiran Chen, Hai Li


+ [ Hard Label Black Box Node Injection Attack on Graph Neural Networks](https://arxiv.org/abs/2311.13244)

	Yu Zhou, Zihao Dong, Guofeng Zhang, Jingchen Tang


+ [ Transfer Attacks and Defenses for Large Language Models on Coding Tasks](https://arxiv.org/abs/2311.13445)

	Chi Zhang, Zifan Wang, Ravi Mangal, Matt Fredrikson, Limin Jia, Corina Pasareanu


## 2023-11-21
+ [ Boost Adversarial Transferability by Uniform Scale and Mix Mask Method](https://arxiv.org/abs/2311.12051)

	Tao Wang, Zijian Ying, Qianmu Li, zhichao Lian


+ [ BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive  Learning](https://arxiv.org/abs/2311.12075)

	Siyuan Liang, Mingli Zhu, Aishan Liu, Baoyuan Wu, Xiaochun Cao, Ee-Chien Chang


+ [ Investigating Weight-Perturbed Deep Neural Networks With Application in  Iris Presentation Attack Detection](https://arxiv.org/abs/2311.12764)

	Renu Sharma, Redwan Sony, Arun Ross


+ [ Iris Presentation Attack: Assessing the Impact of Combining Vanadium  Dioxide Films with Artificial Eyes](https://arxiv.org/abs/2311.12773)

	Darshika Jauhari, Renu Sharma, Cunjian Chen, Nelson Sepulveda, Arun Ross


+ [ ODDR: Outlier Detection & Dimension Reduction Based Defense Against  Adversarial Patches](https://arxiv.org/abs/2311.12084)

	Nandish Chattopadhyay, Amira Guesmi, Muhammad Abdullah Hanif, Bassem Ouni, Muhammad Shafique


+ [ Attacking Motion Planners Using Adversarial Perception Errors](https://arxiv.org/abs/2311.12722)

	Jonathan Sadeghi, Nicholas A. Lord, John Redford, Romain Mueller


+ [ Masked Autoencoders Are Robust Neural Architecture Search Learners](https://arxiv.org/abs/2311.12086)

	Yiming Hu, Xiangxiang Chu, Bo Zhang


+ [ Adversarial Reweighting Guided by Wasserstein Distance for Bias  Mitigation](https://arxiv.org/abs/2311.12684)

	Xuan Zhao, Simone Fabbrizzi, Paula Reyero Lobo, Siamak Ghodsi, Klaus Broelemann, Steffen Staab, Gjergji Kasneci


+ [ Attacks of fairness in Federated Learning](https://arxiv.org/abs/2311.12715)

	Joseph Rance, Filip Svoboda


+ [ DefensiveDR: Defending against Adversarial Patches using Dimensionality  Reduction](https://arxiv.org/abs/2311.12211)

	Nandish Chattopadhyay, Amira Guesmi, Muhammad Abdullah Hanif, Bassem Ouni, Muhammad Shafique


## 2023-11-20
+ [ Safety-aware Causal Representation for Trustworthy Reinforcement  Learning in Autonomous Driving ](https://arxiv.org/abs/2311.10747)

	Haohong Lin, Wenhao Ding, Zuxin Liu, Yaru Niu, Jiacheng Zhu, Yuming Niu, Ding Zhao


+ [ Assessing Prompt Injection Risks in 200+ Custom GPTs ](https://arxiv.org/abs/2311.11538)

	Jiahao Yu, Yuhang Wu, Dong Shu, Mingyu Jin, Xinyu Xin


+ [ Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI  Systems ](https://arxiv.org/abs/2311.11796)

	Guangjing Wang, Ce Zhou, Yuanda Wang, Bocheng Chen, Hanqing Guo, Qiben Yan


+ [ Generating Valid and Natural Adversarial Examples with Large Language Models ](http://arxiv.org/abs/2311.11861)

	Zimu Wang, Wei Wang, Qi Chen, Qiufeng Wang, Anh Nguyen


+ [ AdvGen: Physical Adversarial Attack on Face Presentation Attack Detection Systems ](http://arxiv.org/abs/2311.11753)

	Sai Amrit Patnaik, Shivali Chansoriya, Anil K. Jain, Anoop M. Namboodiri


+ [ Understanding Variation in Subpopulation Susceptibility to Poisoning Attacks ](http://arxiv.org/abs/2311.11544)

	Evan Rose, Fnu Suya, David Evans


+ [ Training robust and generalizable quantum models ](http://arxiv.org/abs/2311.11871)

	Julian Berberich, Daniel Fink, Daniel Pranjić, Christian Tutschku, Christian Holm       


+ [ BrainWash: A Poisoning Attack to Forget in Continual Learning ](http://arxiv.org/abs/2311.11995)

	Ali Abbasi, Parsa Nooralinejad, Hamed Pirsiavash, Soheil Kolouri


## 2023-11-19
+ [ Robust Network Slicing: Multi-Agent Policies, Adversarial Attacks, and  Defensive Strategies ](https://arxiv.org/abs/2311.11206)

	Feng Wang, M. Cenk Gursoy, Senem Velipasalar


+ [ Adversarial Prompt Tuning for Vision-Language Models ](http://arxiv.org/abs/2311.11261)

	Jiaming Zhang, Xingjun Ma, Xin Wang, Lingyu Qiu, Jiaqi Wang, Yu-Gang Jiang, Jitao Sang  


+ [ TextGuard: Provable Defense against Backdoor Attacks on Text  Classification ](https://arxiv.org/abs/2311.11225)

	Hengzhi Pei, Jinyuan Jia, Wenbo Guo, Bo Li, Dawn Song


## 2023-11-18
+ [ Improving Adversarial Transferability by Stable Diffusion ](http://arxiv.org/abs/2311.11017)

 	Jiayang Liu, Siyu Zhu, Siyuan Liang, Jie Zhang, Han Fang, Weiming Zhang, Ee-Chien Chang 


+ [ Attention-Based Real-Time Defenses for Physical Adversarial Attacks in Vision Applications ](http://arxiv.org/abs/2311.11191)

	Giulio Rossolini, Alessandro Biondi, Giorgio Buttazzo


+ [ PACOL: Poisoning Attacks Against Continual Learners ](https://arxiv.org/abs/2311.10919)

	Huayu Li, Gregory Ditzler


+ [ Robust Network Slicing: Multi-Agent Policies, Adversarial Attacks, and Defensive Strategies ](http://arxiv.org/abs/2311.11206)

 	Feng Wang, M. Cenk Gursoy, Senem Velipasalar


## 2023-11-17
+ [ Robustness Enhancement in Neural Networks with Alpha-Stable Training  Noise ](https://arxiv.org/abs/2311.10803)

	Xueqiong Yuan, Jipeng Li, Ercan Engin Kuruoğlu

	
+ [ Breaking Temporal Consistency: Generating Video Universal Adversarial Perturbations Using Image Models ](http://arxiv.org/abs/2311.10366)

 	Hee-Seon Kim, Minji Son, Minbeom Kim, Myung-Joon Kwon, Changick Kim


+ [ PACOL: Poisoning Attacks Against Continual Learners ](http://arxiv.org/abs/2311.10919)

	Huayu Li, Gregory Ditzler


+ [ Two-Factor Authentication Approach Based on Behavior Patterns for Defeating Puppet Attacks ](http://arxiv.org/abs/2311.10389)

	Wenhao Wang, Guyue Li, Zhiming Chu, Haobo Li, Daniele Faccio


## 2023-11-16
+ [Breaking Boundaries: Balancing Performance and Robustness in Deep Wireless Traffic Forecasting](http://arxiv.org/abs/2311.09790)

    Ilbert Romain, V. Hoang Thai, Zhang Zonghua, Palpanas Themis


+ [ Hijacking Large Language Models via Adversarial In-Context Learning](http://arxiv.org/abs/2311.09948)

    Yao Qiang, Xiangyu Zhou, Dongxiao Zhu


+ [ Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking](http://arxiv.org/abs/2311.09827)

    Nan Xu, Fei Wang, Ben Zhou, Bang Zheng Li, Chaowei Xiao, Muhao Chen


+ [ Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations](http://arxiv.org/abs/2311.09763)

    Wenjie Mo, Jiashu Xu, Qin Liu, Jiongxiao Wang, Jun Yan, Chaowei Xiao, Muhao Chen


+ [ On the Exploitability of Reinforcement Learning with Human Feedback for Large Language Models](http://arxiv.org/abs/2311.09641)

    Jiongxiao Wang, Junlin Wu, Muhao Chen, Yevgeniy Vorobeychik, Chaowei Xiao


+ [ Towards more Practical Threat Models in Artificial Intelligence Security](http://arxiv.org/abs/2311.09994)

    Kathrin Grosse, Lukas Bieringer, Tarek Richard Besold, Alexandre Alahi


## 2023-11-15
+ [ Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts](http://arxiv.org/abs/2311.09127)

    Yuanwei Wu, Xiang Li, Yixin Liu, Pan Zhou, Lichao Sun


+ [ Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment](http://arxiv.org/abs/2311.09433)  

    Haoran Wang, Kai Shu


+ [ Fast Certification of Vision-Language Models Using Incremental Randomized Smoothing](http://arxiv.org/abs/2311.09024)

    A K Iowa State University Nirala, A New York University Joshi, C New York University Hegde, S Iowa State University Sarkar


+ [ Adversarially Robust Spiking Neural Networks Through Conversion](http://arxiv.org/abs/2311.09266)

    Ozan Özdenizci, Robert Legenstein


+ [ Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization](http://arxiv.org/abs/2311.09096)

    Zhexin Zhang, Junxiao Yang, Pei Ke, Minlie Huang


+ [ Privacy Threats in Stable Diffusion Models](http://arxiv.org/abs/2311.09355)

    Thomas Cilloni, Charles Fleming, Charles Walter


+ [ How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities](http://arxiv.org/abs/2311.09447)

    Lingbo Mo, Boshi Wang, Muhao Chen, Huan Sun


+ [ MirrorNet: A TEE-Friendly Framework for Secure On-device DNN Inference](http://arxiv.org/abs/2311.09489)

    Ziyu Liu, Yukui Luo, Shijin Duan, Tong Zhou, Xiaolin Xu


+ [ Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models](http://arxiv.org/abs/2311.09428)

    Yueqing Liang, Lu Cheng, Ali Payani, Kai Shu


+ [ JAB: Joint Adversarial Prompting and Belief Augmentation](http://arxiv.org/abs/2311.09473)

    Ninareh Mehrabi, Palash Goyal, Anil Ramakrishna, Jwala Dhamala, Shalini Ghosh, Richard Zemel, Kai-Wei Chang, Aram Galstyan, Rahul Gupta


## 2023-11-14
+ [ Towards Improving Robustness Against Common Corruptions in Object Detectors Using Adversarial Contrastive Learning](http://arxiv.org/abs/2311.07928)

    Shashank Kotyan, Danilo Vasconcellos Vargas


+ [ Physical Adversarial Examples for Multi-Camera Systems](http://arxiv.org/abs/2311.08539)

    Ana Răduţoiu, Jan-Philipp Schulze, Philip Sperl, Konstantin Böttinger


+ [ DALA: A Distribution-Aware LoRA-Based Adversarial Attack against Pre-trained Language Models](http://arxiv.org/abs/2311.08598)

    Yibo Wang, Xiangjue Dong, James Caverlee, Philip S. Yu


+ [ On The Relationship Between Universal Adversarial Attacks And Sparse Representations](http://arxiv.org/abs/2311.08265)

    Dana Weitzner, Raja Giryes


+ [ A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily](http://arxiv.org/abs/2311.08268)   

    Peng Ding, Jun Kuang, Dan Ma, Xuezhi Cao, Yunsen Xian, Jiajun Chen, Shujian Huang


+ [ Multi-Set Inoculation: Assessing Model Robustness Across Multiple Challenge Sets](http://arxiv.org/abs/2311.08662)

    Vatsal Gupta, Pranshu Pandya, Tushar Kataria, Vivek Gupta, Dan Roth


+ [ The Perception-Robustness Tradeoff in Deterministic Image Restoration](http://arxiv.org/abs/2311.09253)

    Guy Ohayon, Tomer Michaeli, Michael Elad


## 2023-11-13
+ [ Adversarial Purification for Data-Driven Power System Event Classifiers with Diffusion Models](http://arxiv.org/abs/2311.07110)

    Yuanbin Cheng, Koji Yamashita, Jim Follum, Nanpeng Yu


+ [ Parrot-Trained Adversarial Examples: Pushing the Practicality of Black-Box Audio Attacks against Speaker Recognition Models](http://arxiv.org/abs/2311.07780)

    Rui Duan, Zhe Qu, Leah Ding, Yao Liu, Zhuo Lu


+ [ An Extensive Study on Adversarial Attack against Pre-trained Models of Code](http://arxiv.org/abs/2311.07553)

    Xiaohu Du, Ming Wen, Zichao Wei, Shangwen Wang, Hai Jin


+ [ Untargeted Black-box Attacks for Social Recommendations](http://arxiv.org/abs/2311.07127)

    Wenqi Fan, Shijie Wang, Xiao-yong Wei, Xiaowei Mei, Qing Li


+ [ On the Robustness of Neural Collapse and the Neural Collapse of Robustness](http://arxiv.org/abs/2311.07444)

    Jingtong Su, Ya Shi Zhang, Nikolaos Tsilivis, Julia Kempe


+ [ Tabdoor: Backdoor Vulnerabilities in Transformer-based Neural Networks for Tabular Data ](http://arxiv.org/abs/2311.07550)

    Bart Pleiter, Behrad Tajalli, Stefanos Koffas, Gorka Abad, Jing Xu, Martha Larson, Stjepan Picek


## 2023-11-12
+ [ Learning Globally Optimized Language Structure via Adversarial Training](http://arxiv.org/abs/2311.06771)

	Xuwang Yin


+ [ Contractive Systems Improve Graph Neural Networks Against Adversarial Attacks](http://arxiv.org/abs/2311.06942)

    Moshe Eliasof, Davide Murari, Ferdia Sherry, Carola-Bibiane Schönlieb


+ [ Analytical Verification of Deep Neural Network Performance for Time-Synchronized Distribution System State Estimation](http://arxiv.org/abs/2311.06973)

        Behrouz Azimian, Shiva Moshtagh, Anamitra Pal, Shanshan Ma


+ [ DialMAT: Dialogue-Enabled Transformer with Moment-Based Adversarial Training](http://arxiv.org/abs/2311.06855)

    Kanta Kaneda, Ryosuke Korekata, Yuiga Wada, Shunya Nagashima, Motonari Kambara, Yui Iioka, Haruka Matsuo, Yuto Imai, Takayuki Nishimura, Komei Sugiura


## 2023-11-10
+ [ Robust Adversarial Attacks Detection for Deep Learning based Relative  Pose Estimation for Space Rendezvous](https://arxiv.org/abs/2311.05992)

	Ziwei Wang, Nabil Aouf, Jose Pizarro, Christophe Honvault


+ [ Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the  Wild](https://arxiv.org/abs/2311.06237)

	Nanna Inie, Jonathan Stray, Leon Derczynski


+ [ Scale-MIA: A Scalable Model Inversion Attack against Secure Federated  Learning via Latent Space Reconstruction](https://arxiv.org/abs/2311.05808)

	Shanghao Shi, Ning Wang, Yang Xiao, Chaoyu Zhang, Yi Shi, Y.Thomas Hou, Wenjing Lou


+ [ Does Differential Privacy Prevent Backdoor Attacks in Practice?](https://arxiv.org/abs/2311.06227)

	Fereshteh Razmi, Jian Lou, Li Xiong


+ [Flatness-aware Adversarial Attack](https://arxiv.org/abs/2311.06423)

	Mingyuan Fan, Xiaodan Li, Cen Chen, Yinggui Wang


+ [Robust Adversarial Attacks Detection for Deep Learning based Relative Pose Estimation for Space Rendezvous.](https://arxiv.org/abs/2311.05992)

	Ziwei Wang, Nabil Aouf, Jose Pizarro, Christophe Honvault


+ [Fight Fire with Fire: Combating Adversarial Patch Attacks using Pattern-randomized Defensive Patches](https://arxiv.org/abs/2311.06122)
	
	Jianan Feng, Jiachun Li, Changqing Miao, Jianjun Huang, Wei You, Wenchang Shi, Bin Liang


+ [Resilient and constrained consensus against adversarial attacks: A distributed MPC framework](https://arxiv.org/abs/2311.05935)

	Henglai Wei, Kunwu Zhang, Hui Zhang, Yang Shi


+ [CALLOC: Curriculum Adversarial Learning for Secure and Robust Indoor Localization](https://arxiv.org/abs/2311.06361)

	Danish Gufran, Sudeep Pasricha

+ [Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration](https://arxiv.org/abs/2311.06062)

	Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, Tao Jiang


## 2023-11-09
+ [ Counter-Empirical Attacking based on Adversarial Reinforcement Learning  for Time-Relevant Scoring System](https://arxiv.org/abs/2311.05144)

	Xiangguo Sun, Hong Cheng, Hang Dong, Bo Qiao, Si Qin, Qingwei Lin


+ [ SynFacePAD 2023: Competition on Face Presentation Attack Detection Based  on Privacy-aware Synthetic Training Data](https://arxiv.org/abs/2311.05336)

	Meiling Fang, Marco Huber, Julian Fierrez, Raghavendra Ramachandra, Naser Damer, Alhasan Alkhaddour, Maksim Kasantcev, Vasiliy Pryadchenko, Ziyuan Yang, Huijie Huangfu, Yingyu Chen, Yi Zhang, Yuchen Pan, Junjun Jiang, Xianming Liu, Xianyun Sun, Caiyong Wang, Xingyu Liu, Zhaohua Chang, Guangzhe Zhao, Juan Tapia, Lazaro Gonzalez-Soler, Carlos Aravena, Daniel Schulz


+ [ Counter-Empirical Attacking based on Adversarial Reinforcement Learning  for Time-Relevant Scoring System](https://arxiv.org/abs/2311.05144)

	Xiangguo Sun, Hong Cheng, Hang Dong, Bo Qiao, Si Qin, Qingwei Lin


+ [ SCAAT: Improving Neural Network Interpretability via Saliency  Constrained Adaptive Adversarial Training](https://arxiv.org/abs/2311.05143)

	Rui Xu, Wenkang Qin, Peixiang Huang, Haowang, Lin Luo


+ [ Training Robust Deep Physiological Measurement Models with Synthetic  Video-based Data](https://arxiv.org/abs/2311.05371)

	Yuxuan Ou, Yuzhe Zhang, Yuntang Wang, Shwetak Patel, Daniel McDuf, Xin Liu

+ [ FigStep: Jailbreaking Large Vision-language Models via Typographic  Visual Prompts](https://arxiv.org/abs/2311.05608)

	Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, Xiaoyun Wang


## 2023-11-8
+ [ Familiarity-Based Open-Set Recognition Under Adversarial Attacks](https://arxiv.org/abs/2311.05006)

	Philip Enevoldsen, Christian Gundersen, Nico Lang, Serge Belongie, Christian Igel


+ [ Edge-assisted U-Shaped Split Federated Learning with Privacy-preserving  for Internet of Things](https://arxiv.org/abs/2311.04944)

	Hengliang Tang, Zihang Zhao, Detian Liu, Yang Cao, Shiqiang Zhang, Siqing You


+ [ DEMASQ: Unmasking the ChatGPT Wordsmith](https://arxiv.org/abs/2311.05019)

	Kavita Kumari, Alessandro Pegoraro, Hossein Fereidooni, Ahmad-Reza Sadeghi


+ [ On the steerability of large language models toward data-driven personas](https://arxiv.org/abs/2311.04978)

	Junyi Li, Ninareh Mehrabi, Charith Peris, Palash Goyal, Kai-Wei Chang, Aram Galstyan, Richard Zemel, Rahul Gupta


## 2023-11-7
+ [ FD-MIA: Efficient Attacks on Fairness-enhanced Models](https://arxiv.org/abs/2311.03865)

	Huan Tian, Guangsheng Zhang, Bo Liu, Tianqing Zhu, Ming Ding, Wanlei Zhou


+ [ Unveiling Safety Vulnerabilities of Large Language Models](https://arxiv.org/abs/2311.04124)
	
	George Kour, Marcel Zalmanovici, Naama Zwerdling, Esther Goldbraich, Ora Nova Fandina, Ateret Anaby-Tavor, Orna Raz, Eitan Farchi


## 2023-11-6
+ [ A Preference Learning Approach to Develop Safe and Personalizable  Autonomous Vehicles](https://arxiv.org/abs/2311.02099)

	Ruya Karagulle, Nikos Arechiga, Andrew Best, Jonathan DeCastro, Necmiye Ozay


+ [ Making Harmful Behaviors Unlearnable for Large Language Models](https://arxiv.org/abs/2311.02105)

	Xin Zhou, Yi Lu, Ruotian Ma, Tao Gui, Qi Zhang, Xuanjing Huang


+ [ Uncertainty Quantification of Deep Learning for Spatiotemporal Data:  Challenges and Opportunities](https://arxiv.org/abs/2311.02485)

	Wenchong He, Zhe Jiang


+ [ On the Intersection of Self-Correction and Trust in Language Models](https://arxiv.org/abs/2311.02801)

	Satyapriya Krishna


+ [ Preserving Privacy in GANs Against Membership Inference Attack](https://arxiv.org/abs/2311.03172)

	Mohammadhadi Shateri, Francisco Messina, Fabrice Labeau, Pablo Piantanida


+ [ DeepInception: Hypnotize Large Language Model to Be Jailbreaker](https://arxiv.org/abs/2311.03191)

	Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, Bo Han


## 2023-11-5
+ [ Pilot-Based Key Distribution and Encryption for Secure Coherent Passive  Optical Networks](https://arxiv.org/abs/2311.02554)

	Haide Wang, Ji Zhou, Qingxin Lu, Jianrui Zeng, Yongqing Liao, Weiping Liu, Changyuan Yu, Zhaohui Li


+ [ ELEGANT: Certified Defense on the Fairness of Graph Neural Networks](https://arxiv.org/abs/2311.02757)

	Yushun Dong; Binchi Zhang; Hanghang Tong; Jundong Li


## 2023-11-4
+ [ From Trojan Horses to Castle Walls: Unveiling Bilateral Backdoor Effects  in Diffusion Models](https://arxiv.org/abs/2311.02373)

	Zhuoshi Pan, Yuguang Yao, Gaowen Liu, Bingquan Shen, H. Vicky Zhao, Ramana Rao Kompella, Sijia Liu


## 2023-11-3
+ [ Can AI Mitigate Human Perceptual Biases? A Pilot Study](https://arxiv.org/abs/2311.00706)

	Ross Geuy, Nate Rising, Tiancheng Shi, Meng Ling, Jian Chen


+ [ SCPO: Safe Reinforcement Learning with Safety Critic Policy Optimization](https://arxiv.org/abs/2311.00880)

	Jaafar Mhamed, Shangding Gu


+ [ Robust Identity Perceptual Watermark Against Deepfake Face Swapping](https://arxiv.org/abs/2311.01357)

	Tianyi Wang, Mengxiao Huang, Harry Cheng, Bin Ma, Yinglong Wang


+ [ A Call to Arms: AI Should be Critical for Social Media Analysis of  Conflict Zones](https://arxiv.org/abs/2311.00810)

	Afia Abedin, Abdul Bais, Cody Buntain, Laura Courchesne, Brian McQuinn, Matthew E. Taylor, Muhib Ullah


+ [ Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game](https://arxiv.org/abs/2311.01011)

	Sam Toyer, Olivia Watkins, Ethan Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor Darrell, Alan Ritter, Stuart Russell


+ [ MIST: Defending Against Membership Inference Attacks Through  Membership-Invariant Subspace Training](https://arxiv.org/abs/2311.00919)

	Jiacheng Li, Ninghui Li, Bruno Ribeiro


+ [ Reputation Systems for Supply Chains: The Challenge of Achieving Privacy  Preservation](https://arxiv.org/abs/2311.01060)

	Lennart Bader, Jan Pennekamp, Emildeon Thevaraj, Maria Spiß, Salil S. Kanhere, Klaus Wehrle


## 2023-11-2
+ [ Optimal Cost Constrained Adversarial Attacks For Multiple Agent Systems](https://arxiv.org/abs/2311.00859)

	Ziqing Lu, Guanlin Liu, Lifeng Cai, Weiyu Xu


+ [ SCPO: Safe Reinforcement Learning with Safety Critic Policy Optimization](https://arxiv.org/abs/2311.00880)

	Jaafar Mhamed, Shangding Gu


+ [ Generate and Pray: Using SALLMS to Evaluate the Security of LLM  Generated Code](https://arxiv.org/abs/2311.00889)

	Mohammed Latif Siddiq, Joanna C. S. Santos


+ [ Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Lehman Go  Indifferent](https://arxiv.org/abs/2311.01205)

	Lorenz Kummer, Samir Moustafa, Nils N. Kriege, Wilfried N. Gansterer


+ [ Towards Evaluating Transfer-based Attacks Systematically, Practically, and Fairly](https://arxiv.org/abs/2311.01323)

	Qizhang Li, Yiwen Guo, Wangmeng Zuo, Hao Chen


+ [ Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game](https://arxiv.org/abs/2311.01011)

	Sam Toyer, Olivia Watkins, Ethan Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor Darrell, Alan Ritter, Stuart Russell


+ [ In Defense of Softmax Parametrization for Calibrated and Consistent  Learning to Defer](https://arxiv.org/abs/2311.01106)

	Yuzhou Cao, Hussein Mozannar, Lei Feng, Hongxin Wei, Bo An


+ [ MIST: Defending Against Membership Inference Attacks Through  Membership-Invariant Subspace Training](https://arxiv.org/abs/2311.00919)

	Jiacheng Li, Ninghui Li, Bruno Ribeiro


## 2023-11-1
+ [ FAIRLABEL: Correcting Bias in Labels](https://arxiv.org/abs/2311.00638)

	Srinivasan H Sengamedu, Hien Pham


+ [ Probing Explicit and Implicit Gender Bias through LLM Conditional Text  Generation](https://arxiv.org/abs/2311.00306)

	Xiangjue Dong, Yibo Wang, Philip S. Yu, James Caverlee


+ [ Robustness Tests for Automatic Machine Translation Metrics with  Adversarial Attacks](https://arxiv.org/abs/2311.00508)

	Yichen Huang, Timothy Baldwin


+ [ Medi-CAT: Contrastive Adversarial Training for Medical Image  Classification](https://arxiv.org/abs/2311.00154)

	Pervaiz Iqbal Khan, Andreas Dengel, Sheraz Ahmed


+ [ Uncertainty quantification and out-of-distribution detection using  surjective normalizing flows](https://arxiv.org/abs/2311.00377)

	Simon Dirmeier, Ye Hong, Yanan Xin, Fernando Perez-Cruz


+ [ NEO-KD: Knowledge-Distillation-Based Adversarial Training for Robust  Multi-Exit Neural Networks](https://arxiv.org/abs/2311.00428)

	Seokil Ham, Jungwuk Park, Dong-Jun Han, Jaekyun Moon


## 2023-10-31
+ [ Unmasking Bias and Inequities: A Systematic Review of Bias Detection and  Mitigation in Healthcare Artificial Intelligence Using Electronic Health  Records](https://arxiv.org/abs/2310.19917)

	Feng Chen, Liqin Wang, Julie Hong, Jiaqi Jiang, Li Zhou


+ [ Is Robustness Transferable across Languages in Multilingual Neural  Machine Translation?](https://arxiv.org/abs/2310.20162)

	Leiyu Pan, Supryadi, Deyi Xiong


+ [ LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B](https://arxiv.org/abs/2310.20624)

	Simon Lermen, Charlie Rogers-Smith, Jeffrey Ladish


+ [ DEPN: Detecting and Editing Privacy Neurons in Pretrained Language  Models](https://arxiv.org/abs/2310.20138)

	Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, Deyi Xiong


+ [ Verification of Neural Networks Local Differential Classification  Privacy](https://arxiv.org/abs/2310.20299)

	Roie Reshef, Anan Kabaha, Olga Seleznova, Dana Drachsler-Cohen


+ [ Initialization Matters: Privacy-Utility Analysis of Overparameterized  Neural Networks](https://arxiv.org/abs/2310.20579)

	Jiayuan Ye, Zhenyu Zhu, Fanghui Liu, Reza Shokri, Volkan Cevher


## 2023-10-30
+ [ PriPrune: Quantifying and Preserving Privacy in Pruned Federated  Learning](https://arxiv.org/abs/2310.19958)

	Tianyue Chu, Mengwei Yang, Nikolaos Laoutaris, Athina Markopoulou


+ [ LipSim: A Provably Robust Perceptual Similarity Metric](https://arxiv.org/abs/2310.18274)

	Sara Ghazanfari, Alexandre Araujo, Prashanth Krishnamurthy, Farshad Khorrami, Siddharth Garg


+ [ Counterfactual Fairness for Predictions using Generative Adversarial  Networks](https://arxiv.org/abs/2310.17687)

	Yuchen Ma, Dennis Frauen, Valentyn Melnychuk, Stefan Feuerriegel


## 2023-10-29
+ [ Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection  Method](https://arxiv.org/abs/2310.17918)

	Yukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang Xing, Chong Meng, Shuaiqiang Wang, Zhicong Cheng, Zhaochun Ren, Dawei Yin


+ [ Fine tuning Pre trained Models for Robustness Under Noisy Labels](https://arxiv.org/abs/2310.17668)

	Sumyeong Ahn, Sihyeon Kim, Jongwoo Ko, Se-Young Yun


+ [ Adversarial Anomaly Detection using Gaussian Priors and Nonlinear  Anomaly Scores](https://arxiv.org/abs/2310.18091)

	Fiete Lüer, Tobias Weber, Maxim Dolgich, Christian Böhm


+ [ $α$-Mutual Information: A Tunable Privacy Measure for Privacy  Protection in Data Sharing](https://arxiv.org/abs/2310.18241)

	MirHamed Jafarzadeh Asl, Mohammadhadi Shateri, Fabrice Labeau


+ [ BlackJack: Secure machine learning on IoT devices through hardware-based  shuffling](https://arxiv.org/abs/2310.17804)

	Karthik Ganesan, Michal Fishkin, Ourong Lin, Natalie Enright Jerger



## 2023-10-27
+ [ ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in  Real-World User-AI Conversation](https://arxiv.org/abs/2310.17389)

	Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, Jingbo Shang


+ [ Where you go is who you are -- A study on machine learning based  semantic privacy attacks](https://arxiv.org/abs/2310.17643)

	Nina Wiedemann, Ourania Kounadi, Martin Raubal, Krzysztof Janowicz


+ [ A near-autonomous and incremental intrusion detection system through  active learning of known and unknown attacks](https://arxiv.org/abs/2310.17430)

	Lynda Boukela, Gongxuan Zhang, Meziane Yacoub, Samia Bouzefrane


## 2023-10-26
+ [ CBD: A Certified Backdoor Detector Based on Local Dominant Probability](https://arxiv.org/abs/2310.17498)

	Zhen Xiang, Zidi Xiong, Bo Li


+ [ A Survey on Transferability of Adversarial Examples across Deep Neural  Networks](https://arxiv.org/abs/2310.17626)

	Jindong Gu, Xiaojun Jia, Pau de Jorge, Wenqain Yu, Xinwei Liu, Avery Ma, Yuan Xun, Anjun Hu, Ashkan Khakzar, Zhijiang Li, Xiaochun Cao, Philip Torr


+ [ Uncertainty-weighted Loss Functions for Improved Adversarial Attacks on  Semantic Segmentation](https://arxiv.org/abs/2310.17436)

	Kira Maag, Asja Fischer


+ [ Detecting stealthy cyberattacks on adaptive cruise control vehicles: A  machine learning approach](https://arxiv.org/abs/2310.17091)

	Tianyi Li, Mingfeng Shang, Shian Wang, Raphael Stern


+ [ SoK: Pitfalls in Evaluating Black-Box Attacks](https://arxiv.org/abs/2310.17534)

	Fnu Suya, Anshuman Suri, Tingwei Zhang, Jingtao Hong, Yuan Tian, David Evans


+ [ Defending Against Transfer Attacks From Public Models](https://arxiv.org/abs/2310.17645)

	Chawin Sitawarin, Jaewon Chang, David Huang, Wesson Altoyan, David Wagner


+ [ Proving Test Set Contamination in Black Box Language Models](https://arxiv.org/abs/2310.17623)

	Yonatan Oren, Nicole Meister, Niladri Chatterji, Faisal Ladhak, Tatsunori B. Hashimoto


+ [ Detection Defenses: An Empty Promise against Adversarial Patch Attacks  on Optical Flow](https://arxiv.org/abs/2310.17403)

	Erik Scheurer, Jenny Schmalfuss, Alexander Lis, Andrés Bruhn


## 2023-10-25
+ [ Trust, but Verify: Robust Image Segmentation using Deep Learning](https://arxiv.org/abs/2310.16999)

	Fahim Ahmed Zaman, Xiaodong Wu, Weiyu Xu, Milan Sonka, Raghuraman Mudumbai


+ [ Break it, Imitate it, Fix it: Robustness by Generating Human-Like  Attacks](https://arxiv.org/abs/2310.16955)

	Aradhana Sinha, Ananth Balashankar, Ahmad Beirami, Thi Avrahami, Jilin Chen, Alex Beutel


+ [ Improving Few-shot Generalization of Safety Classifiers via Data  Augmented Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2310.16959)

	Ananth Balashankar, Xiao Ma, Aradhana Sinha, Ahmad Beirami, Yao Qin, Jilin Chen, Alex Beutel


+ [ Confounder Balancing in Adversarial Domain Adaptation for Pre-Trained  Large Models Fine-Tuning](https://arxiv.org/abs/2310.16062)

	Shuoran Jiang, Qingcai Chen, Yang Xiang, Youcheng Pan, Xiangping Wu


## 2023-10-24
+ [ Enhancing Large Language Models for Secure Code Generation: A  Dataset-driven Study on Vulnerability Mitigation](https://arxiv.org/abs/2310.16263)

	Jiexin Wang, Liuwen Cao, Xitong Luo, Zhiping Zhou, Jiayuan Xie, Adam Jatowt, Yi Cai


+ [ Segue: Side-information Guided Generative Unlearnable Examples for  Facial Privacy Protection in Real World](https://arxiv.org/abs/2310.16061)

	Zhiling Zhang, Jie Zhang, Kui Zhang, Wenbo Zhou, Weiming Zhang, Nenghai Yu


+ [ Defense Against Model Extraction Attacks on Recommender Systems](https://arxiv.org/abs/2310.16335)

	Sixiao Zhang, Hongzhi Yin, Hongxu Chen, Cheng Long


+ [ Robust and Actively Secure Serverless Collaborative Learning](https://arxiv.org/abs/2310.16678)

	Olive Franzese, Adam Dziedzic, Christopher A. Choquette-Choo, Mark R. Thomas, Muhammad Ahmad Kaleem, Stephan Rabanser, Congyu Fang, Somesh Jha, Nicolas Papernot, Xiao Wang


+ [ AI Hazard Management: A framework for the systematic management of root  causes for AI risks](https://arxiv.org/abs/2310.16727)

	Ronald Schnitzer, Andreas Hapfelmeier, Sven Gaube, Sonja Zillner


+ [ FLTrojan: Privacy Leakage Attacks against Federated Language Models  Through Selective Weight Tampering](https://arxiv.org/abs/2310.16152)

	Md Rafi Ur Rashid, Vishnu Asutosh Dasu, Kang Gu, Najrin Sultana, Shagufta Mehnaz


+ [ Poison is Not Traceless: Fully-Agnostic Detection of Poisoning Attacks](https://arxiv.org/abs/2310.16224)

	Xinglong Chang, Katharina Dost, Gillian Dobbie, Jörg Wicker


+ [ A model for multi-attack classification to improve intrusion detection  performance using deep learning approaches](https://arxiv.org/abs/2310.16380)

	Arun Kumar Silivery, Ram Mohan Rao Kovvur


## 2023-10-23
+ [ 3D Masked Autoencoders for Enhanced Privacy in MRI Scans](https://arxiv.org/abs/2310.15778)

	Lennart Alexander Van der Goten, Kevin Smith


+ [ Self-Guard: Empower the LLM to Safeguard Itself](https://arxiv.org/abs/2310.15851)

	Zezhong Wang, Fangkai Yang, Lu Wang, Pu Zhao, Hongru Wang, Liang Chen, Qingwei Lin, Kam-Fai Wong


+ [ The Janus Interface: How Fine-Tuning in Large Language Models Amplifies  the Privacy Risks](https://arxiv.org/abs/2310.15469)

	Xiaoyi Chen, Siyuan Tang, Rui Zhu, Shijun Yan, Lei Jin, Zihao Wang, Liya Su, XiaoFeng Wang, Haixu Tang


+ [ Deceptive Fairness Attacks on Graphs via Meta Learning](https://arxiv.org/abs/2310.15653)

	Jian Kang, Yinglong Xia, Ross Maciejewski, Jiebo Luo, Hanghang Tong


+ [ Momentum Gradient-based Untargeted Attack on Hypergraph Neural Networks](https://arxiv.org/abs/2310.15656)

	Yang Chen, Stjepan Picek, Zhonglin Ye, Zhaoyang Wang, Haixing Zhao


## 2023-10-22
+ [ Fundamental Limits of Membership Inference Attacks on Machine Learning  Models](https://arxiv.org/abs/2310.13786)

	Eric Aubinais, Elisabeth Gassiat, Pablo Piantanida


+ [ Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models](https://arxiv.org/abs/2310.13828)

	Shawn Shan, Wenxin Ding, Josephine Passananti, Haitao Zheng, Ben Y. Zhao


+ [ MoPe: Model Perturbation-based Privacy Attacks on Language Models](https://arxiv.org/abs/2310.14369)

	Marvin Li, Jason Wang, Jeffrey Wang, Seth Neel


+ [ On existence, uniqueness and scalability of adversarial robustness  measures for AI classifiers](https://arxiv.org/abs/2310.14421)

	Illia Horenko


+ [ AutoDAN: Automatic and Interpretable Adversarial Attacks on Large  Language Models](https://arxiv.org/abs/2310.15140)

	Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, Tong Sun


+ [ Toward Stronger Textual Attack Detectors](https://arxiv.org/abs/2310.14001)

	Pierre Colombo, Marine Picot, Nathan Noiry, Guillaume Staerman, Pablo Piantanida


+ [ CT-GAT: Cross-Task Generative Adversarial Attack based on  Transferability](https://arxiv.org/abs/2310.14265)

	Minxuan Lv, Chengwei Dai, Kun Li, Wei Zhou, Songlin Hu


+ [ Data-Free Knowledge Distillation Using Adversarially Perturbed OpenGL  Shader Images](https://arxiv.org/abs/2310.13782)

	Logan Frank, Jim Davis


+ [ Bi-discriminator Domain Adversarial Neural Networks with Class-Level  Gradient Alignment](https://arxiv.org/abs/2310.13959)

	Chuang Zhao, Hongke Zhao, Hengshu Zhu, Zhenya Huang, Nan Feng, Enhong Chen, Hui Xiong


+ [ ADoPT: LiDAR Spoofing Attack Detection Based on Point-Level Temporal  Consistency](https://arxiv.org/abs/2310.14504)

	Minkyoung Cho, Yulong Cao, Zixiang Zhou, Z. Morley Mao


+ [ F$^2$AT: Feature-Focusing Adversarial Training via Disentanglement of  Natural and Perturbed Patterns](https://arxiv.org/abs/2310.14561)

	Yaguan Qian, Chenyu Zhao, Zhaoquan Gu, Bin Wang, Shouling Ji, Wei Wang, Boyang Zhou, Pan Zhou


+ [ Semantic-Aware Adversarial Training for Reliable Deep Hashing Retrieval](https://arxiv.org/abs/2310.14637)

	Xu Yuan, Zheng Zhang, Xunguang Wang, Lin Wu


+ [ On the Detection of Image-Scaling Attacks in Machine Learning](https://arxiv.org/abs/2310.15085)

	Erwin Quiring, Andreas Müller, Konrad Rieck


+ [ Adversarial Attacks on Fairness of Graph Neural Networks](https://arxiv.org/abs/2310.13822)

	Binchi Zhang, Yushun Dong, Chen Chen, Yada Zhu, Minnan Luo, Jundong Li


+ [ Competitive Advantage Attacks to Decentralized Federated Learning](https://arxiv.org/abs/2310.13862)

	Yuqi Jia, Minghong Fang, Neil Zhenqiang Gong


+ [ Enhancing Accuracy-Privacy Trade-off in Differentially Private Split  Learning](https://arxiv.org/abs/2310.14434)

	Ngoc Duy Pham, Khoa Tran Phan, Naveen Chilamkurti


## 2023-10-21
+ [ GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for  Reasoning Problems](https://arxiv.org/abs/2310.12397)

	Kaya Stechly, Matthew Marquez, Subbarao Kambhampati


+ [ Prompt Injection Attacks and Defenses in LLM-Integrated Applications](https://arxiv.org/abs/2310.12815)

	Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, Neil Zhenqiang Gong


+ [ Probing LLMs for hate speech detection: strengths and vulnerabilities](https://arxiv.org/abs/2310.12860)

	Sarthak Roy, Ashish Harshavardhan, Animesh Mukherjee, Punyajoy Saha


## 2023-10-20
+ [ PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models](https://arxiv.org/abs/2310.12439)

	Hongwei Yao, Jian Lou, Zhan Qin


+ [ Segment Anything Meets Universal Adversarial Perturbation](https://arxiv.org/abs/2310.12431)

	Dongshen Han, Sheng Zheng, Chaoning Zhang


+ [ Fast Model Debias with Machine Unlearning](https://arxiv.org/abs/2310.12560)

	Ruizhe Chen, Jianfei Yang, Huimin Xiong, Jianhong Bai, Tianxiang Hu, Jin Hao, Yang Feng, Joey Tianyi Zhou, Jian Wu, Zuozhu Liu


## 2023-10-19
+ [ Automatic Hallucination Assessment for Aligned Large Language Models via  Transferable Adversarial Attacks](https://arxiv.org/abs/2310.12516)

	Xiaodong Yu, Hao Cheng, Xiaodong Liu, Dan Roth, Jianfeng Gao


+ [ Attack Prompt Generation for Red Teaming and Defending Large Language  Models](https://arxiv.org/abs/2310.12505)

	Boyi Deng, Wenjie Wang, Fuli Feng, Yang Deng, Qifan Wang, Xiangnan He


+ [ Recoverable Privacy-Preserving Image Classification through Noise-like  Adversarial Examples](https://arxiv.org/abs/2310.12707)

	Jun Liu, Jiantao Zhou, Jinyu Tian, Weiwei Sun


+ [ REVAMP: Automated Simulations of Adversarial Attacks on Arbitrary  Objects in Realistic Scenes](https://arxiv.org/abs/2310.12243)

	Matthew Hull, Zijie J. Wang, Duen Horng Chau


+ [ Generating Robust Adversarial Examples against Online Social Networks  (OSNs)](https://arxiv.org/abs/2310.12708)

	Jun Liu, Jiantao Zhou, Haiwei Wu, Weiwei Sun, Jinyu Tian


+ [ OODRobustBench: benchmarking and analyzing adversarial robustness under  distribution shift](https://arxiv.org/abs/2310.12793)

	Lin Li, Yifei Wang, Chawin Sitawarin, Michael Spratling


+ [ CAT: Closed-loop Adversarial Training for Safe End-to-End Driving](https://arxiv.org/abs/2310.12432)

	Linrui Zhang, Zhenghao Peng, Quanyi Li, Bolei Zhou


+ [ Knowledge from Uncertainty in Evidential Deep Learning](https://arxiv.org/abs/2310.12663)

	Cai Davies, Marc Roig Vilamala, Alun D. Preece, Federico Cerutti, Lance M. Kaplan, Supriyo Chakraborty


+ [ Learn from the Past: A Proxy based Adversarial Defense Framework to  Boost Robustness](https://arxiv.org/abs/2310.12713)

	Yaohua Liu, Jiaxin Gao, Zhu Liu, Xianghao Jiao, Xin Fan, Risheng Liu


+ [ PrivInfer: Privacy-Preserving Inference for Black-box Large Language  Model](https://arxiv.org/abs/2310.12214)

	Meng Tong, Kejiang Chen, Yuang Qi, Jie Zhang, Weiming Zhang, Nenghai Yu


+ [ Privacy Preserving Large Language Models: ChatGPT Case Study Based  Vision and Framework](https://arxiv.org/abs/2310.12523)

	Imdad Ullah, Najm Hassan, Sukhpal Singh Gill, Basem Suleiman, Tariq Ahamed Ahanger, Zawar Shah, Junaid Qadir, Salil S. Kanhere


+ [ SecurityNet: Assessing Machine Learning Vulnerabilities on Public Models](https://arxiv.org/abs/2310.12665)

	Boyang Zhang, Zheng Li, Ziqing Yang, Xinlei He, Michael Backes, Mario Fritz, Yang Zhang


## 2023-10-18
+ [ Adversarial Robustness Unhardening via Backdoor Attacks in Federated  Learning](https://arxiv.org/abs/2310.11594)

	Taejin Kim, Jiarui Li, Shubhranshu Singh, Nikhil Madaan, Carlee Joe-Wong


+ [ WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks  Against Deep Neural Networks](https://arxiv.org/abs/2310.11595)

	Jun Xia, Zhihao Yue, Yingbo Zhou, Zhiwei Ling, Xian Wei, Mingsong Chen


+ [ The Efficacy of Transformer-based Adversarial Attacks in Security  Domains](https://arxiv.org/abs/2310.11597)

	Kunyang Li, Kyle Domico, Jean-Charles Noirot Ferrand, Patrick McDaniel


+ [ Black-Box Training Data Identification in GANs via Detector Networks](https://arxiv.org/abs/2310.12063)

	Lukman Olagoke, Salil Vadhan, Seth Neel


+ [ A Cautionary Tale: On the Role of Reference Data in Empirical Privacy  Defenses](https://arxiv.org/abs/2310.12112)

	Caelin G. Kaplan, Chuan Xu, Othmane Marfoq, Giovanni Neglia, Anderson Santana de Oliveira


+ [ Domain-Generalized Face Anti-Spoofing with Unknown Attacks](https://arxiv.org/abs/2310.11758)

	Zong-Wei Hong, Yu-Chen Lin, Hsuan-Tung Liu, Yi-Ren Yeh, Chu-Song Chen


+ [ To Generate or Not? Safety-Driven Unlearned Diffusion Models Are Still  Easy To Generate Unsafe Images ... For Now](https://arxiv.org/abs/2310.11868)

	Yimeng Zhang, Jinghan Jia, Xin Chen, Aochuan Chen, Yihua Zhang, Jiancheng Liu, Ke Ding, Sijia Liu


+ [ Exploring Decision-based Black-box Attacks on Face Forgery Detection](https://arxiv.org/abs/2310.12017)

	Zhaoyu Chen, Bo Li, Kaixun Jiang, Shuang Wu, Shouhong Ding, Wenqiang Zhang


+ [ Revisiting Transferable Adversarial Image Examples: Attack  Categorization, Evaluation Guidelines, and New Insights](https://arxiv.org/abs/2310.11850)

	Zhengyu Zhao, Hanwei Zhang, Renjue Li, Ronan Sicre, Laurent Amsaleg, Michael Backes, Qi Li, Chao Shen


+ [ In defense of parameter sharing for model-compression](https://arxiv.org/abs/2310.11611)

	Aditya Desai, Anshumali Shrivastava


+ [ Adversarial Training for Physics-Informed Neural Networks](https://arxiv.org/abs/2310.11789)

	Yao Li, Shengzhu Shi, Zhichang Guo, Boying Wu


+ [ Quantifying Privacy Risks of Prompts in Visual Prompt Learning](https://arxiv.org/abs/2310.11970)

	Yixin Wu, Rui Wen, Michael Backes, Pascal Berrang, Mathias Humbert, Yun Shen, Yang Zhang



## 2023-10-17
+ [ Demystifying Poisoning Backdoor Attacks from a Statistical Perspective](https://arxiv.org/abs/2310.10780)

	Xun Xian, Ganghua Wang, Jayanth Srinivasa, Ashish Kundu, Xuan Bi, Mingyi Hong, Jie Ding


+ [ Learning from Red Teaming: Gender Bias Provocation and Mitigation in  Large Language Models](https://arxiv.org/abs/2310.11079)

	Hsuan Su, Cheng-Chu Cheng, Hua Farn, Shachi H Kumar, Saurav Sahay, Shang-Tse Chen, Hung-yi Lee


+ [ Quantifying Language Models' Sensitivity to Spurious Features in Prompt  Design or: How I learned to start worrying about prompt formatting](https://arxiv.org/abs/2310.11324)

	Melanie Sclar, Yejin Choi, Yulia Tsvetkov, Alane Suhr


+ [ Functional Invariants to Watermark Large Transformers](https://arxiv.org/abs/2310.11446)

	Fernandez Pierre, Couairon Guillaume, Furon Teddy, Douze Matthijs


+ [ Fake News in Sheep's Clothing: Robust Fake News Detection Against  LLM-Empowered Style Attacks](https://arxiv.org/abs/2310.10830)

	Jiaying Wu, Bryan Hooi


+ [ Survey of Vulnerabilities in Large Language Models Revealed by  Adversarial Attacks](https://arxiv.org/abs/2310.10844)

	Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, Nael Abu-Ghazaleh


+ [ Will the Prince Get True Love's Kiss? On the Model Sensitivity to Gender  Perturbation over Fairytale Texts](https://arxiv.org/abs/2310.10865)

	Christina Chance, Da Yin, Dakuo Wang, Kai-Wei Chang


+ [ Backdoor Attack through Machine Unlearning](https://arxiv.org/abs/2310.10659)

	Peixin Zhang, Jun Sun, Mingtian Tan, Xinyu Wang


+ [ Regularization properties of adversarially-trained linear regression](https://arxiv.org/abs/2310.10807)

	Antônio H. Ribeiro, Dave Zachariah, Francis Bach, Thomas B. Schön


+ [ Locally Differentially Private Graph Embedding](https://arxiv.org/abs/2310.11060)

	Zening Li, Rong-Hua Li, Meihao Liao, Fusheng Jin, Guoren Wang


+ [ Last One Standing: A Comparative Analysis of Security and Privacy of  Soft Prompt Tuning, LoRA, and In-Context Learning](https://arxiv.org/abs/2310.11397)

	Rui Wen, Tianhao Wang, Michael Backes, Yang Zhang, Ahmed Salem


+ [ Unbiased Watermark for Large Language Models](https://arxiv.org/abs/2310.10669)

	Zhengmian Hu, Lichang Chen, Xidong Wu, Yihan Wu, Hongyang Zhang, Heng Huang


## 2023-10-16
+ [ DANAA: Towards transferable attacks with double adversarial neuron  attribution](https://arxiv.org/abs/2310.10427)

	Zhibo Jin, Zhiyu Zhu, Xinyi Wang, Jiayu Zhang, Jun Shen, Huaming Chen


+ [ Privacy in Large Language Models: Attacks, Defenses and Future  Directions](https://arxiv.org/abs/2310.10383)

	Haoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi Hu, Chunkit Chan, Yangqiu Song


+ [ Prompt Packer: Deceiving LLMs through Compositional Instruction with  Hidden Attacks](https://arxiv.org/abs/2310.10077)

	Shuyu Jiang, Xingshu Chen, Rui Tang


+ [ ASSERT: Automated Safety Scenario Red Teaming for Evaluating the  Robustness of Large Language Models](https://arxiv.org/abs/2310.09624)

	Alex Mei, Sharon Levy, William Yang Wang


+ [ Orthogonal Uncertainty Representation of Data Manifold for Robust  Long-Tailed Learning](https://arxiv.org/abs/2310.10090)

	Yanbiao Ma, Licheng Jiao, Fang Liu, Shuyuan Yang, Xu Liu, Lingling Li


+ [ Quantifying Assistive Robustness Via the Natural-Adversarial Frontier](https://arxiv.org/abs/2310.10610)

	Jerry Zhi-Yang He, Zackory Erickson, Daniel S. Brown, Anca D. Dragan


## 2023-10-15
+ [ SCME: A Self-Contrastive Method for Data-free and Query-Limited Model  Extraction Attack](https://arxiv.org/abs/2310.09792)

	Renyang Liu, Jinhong Zhang, Kwok-Yan Lam, Jun Zhao, Wei Zhou


+ [ AFLOW: Developing Adversarial Examples under Extremely Noise-limited  Settings](https://arxiv.org/abs/2310.09795)

	Renyang Liu, Jinhong Zhang, Haoran Li, Jin Zhang, Yuanyu Wang, Wei Zhou


+ [ Black-box Targeted Adversarial Attack on Segment Anything (SAM)](https://arxiv.org/abs/2310.10010)

	Sheng Zheng, Chaoning Zhang


+ [ Explore the Effect of Data Selection on Poison Efficiency in Backdoor  Attacks](https://arxiv.org/abs/2310.09744)

	Ziqiang Li, Pengfei Xia, Hong Sun, Yueqi Zeng, Wei Zhang, Bin Li


+ [ Model Inversion Attacks on Homogeneous and Heterogeneous Graph Neural  Networks](https://arxiv.org/abs/2310.09800)

	Renyang Liu, Wei Zhou, Jinhong Zhang, Xiaoyuan Liu, Peiyuan Si, Haoran Li


+ [ Evaluating Robustness of Visual Representations for Object Assembly Task  Requiring Spatio-Geometrical Reasoning](https://arxiv.org/abs/2310.09943)

	Chahyon Ku, Carl Winge, Ryan Diaz, Wentao Yuan, Karthik Desingh


+ [ Is Certifying $\ell_p$ Robustness Still Worthwhile?](https://arxiv.org/abs/2310.09361)

	Ravi Mangal, Klas Leino, Zifan Wang, Kai Hu, Weicheng Yu, Corina Pasareanu, Anupam Datta, Matt Fredrikson


+ [ MAGIC: Detecting Advanced Persistent Threats via Masked Graph  Representation Learning](https://arxiv.org/abs/2310.09831)

	Zian Jia, Yun Xiong, Yuhong Nan, Yao Zhang, Jinjing Zhao, Mi Wen


+ [ A Comprehensive Study of Privacy Risks in Curriculum Learning](https://arxiv.org/abs/2310.10124)

	Joann Qiongna Chen, Xinlei He, Zheng Li, Yang Zhang, Zhou Li


+ [ Prime Match: A Privacy-Preserving Inventory Matching System](https://arxiv.org/abs/2310.09621)

	Antigoni Polychroniadou, Gilad Asharov, Benjamin Diamond, Tucker Balch, Hans Buehler, Richard Hua, Suwen Gu, Greg Gimler, Manuela Veloso


+ [ BufferSearch: Generating Black-Box Adversarial Texts With Lower Queries](https://arxiv.org/abs/2310.09652)

	Wenjie Lv, Zhen Wang, Yitao Zheng, Zhehua Zhong, Qi Xuan, Tianyi Chen


## 2023-10-14
+ [ Defending Our Privacy With Backdoors](https://arxiv.org/abs/2310.08320)

	Dominik Hintersdorf, Lukas Struppek, Daniel Neider, Kristian Kersting


+ [ Samples on Thin Ice: Re-Evaluating Adversarial Pruning of Neural  Networks](https://arxiv.org/abs/2310.08073)

	Giorgio Piras, Maura Pintor, Ambra Demontis, Battista Biggio


## 2023-10-13
+ [ Towards the Vulnerability of Watermarking Artificial Intelligence  Generated Content](https://arxiv.org/abs/2310.07726)

	Guanlin Li, Yifei Chen, Jie Zhang, Jiwei Li, Shangwei Guo, Tianwei Zhang


+ [ Sentinel: An Aggregation Function to Secure Decentralized Federated  Learning](https://arxiv.org/abs/2310.08097)

	Chao Feng, Alberto Huertas Celdran, Janosch Baltensperger, Enrique Tomas Matınez Bertran, Gerome Bovet, Burkhard Stiller


+ [ Trustworthy Machine Learning](https://arxiv.org/abs/2310.08215)

	Bálint Mucsányi, Michael Kirchhof, Elisa Nguyen, Alexander Rubinstein, Seong Joon Oh


+ [ Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders](https://arxiv.org/abs/2310.08571)

	Jan Dubiński, Stanisław Pawlak, Franziska Boenisch, Tomasz Trzciński, Adam Dziedzic

## 2023-10-12
+ [ Effects of Human Adversarial and Affable Samples on BERT  Generalizability](https://arxiv.org/abs/2310.08008)

	Aparna Elangovan, Jiayuan He, Yuan Li, Karin Verspoor


+ [ Concealed Electronic Countermeasures of Radar Signal with Adversarial  Examples](https://arxiv.org/abs/2310.08292)

	Ruinan Ma, Canjie Zhu, Mingfeng Lu, Yunjie Li, Yu-an Tan, Ruibin Zhang, Ran Tao


+ [ Jailbreaking Black Box Large Language Models in Twenty Queries](https://arxiv.org/abs/2310.08419)

	Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J. Pappas, Eric Wong


+ [ Towards Robust Multi-Modal Reasoning via Model Selection](https://arxiv.org/abs/2310.08446)

	Xiangyan Liu, Rongxue Li, Wei Ji, Tao Lin


+ [ Improving Fast Minimum-Norm Attacks with Hyperparameter Optimization](https://arxiv.org/abs/2310.08177)

	Giuseppe Floris, Raffaele Mura, Luca Scionis, Giorgio Piras, Maura Pintor, Ambra Demontis, Battista Biggio


+ [ Invisible Threats: Backdoor Attack in OCR Systems](https://arxiv.org/abs/2310.08259)

	Mauro Conti, Nicola Farronato, Stefanos Koffas, Luca Pajola, Stjepan Picek


+ [ Promoting Robustness of Randomized Smoothing: Two Cost-Effective  Approaches](https://arxiv.org/abs/2310.07780)

	Linbo Liu, Trong Nghia Hoang, Lam M. Nguyen, Tsui-Wei Weng


+ [ Towards Causal Deep Learning for Vulnerability Detection](https://arxiv.org/abs/2310.07958)

	Md Mahbubur Rahman, Ira Ceka, Chengzhi Mao, Saikat Chakraborty, Baishakhi Ray, Wei Le


## 2023-10-11
+ [ RobustEdge: Low Power Adversarial Detection for Cloud-Edge Systems](https://arxiv.org/abs/2310.06845)

	Abhishek Moitra, Abhiroop Bhattacharjee, Youngeun Kim, Priyadarshini Panda


+ [ Genetic Algorithm-Based Dynamic Backdoor Attack on Federated  Learning-Based Network Traffic Classification](https://arxiv.org/abs/2310.06855)

	Mahmoud Nazzal, Nura Aljaafari, Ahmed Sawalmeh, Abdallah Khreishah, Muhammad Anan, Abdulelah Algosaibi, Mohammed Alnaeem, Adel Aldalbahi, Abdulaziz Alhumam, Conrado P. Vizcarra, Shadan Alhamed


+ [ Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation](https://arxiv.org/abs/2310.06987)

	Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, Danqi Chen


+ [ No Privacy Left Outside: On the (In-)Security of TEE-Shielded DNN  Partition for On-Device ML](https://arxiv.org/abs/2310.07152)

	Ziqi Zhang, Chen Gong, Yifeng Cai, Yuanyuan Yuan, Bingyan Liu, Ding Li, Yao Guo, Xiangqun Chen


+ [ Boosting Black-box Attack to Deep Neural Networks with Conditional  Diffusion Models](https://arxiv.org/abs/2310.07492)

	Renyang Liu, Wei Zhou, Tianwei Zhang, Kangjie Chen, Jun Zhao, Kwok-Yan Lam


+ [ Composite Backdoor Attacks Against Large Language Models](https://arxiv.org/abs/2310.07676)

	Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang


+ [ Comparing the robustness of modern no-reference image- and video-quality  metrics to adversarial attacks](https://arxiv.org/abs/2310.06958)

	Anastasia Antsiferova, Khaled Abud, Aleksandr Gushchin, Sergey Lavrushkin, Ekaterina Shumitskaya, Maksim Velikanov, Dmitriy Vatolin


+ [ Robust Safe Reinforcement Learning under Adversarial Disturbances](https://arxiv.org/abs/2310.07207)

	Zeyang Li, Chuxiong Hu, Shengbo Eben Li, Jia Cheng, Yunan Wang


## 2023-10-10
+ [ Fingerprint Attack: Client De-Anonymization in Federated Learning](https://arxiv.org/abs/2310.05960)

	Qiongkai Xu, Trevor Cohn, Olga Ohrimenko


+ [ Suppressing Overestimation in Q-Learning through Adversarial Behaviors](https://arxiv.org/abs/2310.06286)

	HyeAnn Lee, Donghwan Lee


+ [ Jailbreak and Guard Aligned Language Models with Only Few In-Context  Demonstrations](https://arxiv.org/abs/2310.06387)

	Zeming Wei, Yifei Wang, Yisen Wang


+ [ Multilingual Jailbreak Challenges in Large Language Models](https://arxiv.org/abs/2310.06474)

	Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing


+ [ A Semantic Invariant Robust Watermark for Large Language Models](https://arxiv.org/abs/2310.06356)

	Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng, Lijie Wen


+ [ Adversarial Masked Image Inpainting for Robust Detection of Mpox and  Non-Mpox](https://arxiv.org/abs/2310.06318)

	Yubiao Yue, Zhenzhang Li


+ [ Leveraging Diffusion-Based Image Variations for Robust Training on  Poisoned Data](https://arxiv.org/abs/2310.06372)

	Lukas Struppek, Martin B. Hentschel, Clifton Poth, Dominik Hintersdorf, Kristian Kersting


+ [ Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield  but Also a Catalyst for Model Inversion Attacks](https://arxiv.org/abs/2310.06549)

	Lukas Struppek, Dominik Hintersdorf, Kristian Kersting


+ [ Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK  Approach](https://arxiv.org/abs/2310.06112)

	Shaopeng Fu, Di Wang


+ [ PAC-Bayesian Spectrally-Normalized Bounds for Adversarially Robust  Generalization](https://arxiv.org/abs/2310.06182)

	Jiancong Xiao, Ruoyu Sun, Zhi-quan Luo


+ [ Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach](https://arxiv.org/abs/2310.06396)

	Kai Zhao, Qiyu Kang, Yang Song, Rui She, Sijie Wang, Wee Peng Tay


+ [ Exploring adversarial attacks in federated learning for medical imaging](https://arxiv.org/abs/2310.06227)

	Erfan Darzi, Florian Dubost, N.M. Sijtsema, P.M.A van Ooijen


## 2023-10-09

+ [ The Troubling Emergence of Hallucination in Large Language Models -- An  Extensive Definition, Quantification, and Prescriptive Remediations](https://arxiv.org/abs/2310.04988)

	Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S.M Towhidul Islam Tonmoy, Aman Chadha, Amit P. Sheth, Amitava Das


+ [ Large Language Models Can Be Good Privacy Protection Learners](https://arxiv.org/abs/2310.02469)

	Yijia Xiao, Yiqiao Jin, Yushi Bai, Yue Wu, Xianjun Yang, Xiao Luo, Wenchao Yu, Xujiang Zhao, Yanchi Liu, Haifeng Chen, Wei Wang, Wei Cheng


+ [ LoFT: Local Proxy Fine-tuning For Improving Transferability Of  Adversarial Attacks Against Large Language Model](https://arxiv.org/abs/2310.04445)

	Muhammad Ahmed Shah, Roshan Sharma, Hira Dhamyal, Raphael Olivier, Ankit Shah, Dareen Alharthi, Hazim T Bukhari, Massa Baali, Soham Deshmukh, Michael Kuhlmann, Bhiksha Raj, Rita Singh


+ [ AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language  Models](https://arxiv.org/abs/2310.04451)

	Xiaogeng Liu, Nan Xu, Muhao Chen, Chaowei Xiao



+ [ Understanding and Improving Adversarial Attacks on Latent Diffusion  Model](https://arxiv.org/abs/2310.04687)

	Boyang Zheng, Chumeng Liang, Xiaoyu Wu, Yan Liu


+ [ Robustness-enhanced Uplift Modeling with Adversarial Feature  Desensitization](https://arxiv.org/abs/2310.04693)

	Zexu Sun, Bowei He, Ming Ma, Jiakai Tang, Yuchen Wang, Chen Ma, Dugang Liu



+ [ Better Safe than Sorry: Pre-training CLIP against Targeted Data  Poisoning and Backdoor Attacks](https://arxiv.org/abs/2310.05862)

	Wenhan Yang, Jingdong Gao, Baharan Mirzasoleiman


+ [ BRAINTEASER: Lateral Thinking Puzzles for Large Language Model](https://arxiv.org/abs/2310.05057)

	Yifan Jiang, Filip Ilievski, Kaixin Ma


+ [ Do Large Language Models Know about Facts?](https://arxiv.org/abs/2310.05177)

	Xuming Hu, Junzhe Chen, Xiaochuan Li, Yufei Guo, Lijie Wen, Philip S. Yu, Zhijiang Guo


+ [ SC-Safety: A Multi-round Open-ended Question Adversarial Safety  Benchmark for Large Language Models in Chinese](https://arxiv.org/abs/2310.05818)

	Liang Xu, Kangkang Zhao, Lei Zhu, Hang Xue


+ [ IPMix: Label-Preserving Data Augmentation Method for Training Robust  Classifiers](https://arxiv.org/abs/2310.04780)

	Zhenglin Huang, Xianan Bao, Na Zhang, Qingqi Zhang, Xiaomei Tu, Biao Wu, Xi Yang


+ [ VLAttack: Multimodal Adversarial Attacks on Vision-Language Tasks via  Pre-trained Models](https://arxiv.org/abs/2310.04655)

	Ziyi Yin, Muchao Ye, Tianrong Zhang, Tianyu Du, Jinguo Zhu, Han Liu, Jinghui Chen, Ting Wang, Fenglong Ma


+ [ GReAT: A Graph Regularized Adversarial Training Method](https://arxiv.org/abs/2310.05336)

	Samet Bayram, Kenneth Barner


+ [ Generating Less Certain Adversarial Examples Improves Robust  Generalization](https://arxiv.org/abs/2310.04539)

	Minxing Zhang, Michael Backes, Xiao Zhang


+ [ Protecting Sensitive Data through Federated Co-Training](https://arxiv.org/abs/2310.05696)

	Amr Abourayya, Jens Kleesiek, Kanishka Rao, Erman Ayday, Bharat Rao, Geoff Webb, Michael Kamp


+ [ Tight Certified Robustness via Min-Max Representations of ReLU Neural  Networks](https://arxiv.org/abs/2310.04916)

	Brendon G. Anderson, Samuel Pfrommer, Somayeh Sojoudi


## 2023-10-08
+ [ Lightweight Boosting Models for User Response Prediction Using  Adversarial Validation](https://arxiv.org/abs/2310.03778)

	Hyeonwoo Kim, Wonsung Lee


+ [ Assessing Robustness via Score-Based Adversarial Image Generation](https://arxiv.org/abs/2310.04285)

	Marcel Kollovieh, Lukas Gosch, Yan Scholten, Marten Lienen, Stephan Günnemann


+ [ Indirect Meltdown: Building Novel Side-Channel Attacks from  Transient-Execution Attacks](https://arxiv.org/abs/2310.04183)

	Daniel Weber, Fabian Thomas, Lukas Gerlach, Ruiyi Zhang, Michael Schwarz


## 2023-10-07
+ [ Benchmarking Local Robustness of High-Accuracy Binary Neural Networks  for Enhanced Traffic Sign Recognition](https://arxiv.org/abs/2310.03033)

	Andreea Postovan, Mădălina Eraşcu
	

+ [ Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models](https://arxiv.org/abs/2310.03123)

	Zihao Lin, Yan Sun, Yifan Shi, Xueqian Wang, Lifu Huang, Li Shen, Dacheng Tao


+ [ Fine-tuning Aligned Language Models Compromises Safety, Even When Users  Do Not Intend To!](https://arxiv.org/abs/2310.03693)

	Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, Peter Henderson


+ [ Ask for Alice: Online Covert Distress Signal in the Presence of a Strong  Adversary](https://arxiv.org/abs/2310.03237)

	Hayyu Imanda, Kasper Rasmussen


## 2023-10-06
+ [ Misusing Tools in Large Language Models With Visual Adversarial Examples](https://arxiv.org/abs/2310.03185)

	Xiaohan Fu, Zihan Wang, Shuheng Li, Rajesh K. Gupta, Niloofar Mireshghallah, Taylor Berg-Kirkpatrick, Earlence Fernandes


+ [ Robust Representation Learning via Asymmetric Negative Contrast and  Reverse Attention](https://arxiv.org/abs/2310.03358)

	Nuoyan Zhou, Decheng Liu, Dawei Zhou, Xinbo Gao, Nannan Wang


+ [ SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks](https://arxiv.org/abs/2310.03684)

	Alexander Robey, Eric Wong, Hamed Hassani, George J. Pappas


+ [ A Formalism and Approach for Improving Robustness of Large Language  Models Using Risk-Adjusted Confidence Scores](https://arxiv.org/abs/2310.03283)

	Ke Shen, Mayank Kejriwal


+ [ Shielding the Unseen: Privacy Protection through Poisoning NeRF with  Spatial Deformation](https://arxiv.org/abs/2310.03125)

	Yihan Wu, Brandon Y. Feng, Heng Huang


+ [ CSI: Enhancing the Robustness of 3D Point Cloud Recognition against  Corruption](https://arxiv.org/abs/2310.03360)

	Zhuoyuan Wu, Jiachen Sun, Chaowei Xiao


+ [ OMG-ATTACK: Self-Supervised On-Manifold Generation of Transferable  Evasion Attacks](https://arxiv.org/abs/2310.03707)

	Ofir Bar Tal, Adi Haviv, Amit H. Bermano


+ [ Untargeted White-box Adversarial Attack with Heuristic Defence Methods  in Real-time Deep Learning based Network Intrusion Detection System](https://arxiv.org/abs/2310.03334)

	Khushnaseeb Roshan, Aasim Zafar, Sheikh Burhan Ul Haque


+ [ Targeted Adversarial Attacks on Generalizable Neural Radiance Fields](https://arxiv.org/abs/2310.03578)

	Andras Horvath, Csaba M. Jozsa


+ [ Adversarial Machine Learning for Social Good: Reframing the Adversary as  an Ally](https://arxiv.org/abs/2310.03614)

	Shawqi Al-Maliki, Adnan Qayyum, Hassan Ali, Mohamed Abdallah, Junaid Qadir, Dinh Thai Hoang, Dusit Niyato, Ala Al-Fuqaha


+ [ Raze to the Ground: Query-Efficient Adversarial HTML Attacks on  Machine-Learning Phishing Webpage Detectors](https://arxiv.org/abs/2310.03166)

	Biagio Montaruli, Luca Demetrio, Maura Pintor, Luca Compagna, Davide Balzarotti, Battista Biggio


+ [ Regret Analysis of Distributed Online Control for LTI Systems with  Adversarial Disturbances](https://arxiv.org/abs/2310.03206)

	Ting-Jui Chang, Shahin Shahrampour


+ [ Certifiably Robust Graph Contrastive Learning](https://arxiv.org/abs/2310.03312)

	Minhua Lin, Teng Xiao, Enyan Dai, Xiang Zhang, Suhang Wang


+ [ An Integrated Algorithm for Robust and Imperceptible Audio Adversarial  Examples](https://arxiv.org/abs/2310.03349)

	Armin Ettenhofer, Jan-Philipp Schulze, Karla Pizzi


## 2023-10-05
+ [ Low-Resource Languages Jailbreak GPT-4](https://arxiv.org/abs/2310.02446)

	Zheng-Xin Yong, Cristina Menghini, Stephen H. Bach


+ [ Discovering General Reinforcement Learning Algorithms with Adversarial  Environment Design](https://arxiv.org/abs/2310.02782)

	Matthew Thomas Jackson, Minqi Jiang, Jack Parker-Holder, Risto Vuorio, Chris Lu, Gregory Farquhar, Shimon Whiteson, Jakob Nicolaus Foerster


+ [ Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models](https://arxiv.org/abs/2310.02949)

	Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, Dahua Lin


+ [ Can Large Language Models Provide Security & Privacy Advice? Measuring  the Ability of LLMs to Refute Misconceptions](https://arxiv.org/abs/2310.02431)

	Yufan Chen, Arjun Arunasalam, Z. Berkay Celik


+ [ SlowFormer: Universal Adversarial Patch for Attack on Compute and Energy  Efficiency of Inference Efficient Vision Transformers](https://arxiv.org/abs/2310.02544)

	KL Navaneet, Soroush Abbasi Koohpayegani, Essam Sleiman, Hamed Pirsiavash


+ [ Splitting the Difference on Adversarial Training](https://arxiv.org/abs/2310.02480)

	Matan Levi, Aryeh Kontorovich


+ [ A Recipe for Improved Certifiable Robustness: Capacity and Data](https://arxiv.org/abs/2310.02513)

	Kai Hu, Klas Leino, Zifan Wang, Matt Fredrikson


+ [ Jailbreaker in Jail: Moving Target Defense for Large Language Models](https://arxiv.org/abs/2310.02417)

	Bocheng Chen, Advait Paliwal, Qiben Yan


## 2023-10-04
+ [ Identifying and Mitigating Privacy Risks Stemming from Language Models:  A Survey](https://arxiv.org/abs/2310.01424)

	Victoria Smith, Ali Shahin Shamsabadi, Carolyn Ashurst, Adrian Weller


+ [ Fooling the Textual Fooler via Randomizing Latent Representations](https://arxiv.org/abs/2310.01452)

	Duy C. Hoang, Quang H. Nguyen, Saurav Manchanda, MinLong Peng, Kok-Seng Wong, Khoa D. Doan


+ [ On the Safety of Open-Sourced Large Language Models: Does Alignment  Really Prevent Them From Being Misused?](https://arxiv.org/abs/2310.01581)

	Hangfan Zhang, Zhimeng Guo, Huaisheng Zhu, Bochuan Cao, Lu Lin, Jinyuan Jia, Jinghui Chen, Dinghao Wu


+ [ Towards Stable Backdoor Purification through Feature Shift Tuning](https://arxiv.org/abs/2310.01875)

	Rui Min, Zeyu Qin, Li Shen, Minhao Cheng


+ [ Defending Against Authorship Identification Attacks](https://arxiv.org/abs/2310.01568)

	Haining Wang


+ [ Can Language Models be Instructed to Protect Personal Information?](https://arxiv.org/abs/2310.02224)

	Yang Chen, Ethan Mendes, Sauvik Das, Wei Xu, Alan Ritter


+ [ Adversarial Client Detection via Non-parametric Subspace Monitoring in  the Internet of Federated Things](https://arxiv.org/abs/2310.01537)

	Xianjian Xie, Xiaochen Xian, Dan Li, Andi Wang


+ [ Fool Your (Vision and) Language Model With Embarrassingly Simple  Permutations](https://arxiv.org/abs/2310.01651)

	Yongshuo Zong, Tingyang Yu, Bingchen Zhao, Ruchika Chavhan, Timothy Hospedales


+ [ Beyond Labeling Oracles: What does it mean to steal ML models?](https://arxiv.org/abs/2310.01959)

	Avital Shafran, Ilia Shumailov, Murat A. Erdogdu, Nicolas Papernot


+ [ FLEDGE: Ledger-based Federated Learning Resilient to Inference and  Backdoor Attacks](https://arxiv.org/abs/2310.02113)

	Jorge Castillo, Phillip Rieger, Hossein Fereidooni, Qian Chen, Ahmad Sadeghi


+ [ Waveform Manipulation Against DNN-based Modulation Classification  Attacks](https://arxiv.org/abs/2310.01894)

	Dimitrios Varkatzas, Antonios Argyriou


## 2023-10-03
+ [ Adversarial Driving Behavior Generation Incorporating Human Risk  Cognition for Autonomous Vehicle Evaluation](https://arxiv.org/abs/2310.00029)

	Zhen Liu, Hang Gao, Hao Ma, Shuo Cai, Yunfeng Hu, Ting Qu, Hong Chen, Xun Gong


+ [ Certified Robustness via Dynamic Margin Maximization and Improved  Lipschitz Regularization](https://arxiv.org/abs/2310.00116)

	Mahyar Fazlyab, Taha Entesari, Aniket Roy, Rama Chellappa


+ [ Beyond Random Noise: Insights on Anonymization Strategies from a Latent  Bandit Study](https://arxiv.org/abs/2310.00221)

	Alexander Galozy, Sadi Alawadi, Victor Kebande, Sławomir Nowaczyk


+ [ Understanding the Robustness of Randomized Feature Defense Against  Query-Based Adversarial Attacks](https://arxiv.org/abs/2310.00567)

	Quang H. Nguyen, Yingjie Lao, Tung Pham, Kok-Seng Wong, Khoa D. Doan


+ [ Faithful Explanations of Black-box NLP Models Using LLM-generated  Counterfactuals](https://arxiv.org/abs/2310.00603)

	Yair Gat, Nitay Calderon, Amir Feder, Alexander Chapanin, Amit Sharma, Roi Reichart


+ [ A Survey of Robustness and Safety of 2D and 3D Deep Learning Models  Against Adversarial Attacks](https://arxiv.org/abs/2310.00633)

	Yanjie Li, Bin Xie, Songtao Guo, Yuanyuan Yang, Bin Xiao



+ [ All Languages Matter: On the Multilingual Safety of Large Language  Models](https://arxiv.org/abs/2310.00905)

	Wenxuan Wang, Zhaopeng Tu, Chang Chen, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, Michael R. Lyu


+ [ Large Language Model-Powered Smart Contract Vulnerability Detection: New  Perspectives](https://arxiv.org/abs/2310.01152)

	Sihao Hu, Tiansheng Huang, Fatih İlhan, Selim Fukan Tekin, Ling Liu



+ [ Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language  Models](https://arxiv.org/abs/2310.00322)

	Chengdong Ma, Ziran Yang, Minquan Gao, Hai Ci, Jun Gao, Xuehai Pan, Yaodong Yang


+ [ Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2310.00648)

	Lauren Hong, Ting Wang


+ [ Robustness of AI-Image Detectors: Fundamental Limits and Practical  Attacks](https://arxiv.org/abs/2310.00076)

	Mehrdad Saberi, Vinu Sankar Sadasivan, Keivan Rezaei, Aounon Kumar, Atoosa Chegini, Wenxiao Wang, Soheil Feizi


+ [ Human-Producible Adversarial Examples](https://arxiv.org/abs/2310.00438)

	David Khachaturov, Yue Gao, Ilia Shumailov, Robert Mullins, Ross Anderson, Kassem Fawaz


+ [ Black-box Attacks on Image Activity Prediction and its Natural Language  Explanations](https://arxiv.org/abs/2310.00503)

	Alina Elena Baia, Valentina Poggioni, Andrea Cavallaro


+ [ GhostEncoder: Stealthy Backdoor Attacks with Dynamic Triggers to  Pre-trained Encoders in Self-supervised Learning](https://arxiv.org/abs/2310.00626)

	Qiannan Wang, Changchun Yin, Zhe Liu, Liming Fang, Run Wang, Chenhao Lin


+ [ Counterfactual Image Generation for adversarially robust and  interpretable Classifiers](https://arxiv.org/abs/2310.00761)

	Rafael Bischof, Florian Scheidegger, Michael A. Kraus, A. Cristiano I. Malossi


+ [ Practical Membership Inference Attacks Against Large-Scale Multi-Modal  Models: A Pilot Study](https://arxiv.org/abs/2310.00108)

	Myeongseob Ko, Ming Jin, Chenguang Wang, Ruoxi Jia


+ [ Understanding Adversarial Transferability in Federated Learning](https://arxiv.org/abs/2310.00616)

	Yijiang Li, Ying Gao, Haohan Wang


+ [ On the Onset of Robust Overfitting in Adversarial Training](https://arxiv.org/abs/2310.00607)

	Chaojian Yu, Xiaolong Shi, Jun Yu, Bo Han, Tongliang Liu


+ [ Balancing Efficiency vs. Effectiveness and Providing Missing Label  Robustness in Multi-Label Stream Classification](https://arxiv.org/abs/2310.00665)

	Sepehr Bakhshi, Fazli Can


+ [ Adversarial Explainability: Utilizing Explainable Machine Learning in  Bypassing IoT Botnet Detection Systems](https://arxiv.org/abs/2310.00070)

	Mohammed M. Alani, Atefeh Mashatan, Ali Miri


+ [ Source Inference Attacks: Beyond Membership Inference Attacks in  Federated Learning](https://arxiv.org/abs/2310.00222)

	Hongsheng Hu, Xuyun Zhang, Zoran Salcic, Lichao Sun, Kim-Kwang Raymond Choo, Gillian Dobbie


## 2023-10-02
+ [ AIR: Threats of Adversarial Attacks on Deep Learning-Based Information  Recovery](https://arxiv.org/abs/2309.16706)

	Jinyin Chen, Jie Ge, Shilian Zheng, Linhui Ye, Haibin Zheng, Weiguo Shen, Keqiang Yue, Xiaoniu Yang


+ [ General Lipschitz: Certified Robustness Against Resolvable Semantic  Transformations via Transformation-Dependent Randomized Smoothing](https://arxiv.org/abs/2309.16710)

	Dmitrii Korzh, Mikhail Pautov, Olga Tsymboi, Ivan Oseledets


+ [ Investigating Human-Identifiable Features Hidden in Adversarial  Perturbations](https://arxiv.org/abs/2309.16878)

	Dennis Y. Menn, Tzu-hsun Feng, Sriram Vishwanath, Hung-yi Lee


+ [ Medical Foundation Models are Susceptible to Targeted Misinformation  Attacks](https://arxiv.org/abs/2309.17007)

	Tianyu Han, Sven Nebelung, Firas Khader, Tianci Wang, Gustav Mueller-Franzes, Christiane Kuhl, Sebastian Försch, Jens Kleesiek, Christoph Haarburger, Keno K. Bressem, Jakob Nikolas Kather, Daniel Truhn



+ [ Adversarial Machine Learning in Latent Representations of Neural  Networks](https://arxiv.org/abs/2309.17401)

	Milin Zhang, Mohammad Abdi, Francesco Restuccia


+ [ Can Sensitive Information Be Deleted From LLMs? Objectives for Defending  Against Extraction Attacks](https://arxiv.org/abs/2309.17410)

	Vaidehi Patil, Peter Hase, Mohit Bansal


+ [ LatticeGen: A Cooperative Framework which Hides Generated Text in a  Lattice for Privacy-Aware Generation on Cloud](https://arxiv.org/abs/2309.17157)

	Mengke Zhang, Tianxing He, Tianle Wang, Fatemehsadat Mireshghallah, Binyi Chen, Hao Wang, Yulia Tsvetkov


+ [ Towards Robust Offline-to-Online Reinforcement Learning via Uncertainty  and Smoothness](https://arxiv.org/abs/2309.16973)

	Xiaoyu Wen, Xudong Yu, Rui Yang, Chenjia Bai, Zhen Wang


+ [ Efficient Biologically Plausible Adversarial Training](https://arxiv.org/abs/2309.17348)

	Matilde Tristany Farinha, Thomas Ortner, Giorgia Dellaferrera, Benjamin Grewe, Angeliki Pantazi


+ [ Adversarial Imitation Learning from Visual Observations using Latent  Information](https://arxiv.org/abs/2309.17371)

	Vittorio Giammarino, James Queeney, Ioannis Ch. Paschalidis


## 2023-10-01
+ [ Towards Efficient and Trustworthy AI Through  Hardware-Algorithm-Communication Co-Design](https://arxiv.org/abs/2309.15942)

	Bipin Rajendran, Osvaldo Simeone, Bashir M. Al-Hashimi


+ [ VDC: Versatile Data Cleanser for Detecting Dirty Samples via  Visual-Linguistic Inconsistency](https://arxiv.org/abs/2309.16211)

	Zihao Zhu, Mingda Zhang, Shaokui Wei, Bingzhe Wu, Baoyuan Wu


## 2023-09-30


+ [ Recent Advances of Differential Privacy in Centralized Deep Learning: A  Systematic Survey](https://arxiv.org/abs/2309.16398)

	Lea Demelius, Roman Kern, Andreas Trügler


+ [ Robust Offline Reinforcement Learning -- Certify the Confidence Interval](https://arxiv.org/abs/2309.16631)

	Jiarui Yao, Simon Shaolei Du


## 2023-09-29
+ [ Adversarial Examples Might be Avoidable: The Role of Data Concentration  in Adversarial Robustness](https://arxiv.org/abs/2309.16096)

	Ambar Pal, Jeremias Sulam, René Vidal


+ [ Parameter-Saving Adversarial Training: Reinforcing Multi-Perturbation  Robustness via Hypernetworks](https://arxiv.org/abs/2309.16207)

	Huihui Gong, Minjing Dong, Siqi Ma, Seyit Camtepe, Surya Nepal, Chang Xu


+ [ On the Trade-offs between Adversarial Robustness and Actionable  Explanations](https://arxiv.org/abs/2309.16452)

	Satyapriya Krishna, Chirag Agarwal, Himabindu Lakkaraju


+ [ Resisting Backdoor Attacks in Federated Learning via Bidirectional  Elections and Individual Perspective](https://arxiv.org/abs/2309.16456)

	Zhen Qin, Feiyi Chen, Chen Zhi, Xueqiang Yan, Shuiguang Deng


+ [ Towards Poisoning Fair Representations](https://arxiv.org/abs/2309.16487)

	Tianci Liu, Haoyu Wang, Feijie Wu, Hengtong Zhang, Pan Li, Lu Su, Jing Gao


+ [ Compilation as a Defense: Enhancing DL Model Attack Robustness via  Tensor Optimization](https://arxiv.org/abs/2309.16577)

	Stefan Trawicki, William Hackett, Lewis Birch, Neeraj Suri, Peter Garraghan


+ [ Cyber Sentinel: Exploring Conversational Agents in Streamlining Security  Tasks with GPT-4](https://arxiv.org/abs/2309.16422)

	Mehrdad Kaheh, Danial Khosh Kholgh, Panos Kostakos

## 2023-09-28

+ [ How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking  Unrelated Questions](https://arxiv.org/abs/2309.15840)

	Lorenzo Pacchiardi, Alex J. Chan, Sören Mindermann, Ilan Moscovitz, Alexa Y. Pan, Yarin Gal, Owain Evans, Jan Brauner


+ [ The Robust Semantic Segmentation UNCV2023 Challenge Results](https://arxiv.org/abs/2309.15478)

	Xuanlong Yu, Yi Zuo, Zitao Wang, Xiaowen Zhang, Jiaxuan Zhao, Yuting Yang, Licheng Jiao, Rui Peng, Xinyi Wang, Junpei Zhang, Kexin Zhang, Fang Liu, Roberto Alcover-Couso, Juan C. SanMiguel, Marcos Escudero-Viñolo, Hanlin Tian, Kenta Matsui, Tianhao Wang, Fahmy Adan, Zhitong Gao, Xuming He, Quentin Bouniot, Hossein Moghaddam, Shyam Nandan Rai, Fabio Cermelli, Carlo Masone, Andrea Pilzer, Elisa Ricci, Andrei Bursuc, Arno Solin, Martin Trapp, Rui Li, Angela Yao, Wenlong Chen, Ivor Simpson, Neill D. F. Campbell, Gianni Franchi


+ [ A Unified View of Differentially Private Deep Generative Modeling](https://arxiv.org/abs/2309.15696)

	Dingfan Chen, Raouf Kerkouche, Mario Fritz


+ [ On Computational Entanglement and Its Interpretation in Adversarial  Machine Learning](https://arxiv.org/abs/2309.15669)

	YenLung Lai, Xingbo Dong, Zhe Jin


+ [ Automatic Feature Fairness in Recommendation via Adversaries](https://arxiv.org/abs/2309.15418)

	Hengchang Hu, Yiming Cao, Zhankui He, Samson Tan, Min-Yen Kan


## 2023-09-27
+ [ Bias Assessment and Mitigation in LLM-based Code Generation](https://arxiv.org/abs/2309.14345)

	Dong Huang, Qingwen Bu, Jie Zhang, Xiaofei Xie, Junjie Chen, Heming Cui


+ [ Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM](https://arxiv.org/abs/2309.14348)

	Bochuan Cao, Yuanpu Cao, Lu Lin, Jinghui Chen


+ [ Survey of Social Bias in Vision-Language Models](https://arxiv.org/abs/2309.14381)

	Nayeon Lee, Yejin Bang, Holy Lovenia, Samuel Cahyawijaya, Wenliang Dai, Pascale Fung


+ [ XGV-BERT: Leveraging Contextualized Language Model and Graph Neural  Network for Efficient Software Vulnerability Detection](https://arxiv.org/abs/2309.14677)

	Vu Le Anh Quan, Chau Thuan Phat, Kiet Van Nguyen, Phan The Duy, Van-Hau Pham


+ [ DifAttack: Query-Efficient Black-Box Attack via Disentangled Feature  Space](https://arxiv.org/abs/2309.14585)

	Liu Jun, Zhou Jiantao, Zeng Jiandian, Jinyu Tian


+ [ Structure Invariant Transformation for better Adversarial  Transferability](https://arxiv.org/abs/2309.14700)

	Xiaosen Wang, Zeliang Zhang, Jianping Zhang


+ [ Frugal Satellite Image Change Detection with Deep-Net Inversion](https://arxiv.org/abs/2309.14781)

	Hichem Sahbi, Sebastien Deschamps


+ [ The Surveillance AI Pipeline](https://arxiv.org/abs/2309.15084)

	Pratyusha Ria Kalluri, William Agnew, Myra Cheng, Kentrell Owens, Luca Soldaini, Abeba Birhane


+ [ Unveiling Fairness Biases in Deep Learning-Based Brain MRI  Reconstruction](https://arxiv.org/abs/2309.14392)

	Yuning Du, Yuyang Xue, Rohan Dharmakumar, Sotirios A. Tsaftaris


+ [ LogGPT: Log Anomaly Detection via GPT](https://arxiv.org/abs/2309.14482)

	Xiao Han, Shuhan Yuan, Mohamed Trabelsi


+ [ Privacy-preserving and Privacy-attacking Approaches for Speech and Audio  -- A Survey](https://arxiv.org/abs/2309.15087)

	Yuchen Liu, Apu Kapadia, Donald Williamson

## 2023-09-26
+ [ Investigating Efficient Deep Learning Architectures For Side-Channel  Attacks on AES](https://arxiv.org/abs/2309.13170)

	Yohaï-Eliel Berreby, Laurent Sauvage


+ [ Towards Green AI in Fine-tuning Large Language Models via Adaptive  Backpropagation](https://arxiv.org/abs/2309.13192)

	Kai Huang, Hanyun Yin, Heng Huang, Wei Gao


+ [ Defending Pre-trained Language Models as Few-shot Learners against  Backdoor Attacks](https://arxiv.org/abs/2309.13256)

	Zhaohan Xi, Tianyu Du, Changjiang Li, Ren Pang, Shouling Ji, Jinghui Chen, Fenglong Ma, Ting Wang


+ [ LLMs as Counterfactual Explanation Modules: Can ChatGPT Explain  Black-box Text Classifiers?](https://arxiv.org/abs/2309.13340)

	Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, Huan Liu


+ [ Seeing Is Not Always Believing: Invisible Collision Attack and Defence  on Pre-Trained Models](https://arxiv.org/abs/2309.13579)

	Minghang Deng, Zhong Zhang, Junming Shao


+ [ PRIS: Practical robust invertible network for image steganography](https://arxiv.org/abs/2309.13620)

	Hang Yang, Yitian Xu, Xuhua Liu, Xiaodong Ma


+ [ GHN-QAT: Training Graph Hypernetworks to Predict Quantization-Robust  Parameters of Unseen Limited Precision Neural Networks](https://arxiv.org/abs/2309.13773)

	Stone Yun, Alexander Wong


+ [ Can LLM-Generated Misinformation Be Detected?](https://arxiv.org/abs/2309.13788)

	Canyu Chen, Kai Shu


+ [ RBFormer: Improve Adversarial Robustness of Transformer by Robust Bias](https://arxiv.org/abs/2309.13245)

	Hao Cheng, Jinhao Duan, Hui Li, Lyutianyang Zhang, Jiahang Cao, Ping Wang, Jize Zhang, Kaidi Xu, Renjing Xu


+ [ DFRD: Data-Free Robustness Distillation for Heterogeneous Federated  Learning](https://arxiv.org/abs/2309.13546)

	Kangyang Luo, Shuai Wang, Yexuan Fu, Xiang Li, Yunshi Lan, Ming Gao


+ [ Vulnerabilities in Video Quality Assessment Models: The Challenge of  Adversarial Attacks](https://arxiv.org/abs/2309.13609)

	Ao-Xiang Zhang, Yu Ran, Weixuan Tang, Yuan-Gen Wang


+ [ Video Adverse-Weather-Component Suppression Network via Weather  Messenger and Adversarial Backpropagation](https://arxiv.org/abs/2309.13700)

	Yijun Yang, Angelica I. Aviles-Rivero, Huazhu Fu, Ye Liu, Weiming Wang, Lei Zhu


+ [ Combining Two Adversarial Attacks Against Person Re-Identification  Systems](https://arxiv.org/abs/2309.13763)

	Eduardo de O. Andrade, Igor Garcia Ballhausen Sampaio, Joris Guérin, José Viterbo


+ [ Adversarial Attacks on Video Object Segmentation with Hard Region  Discovery](https://arxiv.org/abs/2309.13857)

	Ping Li, Yu Zhang, Li Yuan, Jian Zhao, Xianghua Xu, Xiaoqin Zhang


+ [ SurrogatePrompt: Bypassing the Safety Filter of Text-To-Image Models via  Substitution](https://arxiv.org/abs/2309.14122)

	Zhongjie Ba, Jieming Zhong, Jiachen Lei, Peng Cheng, Qinglong Wang, Zhan Qin, Zhibo Wang, Kui Ren



+ [ Spatial-frequency channels, shape bias, and adversarial robustness](https://arxiv.org/abs/2309.13190)

	Ajay Subramanian, Elena Sizikova, Najib J. Majaj, Denis G. Pelli


+ [ Beyond Fairness: Age-Harmless Parkinson's Detection via Voice](https://arxiv.org/abs/2309.13292)

	Yicheng Wang, Xiaotian Han, Leisheng Yu, Na Zou


+ [ Improving Robustness of Deep Convolutional Neural Networks via  Multiresolution Learning](https://arxiv.org/abs/2309.13752)

	Hongyan Zhou, Yao Liang


+ [ Invisible Watermarking for Audio Generation Diffusion Models](https://arxiv.org/abs/2309.13166)

	Xirong Cao, Xiang Li, Divyesh Jadav, Yanzhao Wu, Zhehui Chen, Chen Zeng, Wenqi Wei


+ [ On the Effectiveness of Adversarial Samples against Ensemble  Learning-based Windows PE Malware Detectors](https://arxiv.org/abs/2309.13841)

	Trong-Nghia To, Danh Le Kim, Do Thi Thu Hien, Nghi Hoang Khoa, Hien Do Hoang, Phan The Duy, Van-Hau Pham


## 2023-09-25
+ [ Provably Robust and Plausible Counterfactual Explanations for Neural  Networks via Robust Optimisation](https://arxiv.org/abs/2309.12545)

	Junqi Jiang, Jianglin Lan, Francesco Leofante, Antonio Rago, Francesca Toni


+ [ HANS, are you clever? Clever Hans Effect Analysis of Neural Systems](https://arxiv.org/abs/2309.12481)

	Leonardo Ranaldi, Fabio Massimo Zanzotto


+ [ Privacy Assessment on Reconstructed Images: Are Existing Evaluation  Metrics Faithful to Human Perception?](https://arxiv.org/abs/2309.13038)

	Xiaoxiao Sun, Nidham Gazagnadou, Vivek Sharma, Lingjuan Lyu, Hongdong Li, Liang Zheng


+ [ Improving Machine Learning Robustness via Adversarial Training](https://arxiv.org/abs/2309.12593)

	Long Dang, Thushari Hapuarachchi, Kaiqi Xiong, Jing Lin


+ [ On Data Fabrication in Collaborative Vehicular Perception: Attacks and  Countermeasures](https://arxiv.org/abs/2309.12955)

	Qingzhao Zhang, Shuowei Jin, Jiachen Sun, Xumiao Zhang, Ruiyang Zhu, Qi Alfred Chen, Z. Morley Mao


+ [ Robotic Handling of Compliant Food Objects by Robust Learning from  Demonstration](https://arxiv.org/abs/2309.12856)

	Ekrem Misimi, Alexander Olofsson, Aleksander Eilertsen, Elling Ruud Øye, John Reidar Mathiassen


## 2023-09-24
+ [ Distilling Adversarial Prompts from Safety Benchmarks: Report for the  Adversarial Nibbler Challenge](https://arxiv.org/abs/2309.11575)

	Manuel Brack, Patrick Schramowski, Kristian Kersting


+ [ The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A"](https://arxiv.org/abs/2309.12288)

	Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, Owain Evans


## 2023-09-23
+ [ Bad Actor, Good Advisor: Exploring the Role of Large Language Models in  Fake News Detection](https://arxiv.org/abs/2309.12247)

	Beizhe Hu, Qiang Sheng, Juan Cao, Yuhui Shi, Yang Li, Danding Wang, Peng Qi


+ [ A Chinese Prompt Attack Dataset for LLMs with Evil Content](https://arxiv.org/abs/2309.11830)

	Chengyuan Liu, Fubang Zhao, Lizhi Qing, Yangyang Kang, Changlong Sun, Kun Kuang, Fei Wu


+ [ Vulnerability of 3D Face Recognition Systems to Morphing Attacks](https://arxiv.org/abs/2309.12118)

	Sanjeet Vardam, Luuk Spreeuwers


+ [ Towards Differential Privacy in Sequential Recommendation: A Noisy Graph  Neural Network Approach](https://arxiv.org/abs/2309.11515)

	Wentao Hu, Hui Fang


## 2023-09-22
+ [ CATS: Conditional Adversarial Trajectory Synthesis for  Privacy-Preserving Trajectory Data Publication Using Deep Learning Approaches](https://arxiv.org/abs/2309.11587)

	Jinmeng Rao, Song Gao, Sijia Zhu


+ [ How Robust is Google's Bard to Adversarial Image Attacks?](https://arxiv.org/abs/2309.11751)

	Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang, Yichi Zhang, Yu Tian, Hang Su, Jun Zhu


+ [ Knowledge Sanitization of Large Language Models](https://arxiv.org/abs/2309.11852)

	Yoichi Ishibashi, Hidetoshi Shimodaira


+ [ On the Relationship between Skill Neurons and Robustness in Prompt  Tuning](https://arxiv.org/abs/2309.12263)

	Leon Ackermann, Xenia Ohmer


+ [ TextCLIP: Text-Guided Face Image Generation And Manipulation Without  Adversarial Training](https://arxiv.org/abs/2309.11923)

	Xiaozhou You, Jian Zhang


+ [ Dictionary Attack on IMU-based Gait Authentication](https://arxiv.org/abs/2309.11766)

	Rajesh Kumar, Can Isik, Chilukuri K. Mohan
  

+ [ Privacy-Preserving In-Context Learning with Differentially Private  Few-Shot Generation](https://arxiv.org/abs/2309.11765)

	Xinyu Tang, Richard Shin, Huseyin A. Inan, Andre Manoel, Fatemehsadat Mireshghallah, Zinan Lin, Sivakanth Gopi, Janardhan Kulkarni, Robert Sim


+ [ MarkNerf:Watermarking for Neural Radiance Field](https://arxiv.org/abs/2309.11747)

	Lifeng Chen, Jia Liu, Yan Ke, Wenquan Sun, Weina Dong, Xiaozhong Pan


+ [ DeepTheft: Stealing DNN Model Architectures through Power Side Channel](https://arxiv.org/abs/2309.11894)

	Yansong Gao, Huming Qiu, Zhi Zhang, Binghui Wang, Hua Ma, Alsharif Abuadbba, Minhui Xue, Anmin Fu, Surya Nepal


## 2023-09-21
+ [ When to Trust AI: Advances and Challenges for Certification of Neural  Networks](https://arxiv.org/abs/2309.11196)

	Marta Kwiatkowska, Xiyue Zhang


+ [ C$\cdot$ASE: Learning Conditional Adversarial Skill Embeddings for  Physics-based Characters](https://arxiv.org/abs/2309.11351)

	Zhiyang Dou, Xuelin Chen, Qingnan Fan, Taku Komura, Wenping Wang


+ [ What Learned Representations and Influence Functions Can Tell Us About  Adversarial Examples](https://arxiv.org/abs/2309.10916)

	Shakila Mahjabin Tonni, Mark Dras


+ [ PRAT: PRofiling Adversarial aTtacks](https://arxiv.org/abs/2309.11111)

	Rahul Ambati, Naveed Akhtar, Ajmal Mian, Yogesh Singh Rawat


+ [ It's Simplex! Disaggregating Measures to Improve Certified Robustness](https://arxiv.org/abs/2309.11005)

	Andrew C. Cullen, Paul Montague, Shijie Liu, Sarah M. Erfani, Benjamin I.P. Rubinstein


+ [ Learning Patient Static Information from Time-series EHR and an Approach  for Safeguarding Privacy and Fairness](https://arxiv.org/abs/2309.11373)

	Wei Liao, Joel Voldman


+ [ Fed-LSAE: Thwarting Poisoning Attacks against Federated Cyber Threat  Detection System via Autoencoder-based Latent Space Inspection](https://arxiv.org/abs/2309.11053)

	Tran Duc Luong, Vuong Minh Tien, Nguyen Huu Quyen, Do Thi Thu Hien, Phan The Duy, Van-Hau Pham


## 2023-09-20
+ [ GPTFUZZER : Red Teaming Large Language Models with Auto-Generated  Jailbreak Prompts](https://arxiv.org/abs/2309.10253) 

  Jiahao Yu, Xingwei Lin, Xinyu Xing


+ [ Exploring the Dark Side of AI: Advanced Phishing Attack Design and  Deployment Using ChatGPT](https://arxiv.org/abs/2309.10463) 

  Nils Begou, Jeremy Vinoy, Andrzej Duda, Maciej Korczynski


+ [ Transferable Adversarial Attack on Image Tampering Localization](https://arxiv.org/abs/2309.10243)

  Yuqi Wang, Gang Cao, Zijie Lou, Haochen Zhu


+ [ RECALL+: Adversarial Web-based Replay for Continual Learning in Semantic  Segmentation](https://arxiv.org/abs/2309.10479)

  Chang Liu, Giulia Rizzoli, Francesco Barbato, Umberto Michieli, Yi Niu, Pietro Zanuttigh


+ [ Realistic Website Fingerprinting By Augmenting Network Trace](https://arxiv.org/abs/2309.10147)

  Alireza Bahramali, Ardavan Bozorgi, Amir Houmansadr


+ [ Love or Hate? Share or Split? Privacy-Preserving Training Using Split  Learning and Homomorphic Encryption](https://arxiv.org/abs/2309.10517) 

  Tanveer Khan, Khoa Nguyen, Antonis Michalas, Alexandros Bakas


+ [ Disentangled Information Bottleneck guided Privacy-Protective JSCC for  Image Transmission](https://arxiv.org/abs/2309.10263)

  Lunan Sun, Yang Yang, Mingzhe Chen, Caili Guo


+ [ SPFL: A Self-purified Federated Learning Method Against Poisoning  Attacks](https://arxiv.org/abs/2309.10607) 

  Zizhen Liu, Weiyang He, Chip-Hong Chang, Jing Ye, Huawei Li, Xiaowei Li


## 2023-09-19
+ [ Plug in the Safety Chip: Enforcing Constraints for LLM-driven Robot Agents](https://arxiv.org/abs/2309.09919)

  Ziyi Yang, Shreyas S. Raman, Ankit Shah, Stefanie Tellex


+ [ Efficient Avoidance of Vulnerabilities in Auto-completed Smart Contract  Code Using Vulnerability-constrained Decoding](https://arxiv.org/abs/2309.09826)

  André Storhaug, Jingyue Li, Tianyuan Hu


+ [ Bias of AI-Generated Content: An Examination of News Produced by Large  Language Models](https://arxiv.org/abs/2309.09825)

  Xiao Fang, Shangkun Che, Minjia Mao, Hongzhe Zhang, Ming Zhao, Xiaohang Zhao


+ [ Reducing Adversarial Training Cost with Gradient Approximation](https://arxiv.org/abs/2309.09464)

  Huihui Gong, Shuo Yang, Siqi Ma, Seyit Camtepe, Surya Nepal, Chang Xu


+ [ Stealthy Physical Masked Face Recognition Attack via Adversarial Style  Optimization](https://arxiv.org/abs/2309.09480)

  Huihui Gong, Minjing Dong, Siqi Ma, Seyit Camtepe, Surya Nepal, Chang Xu


+ [ Evaluating Adversarial Robustness with Expected Viable Performance](https://arxiv.org/abs/2309.09928)

  Ryan McCoppin, Colin Dawson, Sean M. Kennedy, Leslie M. Blaha

## 2023-09-17

+ [ Convex Latent-Optimized Adversarial Regularizers for Imaging Inverse  Problems](https://arxiv.org/abs/2309.09250)

  Huayu Wang, Chen Luo, Taofeng Xie, Qiyu Jin, Guoqing Chen, Zhuo-Xu Cui, Dong Liang

## 2023-09-16


+ [ Robust Backdoor Attacks on Object Detection in Real World](https://arxiv.org/abs/2309.08953)

  Yaguan Qian, Boyuan Ji, Shuke He, Shenhui Huang, Xiang Ling, Bin Wang, Wei Wang


+ [ Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and  Nationality Bias in Generative Models](https://arxiv.org/abs/2309.08902)

  Mahammed Kamruzzaman, Md. Minul Islam Shovon, Gene Louis Kim
 

+ [ Context-aware Adversarial Attack on Named Entity Recognition](https://arxiv.org/abs/2309.08999)

  Shuguang Chen, Leonardo Neves, Thamar Solorio
 
## 2023-09-15

+ [ Adversarial Attacks on Tables with Entity Swap](https://arxiv.org/abs/2309.08650)

  Aneta Koleva, Martin Ringsquandl, Volker Tresp


+ [ A More Secure Split: Enhancing the Security of Privacy-Preserving Split  Learning](https://arxiv.org/abs/2309.08697)

  Tanveer Khan, Khoa Nguyen, Antonis Michalas

  
+ [ Detecting Unknown Attacks in IoT Environments: An Open Set Classifier  for Enhanced Network Intrusion Detection](https://arxiv.org/abs/2309.07461)

  Yasir Ali Farrukh, Syed Wali, Irfan Khan, Nathaniel D. Bastian


+ [ Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated  Text](https://arxiv.org/abs/2309.07689)

  Mahdi Dhaini, Wessel Poelman, Ege Erdogan


+ [ Keep your Identity Small: Privacy-preserving Client-side Fingerprinting](https://arxiv.org/abs/2309.07563)

  Alberto Fernandez-de-Retana, Igor Santos-Grueiro


+ [ Fake News Detectors are Biased against Texts Generated by Large Language  Models](https://arxiv.org/abs/2309.08674)

  Jinyan Su, Terry Yue Zhuo, Jonibek Mansurov, Di Wang, Preslav Nakov
