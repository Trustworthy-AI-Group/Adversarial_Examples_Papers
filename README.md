
# A complete list of papers about adversarial examples

It appears that the [List of All Adversarial Example Papers](https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html) has been experiencing crashes over the past few days. In the absence of this valuable resource, staying up-to-date with the latest research papers in this field has become challenging. Consequently, I created a repository aimed at aggregating and maintaining the most current papers in this domain. While this repository may not encompass every paper, I did try. If you find any papers we have missed, just drop me an [email](mailto:xswanghuster@gmail.com). We have included the [data](./nicholas.md) from [List of All Adversarial Example Papers](https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html) till 2023-09-01. We also provide a list of papers about transfer-based attacks [here](https://xiaosenwang.com/transfer_based_attack_papers.html).

# 2024-04-15
+ [ Privacy at a Price: Exploring its Dual Impact on AI Fairness](https://arxiv.org//abs/2404.09391)

	Mengmeng Yang, Ming Ding, Youyang Qu, Wei Ni, David Smith, Thierry Rakotoarivelo


+ [ Watermark-embedded Adversarial Examples for Copyright Protection against  Diffusion Models](https://arxiv.org//abs/2404.09401)

	Peifei Zhu, Tsubasa Takahashi, Hirokatsu Kataoka


+ [ Improving Weakly-Supervised Object Localization Using Adversarial  Erasing and Pseudo Label](https://arxiv.org//abs/2404.09475)

	Byeongkeun Kang, Sinhae Cha, Yeejin Lee


+ [ Beyond Noise: Privacy-Preserving Decentralized Learning with Virtual  Nodes](https://arxiv.org//abs/2404.09536)

	Sayan Biswas, Mathieu Even, Anne-Marie Kermarrec, Laurent Massoulie, Rafael Pires, Rishi Sharma, Martijn de Vos


+ [ Privacy-Preserving Intrusion Detection using Convolutional Neural  Networks](https://arxiv.org//abs/2404.09625)

	Martin Kodys, Zhongmin Dai, Vrizlynn L. L. Thing


+ [ Mitigating the Curse of Dimensionality for Certified Robustness via Dual  Randomized Smoothing](https://arxiv.org//abs/2404.09586)

	Song Xia, Yu Yi, Xudong Jiang, Henghui Ding


+ [ Ti-Patch: Tiled Physical Adversarial Patch for no-reference video  quality metrics](https://arxiv.org//abs/2404.09961)

	Victoria Leonenkova, Ekaterina Shumitskaya, Anastasia Antsiferova, Dmitriy Vatolin


+ [ On the Efficiency of Privacy Attacks in Federated Learning](https://arxiv.org//abs/2404.09430)

	Nawrin Tabassum, Ka-Ho Chow, Xuyu Wang, Wenbin Zhang, Yanzhao Wu


+ [ Privacy-Preserving Federated Unlearning with Certified Client Removal](https://arxiv.org//abs/2404.09724)

	Ziyao Liu, Huanyi Ye, Yu Jiang, Jiyuan Shen, Jiale Guo, Ivan Tjuawinata, Kwok-Yan Lam




# 2024-04-14
+ [ Make Split, not Hijack: Preventing Feature-Space Hijacking Attacks in  Split Learning](https://arxiv.org//abs/2404.09265)

	Tanveer Khan, Mindaugas Budzys, Antonis Michalas


+ [ FaceCat: Enhancing Face Recognition Security with a Unified Generative  Model Framework](https://arxiv.org//abs/2404.09193)

	Jiawei Chen, Xiao Yang, Yinpeng Dong, Hang Su, Jianteng Peng, Zhaoxia Yin


+ [ Adversarial Robustness Limits via Scaling-Law and Human-Alignment  Studies](https://arxiv.org//abs/2404.09349)

	Brian R. Bartoldson, James Diffenderfer, Konstantinos Parasyris, Bhavya Kailkhura


# 2024-04-13
+ [ Proof-of-Learning with Incentive Security](https://arxiv.org//abs/2404.09005)

	Zishuo Zhao, Zhixuan Fang, Xuechao Wang, Yuan Zhou


+ [ CodeCloak: A Method for Evaluating and Mitigating Code Leakage by LLM  Code Assistants](https://arxiv.org//abs/2404.09066)

	Amit Finkman, Eden Bar-Kochva, Avishag Shapira, Dudu Mimran, Yuval Elovici, Asaf Shabtai


+ [ Stability and Generalization in Free Adversarial Training](https://arxiv.org//abs/2404.08980)

	Xiwei Cheng, Kexin Fu, Farzan Farnia



# 2024-04-12
+ [ A Survey of Neural Network Robustness Assessment in Image Recognition](https://arxiv.org//abs/2404.08285)

	Jie Wang, Jun Ai, Minyan Lu, Haoran Su, Dan Yu, Yutao Zhang, Junda Zhu, Jingyu Liu


+ [ Adversarial Imitation Learning via Boosting](https://arxiv.org//abs/2404.08513)

	Jonathan D. Chang, Dhruv Sreenivas, Yingbing Huang, Kianté Brantley, Wen Sun


+ [ VertAttack: Taking advantage of Text Classifiers' horizontal vision](https://arxiv.org//abs/2404.08538)

	Jonathan Rusert


+ [ Practical Region-level Attack against Segment Anything Models](https://arxiv.org//abs/2404.08255)

	Yifan Shen, Zhengyuan Li, Gang Wang


+ [ Struggle with Adversarial Defense? Try Diffusion](https://arxiv.org//abs/2404.08273)

	Yujie Li, Yanbin Wang, Haitao xu, Bin Liu, Jianguo Sun, Zhenhao Guo, Wenrui Ma


+ [ Counterfactual Explanations for Face Forgery Detection via Adversarial  Removal of Artifacts](https://arxiv.org//abs/2404.08341)

	Yang Li, Songlin Yang, Wei Wang, Ziwen He, Bo Peng, Jing Dong


+ [ Joint Physical-Digital Facial Attack Detection Via Simulating Spoofing  Clues](https://arxiv.org//abs/2404.08450)

	Xianhua He, Dashuang Liang, Song Yang, Zhanlong Hao, Hui Ma, Binjie Mao, Xi Li, Yao Wang, Pengfei Yan, Ajian Liu


+ [ On the Robustness of Language Guidance for Low-Level Vision Tasks:  Findings from Depth Estimation](https://arxiv.org//abs/2404.08540)

	Agneet Chatterjee, Tejas Gokhale, Chitta Baral, Yezhou Yang


+ [ Anti-Byzantine Attacks Enabled Vehicle Selection for Asynchronous  Federated Learning in Vehicular Edge Computing](https://arxiv.org//abs/2404.08444)

	Cui Zhang, Xiao Xu, Qiong Wu, Pingyi Fan, Qiang Fan, Huiling Zhu, Jiangzhou Wang


+ [ FCert: Certifiably Robust Few-Shot Classification in the Era of  Foundation Models](https://arxiv.org//abs/2404.08631)

	Yanting Wang, Wei Zou, Jinyuan Jia


+ [ LazyDP: Co-Designing Algorithm-Software for Scalable Training of  Differentially Private Recommendation Models](https://arxiv.org//abs/2404.08847)

	Juntaek Lim, Youngeun Kwon, Ranggi Hwang, Kiwan Maeng, G. Edward Suh, Minsoo Rhu


# 2024-04-11
+ [ Differentially Private GANs for Generating Synthetic Indoor Location  Data](https://arxiv.org//abs/2404.07366)

	Vahideh Moghtadaiee, Mina Alishahi, Milad Rabiei


+ [ Differentially Private Reinforcement Learning with Self-Play](https://arxiv.org//abs/2404.07559)

	Dan Qiao, Yu-Xiang Wang


+ [ Fragile Model Watermark for integrity protection: leveraging boundary  volatility and sensitive sample-pairing](https://arxiv.org//abs/2404.07572)

	ZhenZhe Gao, Zhenjun Tang, Zhaoxia Yin, Baoyuan Wu, Yue Lu


+ [ AmpleGCG: Learning a Universal and Transferable Generative Model of  Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs](https://arxiv.org//abs/2404.07921)

	Zeyi Liao, Huan Sun


+ [ Privacy preserving layer partitioning for Deep Neural Network models](https://arxiv.org//abs/2404.07437)

	Kishore Rajasekar, Randolph Loh, Kar Wai Fok, Vrizlynn L. L. Thing


+ [ Enhancing Network Intrusion Detection Performance using Generative  Adversarial Networks](https://arxiv.org//abs/2404.07464)

	Xinxing Zhao, Kar Wai Fok, Vrizlynn L. L. Thing


+ [ Backdoor Contrastive Learning via Bi-level Trigger Optimization](https://arxiv.org//abs/2404.07863)

	Weiyu Sun, Xinyu Zhang, Hao Lu, Yingcong Chen, Ting Wang, Jinghui Chen, Lu Lin


+ [ Latent Guard: a Safety Framework for Text-to-image Generation](https://arxiv.org//abs/2404.08031)

	Runtao Liu, Ashkan Khakzar, Jindong Gu, Qifeng Chen, Philip Torr, Fabio Pizzati


+ [ LLM Agents can Autonomously Exploit One-day Vulnerabilities](https://arxiv.org//abs/2404.08144)

	Richard Fang, Rohan Bindu, Akul Gupta, Daniel Kang


+ [ Persistent Classification: A New Approach to Stability of Data and  Adversarial Examples](https://arxiv.org//abs/2404.08069)

	Brian Bell, Michael Geyer, David Glickenstein, Keaton Hamm, Carlos Scheidegger, Amanda Fernandez, Juston Moore


+ [ Eliminating Catastrophic Overfitting Via Abnormal Adversarial Examples  Regularization](https://arxiv.org//abs/2404.08154)

	Runqi Lin, Chaojian Yu, Tongliang Liu


# 2024-04-10
+ [ Towards a Game-theoretic Understanding of Explanation-based Membership  Inference Attacks](https://arxiv.org//abs/2404.07139)

	Kavita Kumari, Murtuza Jadliwala, Sumit Kumar Jha, Anindya Maiti


+ [ SafeGen: Mitigating Unsafe Content Generation in Text-to-Image Models](https://arxiv.org//abs/2404.06666)

	Xinfeng Li, Yuchen Yang, Jiangyi Deng, Chen Yan, Yanjiao Chen, Xiaoyu Ji, Wenyuan Xu


+ [ How to Craft Backdoors with Unlabeled Data Alone?](https://arxiv.org//abs/2404.06694)

	Yifei Wang, Wenhan Ma, Yisen Wang


+ [ Logit Calibration and Feature Contrast for Robust Federated Learning on  Non-IID Data](https://arxiv.org//abs/2404.06776)

	Yu Qiao, Chaoning Zhang, Apurba Adhikary, Choong Seon Hong


+ [ Adversarial purification for no-reference image-quality metrics:  applicability study and new methods](https://arxiv.org//abs/2404.06957)

	Aleksandr Gushchin, Anna Chistyakova, Vladislav Minashkin, Anastasia Antsiferova, Dmitriy Vatolin


+ [ Simpler becomes Harder: Do LLMs Exhibit a Coherent Behavior on  Simplified Corpora?](https://arxiv.org//abs/2404.06838)

	Miriam Anschütz, Edoardo Mosca, Georg Groh


+ [ Poisoning Prevention in Federated Learning and Differential Privacy via  Stateful Proofs of Execution](https://arxiv.org//abs/2404.06721)

	Norrathep Rattanavipanon, Ivan de Oliviera Nunes



# 2024-04-09
+ [ Differential Privacy for Anomaly Detection: Analyzing the Trade-off  Between Privacy and Explainability](https://arxiv.org//abs/2404.06144)

	Fatima Ezzeddine, Mirna Saad, Omran Ayoub, Davide Andreoletti, Martin Gjoreski, Ihab Sbeity, Marc Langheinrich, Silvia Giordano


+ [ LRR: Language-Driven Resamplable Continuous Representation against  Adversarial Tracking Attacks](https://arxiv.org//abs/2404.06247)

	Jianlang Chen, Xuhong Ren, Qing Guo, Felix Juefei-Xu, Di Lin, Wei Feng, Lei Ma, Jianjun Zhao


+ [ On adversarial training and the 1 Nearest Neighbor classifier](https://arxiv.org//abs/2404.06313)

	Amir Hagai, Yair Weiss


+ [ Towards Robust Domain Generation Algorithm Classification](https://arxiv.org//abs/2404.06236)

	Arthur Drichel, Marc Meyer, Ulrike Meyer


+ [ Privacy-preserving Scanpath Comparison for Pervasive Eye Tracking](https://arxiv.org//abs/2404.06216)

	Suleyman Ozdel, Efe Bozkir, Enkelejda Kasneci


+ [ Sandwich attack: Multi-language Mixture Adaptive Attack on LLMs](https://arxiv.org//abs/2404.07242)

	Bibek Upadhayay, Vahid Behzadan


+ [ Towards Building a Robust Toxicity Predictor](https://arxiv.org//abs/2404.08690)

	Dmitriy Bespalov, Sourav Bhabesh, Yi Xiang, Liutong Zhou, Yanjun Qi



# 2024-04-08
+ [ SoK: Gradient Leakage in Federated Learning](https://arxiv.org//abs/2404.05403)

	Jiacheng Du, Jiahui Hu, Zhibo Wang, Peng Sun, Neil Zhenqiang Gong, Kui Ren


+ [ Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data](https://arxiv.org//abs/2404.05530)

	Tim Baumgärtner, Yang Gao, Dana Alon, Donald Metzler


+ [ Investigating the Impact of Quantization on Adversarial Robustness](https://arxiv.org//abs/2404.05639)

	Qun Li, Yuan Meng, Chen Tang, Jiacheng Jiang, Zhi Wang


+ [ David and Goliath: An Empirical Evaluation of Attacks and Defenses for  QNNs at the Deep Edge](https://arxiv.org//abs/2404.05688)

	Miguel Costa, Sandro Pinto


+ [ Semantic Stealth: Adversarial Text Attacks on NLP Using Several Methods](https://arxiv.org//abs/2404.05159)

	Roopkatha Dey, Aivy Debnath, Sayak Kumar Dutta, Kaustav Ghosh, Arijit Mitra, Arghya Roy Chowdhury, Jaydip Sen


+ [ Out-of-Distribution Data: An Acquaintance of Adversarial Examples -- A  Survey](https://arxiv.org//abs/2404.05219)

	Naveen Karunanayake, Ravin Gunawardena, Suranga Seneviratne, Sanjay Chawla


+ [ BruSLeAttack: A Query-Efficient Score-Based Black-Box Sparse Adversarial  Attack](https://arxiv.org//abs/2404.05311)

	Viet Quoc Vo, Ehsan Abbasnejad, Damith C. Ranasinghe


+ [ Certified PEFTSmoothing: Parameter-Efficient Fine-Tuning with Randomized  Smoothing](https://arxiv.org//abs/2404.05350)

	Chengyan Fu, Wenjie Wang


+ [ Flexible Fairness Learning via Inverse Conditional Permutation](https://arxiv.org//abs/2404.05678)

	Yuheng Lai, Leying Guan


+ [ Enabling Privacy-Preserving Cyber Threat Detection with Federated  Learning](https://arxiv.org//abs/2404.05130)

	Yu Bi, Yekai Li, Xuan Feng, Xianghang Mi


+ [ Negative Preference Optimization: From Catastrophic Collapse to  Effective Unlearning](https://arxiv.org//abs/2404.05868)

	Ruiqi Zhang, Licong Lin, Yu Bai, Song Mei


+ [ Eraser: Jailbreaking Defense in Large Language Models via Unlearning  Harmful Knowledge](https://arxiv.org//abs/2404.05880)

	Weikai Lu, Ziqian Zeng, Jianwei Wang, Zhengdong Lu, Zelin Chen, Huiping Zhuang, Cen Chen


+ [ Privacy-Preserving Deep Learning Using Deformable Operators for Secure  Task Learning](https://arxiv.org//abs/2404.05828)

	Fabian Perez, Jhon Lopez, Henry Arguello


+ [ Quantum Adversarial Learning for Kernel Methods](https://arxiv.org//abs/2404.05824)

	Giuseppe Montalbano, Leonardo Banchi


# 2024-04-07
+ [ Inference-Time Rule Eraser: Distilling and Removing Bias Rules to  Mitigate Bias in Deployed Models](https://arxiv.org//abs/2404.04814)

	Yi Zhang, Jitao Sang


+ [ Hidden You Malicious Goal Into Benigh Narratives: Jailbreak Large  Language Models through Logic Chain Injection](https://arxiv.org//abs/2404.04849)

	Zhilong Wang, Yebo Cao, Peng Liu


+ [ SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for  Clinical Trials](https://arxiv.org//abs/2404.04963)

	Mael Jullien, Marco Valentino, André Freitas


+ [ How much reliable is ChatGPT's prediction on Information Extraction  under Input Perturbations?](https://arxiv.org//abs/2404.05088)

	Ishani Mondal, Abhilasha Sancheti


+ [ Privacy-Preserving Traceable Functional Encryption for Inner Product](https://arxiv.org//abs/2404.04861)

	Muyao Qiu, Jinguang Han



# 2024-04-06
+ [ Trustless Audits without Revealing Data or Models](https://arxiv.org//abs/2404.04500)

	Suppakit Waiwitlikhit, Ion Stoica, Yi Sun, Tatsunori Hashimoto, Daniel Kang


+ [ Data Poisoning Attacks on Off-Policy Policy Evaluation Methods](https://arxiv.org//abs/2404.04714)

	Elita Lobo, Harvineet Singh, Marek Petrik, Cynthia Rudin, Himabindu Lakkaraju


+ [ D$^3$: Scaling Up Deepfake Detection by Learning from Discrepancy](https://arxiv.org//abs/2404.04584)

	Yongqi Yang, Zhihao Qian, Ye Zhu, Yu Wu


+ [ Structured Gradient-based Interpretations via Norm-Regularized  Adversarial Training](https://arxiv.org//abs/2404.04647)

	Shizhan Gong, Qi Dou, Farzan Farnia


+ [ CANEDERLI: On The Impact of Adversarial Training and Transferability on  CAN Intrusion Detection Systems](https://arxiv.org//abs/2404.04648)

	Francesco Marchiori, Mauro Conti


+ [ Goal-guided Generative Prompt Injection Attack on Large Language Models](https://arxiv.org//abs/2404.07234)

	Chong Zhang, Mingyu Jin, Qinkai Yu, Chengzhi Liu, Haochen Xue, Xiaobo Jin


+ [ ALERT: A Comprehensive Benchmark for Assessing Large Language Models'  Safety through Red Teaming](https://arxiv.org//abs/2404.08676)

	Simone Tedeschi, Felix Friedrich, Patrick Schramowski, Kristian Kersting, Roberto Navigli, Huu Nguyen, Bo Li


## 2024-04-05
+ [ Precision Guided Approach to Mitigate Data Poisoning Attacks in  Federated Learning](https://arxiv.org//abs/2404.04139)

	K Naveen Kumar, C Krishna Mohan, Aravind Machiry


+ [ Watermark-based Detection and Attribution of AI-Generated Content](https://arxiv.org//abs/2404.04254)

	Zhengyuan Jiang, Moyang Guo, Yuepeng Hu, Neil Zhenqiang Gong


+ [ Evaluating Adversarial Robustness: A Comparison Of FGSM, Carlini-Wagner  Attacks, And The Role of Distillation as Defense Mechanism](https://arxiv.org//abs/2404.04245)

	Trilokesh Ranjan Sarkar, Nilanjan Das, Pralay Sankar Maitra, Bijoy Some, Ritwik Saha, Orijita Adhikary, Bishal Bose, Jaydip Sen


+ [ Re-pseudonymization Strategies for Smart Meter Data Are Not Robust to  Deep Learning Profiling Attacks](https://arxiv.org//abs/2404.03948)

	Ana-Maria Cretu, Miruna Rusu, Yves-Alexandre de Montjoye


+ [ You Can Use But Cannot Recognize: Preserving Visual Privacy in Deep  Neural Networks](https://arxiv.org//abs/2404.04098)

	Qiushi Li, Yan Zhang, Ju Ren, Qi Li, Yaoxue Zhang


+ [ Increased LLM Vulnerabilities from Fine-tuning and Quantization](https://arxiv.org//abs/2404.04392)

	Divyanshu Kumar, Anurakt Kumar, Sahil Agarwal, Prashanth Harshangi



## 2024-04-04
+ [ Knowledge Distillation-Based Model Extraction Attack using Private  Counterfactual Explanations](https://arxiv.org/abs/2404.03348)

	Fatima Ezzeddine, Omran Ayoub, Silvia Giordano


+ [ Learn When (not) to Trust Language Models: A Privacy-Centric Adaptive  Model-Aware Approach](https://arxiv.org/abs/2404.03514)

	Chengkai Huang, Rui Wang, Kaige Xie, Tong Yu, Lina Yao


+ [ A Comparative Analysis of Word-Level Metric Differential Privacy:  Benchmarking The Privacy-Utility Trade-off](https://arxiv.org/abs/2404.03324)

	Stephen Meisenbacher, Nihildev Nandakumar, Alexandra Klymenko, Florian Matthes


+ [ Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak  Attacks?](https://arxiv.org/abs/2404.03411)

	Shuo Chen, Zhen Han, Bailan He, Zifeng Ding, Wenqian Yu, Philip Torr, Volker Tresp, Jindong Gu


+ [ Meta Invariance Defense Towards Generalizable Robustness to Unknown  Adversarial Attacks](https://arxiv.org/abs/2404.03340)

	Lei Zhang, Yuhang Zhou, Yi Yang, Xinbo Gao


+ [ Learn What You Want to Unlearn: Unlearning Inversion Attacks against  Machine Unlearning](https://arxiv.org/abs/2404.03233)

	Hongsheng Hu, Shuo Wang, Tian Dong, Minhui Xue


+ [ Privacy-Enhancing Technologies for Artificial Intelligence-Enabled  Systems](https://arxiv.org/abs/2404.03509)

	Liv d'Aliberti, Evan Gronberg, Joseph Kovba


+ [ Accurate Low-Degree Polynomial Approximation of Non-polynomial Operators  for Fast Private Inference in Homomorphic Encryption](https://arxiv.org/abs/2404.03216)

	Jianming Tong, Jingtian Dang, Anupam Golder, Callie Hao, Arijit Raychowdhury, Tushar Krishna


## 2024-04-03
+ [ JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal  Large Language Models against Jailbreak Attacks](https://arxiv.org/abs/2404.03027)

	Weidi Luo, Siyuan Ma, Xiaogeng Liu, Xiaoyu Guo, Chaowei Xiao


+ [ Adversarial Attacks and Dimensionality in Text Classifiers](https://arxiv.org/abs/2404.02660)

	Nandish Chattopadhyay, Atreya Goswami, Anupam Chattopadhyay


+ [ Learn to Disguise: Avoid Refusal Responses in LLM's Defense via a Multi-agent Attacker-Disguiser Game](https://arxiv.org/abs/2404.02532)

	Qianqiao Xu, Zhiliang Tian, Hongyan Wu, Zhen Huang, Yiping Song, Feng Liu, Dongsheng Li


## 2024-04-02
+ [ Jailbreaking Prompt Attack: A Controllable Adversarial Attack against  Diffusion Models](https://arxiv.org/abs/2404.02928)

	Jiachen Ma, Anda Cao, Zhiqing Xiao, Jie Zhang, Chao Ye, Junbo Zhao


+ [ Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks](https://arxiv.org/abs/2404.02151)

	Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion


+ [ Humanizing Machine-Generated Content: Evading AI-Text Detection through Adversarial Attack](https://arxiv.org/abs/2404.01907)

	Ying Zhou, Ben He, Le Sun


+ [ Red-Teaming Segment Anything Model](https://arxiv.org/abs/2404.02067)

	Krzysztof Jankowski, Bartlomiej Sobieski, Mateusz Kwiatkowski, Jakub Szulc, Michal Janik, Hubert Baniecki, Przemyslaw Biecek


+ [ Towards Robust 3D Pose Transfer with Adversarial Learning](https://arxiv.org/abs/2404.02242)

	Haoyu Chen, Hao Tang, Ehsan Adeli, Guoying Zhao


+ [ Exploring Backdoor Vulnerabilities of Chat Models](https://arxiv.org/abs/2404.02406)

	Yunzhuo Hao, Wenkai Yang, Yankai Lin


## 2024-04-01
+ [ BadPart: Unified Black-box Adversarial Patch Attacks against Pixel-wise Regression Tasks](https://arxiv.org/abs/2404.00924)

	Zhiyuan Cheng, Zhaoyi Liu, Tengda Guo, Shiwei Feng, Dongfang Liu, Mingjie Tang, Xiangyu Zhang


+ [ Multi-granular Adversarial Attacks against Black-box Neural Ranking Models](https://arxiv.org/abs/2404.01574)

	Yu-An Liu, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, Xueqi Cheng


+ [ UFID: A Unified Framework for Input-level Backdoor Detection on Diffusion Models](https://arxiv.org/abs/2404.01101)

	Zihan Guan, Mengxuan Hu, Sheng Li, Anil Vullikanti


## 2024-03-31
+ [ An Unsupervised Adversarial Autoencoder for Cyber Attack Detection in  Power Distribution Grids](https://arxiv.org/abs/2404.02923)

	Mehdi Jabbari Zideh, Mohammad Reza Khalghani, Sarika Khushalani Solanki


## 2024-03-30
+ [ STBA: Towards Evaluating the Robustness of DNNs for Query-Limited Black-box Scenario](https://arxiv.org/abs/2404.00362)

	Renyang Liu, Kwok-Yan Lam, Wei Zhou, Sixing Wu, Jun Zhao, Dongting Hu, Mingming Gong


## 2024-03-29
+ [ Benchmarking the Robustness of Temporal Action Detection Models Against  Temporal Corruptions](https://arxiv.org/abs/2403.20254)

	Runhao Zeng, Xiaoyong Chen, Jiaming Liang, Huisi Wu, Guangzhong Cao, Yong Guo


## 2024-03-28
+ [ MMCert: Provable Defense against Adversarial Attacks to Multi-modal  Models](https://arxiv.org/abs/2403.19080)

	Yanting Wang, Hongye Fu, Wei Zou, Jinyuan Jia


+ [ MedBN: Robust Test-Time Adaptation against Malicious Test Samples](https://arxiv.org/abs/2403.19326)

	Hyejin Park, Jeongyeon Hwang, Sunung Mun, Sangdon Park, Jungseul Ok


+ [ Towards Understanding Dual BN In Hybrid Adversarial Training](https://arxiv.org/abs/2403.19150)

	Chenshuang Zhang, Chaoning Zhang, Kang Zhang, Axi Niu, Junmo Kim, In So Kweon


+ [ Improving Adversarial Data Collection by Supporting Annotators: Lessons  from GAHD, a German Hate Speech Dataset](https://arxiv.org/abs/2403.19559)

	Janis Goldzycher, Paul Röttger, Gerold Schneider


## 2024-03-27
+ [ Manipulating Neural Path Planners via Slight Perturbations](https://arxiv.org/abs/2403.18256)

	Zikang Xiong, Suresh Jagannathan


+ [ CosalPure: Learning Concept from Group Images for Robust Co-Saliency  Detection](https://arxiv.org/abs/2403.18554)

	Jiayi Zhu, Qing Guo, Felix Juefei-Xu, Yihao Huang, Yang Liu, Geguang Pu


+ [ Safe and Robust Reinforcement-Learning: Principles and Practice](https://arxiv.org/abs/2403.18539)

	Taku Yamagata, Raul Santos-Rodriguez


+ [ Bayesian Learned Models Can Detect Adversarial Malware For Free](https://arxiv.org/abs/2403.18309)

	Bao Gia Doan, Dang Quang Nguyen, Paul Montague, Tamas Abraham, Olivier De Vel, Seyit Camtepe, Salil S. Kanhere, Ehsan Abbasnejad, Damith C. Ranasinghe


+ [ MisGUIDE : Defense Against Data-Free Deep Learning Model Extraction](https://arxiv.org/abs/2403.18580)

	Mahendra Gurve, Sankar Behera, Satyadev Ahlawat, Yamuna Prasad


+ [ Robustness and Visual Explanation for Black Box Image, Video, and ECG  Signal Classification with Reinforcement Learning](https://arxiv.org/abs/2403.18985)

	Soumyendu Sarkar, Ashwin Ramesh Babu, Sajad Mousavi, Vineet Gundecha, Avisek Naug, Sahand Ghorbanpour


## 2024-03-26
+ [ Out-of-distribution Rumor Detection via Test-Time Adaptation](https://arxiv.org/abs/2403.17735)

	Xiang Tao, Mingqing Zhang, Qiang Liu, Shu Wu, Liang Wang


+ [ DataCook: Crafting Anti-Adversarial Examples for Healthcare Data  Copyright Protection](https://arxiv.org/abs/2403.17755)

	Sihan Shang, Jiancheng Yang, Zhenglong Sun, Pascal Fua


+ [ UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object  Detection with Sparse LiDAR and Large Domain Gaps](https://arxiv.org/abs/2403.17633)

	Maciej K Wozniak, Mattias Hansson, Marko Thiel, Patric Jensfelt


+ [ Optimization-based Prompt Injection Attack to LLM-as-a-Judge](https://arxiv.org/abs/2403.17710)

	Jiawen Shi, Zenghui Yuan, Yinuo Liu, Yue Huang, Pan Zhou, Lichao Sun, Neil Zhenqiang Gong


+ [ Physical 3D Adversarial Attacks against Monocular Depth Estimation in  Autonomous Driving](https://arxiv.org/abs/2403.17301)

	Junhao Zheng, Chenhao Lin, Jiahao Sun, Zhengyu Zhao, Qian Li, Chao Shen


+ [ Boosting Adversarial Training via Fisher-Rao Norm-based Regularization](https://arxiv.org/abs/2403.17520)

	Xiangyu Yin, Wenjie Ruan


+ [ Securing GNNs: Explanation-Based Identification of Backdoored Training  Graphs](https://arxiv.org/abs/2403.18136)

	Jane Downer, Ren Wang, Binghui Wang


+ [ Targeted Visualization of the Backbone of Encoder LLMs](https://arxiv.org/abs/2403.18872)

	Isaac Roberts, Alexander Schulz, Luca Hermes, Barbara Hammer


## 2024-03-25
+ [ $\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on  Prompt-based Language Models](https://arxiv.org/abs/2403.16432)

	Yue Xu, Wenjie Wang


+ [ The Anatomy of Adversarial Attacks: Concept-based XAI Dissection](https://arxiv.org/abs/2403.16782)

	Georgii Mikriukov, Gesina Schwalbe, Franz Motzkus, Korinna Bade


+ [ Generating Potent Poisons and Backdoors from Scratch with Guided  Diffusion](https://arxiv.org/abs/2403.16365)

	Hossein Souri, Arpit Bansal, Hamid Kazemi, Liam Fowl, Aniruddha Saha, Jonas Geiping, Andrew Gordon Wilson, Rama Chellappa, Tom Goldstein, Micah Goldblum


+ [ Ensemble Adversarial Defense via Integration of Multiple Dispersed Low  Curvature Models](https://arxiv.org/abs/2403.16405)

	Kaikang Zhao, Xi Chen, Wei Huang, Liuxin Ding, Xianglong Kong, Fan Zhang


+ [ Revealing Vulnerabilities of Neural Networks in Parameter Learning and  Defense Against Explanation-Aware Backdoors](https://arxiv.org/abs/2403.16569)

	Md Abdul Kadir, GowthamKrishna Addluri, Daniel Sonntag


+ [ CipherFormer: Efficient Transformer Private Inference with Low Round  Complexity](https://arxiv.org/abs/2403.16860)

	Weize Wang, Yi Kuang


+ [ Task-Agnostic Detector for Insertion-Based Backdoor Attacks](https://arxiv.org/abs/2403.17155)

	Weimin Lyu, Xiao Lin, Songzhu Zheng, Lu Pang, Haibin Ling, Susmit Jha, Chao Chen


+ [ LOTUS: Evasive and Resilient Backdoor Attacks through Sub-Partitioning](https://arxiv.org/abs/2403.17188)

	Siyuan Cheng, Guanhong Tao, Yingqi Liu, Guangyu Shen, Shengwei An, Shiwei Feng, Xiangzhe Xu, Kaiyuan Zhang, Shiqing Ma, Xiangyu Zhang


## 2024-03-24
+ [ Unlearning Backdoor Threats: Enhancing Backdoor Defense in Multimodal  Contrastive Learning via Local Token Unlearning](https://arxiv.org/abs/2403.16257)

	Siyuan Liang, Kuanrong Liu, Jiajun Gong, Jiawei Liang, Yuan Xun, Ee-Chien Chang, Xiaochun Cao


+ [ Robust Diffusion Models for Adversarial Purification](https://arxiv.org/abs/2403.16067)

	Guang Lin, Zerui Tao, Jianhai Zhang, Toshihisa Tanaka, Qibin Zhao


+ [ Subspace Defense: Discarding Adversarial Perturbations by Learning a  Subspace for Clean Signals](https://arxiv.org/abs/2403.16176)

	Rui Zheng, Yuhao Zhou, Zhiheng Xi, Tao Gui, Qi Zhang, Xuanjing Huang



## 2024-03-23
+ [ Adversarial Defense Teacher for Cross-Domain Object Detection under Poor  Visibility Conditions](https://arxiv.org/abs/2403.15786)

	Kaiwen Wang, Yinzhe Shen, Martin Lauer


+ [ An Embarrassingly Simple Defense Against Backdoor Attacks On SSL](https://arxiv.org/abs/2403.15918)

	Aryan Satpathy, Nilaksh, Dhruva Rajwade


## 2024-03-22
+ [ A Transfer Attack to Image Watermarks](https://arxiv.org/abs/2403.15365)

	Yuepeng Hu, Zhengyuan Jiang, Moyang Guo, Neil Gong


+ [ Clean-image Backdoor Attacks](https://arxiv.org/abs/2403.15010)

	Dazhong Rong, Shuheng Shen, Xinyi Fu, Peng Qian, Jianhai Chen, Qinming He, Xing Fu, Weiqiang Wang


+ [ Robust optimization for adversarial learning with finite sample  complexity guarantees](https://arxiv.org/abs/2403.15207)

	André Bertolace, Konstatinos Gatsis, Kostas Margellos


+ [ Twin Auto-Encoder Model for Learning Separable Representation in  Cyberattack Detection](https://arxiv.org/abs/2403.15509)

	Phai Vu Dinh, Quang Uy Nguyen, Thai Hoang Dinh, Diep N. Nguyen, Bao Son Pham, Eryk Dutkiewicz


+ [ Differentially Private Next-Token Prediction of Large Language Models](https://arxiv.org/abs/2403.15638)

	James Flemings, Meisam Razaviyayn, Murali Annavaram


## 2024-03-21
+ [ SoftPatch: Unsupervised Anomaly Detection with Noisy Data](https://arxiv.org/abs/2403.14233)

	Xi Jiang, Ying Chen, Qiang Nie, Yong Liu, Jianlin Liu, Bin-Bin Gao, Jun Liu, Chengjie Wang, Feng Zheng


+ [ Locating and Mitigating Gender Bias in Large Language Models](https://arxiv.org/abs/2403.14409)

	Yuchen Cai, Ding Cao, Rongxi Guo, Yaqin Wen, Guiquan Liu, Enhong Chen


+ [ Detoxifying Large Language Models via Knowledge Editing](https://arxiv.org/abs/2403.14472)

	Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, Huajun Chen


+ [ MMIDR: Teaching Large Language Model to Interpret Multimodal  Misinformation via Knowledge Distillation](https://arxiv.org/abs/2403.14171)

	Longzheng Wang, Xiaohan Xu, Lei Zhang, Jiarui Lu, Yongxiu Xu, Hongbo Xu, Chuang Zhang


+ [ Adversary-Robust Graph-Based Learning of WSIs](https://arxiv.org/abs/2403.14489)

	Saba Heidari Gheshlaghi, Milan Aryal, Nasim Yahyasoltani, Masoud Ganji


+ [ Rethinking Adversarial Inverse Reinforcement Learning: From the Angles  of Policy Imitation and Transferable Reward Recovery](https://arxiv.org/abs/2403.14593)

	Yangchun Zhang, Yirui Zhou


+ [ Improving the Robustness of Large Language Models via Consistency  Alignment](https://arxiv.org/abs/2403.14221)

	Zhao Yukun, Yan Lingyong, Sun Weiwei, Xing Guoliang, Wang Shuaiqiang, Meng Chong, Cheng Zhicong, Ren Zhaochun, Yin Dawei


+ [ Rethinking Adversarial Inverse Reinforcement Learning: From the Angles  of Policy Imitation and Transferable Reward Recovery](https://arxiv.org/abs/2403.14593)

	Yangchun Zhang, Yirui Zhou


+ [ Adversary-Augmented Simulation to evaluate client-fairness on  HyperLedger Fabric](https://arxiv.org/abs/2403.14342)

	Erwan Mahe, Rouwaida Abdallah, Sara Tucci-Piergiovanni, Pierre-Yves Piriou


+ [ FIT-RAG: Black-Box RAG with Factual Information and Token Reduction](https://arxiv.org/abs/2403.14374)

	Yuren Mao, Xuemei Dong, Wenyi Xu, Yunjun Gao, Bin Wei, Ying Zhang


+ [ Adversary-Robust Graph-Based Learning of WSIs](https://arxiv.org/abs/2403.14489)

	Saba Heidari Gheshlaghi, Milan Aryal, Nasim Yahyasoltani, Masoud Ganji


+ [ Safeguarding Medical Image Segmentation Datasets against Unauthorized  Training via Contour- and Texture-Aware Perturbations](https://arxiv.org/abs/2403.14250)

	Xun Lin, Yi Yu, Song Xia, Jue Jiang, Haoran Wang, Zitong Yu, Yizhong Liu, Ying Fu, Shuai Wang, Wenzhong Tang, Alex Kot


+ [ HETAL: Efficient Privacy-preserving Transfer Learning with Homomorphic  Encryption](https://arxiv.org/abs/2403.14111)

	Seewoo Lee, Garam Lee, Jung Woo Kim, Junbum Shin, Mun-Kyu Lee


+ [ Improving Robustness to Model Inversion Attacks via Sparse Coding  Architectures](https://arxiv.org/abs/2403.14772)

	Sayanton V. Dibbo, Adam Breuer, Juston Moore, Michael Teti


+ [ Protected group bias and stereotypes in Large Language Models](https://arxiv.org/abs/2403.14727)

	Hadas Kotek, David Q. Sun, Zidi Xiu, Margit Bowler, Christopher Klein


+ [ Diffusion Attack: Leveraging Stable Diffusion for Naturalistic Image  Attacking](https://arxiv.org/abs/2403.14778)

	Qianyu Guo, Jiaming Fu, Yawen Lu, Dongming Gan


## 2024-03-20
+ [ BadEdit: Backdooring large language models by model editing](https://arxiv.org/abs/2403.13355)

	Yanzhou Li, Tianlin Li, Kangjie Chen, Jian Zhang, Shangqing Liu, Wenhan Wang, Tianwei Zhang, Yang Liu


+ [ Deepfake Detection without Deepfakes: Generalization via Synthetic  Frequency Patterns Injection](https://arxiv.org/abs/2403.13479)

	Davide Alessandro Coccomini, Roberto Caldelli, Claudio Gennaro, Giuseppe Fiameni, Giuseppe Amato, Fabrizio Falchi


+ [ Have You Poisoned My Data? Defending Neural Networks against Data  Poisoning](https://arxiv.org/abs/2403.13523)

	Fabio De Gaspari, Dorjan Hitaj, Luigi V. Mancini


+ [ Adversarial Attacks and Defenses in Automated Control Systems: A  Comprehensive Benchmark](https://arxiv.org/abs/2403.13502)

	Vitaliy Pozdnyakov, Aleksandr Kovalenko, Ilya Makarov, Mikhail Drobyshevskiy, Kirill Lukyanov


+ [ Reducing Large Language Model Bias with Emphasis on 'Restricted  Industries': Automated Dataset Augmentation and Prejudice Quantification](https://arxiv.org/abs/2403.13925)

	Devam Mondal, Carlo Lipizzi



+ [ Multi-Modal Hallucination Control by Visual Information Grounding](https://arxiv.org/abs/2403.14003)

	Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro Achille, Ashwin Swaminathan, Stefano Soatto


+ [ Optimal Transport for Fairness: Archival Data Repair using Small  Research Data Sets](https://arxiv.org/abs/2403.13864)

	Abigail Langbridge, Anthony Quinn, Robert Shorten



## 2024-03-19
+ [ Electioneering the Network: Dynamic Multi-Step Adversarial Attacks for  Community Canvassing](https://arxiv.org/abs/2403.12399)

	Saurabh Sharma, Ambuj SIngh


+ [ FairSIN: Achieving Fairness in Graph Neural Networks through Sensitive  Information Neutralization](https://arxiv.org/abs/2403.12474)

	Cheng Yang, Jixi Liu, Yunhe Yan, Chuan Shi


+ [ RigorLLM: Resilient Guardrails for Large Language Models against  Undesired Content](https://arxiv.org/abs/2403.13031)

	Zhuowen Yuan, Zidi Xiong, Yi Zeng, Ning Yu, Ruoxi Jia, Dawn Song, Bo Li


+ [ Robust NAS under adversarial training: benchmark, theory, and beyond](https://arxiv.org/abs/2403.13134)

	Yongtao Wu, Fanghui Liu, Carl-Johann Simon-Gabriel, Grigorios G Chrysos, Volkan Cevher


+ [ ADAPT to Robustify Prompt Tuning Vision Transformers](https://arxiv.org/abs/2403.13196)

	Masih Eskandar, Tooba Imtiaz, Zifeng Wang, Jennifer Dy


+ [ Analyzing the Impact of Partial Sharing on the Resilience of Online  Federated Learning Against Model Poisoning Attacks](https://arxiv.org/abs/2403.13108)

	Ehsan Lari, Vinay Chakravarthi Gogineni, Reza Arablouei, Stefan Werner


## 2024-03-18
+ [ Problem space structural adversarial attacks for Network Intrusion  Detection Systems based on Graph Neural Networks](https://arxiv.org/abs/2403.11830)

	Andrea Venturi, Dario Stabili, Mirco Marchetti


+ [ A Toolbox for Surfacing Health Equity Harms and Biases in Large Language  Models](https://arxiv.org/abs/2403.12025)

	Stephen R. Pfohl, Heather Cole-Lewis, Rory Sayres, Darlene Neal, Mercy Asiedu, Awa Dieng, Nenad Tomasev, Qazi Mamunur Rashid, Shekoofeh Azizi, Negar Rostamzadeh, Liam G. McCoy, Leo Anthony Celi, Yun Liu, Mike Schaekermann, Alanna Walton, Alicia Parrish, Chirag Nagpal, Preeti Singh, Akeiylah Dewitt, Philip Mansfield, Sushant Prakash, Katherine Heller, Alan Karthikesalingam, Christopher Semturs, Joelle Barral, Greg Corrado, Yossi Matias, Jamila Smith-Loud, Ivor Horn, Karan Singhal


+ [ Defense Against Adversarial Attacks on No-Reference Image Quality Models  with Gradient Norm Regularization](https://arxiv.org/abs/2403.11397)

	Yujia Liu, Chenxi Yang, Dingquan Li, Jianhao Ding, Tingting Jiang


+ [ SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption  of Monocular Depth Estimation in Autonomous Navigation Applications](https://arxiv.org/abs/2403.11515)

	Amira Guesmi, Muhammad Abdullah Hanif, Ihsen Alouani, Bassem Ouni, Muhammad Shafique


+ [ LocalStyleFool: Regional Video Style Transfer Attack Using Segment  Anything Model](https://arxiv.org/abs/2403.11656)

	Yuxin Cao, Jinghao Li, Xi Xiao, Derui Wang, Minhui Xue, Hao Ge, Wei Liu, Guangwu Hu


+ [ Diffusion Denoising as a Certified Defense against Clean-label Poisoning](https://arxiv.org/abs/2403.11981)

	Sanghyun Hong, Nicholas Carlini, Alexey Kurakin


+ [ Diffusion-Reinforcement Learning Hierarchical Motion Planning in  Adversarial Multi-agent Games](https://arxiv.org/abs/2403.10794)

	Zixuan Wu, Sean Ye, Manisha Natarajan, Matthew C. Gombolay


+ [ Improving LoRA in Privacy-preserving Federated Learning](https://arxiv.org/abs/2403.12313)

	Youbang Sun, Zitao Li, Yaliang Li, Bolin Ding


+ [ Impart: An Imperceptible and Effective Label-Specific Backdoor Attack](https://arxiv.org/abs/2403.13017)

	Jingke Zhao, Zan Wang, Yongwei Wang, Lanjun Wang


+ [ Invisible Backdoor Attack Through Singular Value Decomposition](https://arxiv.org/abs/2403.13018)

	Wenmin Chen, Xiaowei Xu



## 2024-03-17
+ [ RobustSentEmbed: Robust Sentence Embeddings Using Adversarial  Self-Supervised Contrastive Learning](https://arxiv.org/abs/2403.11082)

	Javad Rafiei Asl, Prajwal Panzade, Eduardo Blanco, Daniel Takabi, Zhipeng Cai


+ [ COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via  Probabilistic Circuits](https://arxiv.org/abs/2403.11348)

	Mintong Kang, Nezihe Merve Gürel, Linyi Li, Bo Li


+ [ A Modified Word Saliency-Based Adversarial Attack on Text Classification  Models](https://arxiv.org/abs/2403.11297)

	Hetvi Waghela, Sneha Rakshit, Jaydip Sen


## 2024-03-16
+ [ Improving Adversarial Transferability of Visual-Language Pre-training  Models through Collaborative Multimodal Interaction](https://arxiv.org/abs/2403.10883)

	Jiyuan Fu, Zhaoyu Chen, Kaixun Jiang, Haijing Guo, Jiafeng Wang, Shuyong Gao, Wenqiang Zhang


+ [ Understanding Robustness of Visual State Space Models for Image  Classification](https://arxiv.org/abs/2403.10935)

	Chengbin Du, Yanxi Li, Chang Xu


+ [ Adversarial Knapsack and Secondary Effects of Common Information for  Cyber Operations](https://arxiv.org/abs/2403.10789)

	Jon Goohs, Georgel Savin, Lucas Starks, Josiah Dykstra, William Casey




## 2024-03-15
+ [ Global Convergence Guarantees for Federated Policy Gradient Methods with  Adversaries](https://arxiv.org/abs/2403.09940)

	Swetha Ganesh, Jiayu Chen, Gugan Thoppe, Vaneet Aggarwal


+ [ Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A  Pilot Study](https://arxiv.org/abs/2403.10499)

	Chenguang Wang, Ruoxi Jia, Xin Liu, Dawn Song


+ [ Revisiting Adversarial Training under Long-Tailed Distributions](https://arxiv.org/abs/2403.10073)

	Xinli Yue, Ningping Mou, Qian Wang, Lingchen Zhao


+ [ Benchmarking Adversarial Robustness of Image Shadow Removal with  Shadow-adaptive Attacks](https://arxiv.org/abs/2403.10076)

	Chong Wang, Yi Yu, Lanqing Guo, Bihan Wen


+ [ Mitigating Dialogue Hallucination for Large Multi-modal Models via  Adversarial Instruction Tuning](https://arxiv.org/abs/2403.10492)

	Dongmin Park, Zhaofang Qian, Guangxing Han, Ser-Nam Lim


+ [ Towards Adversarially Robust Dataset Distillation by Curvature  Regularization](https://arxiv.org/abs/2403.10045)

	Eric Xue, Yijiang Li, Haoyang Liu, Yifan Shen, Haohan Wang


+ [ Generation is better than Modification: Combating High Class Homophily  Variance in Graph Anomaly Detection](https://arxiv.org/abs/2403.10339)

	Rui Zhang, Dawei Cheng, Xin Liu, Jie Yang, Yi Ouyang, Xian Wu, Yefeng Zheng


+ [ Federated Learning with Anomaly Detection via Gradient and  Reconstruction Analysis](https://arxiv.org/abs/2403.10000)

	Zahir Alsulaimawi


+ [ Securing Federated Learning with Control-Flow Attestation: A Novel  Framework for Enhanced Integrity and Resilience against Adversarial Attacks](https://arxiv.org/abs/2403.10005)

	Zahir Alsulaimawi


+ [ Interactive Trimming against Evasive Online Data Manipulation Attacks: A  Game-Theoretic Approach](https://arxiv.org/abs/2403.10313)

	Yue Fu, Qingqing Ye, Rong Du, Haibo Hu


+ [ Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized  Scaled Prediction Consistency](https://arxiv.org/abs/2403.10717)

	Soumyadeep Pal, Yuguang Yao, Ren Wang, Bingquan Shen, Sijia Liu



## 2024-03-14
+ [ ADEdgeDrop: Adversarial Edge Dropping for Robust Graph Neural Networks](https://arxiv.org/abs/2403.09171)

	Zhaoliang Chen, Zhihao Wu, Ylli Sadikaj, Claudia Plant, Hong-Ning Dai, Shiping Wang, Wenzhong Guo


+ [ Adversarial Training with OCR Modality Perturbation for Scene-Text  Visual Question Answering](https://arxiv.org/abs/2403.09288)

	Zhixuan Shen, Haonan Luo, Sijia Li, Tianrui Li


+ [ AdaShield: Safeguarding Multimodal Large Language Models from  Structure-based Attack via Adaptive Shield Prompting](https://arxiv.org/abs/2403.09513)

	Yu Wang, Xiaogeng Liu, Yu Li, Muhao Chen, Chaowei Xiao


+ [ VDNA-PR: Using General Dataset Representations for Robust Sequential  Visual Place Recognition](https://arxiv.org/abs/2403.09025)

	Benjamin Ramtoula, Daniele De Martini, Matthew Gadd, Paul Newman


+ [ Impact of Synthetic Images on Morphing Attack Detection Using a Siamese  Network](https://arxiv.org/abs/2403.09380)

	Juan Tapia, Christoph Busch


+ [ Anomaly Detection by Adapting a pre-trained Vision Language Model](https://arxiv.org/abs/2403.09493)

	Yuxuan Cai, Xinwei He, Dingkang Liang, Ao Tong, Xiang Bai


+ [ Soften to Defend: Towards Adversarial Robustness via Self-Guided Label  Refinement](https://arxiv.org/abs/2403.09101)

	Daiwei Yu, Zhuorong Li, Lina Wei, Canghong Jin, Yun Zhang, Sixian Chan


+ [ Adversarial Fine-tuning of Compressed Neural Networks for Joint  Improvement of Robustness and Efficiency](https://arxiv.org/abs/2403.09441)

	Hallgrimur Thorsteinsson, Valdemar J Henriksen, Tong Chen, Raghavendra Selvan


+ [ AVIBench: Towards Evaluating the Robustness of Large Vision-Language  Model on Adversarial Visual-Instructions](https://arxiv.org/abs/2403.09346)

	Hao Zhang, Wenqi Shao, Hong Liu, Yongqiang Ma, Ping Luo, Yu Qiao, Kaipeng Zhang


+ [ Counterfactual contrastive learning: robust representations via causal  image synthesis](https://arxiv.org/abs/2403.09605)

	Melanie Roschewitz, Fabio De Sousa Ribeiro, Tian Xia, Galvin Khara, Ben Glocker


+ [ Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text  Transformation](https://arxiv.org/abs/2403.09572)

	Yunhao Gou, Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung, James T. Kwok, Yu Zhang


+ [ Ciphertext-Only Attack on a Secure $k$-NN Computation on Cloud](https://arxiv.org/abs/2403.09080)

	Shyam Murthy, Santosh Kumar Upadhyaya, Srinivas Vivek


+ [ Optimistic Verifiable Training by Controlling Hardware Nondeterminism](https://arxiv.org/abs/2403.09603)

	Megha Srivastava, Simran Arora, Dan Boneh


+ [ Evaluating LLMs for Gender Disparities in Notable Persons](https://arxiv.org/abs/2403.09148)

	Lauren Rhue, Sofie Goethals, Arun Sundararajan


+ [ Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative  Privacy Risk](https://arxiv.org/abs/2403.09450)

	Zhangheng Li, Junyuan Hong, Bo Li, Zhangyang Wang


+ [ An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts  on Vision-Language Models](https://arxiv.org/abs/2403.09766)

	Haochen Luo, Jindong Gu, Fengyuan Liu, Philip Torr


+ [ Robust Subgraph Learning by Monitoring Early Training Representations](https://arxiv.org/abs/2403.09901)

	Sepideh Neshatfar, Salimeh Yasaei Sekeh


+ [ Counter-Samples: A Stateless Strategy to Neutralize Black Box  Adversarial Attacks](https://arxiv.org/abs/2403.10562)

	Roey Bokobza, Yisroel Mirsky



## 2024-03-13
+ [ Robust Decision Aggregation with Adversarial Experts](https://arxiv.org/abs/2403.08222)

	Yongkang Guo, Yuqing Kong


+ [ Versatile Defense Against Adversarial Attacks on Image Recognition](https://arxiv.org/abs/2403.08170)

	Haibo Zhang, Zhihua Yao, Kouichi Sakurai


+ [ RAF-GI: Towards Robust, Accurate and Fast-Convergent Gradient Inversion  Attack in Federated Learning](https://arxiv.org/abs/2403.08383)

	Can Liu, Jin Wang, Dongyang Yu


+ [ AIGCs Confuse AI Too: Investigating and Explaining Synthetic  Image-induced Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2403.08542)

	Yifei Gao, Jiaqi Wang, Zhiyu Lin, Jitao Sang


+ [ Advancing Security in AI Systems: A Novel Approach to Detecting  Backdoors in Deep Neural Networks](https://arxiv.org/abs/2403.08208)

	Khondoker Murad Hossain, Tim Oates


+ [ SoK: Reducing the Vulnerability of Fine-tuned Language Models to  Membership Inference Attacks](https://arxiv.org/abs/2403.08481)

	Guy Amit, Abigail Goldsteen, Ariel Farkash



## 2024-03-12
+ [ Disentangling Policy from Offline Task Representation Learning via  Adversarial Data Augmentation](https://arxiv.org/abs/2403.07261)

	Chengxing Jia, Fuxiang Zhang, Yi-Chen Li, Chen-Xiao Gao, Xu-Hui Liu, Lei Yuan, Zongzhang Zhang, Yang Yu


+ [ A Bayesian Approach to OOD Robustness in Image Classification](https://arxiv.org/abs/2403.07277)

	Prakhar Kaushik, Adam Kortylewski, Alan Yuille


+ [ Exploring Safety Generalization Challenges of Large Language Models via  Code](https://arxiv.org/abs/2403.07865)

	Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Wai Lam, Lizhuang Ma


+ [ Truth-Aware Context Selection: Mitigating the Hallucinations of Large  Language Models Being Misled by Untruthful Contexts](https://arxiv.org/abs/2403.07556)

	Tian Yu, Shaolei Zhang, Yang Feng


+ [ Calibrating Multi-modal Representations: A Pursuit of Group Robustness  without Annotations](https://arxiv.org/abs/2403.07241)

	Chenyu You, Yifei Min, Weicheng Dai, Jasjeet S. Sekhon, Lawrence Staib, James S. Duncan


+ [ Backdoor Attack with Mode Mixture Latent Modification](https://arxiv.org/abs/2403.07463)

	Hongwei Zhang, Xiaoyin Xu, Dongsheng An, Xianfeng Gu, Min Zhang


+ [ FairRR: Pre-Processing for Group Fairness through Randomized Response](https://arxiv.org/abs/2403.07780)

	Xianli Zeng, Joshua Ward, Guang Cheng


+ [ LiveCodeBench: Holistic and Contamination Free Evaluation of Large  Language Models for Code](https://arxiv.org/abs/2403.07974)

	Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, Ion Stoica


## 2024-03-11
+ [ Poisoning Programs by Un-Repairing Code: Security Concerns of  AI-generated Code](https://arxiv.org/abs/2403.06675)

	Cristina Improta


+ [ Data-Independent Operator: A Training-Free Artifact Representation  Extractor for Generalizable Deepfake Detection](https://arxiv.org/abs/2403.06803)

	Chuangchuang Tan, Ping Liu, RenShuai Tao, Huan Liu, Yao Zhao, Baoyuan Wu, Yunchao Wei


+ [ PeerAiD: Improving Adversarial Distillation from a Specialized Peer  Tutor](https://arxiv.org/abs/2403.06668)

	Jaewon Jung, Hongsun Jang, Jaeyong Song, Jinho Lee


+ [ Dynamic Perturbation-Adaptive Adversarial Training on Medical Image  Classification](https://arxiv.org/abs/2403.06798)

	Shuai Li, Xiaoguang Ma, Shancheng Jiang, Lu Meng


+ [ Intra-Section Code Cave Injection for Adversarial Evasion Attacks on  Windows PE Malware File](https://arxiv.org/abs/2403.06428)

	Kshitiz Aryal, Maanak Gupta, Mahmoud Abdelsalam, Moustafa Saleh


+ [ Real is not True: Backdoor Attacks Against Deepfake Detection](https://arxiv.org/abs/2403.06610)

	Hong Sun, Ziqiang Li, Lei Liu, Bin Li


+ [ Stealing Part of a Production Language Model](https://arxiv.org/abs/2403.06634)

	Nicholas Carlini, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A. Feder Cooper, Katherine Lee, Matthew Jagielski, Milad Nasr, Arthur Conmy, Eric Wallace, David Rolnick, Florian Tramèr


+ [ Improving deep learning with prior knowledge and cognitive models: A  survey on enhancing explainability, adversarial robustness and zero-shot  learning](https://arxiv.org/abs/2403.07078)

	Fuseinin Mumuni, Alhassan Mumuni


## 2024-03-10
+ [ In-context Prompt Learning for Test-time Vision Recognition with Frozen  Vision-language Model](https://arxiv.org/abs/2403.06126)

	Junhui Yin, Xinyu Zhang, Lin Wu, Xianghua Xie, Xiaojie Wang


+ [ Federated Learning: Attacks, Defenses, Opportunities, and Challenges](https://arxiv.org/abs/2403.06067)

	Ghazaleh Shirvani, Saeid Ghasemshirazi, Behzad Beigzadeh



+ [ Attacking Transformers with Feature Diversity Adversarial Perturbation](https://arxiv.org/abs/2403.07942)

	Chenxing Gao, Hang Zhou, Junqing Yu, YuTeng Ye, Jiale Cai, Junle Wang, Wei Yang


## 2024-03-09
+ [ Towards Deviation-Robust Agent Navigation via Perturbation-Aware  Contrastive Learning](https://arxiv.org/abs/2403.05770)

	Bingqian Lin, Yanxin Long, Yi Zhu, Fengda Zhu, Xiaodan Liang, Qixiang Ye, Liang Lin


+ [ Hard-label based Small Query Black-box Adversarial Attack](https://arxiv.org/abs/2403.06014)

	Jeonghwan Park, Paul Miller, Niall McLaughlin


## 2024-03-08
+ [ Predicting Single-cell Drug Sensitivity by Adaptive Weighted Feature for  Adversarial Multi-source Domain Adaptation](https://arxiv.org/abs/2403.05260)

	Wei Duan, Hui Liu


+ [ Exploring the Adversarial Frontier: Quantifying Robustness via  Adversarial Hypervolume](https://arxiv.org/abs/2403.05100)

	Ping Guo, Cheng Gong, Xi Lin, Zhiyuan Yang, Qingfu Zhang


+ [ Overcoming Reward Overoptimization via Adversarial Policy Optimization  with Lightweight Uncertainty Estimation](https://arxiv.org/abs/2403.05171)

	Xiaoying Zhang, Jean-Francois Ton, Wei Shen, Hongning Wang, Yang Liu


+ [ Towards Multimodal Sentiment Analysis Debiasing via Bias Purification](https://arxiv.org/abs/2403.05023)

	Dingkang Yang, Mingcheng Li, Dongling Xiao, Yang Liu, Kun Yang, Zhaoyu Chen, Yuzheng Wang, Peng Zhai, Ke Li, Lihua Zhang


+ [ The Impact of Quantization on the Robustness of Transformer-based Text  Classifiers](https://arxiv.org/abs/2403.05365)

	Seyed Parsa Neshaei, Yasaman Boreshban, Gholamreza Ghassem-Sani, Seyed Abolghasem Mirroshandel


+ [ Hide in Thicket: Generating Imperceptible and Rational Adversarial  Perturbations on 3D Point Clouds](https://arxiv.org/abs/2403.05247)

	Tianrui Lou, Xiaojun Jia, Jindong Gu, Li Liu, Siyuan Liang, Bangyan He, Xiaochun Cao


+ [ Federated Learning Method for Preserving Privacy in Face Recognition  System](https://arxiv.org/abs/2403.05344)

	Enoch Solomon, Abraham Woubie


+ [ EVD4UAV: An Altitude-Sensitive Benchmark to Evade Vehicle Detection in  UAV](https://arxiv.org/abs/2403.05422)

	Huiming Sun, Jiacheng Guo, Zibo Meng, Tianyun Zhang, Jianwu Fang, Yuewei Lin, Hongkai Yu


+ [ Adversarial Sparse Teacher: Defense Against Distillation-Based Model  Stealing Attacks Using Adversarial Examples](https://arxiv.org/abs/2403.05181)

	Eda Yilmaz, Hacer Yalim Keles


## 2024-03-07
+ [ Privacy-preserving Fine-tuning of Large Language Models through Flatness](https://arxiv.org/abs/2403.04124)

	Tiejin Chen, Longchao Da, Huixue Zhou, Pingzhi Li, Kaixiong Zhou, Tianlong Chen, Hua Wei


+ [ A Domain Translation Framework with an Adversarial Denoising Diffusion  Model to Generate Synthetic Datasets of Echocardiography Images](https://arxiv.org/abs/2403.04612)

	Cristiana Tiago, Sten Roar Snare, Jurica Sprem, Kristin McLeod


+ [ Membership Inference Attacks and Privacy in Topic Modeling](https://arxiv.org/abs/2403.04451)

	Nico Manzonelli, Wanrong Zhang, Salil Vadhan


+ [ Automatic and Universal Prompt Injection Attacks against Large Language  Models](https://arxiv.org/abs/2403.04957)

	Xiaogeng Liu, Zhiyuan Yu, Yizhe Zhang, Ning Zhang, Chaowei Xiao


## 2024-03-06
+ [ Do You Trust Your Model? Emerging Malware Threats in the Deep Learning  Ecosystem](https://arxiv.org/abs/2403.03593)

	Dorjan Hitaj, Giulio Pagnotta, Fabio De Gaspari, Sediola Ruko, Briland Hitaj, Luigi V. Mancini, Fernando Perez-Cruz


+ [ Adversarial Infrared Geometry: Using Geometry to Perform Adversarial  Attack against Infrared Pedestrian Detectors](https://arxiv.org/abs/2403.03674)

	Kalibinuer Tiliwalidi


+ [ Advancing Out-of-Distribution Detection through Data Purification and  Dynamic Activation Function Design](https://arxiv.org/abs/2403.03412)

	Yingrui Ji, Yao Zhu, Zhigang Li, Jiansheng Chen, Yunlong Kong, Jingbo Chen


+ [ Probing the Robustness of Time-series Forecasting Models with  CounterfacTS](https://arxiv.org/abs/2403.03508)

	Håkon Hanisch Kjærnli, Lluis Mas-Ribas, Aida Ashrafi, Gleb Sizov, Helge Langseth, Odd Erik Gundersen


+ [ Learning Adversarial MDPs with Stochastic Hard Constraints](https://arxiv.org/abs/2403.03672)

	Francesco Emanuele Stradi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti


+ [ Verified Training for Counterfactual Explanation Robustness under Data  Shift](https://arxiv.org/abs/2403.03773)

	Anna P. Meyer, Yuhao Zhang, Aws Albarghouthi, Loris D'Antoni


+ [ DeepEclipse: How to Break White-Box DNN-Watermarking Schemes](https://arxiv.org/abs/2403.03590)

	Alessandro Pegoraro, Carlotta Segna, Kavita Kumari, Ahmad-Reza Sadeghi


+ [ Neural Exec: Learning (and Learning from) Execution Triggers for Prompt  Injection Attacks](https://arxiv.org/abs/2403.03792)

	Dario Pasquini, Martin Strohmeier, Carmela Troncoso


+ [ Unsupervised Contrastive Learning for Robust RF Device Fingerprinting  Under Time-Domain Shift](https://arxiv.org/abs/2403.04036)

	Jun Chen, Weng-Keen Wong, Bechir Hamdaoui


+ [ Improving Adversarial Training using Vulnerability-Aware Perturbation  Budget](https://arxiv.org/abs/2403.04070)

	Olukorede Fakorede, Modeste Atsague, Jin Tian


+ [ Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability](https://arxiv.org/abs/2403.03967)

	Rajdeep Haldar, Yue Xing, Qifan Song


+ [ Belief-Enriched Pessimistic Q-Learning against Adversarial State  Perturbations](https://arxiv.org/abs/2403.04050)

	Xiaolin Sun, Zizhan Zheng


+ [ Fooling Neural Networks for Motion Forecasting via Adversarial Attacks](https://arxiv.org/abs/2403.04954)

	Edgar Medina, Leyong Loh


## 2024-03-05
+ [ FLGuard: Byzantine-Robust Federated Learning via Ensemble of Contrastive  Models](https://arxiv.org/abs/2403.02846)

	Younghan Lee, Yungi Cho, Woorim Han, Ho Bae, Yunheung Paek


+ [ Recall-Oriented Continual Learning with Generative Adversarial  Meta-Model](https://arxiv.org/abs/2403.03082)

	Haneol Kang, Dong-Wan Choi


+ [ Towards Robust Federated Learning via Logits Calibration on Non-IID Data](https://arxiv.org/abs/2403.02803)

	Yu Qiao, Apurba Adhikary, Chaoning Zhang, Choong Seon Hong


+ [ XAI-Based Detection of Adversarial Attacks on Deepfake Detectors](https://arxiv.org/abs/2403.02955)

	Ben Pinhasov, Raz Lapid, Rony Ohayon, Moshe Sipper, Yehudit Aperstein


+ [ Here Comes The AI Worm: Unleashing Zero-click Worms that Target  GenAI-Powered Applications](https://arxiv.org/abs/2403.02817)

	Stav Cohen, Ron Bitton, Ben Nassi


+ [ Enhancing Security in Federated Learning through Adaptive  Consensus-Based Model Update Validation](https://arxiv.org/abs/2403.04803)

	Zahir Alsulaimawi


## 2024-03-03
+ [ GuardT2I: Defending Text-to-Image Models from Adversarial Prompts](https://arxiv.org/abs/2403.01446)

	Yijun Yang, Ruiyuan Gao, Xiao Yang, Jianyuan Zhong, Qiang Xu


+ [ Breaking Down the Defenses: A Comparative Survey of Attacks on Large  Language Models](https://arxiv.org/abs/2403.04786)

	Arijit Ghosh Chowdhury, Md Mofijul Islam, Vaibhav Kumar, Faysal Hossain Shezan, Vaibhav Kumar, Vinija Jain, Aman Chadha


## 2024-03-02
+ [ Query Recovery from Easy to Hard: Jigsaw Attack against SSE](https://arxiv.org/abs/2403.01155)

	Hao Nie, Wei Wang, Peng Xu, Xianglong Zhang, Laurence T. Yang, Kaitai Liang


+ [ AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks](https://arxiv.org/abs/2403.04783)

	Yifan Zeng, Yiran Wu, Xiao Zhang, Huazheng Wang, Qingyun Wu


## 2024-03-01
+ [ AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language  Model Outputs](https://arxiv.org/abs/2403.00198)

	Sana Ebrahimi, Kaiwen Chen, Abolfazl Asudeh, Gautam Das, Nick Koudas


+ [ Robust Deep Reinforcement Learning Through Adversarial Attacks and  Training : A Survey](https://arxiv.org/abs/2403.00420)

	Lucas Schott, Josephine Delas, Hatem Hajri, Elies Gherbi, Reda Yaich, Nora Boulahia-Cuppens, Frederic Cuppens, Sylvain Lamprier


+ [ DPP-Based Adversarial Prompt Searching for Lanugage Models](https://arxiv.org/abs/2403.00292)

	Xu Zhang, Xiaojun Wan


+ [ Robustifying a Policy in Multi-Agent RL with Diverse Cooperative  Behavior and Adversarial Style Sampling for Assistive Tasks](https://arxiv.org/abs/2403.00344)

	Tayuki Osa, Tatsuya Harada


+ [ Attacking Delay-based PUFs with Minimal Adversary Model](https://arxiv.org/abs/2403.00464)

	Hongming Fei, Owen Millwood, Prosanta Gope, Jack Miskelly, Biplab Sikdar



## 2024-02-29
+ [ Differentially Private Worst-group Risk Minimization](https://arxiv.org/abs/2402.19437)

	Xinyu Zhou, Raef Bassily


+ [ Utilizing Local Hierarchy with Adversarial Training for Hierarchical  Text Classification](https://arxiv.org/abs/2402.18825)

	Zihan Wang, Peiyi Wang, Houfeng Wang


+ [ MPAT: Building Robust Deep Neural Networks against Textual Adversarial  Attacks](https://arxiv.org/abs/2402.18792)

	Fangyuan Zhang, Huichi Zhou, Shuangjiao Li, Hongtao Wang


+ [ PRSA: Prompt Reverse Stealing Attacks against Large Language Models](https://arxiv.org/abs/2402.19200)

	Yong Yang, Xuhong Zhang, Yi Jiang, Xi Chen, Haoyu Wang, Shouling Ji, Zonghui Wang


+ [ Unraveling Adversarial Examples against Speaker Identification --  Techniques for Attack Detection and Victim Model Classification](https://arxiv.org/abs/2402.19355)

	Sonal Joshi, Thomas Thebaud, Jesús Villalba, Najim Dehak


+ [ Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on  Pre-trained Language Models](https://arxiv.org/abs/2402.18945)

	Pengzhou Cheng, Wei Du, Zongru Wu, Fengwei Zhang, Libo Chen, Gongshen Liu


+ [ Watermark Stealing in Large Language Models](https://arxiv.org/abs/2402.19361)

	Nikola Jovanović, Robin Staab, Martin Vechev


+ [ PrivatEyes: Appearance-based Gaze Estimation Using Federated Secure  Multi-Party Computation](https://arxiv.org/abs/2402.18970)

	Mayar Elfares, Pascal Reisert, Zhiming Hu, Wenwu Tang, Ralf Küsters, Andreas Bulling


+ [ Typographic Attacks in Large Multimodal Models Can be Alleviated by More  Informative Prompts](https://arxiv.org/abs/2402.19150)

	Hao Cheng, Erjia Xiao, Renjing Xu


+ [ Assessing Visually-Continuous Corruption Robustness of Neural Networks  Relative to Human Performance](https://arxiv.org/abs/2402.19401)

	Huakun Shen, Boyue Caroline Hu, Krzysztof Czarnecki, Lina Marsso, Marsha Chechik


+ [ Verification of Neural Networks' Global Robustness](https://arxiv.org/abs/2402.19322)

	Anan Kabaha, Dana Drachsler-Cohen


+ [ Whispers that Shake Foundations: Analyzing and Mitigating False Premise  Hallucinations in Large Language Models](https://arxiv.org/abs/2402.19103)

	Hongbang Yuan, Pengfei Cao, Zhuoran Jin, Yubo Chen, Daojian Zeng, Kang Liu, Jun Zhao


+ [ Pointing out the Shortcomings of Relation Extraction Models with  Semantically Motivated Adversarials](https://arxiv.org/abs/2402.19076)

	Gennaro Nolano, Moritz Blum, Basil Ell, Philipp Cimiano


+ [ LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario](https://arxiv.org/abs/2403.00108)

	Hongyi Liu, Zirui Liu, Ruixiang Tang, Jiayi Yuan, Shaochen Zhong, Yu-Neng Chuang, Li Li, Rui Chen, Xia Hu


## 2024-02-28
+ [ Making Them Ask and Answer: Jailbreaking Large Language Models in Few  Queries via Disguise and Reconstruction](https://arxiv.org/abs/2402.18104)

	Tong Liu, Yingjie Zhang, Zhe Zhao, Yinpeng Dong, Guozhu Meng, Kai Chen


+ [ Enhancing Tracking Robustness with Auxiliary Adversarial Defense  Networks](https://arxiv.org/abs/2402.17976)

	Zhewei Wu, Ruilong Yu, Qihe Liu, Shuying Cheng, Shilin Qiu, Shijie Zhou


+ [ Catastrophic Overfitting: A Potential Blessing in Disguise](https://arxiv.org/abs/2402.18211)

	Mengnan Zhao, Lihe Zhang, Yuqiu Kong, Baocai Yin


+ [ A New Era in LLM Security: Exploring Security Concerns in Real-World  LLM-based Systems](https://arxiv.org/abs/2402.18649)

	Fangzhou Wu, Ning Zhang, Somesh Jha, Patrick McDaniel, Chaowei Xiao


+ [ Unveiling Privacy, Memorization, and Input Curvature Links](https://arxiv.org/abs/2402.18726)

	Deepak Ravikumar, Efstathia Soufleri, Abolfazl Hashemi, Kaushik Roy


+ [ Model Pairing Using Embedding Translation for Backdoor Attack Detection  on Open-Set Classification Tasks](https://arxiv.org/abs/2402.18718)

	Alexander Unnervik, Hatef Otroshi Shahreza, Anjith George, Sébastien Marcel


+ [ Pre-training Differentially Private Models with Limited Public Data](https://arxiv.org/abs/2402.18752)

	Zhiqi Bu, Xinwei Zhang, Mingyi Hong, Sheng Zha, George Karypis


+ [ Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An  Adversarial Perspective](https://arxiv.org/abs/2402.18607)

	Xinjian Luo, Yangfan Jiang, Fei Wei, Yuncheng Wu, Xiaokui Xiao, Beng Chin Ooi


## 2024-02-27
+ [ Speak Out of Turn: Safety Vulnerability of Large Language Models in  Multi-turn Dialogue](https://arxiv.org/abs/2402.17262)

	Zhenhong Zhou, Jiuyang Xiang, Haopeng Chen, Quan Liu, Zherui Li, Sen Su


+ [ FairBelief - Assessing Harmful Beliefs in Language Models](https://arxiv.org/abs/2402.17389)

	Mattia Setzu, Marta Marchiori Manerba, Pasquale Minervini, Debora Nozza


+ [ Extreme Miscalibration and the Illusion of Adversarial Robustness](https://arxiv.org/abs/2402.17509)

	Vyas Raina, Samson Tan, Volkan Cevher, Aditya Rawal, Sheng Zha, George Karypis


+ [ Enhancing Quality of Compressed Images by Mitigating Enhancement Bias  Towards Compression Domain](https://arxiv.org/abs/2402.17200)

	Qunliang Xing, Mai Xu, Shengxi Li, Xin Deng, Meisong Zheng, Huaida Liu, Ying Chen


+ [ Preserving Fairness Generalization in Deepfake Detection](https://arxiv.org/abs/2402.17229)

	Li Lin, Xinan He, Yan Ju, Xin Wang, Feng Ding, Shu Hu


+ [ Black-box Adversarial Attacks Against Image Quality Assessment Models](https://arxiv.org/abs/2402.17533)

	Yu Ran, Ao-Xiang Zhang, Mingjie Li, Weixuan Tang, Yuan-Gen Wang


+ [ Structure-Guided Adversarial Training of Diffusion Models](https://arxiv.org/abs/2402.17563)

	Ling Yang, Haotian Qian, Zhilong Zhang, Jingwei Liu, Bin Cui


+ [ Towards Fairness-Aware Adversarial Learning](https://arxiv.org/abs/2402.17729)

	Yanghao Zhang, Tianle Zhang, Ronghui Mu, Xiaowei Huang, Wenjie Ruan


+ [ Model X-ray:Detect Backdoored Models via Decision Boundary](https://arxiv.org/abs/2402.17465)

	Yanghao Su, Jie Zhang, Ting Xu, Tianwei Zhang, Weiming Zhang, Nenghai Yu


+ [ Robustness-Congruent Adversarial Training for Secure Machine Learning  Model Updates](https://arxiv.org/abs/2402.17390)

	Daniele Angioni, Luca Demetrio, Maura Pintor, Luca Oneto, Davide Anguita, Battista Biggio, Fabio Roli


+ [ Evaluation of Predictive Reliability to Foster Trust in Artificial  Intelligence. A case study in Multiple Sclerosis](https://arxiv.org/abs/2402.17554)

	Lorenzo Peracchio, Giovanna Nicora, Enea Parimbelli, Tommaso Mario Buonocore, Roberto Bergamaschi, Eleonora Tavazzi, Arianna Dagliati, Riccardo Bellazzi


+ [ LLM-Resistant Math Word Problem Generation via Adversarial Attacks](https://arxiv.org/abs/2402.17916)

	Roy Xie, Chengxuan Huang, Junlin Wang, Bhuwan Dhingra


+ [ Adversarial example soups: averaging multiple adversarial examples  improves transferability without increasing additional generation time](https://arxiv.org/abs/2402.18370)

	Bo Yang, Hengwei Zhang, Chenwei Li, Jindong Wang


## 2024-02-26
+ [ Referee Can Play: An Alternative Approach to Conditional Generation via  Model Inversion](https://arxiv.org/abs/2402.16305)

	Xuantong Liu, Tianyang Hu, Wenjia Wang, Kenji Kawaguchi, Yuan Yao


+ [ Investigating Deep Watermark Security: An Adversarial Transferability  Perspective](https://arxiv.org/abs/2402.16397)

	Biqing Qi, Junqi Gao, Yiang Luo, Jianxing Liu, Ligang Wu, Bowen Zhou


+ [ Training Implicit Generative Models via an Invariant Statistical Loss](https://arxiv.org/abs/2402.16435)

	José Manuel de Frutos, Pablo M. Olmos, Manuel A. Vázquez, Joaquín Míguez


+ [ CodeChameleon: Personalized Encryption Framework for Jailbreaking Large  Language Models](https://arxiv.org/abs/2402.16717)

	Huijie Lv, Xiao Wang, Yuansen Zhang, Caishuang Huang, Shihan Dou, Junjie Ye, Tao Gui, Qi Zhang, Xuanjing Huang


+ [ Immunization against harmful fine-tuning attacks](https://arxiv.org/abs/2402.16382)

	Domenic Rosati, Jan Wehner, Kai Williams, Łukasz Bartoszcze, Jan Batzner, Hassan Sajjad, Frank Rudzicz


+ [ RoCoIns: Enhancing Robustness of Large Language Models through  Code-Style Instructions](https://arxiv.org/abs/2402.16431)

	Yuansen Zhang, Xiao Wang, Zhiheng Xi, Han Xia, Tao Gui, Qi Zhang, Xuanjing Huang


+ [ ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable  Safety Detectors](https://arxiv.org/abs/2402.16444)

	Zhexin Zhang, Yida Lu, Jingyuan Ma, Di Zhang, Rui Li, Pei Ke, Hao Sun, Lei Sha, Zhifang Sui, Hongning Wang, Minlie Huang


+ [ Unveiling Vulnerability of Self-Attention](https://arxiv.org/abs/2402.16470)

	Khai Jiet Liong, Hongqiu Wu, Hai Zhao


+ [ Edge Detectors Can Make Deep Convolutional Neural Networks More Robust](https://arxiv.org/abs/2402.16479)

	Jin Ding, Jie-Chao Zhao, Yong-Zhi Sun, Ping Tan, Jia-Wei Wang, Ji-En Ma, You-Tong Fang


+ [ Improving the JPEG-resistance of Adversarial Attacks on Face Recognition  by Interpolation Smoothing](https://arxiv.org/abs/2402.16586)

	Kefu Guo, Fengfan Zhou, Hefei Ling, Ping Li, Hui Liu


+ [ On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing  Problem](https://arxiv.org/abs/2402.16926)

	Georg Pichler, Marco Romanelli, Divya Prakash Manivannan, Prashanth Krishnamurthy, Farshad Khorrami, Siddharth Garg


+ [ A Curious Case of Remarkable Resilience to Gradient Attacks via Fully  Convolutional and Differentiable Front End with a Skip Connection](https://arxiv.org/abs/2402.17018)

	Leonid Boytsov, Ameya Joshi, Filipe Condessa


## 2024-02-25
+ [ From Noise to Clarity: Unraveling the Adversarial Suffix of Large  Language Model Attacks via Translation of Text Embeddings](https://arxiv.org/abs/2402.16006)

	Hao Wang, Hao Li, Minlie Huang, Lei Sha


+ [ Defending Large Language Models against Jailbreak Attacks via Semantic  Smoothing](https://arxiv.org/abs/2402.16192)

	Jiabao Ji, Bairu Hou, Alexander Robey, George J. Pappas, Hamed Hassani, Yang Zhang, Eric Wong, Shiyu Chang


+ [ Adversarial-Robust Transfer Learning for Medical Imaging via Domain  Assimilation](https://arxiv.org/abs/2402.16005)

	Xiaohui Chen, Tie Luo


+ [ DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM  Jailbreakers](https://arxiv.org/abs/2402.16914)

	Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, Cho-Jui Hsieh


## 2024-02-24
+ [ LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A  Vision Paper](https://arxiv.org/abs/2402.15727)

	Daoyuan Wu, Shuai Wang, Yang Liu, Ning Liu


+ [ RAUCA: A Novel Physical Adversarial Attack on Vehicle Detectors via  Robust and Accurate Camouflage Generation](https://arxiv.org/abs/2402.15853)

	Jiawei Zhou, Linye Lyu, Daojing He, Yu Li


## 2024-02-23
+ [ On the Duality Between Sharpness-Aware Minimization and Adversarial  Training](https://arxiv.org/abs/2402.15152)

	Yihao Zhang, Hangzhou He, Jingyu Zhu, Huanran Chen, Yifei Wang, Zeming Wei


+ [ ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion  Models against Stochastic Perturbation](https://arxiv.org/abs/2402.15429)

	Yi Zhang, Yun Tang, Wenjie Ruan, Xiaowei Huang, Siddartha Khastgir, Paul Jennings, Xingyu Zhao


+ [ A First Look at GPT Apps: Landscape and Vulnerability](https://arxiv.org/abs/2402.15105)

	Zejun Zhang, Li Zhang, Xin Yuan, Anlan Zhang, Mengwei Xu, Feng Qian


+ [ Fast Adversarial Attacks on Language Models In One GPU Minute](https://arxiv.org/abs/2402.15570)

	Vinu Sankar Sadasivan, Shoumik Saha, Gaurang Sriramanan, Priyatham Kattakinda, Atoosa Chegini, Soheil Feizi


+ [ Distilling Adversarial Robustness Using Heterogeneous Teachers](https://arxiv.org/abs/2402.15586)

	Jieren Deng, Aaron Palmer, Rigel Mahmood, Ethan Rathbun, Jinbo Bi, Kaleel Mahmood, Derek Aguiar


+ [ Low-Frequency Black-Box Backdoor Attack via Evolutionary Algorithm](https://arxiv.org/abs/2402.15653)

	Yanqi Qiao, Dazhuang Liu, Rui Wang, Kaitai Liang


+ [ The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented  Generation (RAG)](https://arxiv.org/abs/2402.16893)

	Shenglai Zeng, Jiankun Zhang, Pengfei He, Yue Xing, Yiding Liu, Han Xu, Jie Ren, Shuaiqiang Wang, Dawei Yin, Yi Chang, Jiliang Tang


## 2024-02-22
+ [ Rethinking Invariance Regularization in Adversarial Training to Improve  Robustness-Accuracy Trade-off](https://arxiv.org/abs/2402.14648)

	Futa Waseda, Isao Echizen


+ [ Does the Generator Mind its Contexts? An Analysis of Generative Model  Faithfulness under Context Transfer](https://arxiv.org/abs/2402.14488)

	Xinshuo Hu, Baotian Hu, Dongfang Li, Xiaoguang Li, Lifeng Shang


+ [ Noise-BERT: A Unified Perturbation-Robust Framework with Noise Alignment  Pre-training for Noisy Slot Filling Task](https://arxiv.org/abs/2402.14494)

	Jinxu Zhao, Guanting Dong, Yueyan Qiu, Tingfeng Hui, Xiaoshuai Song, Daichi Guo, Weiran Xu


+ [ Quadruplet Loss For Improving the Robustness to Face Morphing Attacks](https://arxiv.org/abs/2402.14665)

	Iurii Medvedev, Nuno Gonçalves


+ [ BeTAIL: Behavior Transformer Adversarial Imitation Learning from Human  Racing Gameplay](https://arxiv.org/abs/2402.14194)

	Catherine Weaver, Chen Tang, Ce Hao, Kenta Kawamoto, Masayoshi Tomizuka, Wei Zhan


+ [ COBIAS: Contextual Reliability in Bias Assessment](https://arxiv.org/abs/2402.14889)

	Priyanshul Govil, Vamshi Krishna Bonagiri, Manas Gaur, Ponnurangam Kumaraguru, Sanorita Dey


+ [ Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning  Meets Adversarial Images](https://arxiv.org/abs/2402.14899)

	Zefeng Wang, Zhen Han, Shuo Chen, Fan Xue, Zifeng Ding, Xun Xiao, Volker Tresp, Philip Torr, Jindong Gu


+ [ Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment](https://arxiv.org/abs/2402.14968)

	Jiongxiao Wang, Jiazhao Li, Yiquan Li, Xiangyu Qi, Muhao Chen, Junjie Hu, Yixuan Li, Bo Li, Chaowei Xiao


+ [ Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models](https://arxiv.org/abs/2402.14977)

	Hongbin Liu, Michael K. Reiter, Neil Zhenqiang Gong


+ [ SoK: Analyzing Adversarial Examples: A Framework to Study Adversary  Knowledge](https://arxiv.org/abs/2402.14937)

	Lucas Fenaux, Florian Kerschbaum


## 2024-02-21
+ [ Large Language Models are Vulnerable to Bait-and-Switch Attacks for  Generating Harmful Content](https://arxiv.org/abs/2402.13926)

	Federico Bianchi, James Zou


+ [ SDXL-Lightning: Progressive Adversarial Diffusion Distillation](https://arxiv.org/abs/2402.13929)

	Shanchuan Lin, Anran Wang, Xiao Yang


+ [ GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient  Analysis](https://arxiv.org/abs/2402.13494)

	Yueqi Xie, Minghong Fang, Renjie Pi, Neil Gong


+ [ Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on  Zero-shot LLM Assessment](https://arxiv.org/abs/2402.14016)

	Vyas Raina, Adian Liusie, Mark Gales


+ [ Learning to Poison Large Language Models During Instruction Tuning](https://arxiv.org/abs/2402.13459)

	Yao Qiang, Xiangyu Zhou, Saleh Zare Zade, Mohammad Amin Roshani, Douglas Zytko, Dongxiao Zhu


+ [ Coercing LLMs to do and reveal (almost) anything](https://arxiv.org/abs/2402.14020)

	Jonas Geiping, Alex Stein, Manli Shu, Khalid Saifullah, Yuxin Wen, Tom Goldstein


+ [ Robustness of Deep Neural Networks for Micro-Doppler Radar  Classification](https://arxiv.org/abs/2402.13651)

	Mikolaj Czerkawski, Carmine Clemente, Craig MichieCraig Michie, Christos Tachtatzis


+ [ VL-Trojan: Multimodal Instruction Backdoor Attacks against  Autoregressive Visual Language Models](https://arxiv.org/abs/2402.13851)

	Jiawei Liang, Siyuan Liang, Man Luo, Aishan Liu, Dongchen Han, Ee-Chien Chang, Xiaochun Cao


+ [ Stealthy Adversarial Attacks on Stochastic Multi-Armed Bandits](https://arxiv.org/abs/2402.13487)

	Zhiwei Wang, Huazheng Wang, Hongning Wang


+ [ AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement  Learning](https://arxiv.org/abs/2402.13946)

	Vasudev Gohil, Satwik Patnaik, Dileep Kalathil, Jeyavijayan Rajendran


+ [ Uncertainty-driven and Adversarial Calibration Learning for Epicardial  Adipose Tissue Segmentation](https://arxiv.org/abs/2402.14349)

	Kai Zhao, Zhiming Liu, Jiaqi Liu, Jingbiao Zhou, Bihong Liao, Huifang Tang, Qiuyu Wang, Chunquan Li


+ [ Fake Resume Attacks: Data Poisoning on Online Job Platforms](https://arxiv.org/abs/2402.14124)

	Michiharu Yamashita, Thanh Tran, Dongwon Lee


+ [ Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts  Against Open-source LLMs](https://arxiv.org/abs/2402.14872)

	Xiaoxia Li, Siyuan Liang, Jiyi Zhang, Han Fang, Aishan Liu, Ee-Chien Chang



## 2024-02-20
+ [ TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box  Identification](https://arxiv.org/abs/2402.12991)

	Martin Gubri, Dennis Ulmer, Hwaran Lee, Sangdoo Yun, Seong Joon Oh


+ [ VGMShield: Mitigating Misuse of Video Generative Models](https://arxiv.org/abs/2402.13126)

	Yan Pang, Yang Zhang, Tianhao Wang


+ [ Indiscriminate Data Poisoning Attacks on Pre-trained Feature Extractors](https://arxiv.org/abs/2402.12626)

	Yiwei Lu, Matthew Y.R. Yang, Gautam Kamath, Yaoliang Yu


+ [ Beyond Worst-case Attacks: Robust RL with Adaptive Defense via  Non-dominated Policies](https://arxiv.org/abs/2402.12673)

	Xiangyu Liu, Chenghao Deng, Yanchao Sun, Yongyuan Liang, Furong Huang


+ [ Defending Jailbreak Prompts via In-Context Adversarial Game](https://arxiv.org/abs/2402.13148)

	Yujun Zhou, Yufei Han, Haomin Zhuang, Taicheng Guo, Kehan Guo, Zhenwen Liang, Hongyan Bao, Xiangliang Zhang


## 2024-02-19
+ [ ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs](https://arxiv.org/abs/2402.11753)

	Fengqing Jiang, Zhangchen Xu, Luyao Niu, Zhen Xiang, Bhaskar Ramasubramanian, Bo Li, Radha Poovendran


+ [ ChatGPT Based Data Augmentation for Improved Parameter-Efficient  Debiasing of LLMs](https://arxiv.org/abs/2402.11764)

	Pengrui Han, Rafal Kocielnik, Adhithya Saravanan, Roy Jiang, Or Sharir, Anima Anandkumar


+ [ Acquiring Clean Language Models from Backdoor Poisoned Datasets by  Downscaling Frequency Space](https://arxiv.org/abs/2402.12026)

	Zongru Wu, Zhuosheng Zhang, Pengzhou Cheng, Gongshen Liu


+ [ Defending Against Weight-Poisoning Backdoor Attacks for  Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.12168)

	Shuai Zhao, Leilei Gan, Luu Anh Tuan, Jie Fu, Lingjuan Lyu, Meihuizi Jia, Jinming Wen


+ [ Dynamic Environment Responsive Online Meta-Learning with Fairness  Awareness](https://arxiv.org/abs/2402.12319)

	Chen Zhao, Feng Mi, Xintao Wu, Kai Jiang, Latifur Khan, Feng Chen


+ [ Query-Based Adversarial Prompt Generation](https://arxiv.org/abs/2402.12329)

	Jonathan Hayase, Ema Borevkovic, Nicholas Carlini, Florian Tramèr, Milad Nasr


+ [ AICAttack: Adversarial Image Captioning Attack with Attention-Based  Optimization](https://arxiv.org/abs/2402.11940)

	Jiyao Li, Mingze Ni, Yifei Dong, Tianqing Zhu, Wei Liu


+ [ Adversarial Feature Alignment: Balancing Robustness and Accuracy in Deep  Learning via Adversarial Training](https://arxiv.org/abs/2402.12187)

	Leo Hyun Park, Jaeuk Kim, Myung Gyo Oh, Jaewoo Park, Taekyoung Kwon


+ [ Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models](https://arxiv.org/abs/2402.11989)

	Zihao Luo, Xilie Xu, Feng Liu, Yun Sing Koh, Di Wang, Jingfeng Zhang


+ [ Emulated Disalignment: Safety Alignment for Large Language Models May  Backfire!](https://arxiv.org/abs/2402.12343)

	Zhanhui Zhou, Jie Liu, Zhichen Dong, Jiaheng Liu, Chao Yang, Wanli Ouyang, Yu Qiao


+ [ Attacks on Node Attributes in Graph Neural Networks](https://arxiv.org/abs/2402.12426)

	Ying Xu, Michael Lanier, Anindya Sarkar, Yevgeniy Vorobeychik


## 2024-02-18
+ [ How Susceptible are Large Language Models to Ideological Manipulation?](https://arxiv.org/abs/2402.11725)

	Kai Chen, Zihao He, Jun Yan, Taiwei Shi, Kristina Lerman


+ [ Stealthy Attack on Large Language Model based Recommendation](https://arxiv.org/abs/2402.14836)

	Jinghao Zhang, Yuting Liu, Qiang Liu, Shu Wu, Guibing Guo, Liang Wang


## 2024-02-17
+ [ Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated  Text Detection](https://arxiv.org/abs/2402.11167)

	Fan Huang, Haewoon Kwak, Jisun An


+ [ Maintaining Adversarial Robustness in Continuous Learning](https://arxiv.org/abs/2402.11196)

	Xiaolei Ru, Xiaowei Cao, Zijia Liu, Jack Murdoch Moore, Xin-Ya Zhang, Xia Zhu, Wenjia Wei, Gang Yan


+ [ Disclosure and Mitigation of Gender Bias in LLMs](https://arxiv.org/abs/2402.11190)

	Xiangjue Dong, Yibo Wang, Philip S. Yu, James Caverlee


+ [ A White-Box False Positive Adversarial Attack Method on Contrastive Loss  Based Offline Handwritten Signature Verification Models](https://arxiv.org/abs/2308.08925)

	Zhongliang Guo, Weiye Li, Yifei Qian, Ognjen Arandjelović, Lei Fang


## 2024-02-16
+ [ Connect the dots: Dataset Condensation, Differential Privacy, and  Adversarial Uncertainty](https://arxiv.org/abs/2402.10423)

	Kenneth Odoh


+ [ Adversarial Curriculum Graph Contrastive Learning with Pair-wise  Augmentation](https://arxiv.org/abs/2402.10468)

	Xinjian Zhao, Liang Zhang, Yang Liu, Ruocheng Guo, Xiangyu Zhao


+ [ Zero-shot sampling of adversarial entities in biomedical question  answering](https://arxiv.org/abs/2402.10527)

	R. Patrick Xian, Alex J. Lee, Vincent Wang, Qiming Cui, Russell Ro, Reza Abbasi-Asl


+ [ Universal Prompt Optimizer for Safe Text-to-Image Generation](https://arxiv.org/abs/2402.10882)

	Zongyu Wu, Hongcheng Gao, Yueze Wang, Xiang Zhang, Suhang Wang


+ [ Uncertainty, Calibration, and Membership Inference Attacks: An  Information-Theoretic Perspective](https://arxiv.org/abs/2402.10686)

	Meiyi Zhu, Caili Guo, Chunyan Feng, Osvaldo Simeone


+ [ ASGEA: Exploiting Logic Rules from Align-Subgraphs for Entity Alignment](https://arxiv.org/abs/2402.11000)

	Yangyifei Luo, Zhuo Chen, Lingbing Guo, Qian Li, Wenxuan Zeng, Zhixin Cai, Jianxin Li


+ [ The Male CEO and the Female Assistant: Probing Gender Biases in  Text-To-Image Models Through Paired Stereotype Test](https://arxiv.org/abs/2402.11089)

	Yixin Wan, Kai-Wei Chang


+ [ VQAttack: Transferable Adversarial Attacks on Visual Question Answering  via Pre-trained Models](https://arxiv.org/abs/2402.11083)

	Ziyi Yin, Muchao Ye, Tianrong Zhang, Jiaqi Wang, Han Liu, Jinghui Chen, Ting Wang, Fenglong Ma


+ [ DART: A Principled Approach to Adversarially Robust Unsupervised Domain  Adaptation](https://arxiv.org/abs/2402.11120)

	Yunjuan Wang, Hussein Hazimeh, Natalia Ponomareva, Alexey Kurakin, Ibrahim Hammoud, Raman Arora


## 2024-02-15
+ [ Generating Visual Stimuli from EEG Recordings using Transformer-encoder  based EEG encoder and GAN](https://arxiv.org/abs/2402.10115)

	Rahul Mishra, Arnav Bhavsar


+ [ Improving EEG Signal Classification Accuracy Using Wasserstein  Generative Adversarial Networks](https://arxiv.org/abs/2402.09453)

	Joshua Park, Priyanshu Mahey, Ore Adeniyi


+ [ Examining Pathological Bias in a Generative Adversarial Network  Discriminator: A Case Study on a StyleGAN3 Model](https://arxiv.org/abs/2402.09786)

	Alvin Grissom II, Ryan F. Lei, Jeova Farias Sales Rocha Neto, Bailey Lin, Ryan Trotter


+ [ A Trembling House of Cards? Mapping Adversarial Attacks against Language  Agents](https://arxiv.org/abs/2402.10196)

	Lingbo Mo, Zeyi Liao, Boyuan Zheng, Yu Su, Chaowei Xiao, Huan Sun


+ [ Align before Attend: Aligning Visual and Textual Features for Multimodal  Hateful Content Detection](https://arxiv.org/abs/2402.09738)

	Eftekhar Hossain, Omar Sharif, Mohammed Moshiul Hoque, Sarah M. Preum


+ [ Camouflage is all you need: Evaluating and Enhancing Language Model  Robustness Against Camouflage Adversarial Attacks](https://arxiv.org/abs/2402.09874)

	Álvaro Huertas-García, Alejandro Martín, Javier Huertas-Tato, David Camacho


+ [ FedRDF: A Robust and Dynamic Aggregation Function against Poisoning  Attacks in Federated Learning](https://arxiv.org/abs/2402.10082)

	Enrique Mármol Campos, Aurora González Vidal, José Luis Hernández Ramos, Antonio Skarmeta


+ [ Unlocking the Potential of Transformers in Time Series Forecasting with  Sharpness-Aware Minimization and Channel-Wise Attention](https://arxiv.org/abs/2402.10198)

	Romain Ilbert, Ambroise Odonnat, Vasilii Feofanov, Aladin Virmaux, Giuseppe Paolo, Themis Palpanas, Ievgen Redko


+ [ PAL: Proxy-Guided Black-Box Attack on Large Language Models](https://arxiv.org/abs/2402.09674)

	Chawin Sitawarin, Norman Mu, David Wagner, Alexandre Araujo


+ [ Reward Poisoning Attack Against Offline Reinforcement Learning](https://arxiv.org/abs/2402.09695)

	Yinglun Xu, Rohan Gumaste, Gagandeep Singh


+ [ AbuseGPT: Abuse of Generative AI ChatBots to Create Smishing Campaigns](https://arxiv.org/abs/2402.09728)

	Ashfak Md Shibli, Mir Mehedi A. Pritom, Maanak Gupta


+ [ Privacy Attacks in Decentralized Learning](https://arxiv.org/abs/2402.10001)

	Abdellah El Mrini, Edwige Cyffers, Aurélien Bellet


+ [ How Much Does Each Datapoint Leak Your Privacy? Quantifying the  Per-datum Membership Leakage](https://arxiv.org/abs/2402.10065)

	Achraf Azize, Debabrota Basu


+ [ A Trembling House of Cards? Mapping Adversarial Attacks against Language  Agents](https://arxiv.org/abs/2402.10196)

	Lingbo Mo, Zeyi Liao, Boyuan Zheng, Yu Su, Chaowei Xiao, Huan Sun


+ [ Camouflage is all you need: Evaluating and Enhancing Language Model  Robustness Against Camouflage Adversarial Attacks](https://arxiv.org/abs/2402.09874)

	Álvaro Huertas-García, Alejandro Martín, Javier Huertas-Tato, David Camacho


+ [ Risk-Sensitive Soft Actor-Critic for Robust Deep Reinforcement Learning  under Distribution Shifts](https://arxiv.org/abs/2402.09992)

	Tobias Enders, James Harrison, Maximilian Schiffer


+ [ How Much Does Each Datapoint Leak Your Privacy? Quantifying the  Per-datum Membership Leakage](https://arxiv.org/abs/2402.10065)

	Achraf Azize, Debabrota Basu


+ [ Backdoor Attack against One-Class Sequential Anomaly Detection Models](https://arxiv.org/abs/2402.10283)

	He Cheng, Shuhan Yuan


## 2024-02-14
+ [Exploring the Adversarial Capabilities of Large Language Models](https://arxiv.org/abs/2402.09132)

	Lukas Struppek, Minh Hieu Le, Dominik Hintersdorf, Kristian Kersting


+ [Only My Model On My Data: A Privacy Preserving Approach Protecting one Model and Deceiving Unauthorized Black-Box Models](https://arxiv.org/abs/2402.09316)

	Weiheng Chai, Brian Testa, Huantao Ren, Asif Salekin, Senem Velipasalar


+ [Review-Incorporated Model-Agnostic Profile Injection Attacks on Recommender Systems](https://arxiv.org/abs/2402.09023)

	Shiyi Yang, Lina Yao, Chen Wang, Xiwei Xu, Liming Zhu


+ [Attacking Large Language Models with Projected Gradient Descent](https://arxiv.org/abs/2402.09154)

	Simon Geisler, Tom Wollschläger, M. H. I. Abdalla, Johannes Gasteiger, Stephan Günnemann


+ [Detecting Adversarial Spectrum Attacks via Distance to Decision Boundary Statistics](https://arxiv.org/abs/2402.08986)

	Wenwei Zhao, Xiaowen Li, Shangqing Zhao, Jie Xu, Yao Liu, Zhuo Lu


+ [SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding](https://arxiv.org/abs/2402.08983)

	Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Bill Yuchen Lin, Radha Poovendran


+ [Ten Words Only Still Help: Improving Black-Box AI-Generated Text Detection via Proxy-Guided Efficient Re-Sampling](https://arxiv.org/abs/2402.09199)

	Yuhui Shi, Qiang Sheng, Juan Cao, Hao Mi, Beizhe Hu, Danding Wang


+ [Towards Robust Model-Based Reinforcement Learning Against Adversarial Corruption](https://arxiv.org/abs/2402.08991)

	Chenlu Ye, Jiafan He, Quanquan Gu, Tong Zhang


+ [Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues](https://arxiv.org/abs/2402.09091)

	Zhiyuan Chang, Mingyang Li, Yi Liu, Junjie Wang, Qing Wang, Yang Liu


+ [ Why Does Differential Privacy with Large Epsilon Defend Against  Practical Membership Inference Attacks?](https://arxiv.org/abs/2402.09540)

	Andrew Lowy, Zhuohang Li, Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang


## 2024-02-13
+ [ Data Reconstruction Attacks and Defenses: A Systematic Evaluation](https://arxiv.org/abs/2402.09478)

	Sheng Liu, Zihan Wang, Qi Lei


+ [Faster Repeated Evasion Attacks in Tree Ensembles](https://arxiv.org/abs/2402.08586)

	Lorenzo Cascioli, Laurens Devos, Ondřej Kuželka, Jesse Davis


+ [Generating Universal Adversarial Perturbations for Quantum Classifiers](https://arxiv.org/abs/2402.08648)

	Gautham Anil, Vishnu Vinod, Apurva Narayan


+ [Enhancing Robustness of Indoor Robotic Navigation with Free-Space Segmentation Models Against Adversarial Attacks](https://arxiv.org/abs/2402.08763)

	Qiyuan An, Christos Sevastopoulos, Fillia Makedon


+ [COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability](https://arxiv.org/abs/2402.08679)

	Xingang Guo, Fangxu Yu, Huan Zhang, Lianhui Qin, Bin Hu


+ [Test-Time Backdoor Attacks on Multimodal Large Language Models](https://arxiv.org/abs/2402.08577)

	Dong Lu, Tianyu Pang, Chao Du, Qian Liu, Xianjun Yang, Min Lin


+ [Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast](https://arxiv.org/abs/2402.08567)

	Xiangming Gu, Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Ye Wang, Jing Jiang, Min Lin


+ [ Oracle-Efficient Differentially Private Learning with Public Data](https://arxiv.org/abs/2402.09483)

	Adam Block, Mark Bun, Rathin Desai, Abhishek Shetty, Steven Wu


## 2024-02-12
+ [ PANORAMIA: Privacy Auditing of Machine Learning Models without  Retraining](https://arxiv.org/abs/2402.09477)

	Mishaal Kazmi, Hadrien Lautraite, Alireza Akbari, Mauricio Soroco, Qiaoyue Tang, Tao Wang, Sébastien Gambs, Mathias Lécuyer

+ [Understanding Deep Learning defenses Against Adversarial Examples Through Visualizations for Dynamic Risk Assessment](https://arxiv.org/abs/2402.07496)

	Xabier Echeberria-Barrio, Amaia Gil-Lerchundi, Jon Egana-Zubia, Raul Orduna-Urrutia


+ [Topological safeguard for evasion attack interpreting the neural networks' behavior](https://arxiv.org/abs/2402.07480)

	Xabier Echeberria-Barrio, Amaia Gil-Lerchundi, Iñigo Mendialdua, Raul Orduna-Urrutia


+ [PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2402.07867)

	Wei Zou, Runpeng Geng, Binghui Wang, Jinyuan Jia


+ [OrderBkd: Textual backdoor attack through repositioning](https://arxiv.org/abs/2402.07689)

	Irina Alekseevskaia, Konstantin Arkhipenko


+ [Customizable Perturbation Synthesis for Robust SLAM Benchmarking](https://arxiv.org/abs/2402.08125)

	Xiaohao Xu, Tianyi Zhang, Sibo Wang, Xiang Li, Yongqi Chen, Ye Li, Bhiksha Raj, Matthew Johnson-Roberson, Xiaonan Huang


+ [Multi-Attribute Vision Transformers are Efficient and Robust Learners](https://arxiv.org/abs/2402.08070)

	Hanan Gani, Nada Saadi, Noor Hussein, Karthik Nandakumar


+ [Do Membership Inference Attacks Work on Large Language Models?](https://arxiv.org/abs/2402.07841)

	Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, Hannaneh Hajishirzi


+ [NeuralSentinel: Safeguarding Neural Network Reliability and Trustworthiness](https://arxiv.org/abs/2402.07506)

	Xabier Echeberria-Barrio, Mikel Gorricho, Selene Valencia, Francesco Zola


## 2024-02-11
+ [Accuracy of TextFooler black box adversarial attacks on 01 loss sign activation neural network ensemble](https://arxiv.org/abs/2402.07347)

	Yunzhe Xue, Usman Roshan

+ [A Random Ensemble of Encrypted Vision Transformers for Adversarially Robust Defense](https://arxiv.org/abs/2402.07183)

	Ryota Iijima, Sayaka Shiota, Hitoshi Kiya


## 2024-02-10
+ [Architectural Neural Backdoors from First Principles](https://arxiv.org/abs/2402.06957)

	Harry Langford, Ilia Shumailov, Yiren Zhao, Robert Mullins, Nicolas Papernot


## 2024-02-09
+ [TETRIS: Towards Exploring the Robustness of Interactive Segmentation](https://arxiv.org/abs/2402.06132)

	Andrey Moskalenko, Vlad Shakhuro, Anna Vorontsova, Anton Konushin, Anton Antonov, Alexander Krapukhin, Denis Shepelev, Konstantin Soshin

+ [RAMP: Boosting Adversarial Robustness Against Multiple lp Perturbations](https://arxiv.org/abs/2402.06827)

	Enyi Jiang, Gagandeep Singh


+ [Anomaly Unveiled: Securing Image Classification against Adversarial Patch Attacks](https://arxiv.org/abs/2402.06249)

	Nandish Chattopadhyay, Amira Guesmi, Muhammad Shafique


+ [Evaluating Membership Inference Attacks and Defenses in Federated Learning](https://arxiv.org/abs/2402.06289)

	Gongxi Zhu, Donghao Li, Hanlin Gu, Yuxing Han, Yuan Yao, Lixin Fan, Qiang Yang


+ [Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning](https://arxiv.org/abs/2402.06255)

	Yichuan Mo, Yuji Wang, Zeming Wei, Yisen Wang

+ [StruQ: Defending Against Prompt Injection with Structured Queries](https://arxiv.org/abs/2402.06363)

	Sizhe Chen, Julien Piet, Chawin Sitawarin, David Wagner


+ [Quantifying and Enhancing Multi-modal Robustness with Modality Preference](https://arxiv.org/abs/2402.06244)

	Zequn Yang, Yake Wei, Ce Liang, Di Hu


## 2024-02-08
+ [Reinforcement Learning as a Catalyst for Robust and Fair Federated Learning: Deciphering the Dynamics of Client Contributions](https://arxiv.org/abs/2402.05541)

	Jialuo He, Wei Chen, Xiaojin Zhang


+ [Is Adversarial Training with Compressed Datasets Effective?](https://arxiv.org/abs/2402.05675)

	Tong Chen, Raghavendra Selvan

+ [Investigating White-Box Attacks for On-Device Models](https://arxiv.org/abs/2402.05493)

	Mingyi Zhou, Xiang Gao, Jing Wu, Kui Liu, Hailong Sun, Li Li


+ [Comprehensive Assessment of Jailbreak Attacks Against LLMs](https://arxiv.org/abs/2402.05668)

	Junjie Chu, Yugeng Liu, Ziqing Yang, Xinyue Shen, Michael Backes, Yang Zhang


+ [ Savvy: Trustworthy Autonomous Vehicles Architecture](https://arxiv.org/abs/2402.14580)

	Ali Shoker, Rehana Yasmin, Paulo Esteves-Verissimo


## 2024-02-07
+ [ InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding  Extremely Long Sequences with Training-Free Memory](https://arxiv.org/abs/2402.04617)

	Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, Maosong Sun


+ [ Adversarial Robustness Through Artifact Design](https://arxiv.org/abs/2402.04660)

	Tsufit Shua, Mahmood Sharif


+ [ Group Distributionally Robust Dataset Distillation with Risk  Minimization](https://arxiv.org/abs/2402.04676)

	Saeed Vahidian, Mingyu Wang, Jianyang Gu, Vyacheslav Kungurtsev, Wei Jiang, Yiran Chen


+ [ EvoSeed: Unveiling the Threat on Deep Neural Networks with Real-World  Illusions](https://arxiv.org/abs/2402.04699)

	Shashank Kotyan, PoYuan Mao, Danilo Vasconcellos Vargas


+ [ SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large  Language Models](https://arxiv.org/abs/2402.05044)

	Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu Qiao, Jing Shao


+ [ Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations  from Large Language Models](https://arxiv.org/abs/2402.04614)

	Chirag Agarwal, Sree Harsha Tanneru, Himabindu Lakkaraju


+ [ Channel-Selective Normalization for Label-Shift Robust Test-Time  Adaptation](https://arxiv.org/abs/2402.04958)

	Pedro Vianna, Muawiz Chaudhary, Paria Mehrbod, An Tang, Guy Cloutier, Guy Wolf, Michael Eickenberg, Eugene Belilovsky


+ [ De-amplifying Bias from Differential Privacy in Language Model  Fine-tuning](https://arxiv.org/abs/2402.04489)

	Sanjari Srivastava, Piotr Mardziel, Zhikhun Zhang, Archana Ahlawat, Anupam Datta, John C Mitchell


## 2024-02-06
+ [ Partially Recentralization Softmax Loss for Vision-Language Models  Robustness](https://arxiv.org/abs/2402.03627)

	Hao Wang, Xin Zhang, Jinzhe Jiang, Yaqian Zhao, Chen Li


+ [ A Survey of Privacy Threats and Defense in Vertical Federated Learning:  From Model Life Cycle Perspective](https://arxiv.org/abs/2402.03688)

	Lei Yu, Meng Han, Yiming Li, Changting Lin, Yao Zhang, Mingyang Zhang, Yan Liu, Haiqin Weng, Yuseok Jeon, Ka-Ho Chow, Stacy Patterson


+ [ Boosting Adversarial Transferability across Model Genus by  Deformation-Constrained Warping](https://arxiv.org/abs/2402.03951)

	Qinliang Lin, Cheng Luo, Zenghao Niu, Xilin He, Weicheng Xie, Yuanbo Hou, Linlin Shen, Siyang Song


+ [ Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science](https://arxiv.org/abs/2402.04247)

	Xiangru Tang, Qiao Jin, Kunlun Zhu, Tongxin Yuan, Yichi Zhang, Wangchunshu Zhou, Meng Qu, Yilun Zhao, Jian Tang, Zhuosheng Zhang, Arman Cohan, Zhiyong Lu, Mark Gerstein


+ [ HarmBench: A Standardized Evaluation Framework for Automated Red Teaming  and Robust Refusal](https://arxiv.org/abs/2402.04249)

	Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, David Forsyth, Dan Hendrycks


+ [ Measuring Implicit Bias in Explicitly Unbiased Large Language Models](https://arxiv.org/abs/2402.04105)

	Xuechunzi Bai, Angelina Wang, Ilia Sucholutsky, Thomas L. Griffiths


+ [ Privacy Leakage on DNNs: A Survey of Model Inversion Attacks and  Defenses](https://arxiv.org/abs/2402.04013)

	Hao Fang, Yixiang Qiu, Hongyao Yu, Wenbo Yu, Jiawei Kong, Baoli Chong, Bin Chen, Xuan Wang, Shu-Tao Xia


+ [ Fairness and Privacy Guarantees in Federated Contextual Bandits](https://arxiv.org/abs/2402.03531)

	Sambhav Solanki, Shweta Jain, Sujit Gujar


+ [ PAC-Bayesian Adversarially Robust Generalization Bounds for Graph Neural  Network](https://arxiv.org/abs/2402.04038)

	Tan Sun, Junhong Lin


+ [ Towards Fair, Robust and Efficient Client Contribution Evaluation in  Federated Learning](https://arxiv.org/abs/2402.04409)

	Meiying Zhang, Huan Zhao, Sheldon Ebron, Kan Yang


+ [ PreGIP: Watermarking the Pretraining of Graph Neural Networks for Deep  Intellectual Property Protection](https://arxiv.org/abs/2402.04435)

	Enyan Dai, Minhua Lin, Suhang Wang



+ [ Adversarially Robust Deepfake Detection via Adversarial Feature  Similarity Learning](https://arxiv.org/abs/2403.08806)

	Sarwar Khan


## 2024-02-05
+ [ Exploiting Class Probabilities for Black-box Sentence-level Attacks](https://arxiv.org/abs/2402.02695)

	Raha Moraffah, Huan Liu


+ [ A Generative Approach to Surrogate-based Black-box Attacks](https://arxiv.org/abs/2402.02732)

	Raha Moraffah, Huan Liu


+ [ Evading Data Contamination Detection for Language Models is (too) Easy](https://arxiv.org/abs/2402.02823)

	Jasper Dekoninck, Mark Niklas Müller, Maximilian Baader, Marc Fischer, Martin Vechev


+ [ Homograph Attacks on Maghreb Sentiment Analyzers](https://arxiv.org/abs/2402.03171)

	Fatima Zahra Qachfar, Rakesh M. Verma


+ [ Conversation Reconstruction Attack Against GPT Models](https://arxiv.org/abs/2402.02987)

	Junjie Chu, Zeyang Sha, Michael Backes, Yang Zhang


+ [ GUARD: Role-playing to Generate Natural-language Jailbreakings to Test  Guideline Adherence of Large Language Models](https://arxiv.org/abs/2402.03299)

	Haibo Jin, Ruoxi Chen, Andy Zhou, Jinyin Chen, Yang Zhang, Haohan Wang


+ [ Transcending Adversarial Perturbations: Manifold-Aided Adversarial  Examples with Legitimate Semantics](https://arxiv.org/abs/2402.03095)

	Shuai Li, Xiaoyu Jiang, Xiaoguang Ma


+ [ DisDet: Exploring Detectability of Backdoor Attack on Diffusion Models](https://arxiv.org/abs/2402.02739)

	Yang Sui, Huy Phan, Jinqi Xiao, Tianfang Zhang, Zijie Tang, Cong Shi, Yan Wang, Yingying Chen, Bo Yuan


+ [ Time-Distributed Backdoor Attacks on Federated Spiking Learning](https://arxiv.org/abs/2402.02886)

	Gorka Abad, Stjepan Picek, Aitor Urbieta


+ [ Adversarial Data Augmentation for Robust Speaker Verification](https://arxiv.org/abs/2402.02699)

	Zhenyu Zhou, Junhui Chen, Namin Wang, Lantian Li, Dong Wang


+ [ Arabic Synonym BERT-based Adversarial Examples for Text Classification](https://arxiv.org/abs/2402.03477)

	Norah Alshahrani, Saied Alshahrani, Esma Wali, Jeanna Matthews


+ [ Generalization Properties of Adversarial Training for $\ell_0$-Bounded  Adversarial Attacks](https://arxiv.org/abs/2402.03576)

	Payam Delgosha, Hamed Hassani, Ramtin Pedarsani


## 2024-02-04
+ [ DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms  in Vision Transformers](https://arxiv.org/abs/2402.02554)

	Oryan Yehezkel, Alon Zolfi, Amit Baras, Yuval Elovici, Asaf Shabtai


+ [ PROSAC: Provably Safe Certification for Machine Learning Models under  Adversarial Attacks](https://arxiv.org/abs/2402.02629)

	Ziquan Liu, Zhuo Zhi, Ilija Bogunovic, Carsten Gerner-Beuerle, Miguel Rodrigues

## 2024-02-03
+ [ Seeing is not always believing: The Space of Harmless Perturbations](https://arxiv.org/abs/2402.02095)

	Lu Chen, Shaofeng Li, Benhao Huang, Fan Yang, Zheng Li, Jie Li, Yuan Luo


+ [ Towards Optimal Adversarial Robust Q-learning with Bellman  Infinity-error](https://arxiv.org/abs/2402.02165)

	Haoran Li, Zicheng Zhang, Wang Luo, Congying Han, Yudong Hu, Tiande Guo, Shichen Liao


+ [ Rethinking the Starting Point: Enhancing Performance and Fairness of  Federated Learning via Collaborative Pre-Training](https://arxiv.org/abs/2402.02225)

	Yun-Wei Chu, Dong-Jun Han, Seyyedali Hosseinalipour, Christopher G. Brinton


+ [ Data Poisoning for In-context Learning](https://arxiv.org/abs/2402.02160)

	Pengfei He, Han Xu, Yue Xing, Hui Liu, Makoto Yamada, Jiliang Tang


+ [ Universal Post-Training Reverse-Engineering Defense Against Backdoors in  Deep Neural Networks](https://arxiv.org/abs/2402.02034)

	Xi Li, Hang Wang, David J. Miller, George Kesidis


+ [ MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly  Mixed Classifiers](https://arxiv.org/abs/2402.02263)

	Yatong Bai, Mo Zhou, Vishal M. Patel, Somayeh Sojoudi


## 2024-02-02
+ [ Trustworthy Distributed AI Systems: Robustness, Privacy, and Governance](https://arxiv.org/abs/2402.01096)

	Wenqi Wei, Ling Liu


+ [ STAA-Net: A Sparse and Transferable Adversarial Attack for Speech  Emotion Recognition](https://arxiv.org/abs/2402.01227)

	Yi Chang, Zhao Ren, Zixing Zhang, Xin Jing, Kun Qian, Xi Shao, Bin Hu, Tanja Schultz, Björn W. Schuller


+ [ Delving into Decision-based Black-box Attacks on Semantic Segmentation](https://arxiv.org/abs/2402.01220)

	Zhaoyu Chen, Zhengyang Shan, Jingwen Chang, Kaixun Jiang, Dingkang Yang, Yiting Cheng, Wenqiang Zhang


+ [ Synthetic Data for the Mitigation of Demographic Biases in Face  Recognition](https://arxiv.org/abs/2402.01472)

	Pietro Melzi, Christian Rathgeb, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Dominik Lawatsch, Florian Domin, Maxim Schaubert


+ [ Cheating Suffix: Targeted Attack to Text-To-Image Diffusion Models with  Multi-Modal Priors](https://arxiv.org/abs/2402.01369)

	Dingcheng Yang, Yang Bai, Xiaojun Jia, Yang Liu, Xiaochun Cao, Wenjian Yu


+ [ SignSGD with Federated Defense: Harnessing Adversarial Attacks through  Gradient Sign Decoding](https://arxiv.org/abs/2402.01340)

	Chanho Park, Namyoon Lee


+ [ On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio  Classification](https://arxiv.org/abs/2402.01274)

	Calum Heggan, Sam Budgett, Timothy Hosepedales, Mehrdad Yeghoobi


+ [ $σ$-zero: Gradient-based Optimization of $\ell_0$-norm Adversarial  Examples](https://arxiv.org/abs/2402.01879)

	Antonio Emanuele Cinà, Francesco Villani, Maura Pintor, Lea Schönherr, Battista Biggio, Marcello Pelillo


+ [ Position Paper: Assessing Robustness, Privacy, and Fairness in Federated  Learning Integrated with Foundation Models](https://arxiv.org/abs/2402.01857)

	Xi Li, Jiaqi Wang


+ [ HQA-Attack: Toward High Quality Black-Box Hard-Label Adversarial Attack  on Text](https://arxiv.org/abs/2402.01806)

	Han Liu, Zhi Xu, Xiaotong Zhang, Feng Zhang, Fenglong Ma, Hongyang Chen, Hong Yu, Xianchao Zhang


+ [ Preference Poisoning Attacks on Reward Model Learning](https://arxiv.org/abs/2402.01920)

	Junlin Wu, Jiongxiao Wang, Chaowei Xiao, Chenguang Wang, Ning Zhang, Yevgeniy Vorobeychik


+ [ SpecFormer: Guarding Vision Transformer Robustness via Maximum Singular  Value Penalization](https://arxiv.org/abs/2402.03317)

	Xixu Hu, Runkai Zheng, Jindong Wang, Cheuk Hang Leung, Qi Wu, Xing Xie


## 2024-02-01
+ [ Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated  Student Essay Detection](https://arxiv.org/abs/2402.00412)

	Xinlin Peng, Ying Zhou, Ben He, Le Sun, Yingfei Sun


+ [ Safety of Multimodal Large Language Models on Images and Text](https://arxiv.org/abs/2402.00357)

	Xin Liu, Yichen Zhu, Yunshi Lan, Chao Yang, Yu Qiao


+ [ Masked Conditional Diffusion Model for Enhancing Deepfake Detection](https://arxiv.org/abs/2402.00541)

	Tiewen Chen, Shanmin Yang, Shu Hu, Zhenghan Fang, Ying Fu, Xi Wu, Xin Wang


+ [ Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks](https://arxiv.org/abs/2402.00626)

	Maan Qraitem, Nazia Tasnim, Kate Saenko, Bryan A. Plummer


+ [ Approximating Optimal Morphing Attacks using Template Inversion](https://arxiv.org/abs/2402.00695)

	Laurent Colbois, Hatef Otroshi Shahreza, Sébastien Marcel


+ [ Tropical Decision Boundaries for Neural Networks Are Robust Against  Adversarial Attacks](https://arxiv.org/abs/2402.00576)

	Kurt Pasque, Christopher Teska, Ruriko Yoshida, Keiji Miura, Jefferson Huang


+ [Short: Benchmarking Transferable Adversarial Attacks](https://arxiv.org/abs/2402.00418)

	Zhibo Jin, Jiayu Zhang, Zhiyu Zhu, Huaming Chen


+ [ Large Language Models Based Fuzzing Techniques: A Survey](https://arxiv.org/abs/2402.00350)

	Linghan Huang, Peizhou Zhao, Huaming Chen, Lei Ma


+ [ Investigating Bias Representations in Llama 2 Chat via Activation  Steering](https://arxiv.org/abs/2402.00402)

	Dawn Lu, Nina Rimsky


+ [ Invariance-powered Trustworthy Defense via Remove Then Restore](https://arxiv.org/abs/2402.00304)

	Xiaowei Fu, Yuhang Zhou, Lina Ma, Lei Zhang


+ [ Safety of Multimodal Large Language Models on Images and Text](https://arxiv.org/abs/2402.00357)

	Xin Liu, Yichen Zhu, Yunshi Lan, Chao Yang, Yu Qiao


+ [ Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks](https://arxiv.org/abs/2402.00626)

	Maan Qraitem, Nazia Tasnim, Kate Saenko, Bryan A. Plummer


+ [ Survey of Privacy Threats and Countermeasures in Federated Learning](https://arxiv.org/abs/2402.00342)

	Masahiro Hayashitani, Junki Mori, Isamu Teranishi


+ [ FairEHR-CLP: Towards Fairness-Aware Clinical Predictions with  Contrastive Learning in Multimodal Electronic Health Records](https://arxiv.org/abs/2402.00955)

	Yuqing Wang, Malvika Pillai, Yun Zhao, Catherine Curtin, Tina Hernandez-Boussard


+ [ BrainLeaks: On the Privacy-Preserving Properties of Neuromorphic  Architectures against Model Inversion Attacks](https://arxiv.org/abs/2402.00906)

	Hamed Poursiami, Ihsen Alouani, Maryam Parsa


## 2024-01-31
+ [ Manipulating Predictions over Discrete Inputs in Machine Teaching](https://arxiv.org/abs/2401.17865)

	Xiaodong Wu, Yufei Han, Hayssam Dahrouj, Jianbing Ni, Zhenwen Liang, Xiangliang Zhang


+ [ Unified Physical-Digital Face Attack Detection](https://arxiv.org/abs/2401.17699)

	Hao Fang, Ajian Liu, Haocheng Yuan, Junze Zheng, Dingheng Zeng, Yanhong Liu, Jiankang Deng, Sergio Escalera, Xiaoming Liu, Jun Wan, Zhen Lei


+ [ Logit Poisoning Attack in Distillation-based Federated Learning and its  Countermeasures](https://arxiv.org/abs/2401.17746)

	Yonghao Yu, Shunan Zhu, Jinglu Hu


+ [ Adversarial Quantum Machine Learning: An Information-Theoretic  Generalization Analysis](https://arxiv.org/abs/2402.00176)

	Petros Georgiou, Sharu Theresa Jose, Osvaldo Simeone


+ [ Common Sense Reasoning for Deep Fake Detection](https://arxiv.org/abs/2402.00126)

	Yue Zhang, Ben Colman, Ali Shahriyari, Gaurav Bharaj


+ [ Privacy and Security Implications of Cloud-Based AI Services : A Survey](https://arxiv.org/abs/2402.00896)

	Alka Luqman, Riya Mahesh, Anupam Chattopadhyay


+ [ An Early Categorization of Prompt Injection Attacks on Large Language  Models](https://arxiv.org/abs/2402.00898)

	Sippo Rossi, Alisia Marianne Michel, Raghava Rao Mukkamala, Jason Bennett Thatcher


## 2024-01-30
+ [ Detection and Recovery Against Deep Neural Network Fault Injection  Attacks Based on Contrastive Learning](https://arxiv.org/abs/2401.16766)

	Chenan Wang, Pu Zhao, Siyue Wang, Xue Lin


+ [ Can Large Language Models be Trusted for Evaluation? Scalable  Meta-Evaluation of LLMs as Evaluators via Agent Debate](https://arxiv.org/abs/2401.16788)

	Steffi Chern, Ethan Chern, Graham Neubig, Pengfei Liu


+ [ Finetuning Large Language Models for Vulnerability Detection](https://arxiv.org/abs/2401.17010)

	Alexey Shestov, Anton Cheshkov, Rodion Levichev, Ravil Mussabayev, Pavel Zadorozhny, Evgeny Maslov, Chibirev Vadim, Egor Bulychev


+ [ Robust Prompt Optimization for Defending Language Models Against  Jailbreaking Attacks](https://arxiv.org/abs/2401.17263)

	Andy Zhou, Bo Li, Haohan Wang


+ [ Gradient-Based Language Model Red Teaming](https://arxiv.org/abs/2401.16656)

	Nevan Wichers, Carson Denison, Ahmad Beirami


+ [ Single Word Change is All You Need: Designing Attacks and Defenses for  Text Classifiers](https://arxiv.org/abs/2401.17196)

	Lei Xu, Sarah Alnegheimish, Laure Berti-Equille, Alfredo Cuesta-Infante, Kalyan Veeramachaneni


+ [ Weak-to-Strong Jailbreaking on Large Language Models](https://arxiv.org/abs/2401.17256)

	Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, William Yang Wang


+ [ Optimal-Landmark-Guided Image Blending for Face Morphing Attacks](https://arxiv.org/abs/2401.16722)

	Qiaoyun He, Zongyong Deng, Zuyuan He, Qijun Zhao


+ [ Towards Assessing the Synthetic-to-Measured Adversarial Vulnerability of  SAR ATR](https://arxiv.org/abs/2401.17038)

	Bowen Peng, Bo Peng, Jingyuan Xia, Tianpeng Liu, Yongxiang Liu, Li Liu


+ [ Revisiting Gradient Pruning: A Dual Realization for Defending against  Gradient Attacks](https://arxiv.org/abs/2401.16687)

	Lulu Xue, Shengshan Hu, Ruizhi Zhao, Leo Yu Zhang, Shengqing Hu, Lichao Sun, Dezhong Yao


+ [ Systematically Assessing the Security Risks of AI/ML-enabled Connected  Healthcare Systems](https://arxiv.org/abs/2401.17136)

	Mohammed Elnawawy, Mohammadreza Hallajiyan, Gargi Mitra, Shahrear Iqbal, Karthik Pattabiraman


+ [ Provably Robust Multi-bit Watermarking for AI-generated Text via Error  Correction Code](https://arxiv.org/abs/2401.16820)

	Wenjie Qu, Dong Yin, Zixin He, Wei Zou, Tianyang Tao, Jinyuan Jia, Jiaheng Zhang


+ [ AdvGPS: Adversarial GPS for Multi-Agent Perception Attack](https://arxiv.org/abs/2401.17499)

	Jinlong Li, Baolu Li, Xinyu Liu, Jianwu Fang, Felix Juefei-Xu, Qing Guo, Hongkai Yu


+ [ Security and Privacy Challenges of Large Language Models: A Survey](https://arxiv.org/abs/2402.00888)

	Badhan Chandra Das, M. Hadi Amini, Yanzhao Wu


+ [ Large Language Models in Cybersecurity: State-of-the-Art](https://arxiv.org/abs/2402.00891)

	Farzad Nourmohammadzadeh Motlagh, Mehrdad Hajizadeh, Mehryar Majd, Pejman Najafi, Feng Cheng, Christoph Meinel


## 2024-01-29
+ [ Adversarial Training on Purification (AToP): Advancing Both Robustness  and Generalization](https://arxiv.org/abs/2401.16352)

	Guang Lin, Chao Li, Jianhai Zhang, Toshihisa Tanaka, Qibin Zhao


+ [ Finding Challenging Metaphors that Confuse Pretrained Language Models](https://arxiv.org/abs/2401.16012)

	Yucheng Li, Frank Guerin, Chenghua Lin


+ [ Transparency Attacks: How Imperceptible Image Layers Can Fool AI  Perception](https://arxiv.org/abs/2401.15817)

	Forrest McKee, David Noever


+ [ TransTroj: Transferable Backdoor Attacks to Pre-trained Models via  Embedding Indistinguishability](https://arxiv.org/abs/2401.15883)

	Hao Wang, Tao Xiang, Shangwei Guo, Jialing He, Hangcheng Liu, Tianwei Zhang


+ [ AdvNF: Reducing Mode Collapse in Conditional Normalising Flows using  Adversarial Learning](https://arxiv.org/abs/2401.15948)

	Vikas Kanaujia, Mathias S. Scheurer, Vipul Arora


+ [ Red-Teaming for Generative AI: Silver Bullet or Security Theater?](https://arxiv.org/abs/2401.15897)

	Michael Feffer, Anusha Sinha, Zachary C. Lipton, Hoda Heidari


+ [ LESSON: Multi-Label Adversarial False Data Injection Attack for Deep  Learning Locational Detection](https://arxiv.org/abs/2401.16001)

	Jiwei Tian, Chao Shen, Buhong Wang, Xiaofang Xia, Meng Zhang, Chenhao Lin, Qian Li


+ [ Effective Controllable Bias Mitigation for Classification and Retrieval  using Gate Adapters](https://arxiv.org/abs/2401.16457)

	Shahed Masoudian, Cornelia Volaucnik, Markus Schedl, Shahed Masoudian


## 2024-01-28
+ [ Lips Are Lying: Spotting the Temporal Inconsistency between Audio and  Visual in Lip-Syncing DeepFakes](https://arxiv.org/abs/2401.15668)

	Weifeng Liu, Tianyi She, Jiawei Liu, Run Wang, Dongyu Yao, Ziyou Liang


+ [ Addressing Noise and Efficiency Issues in Graph-Based Machine Learning  Models From the Perspective of Adversarial Attack](https://arxiv.org/abs/2401.15615)

	Yongyu Wang


+ [ Integrating Differential Privacy and Contextual Integrity](https://arxiv.org/abs/2401.15774)

	Sebastian Benthall, Rachel Cummings


## 2024-01-27
+ [ L-AutoDA: Leveraging Large Language Models for Automated Decision-based  Adversarial Attacks](https://arxiv.org/abs/2401.15335)

	Ping Guo, Fei Liu, Xi Lin, Qingchuan Zhao, Qingfu Zhang


+ [ Style-News: Incorporating Stylized News Generation and Adversarial  Verification for Neural Fake News Detection](https://arxiv.org/abs/2401.15509)

	Wei-Yao Wang, Yu-Chieh Chang, Wen-Chih Peng


+ [ Multi-Trigger Backdoor Attacks: More Triggers, More Threats](https://arxiv.org/abs/2401.15295)

	Yige Li, Xingjun Ma, Jiabo He, Hanxun Huang, Yu-Gang Jiang


+ [ Asymptotic Behavior of Adversarial Training Estimator under  $\ell_\infty$-Perturbation](https://arxiv.org/abs/2401.15262)

	Yiling Xie, Xiaoming Huo


## 2024-01-26
+ [ Mitigating Feature Gap for Adversarial Robustness by Feature  Disentanglement](https://arxiv.org/abs/2401.14707)

	Nuoyan Zhou, Dawei Zhou, Decheng Liu, Xinbo Gao, Nannan Wang


+ [ Conserve-Update-Revise to Cure Generalization and Robustness Trade-off  in Adversarial Training](https://arxiv.org/abs/2401.14948)

	Shruthi Gowda, Bahram Zonooz, Elahe Arani


+ [ BackdoorBench: A Comprehensive Benchmark and Analysis of Backdoor  Learning](https://arxiv.org/abs/2401.15002)

	Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, Mingli Zhu, Ruotong Wang, Li Liu, Chao Shen


+ [ Unrecognizable Yet Identifiable: Image Distortion with Preserved  Embeddings](https://arxiv.org/abs/2401.15048)

	Dmytro Zakharov, Oleksandr Kuznetsov, Emanuele Frontoni


+ [ GuardML: Efficient Privacy-Preserving Machine Learning Services Through  Hybrid Homomorphic Encryption](https://arxiv.org/abs/2401.14840)

	Eugene Frimpong, Khoa Nguyen, Mindaugas Budzys, Tanveer Khan, Antonis Michalas


+ [ PrivStream: An Algorithm for Streaming Differentially Private Data](https://arxiv.org/abs/2401.14577)

	Girish Kumar, Thomas Strohmer, Roman Vershynin


+ [ Coca: Improving and Explaining Graph Neural Network-Based Vulnerability  Detection Systems](https://arxiv.org/abs/2401.14886)

	Sicong Cao, Xiaobing Sun, Xiaoxue Wu, David Lo, Lili Bo, Bin Li, Wei Liu


+ [ Better Representations via Adversarial Training in Pre-Training: A  Theoretical Perspective](https://arxiv.org/abs/2401.15248)

	Yue Xing, Xiaofeng Lin, Qifan Song, Yi Xu, Belinda Zeng, Guang Cheng


+ [ MEA-Defender: A Robust Watermark against Model Extraction Attack](https://arxiv.org/abs/2401.15239)

	Peizhuo Lv, Hualong Ma, Kai Chen, Jiachen Zhou, Shengzhi Zhang, Ruigang Liang, Shenchen Zhu, Pan Li, Yingjun Zhang


## 2024-01-25
+ [ Unmasking and Quantifying Racial Bias of Large Language Models in  Medical Report Generation](https://arxiv.org/abs/2401.13867)

	Yifan Yang, Xiaoyu Liu, Qiao Jin, Furong Huang, Zhiyong Lu


+ [ Adaptive Text Watermark for Large Language Models](https://arxiv.org/abs/2401.13927)

	Yepeng Liu, Yuheng Bu



+ [ Sparse and Transferable Universal Singular Vectors Attack](https://arxiv.org/abs/2401.14031)

	Kseniia Kuvshinova, Olga Tsymboi, Ivan Oseledets


+ [ The Risk of Federated Learning to Skew Fine-Tuning Features and  Underperform Out-of-Distribution Robustness](https://arxiv.org/abs/2401.14027)

	Mengyao Du, Miao Zhang, Yuwen Pu, Kai Xu, Shouling Ji, Quanjun Yin


+ [ Information Leakage Detection through Approximate Bayes-optimal  Prediction](https://arxiv.org/abs/2401.14283)

	Pritha Gupta, Marcel Wever, Eyke Hüllermeier


+ [ Improving Pseudo-labelling and Enhancing Robustness for Semi-Supervised  Domain Generalization](https://arxiv.org/abs/2401.13965)

	Adnan Khan, Mai A. Shaaban, Muhammad Haris Khan


+ [ Producing Plankton Classifiers that are Robust to Dataset Shift](https://arxiv.org/abs/2401.14256)

	Cheng Chen, Sreenath Kyathanahally, Marta Reyes, Stefanie Merkli, Ewa Merz, Emanuele Francazi, Marvin Hoege, Francesco Pomati, Marco Baity-Jesi


+ [ Decentralized Federated Learning: A Survey on Security and Privacy](https://arxiv.org/abs/2401.17319)

	Ehsan Hallaji, Roozbeh Razavi-Far, Mehrdad Saif, Boyu Wang, Qiang Yang


## 2024-01-24
+ [ Boosting the Transferability of Adversarial Examples via Local Mixup and  Adaptive Step Size](https://arxiv.org/abs/2401.13205)

	Junlin Liu, Xinchen Lyu


+ [ AdCorDA: Classifier Refinement via Adversarial Correction and Domain  Adaptation](https://arxiv.org/abs/2401.13212)

	Lulan Shen, Ali Edalati, Brett Meyer, Warren Gross, James J. Clark


+ [ Multi-Agent Diagnostics for Robustness via Illuminated Diversity](https://arxiv.org/abs/2401.13460)

	Mikayel Samvelyan, Davide Paglieri, Minqi Jiang, Jack Parker-Holder, Tim Rocktäschel


+ [ Can overfitted deep neural networks in adversarial training generalize?  -- An approximation viewpoint](https://arxiv.org/abs/2401.13624)

	Zhongjie Shi, Fanghui Liu, Yuan Cao, Johan A.K. Suykens


+ [ A Systematic Approach to Robustness Modelling for Deep Convolutional  Neural Networks](https://arxiv.org/abs/2401.13751)

	Charles Meyers, Mohammad Reza Saleh Sedghpour, Tommy Löfstedt, Erik Elmroth


+ [ Don't Push the Button! Exploring Data Leakage Risks in Machine Learning  and Transfer Learning](https://arxiv.org/abs/2401.13796)

	Andrea Apicella, Francesco Isgrò, Roberto Prevete


+ [ LAA-Net: Localized Artifact Attention Network for High-Quality Deepfakes  Detection](https://arxiv.org/abs/2401.13856)

	Dat Nguyen, Nesryne Mejri, Inder Pal Singh, Polina Kuleshova, Marcella Astrid, Anis Kacem, Enjie Ghorbel, Djamila Aouada


+ [ Inference Attacks Against Face Recognition Model without Classification  Layers](https://arxiv.org/abs/2401.13719)

	Yuanqing Huang, Huilong Chen, Yinggui Wang, Lei Wang


+ [ A Systematic Approach to Robustness Modelling for Deep Convolutional  Neural Networks](https://arxiv.org/abs/2401.13751)

	Charles Meyers, Mohammad Reza Saleh Sedghpour, Tommy Löfstedt, Erik Elmroth


## 2024-01-23
+ [ Securing Recommender System via Cooperative Training](https://arxiv.org/abs/2401.12700)

	Qingyang Wang, Chenwang Wu, Defu Lian, Enhong Chen


+ [ DAFA: Distance-Aware Fair Adversarial Training](https://arxiv.org/abs/2401.12532)

	Hyungyu Lee, Saehyung Lee, Hyemi Jang, Junsung Park, Ho Bae, Sungroh Yoon


+ [ Fast Adversarial Training against Textual Adversarial Attacks](https://arxiv.org/abs/2401.12461)

	Yichen Yang, Xin Liu, Kun He


+ [ MAPPING: Debiasing Graph Neural Networks for Fair Node Classification  with Limited Sensitive Information Leakage](https://arxiv.org/abs/2401.12824)

	Ying Song, Balaji Palanisamy


+ [ ToDA: Target-oriented Diffusion Attacker against Recommendation System](https://arxiv.org/abs/2401.12578)

	Xiaohao Liu, Zhulin Tao, Ting Jiang, He Chang, Yunshan Ma, Xianglin Huang


+ [ Digital Divides in Scene Recognition: Uncovering Socioeconomic Biases in  Deep Learning Systems](https://arxiv.org/abs/2401.13097)

	Michelle R. Greene, Mariam Josyula, Wentao Si, Jennifer A. Hart


+ [ The Language Barrier: Dissecting Safety Challenges of LLMs in  Multilingual Contexts](https://arxiv.org/abs/2401.13136)

	Lingfeng Shen, Weiting Tan, Sihao Chen, Yunmo Chen, Jingyu Zhang, Haoran Xu, Boyuan Zheng, Philipp Koehn, Daniel Khashabi


## 2024-01-22
+ [ GI-PIP: Do We Require Impractical Auxiliary Dataset for Gradient  Inversion Attacks?](https://arxiv.org/abs/2401.11748)

	Yu sun, Gaojian Xiong, Xianxun Yao, Kailang Ma, Jian Cui


+ [ Safe and Generalized end-to-end Autonomous Driving System with  Reinforcement Learning and Demonstrations](https://arxiv.org/abs/2401.11792)

	Zuojin Tang, Xiaoyu Chen, YongQiang Li, Jianyu Chen


+ [ Robustness to distribution shifts of compressed networks for edge  devices](https://arxiv.org/abs/2401.12014)

	Lulan Shen, Ali Edalati, Brett Meyer, Warren Gross, James J. Clark


+ [ Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated  Text](https://arxiv.org/abs/2401.12070)

	Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein


+ [ Text Embedding Inversion Attacks on Multilingual Language Models](https://arxiv.org/abs/2401.12192)

	Yiyi Chen, Heather Lent, Johannes Bjerva


+ [ A Training-Free Defense Framework for Robust Learned Image Compression](https://arxiv.org/abs/2401.11902)

	Myungseo Song, Jinyoung Choi, Bohyung Han


+ [ Privacy-Preserving Data Fusion for Traffic State Estimation: A Vertical  Federated Learning Approach](https://arxiv.org/abs/2401.11836)

	Qiqing Wang, Kaidi Yang


+ [ Analyzing the Quality Attributes of AI Vision Models in Open  Repositories Under Adversarial Attacks](https://arxiv.org/abs/2401.12261)

	Zerui Wang, Yan Liu


+ [ GRATH: Gradual Self-Truthifying for Large Language Models](https://arxiv.org/abs/2401.12292)

	Weixin Chen, Bo Li


## 2024-01-21
+ [ Finding a Needle in the Adversarial Haystack: A Targeted Paraphrasing  Approach For Uncovering Edge Cases with Minimal Distribution Distortion](https://arxiv.org/abs/2401.11373)

	Aly M. Kassem, Sherif Saad


+ [ Adversarial Augmentation Training Makes Action Recognition Models More  Robust to Realistic Video Distribution Shifts](https://arxiv.org/abs/2401.11406)

	Kiyoon Kim, Shreyank N Gowda, Panagiotis Eustratiadis, Antreas Antoniou, Robert B Fisher


+ [ TetraLoss: Improving the Robustness of Face Recognition against Morphing  Attacks](https://arxiv.org/abs/2401.11598)

	Mathias Ibsen, Lázaro J. González-Soler, Christian Rathgeb, Christoph Busch


+ [ How Robust Are Energy-Based Models Trained With Equilibrium Propagation?](https://arxiv.org/abs/2401.11543)

	Siddharth Mansingh, Michal Kucer, Garrett Kenyon, Juston Moore, Michael Teti


## 2024-01-20
+ [ CARE: Ensemble Adversarial Robustness Evaluation Against Adaptive  Attackers for Security Applications](https://arxiv.org/abs/2401.11126)

	Hangsheng Zhang, Jiqiang Liu, Jinsong Dong


+ [ BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models](https://arxiv.org/abs/2401.12242)

	Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, Bo Li


## 2024-01-19
+ [ PuriDefense: Randomized Local Implicit Adversarial Purification for  Defending Black-box Query-based Attacks](https://arxiv.org/abs/2401.10586)

	Ping Guo, Zhiyuan Yang, Xi Lin, Qingchuan Zhao, Qingfu Zhang


+ [ Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs  Without Fine-Tuning](https://arxiv.org/abs/2401.10862)

	Adib Hasan, Ileana Rugina, Alex Wang


+ [ Mitigating Hallucinations of Large Language Models via Knowledge  Consistent Alignment](https://arxiv.org/abs/2401.10768)

	Fanqi Wan, Xinting Huang, Leyang Cui, Xiaojun Quan, Wei Bi, Shuming Shi


+ [ Differentially Private and Adversarially Robust Machine Learning: An  Empirical Evaluation](https://arxiv.org/abs/2401.10405)

	Janvi Thakkar, Giulio Zizzo, Sergio Maffeis


+ [ Adversarially Robust Signed Graph Contrastive Learning from Balance  Augmentation](https://arxiv.org/abs/2401.10590)

	Jialong Zhou, Xing Ai, Yuni Lai, Kai Zhou


+ [ Hacking Predictors Means Hacking Cars: Using Sensitivity Analysis to  Identify Trajectory Prediction Vulnerabilities for Autonomous Driving  Security](https://arxiv.org/abs/2401.10313)

	Marsalis Gibson, David Babazadeh, Claire Tomlin, Shankar Sastry


+ [ A Lightweight Multi-Attack CAN Intrusion Detection System on Hybrid  FPGAs](https://arxiv.org/abs/2401.10689)

	Shashwat Khandelwal, Shreejith Shanker


+ [ Real-Time Zero-Day Intrusion Detection System for Automotive Controller  Area Network on FPGAs](https://arxiv.org/abs/2401.10724)

	Shashwat Khandelwal, Shreejith Shanker


+ [ The Surprising Harmfulness of Benign Overfitting for Adversarial  Robustness](https://arxiv.org/abs/2401.12236)

	Yifan Hao, Tong Zhang

	
## 2024-01-18
+ [ Uncertainty-Aware Hardware Trojan Detection Using Multimodal Deep  Learning](https://arxiv.org/abs/2401.09479)

	Rahul Vishwakarma, Amin Rezaei


+ [ Power in Numbers: Robust reading comprehension by finetuning with four  adversarial sentences per example](https://arxiv.org/abs/2401.10091)

	Ariel Marcus


+ [ Marrying Adapters and Mixup to Efficiently Enhance the Adversarial  Robustness of Pre-Trained Language Models for Text Classification](https://arxiv.org/abs/2401.10111)

	Tuc Nguyen, Thai Le


+ [ Artwork Protection Against Neural Style Transfer Using Locally Adaptive  Adversarial Color Attack](https://arxiv.org/abs/2401.09673)

	Zhongliang Guo, Kaixuan Wang, Weiye Li, Yifei Qian, Ognjen Arandjelović, Lei Fang


+ [ Cross-Modality Perturbation Synergy Attack for Person Re-identification](https://arxiv.org/abs/2401.10090)

	Yunpeng Gong, others


+ [ MedBlindTuner: Towards Privacy-preserving Fine-tuning on Biomedical  Images with Transformers and Fully Homomorphic Encryption](https://arxiv.org/abs/2401.09604)

	Prajwal Panzade, Daniel Takabi, Zhipeng Cai


+ [ MITS-GAN: Safeguarding Medical Imaging from Tampering with Generative  Adversarial Networks](https://arxiv.org/abs/2401.09624)

	Giovanni Pasqualino, Luca Guarnera, Alessandro Ortis, Sebastiano Battiato


+ [ Universally Robust Graph Neural Networks by Preserving Neighbor  Similarity](https://arxiv.org/abs/2401.09754)

	Yulin Zhu, Yuni Lai, Xing Ai, Kai Zhou


+ [ HGAttack: Transferable Heterogeneous Graph Adversarial Attack](https://arxiv.org/abs/2401.09945)

	He Zhao, Zhiwei Zeng, Yongwei Wang, Deheng Ye, Chunyan Miao


+ [ Hijacking Attacks against Neural Networks by Analyzing Training Data](https://arxiv.org/abs/2401.09740)

	Yunjie Ge, Qian Wang, Huayang Huang, Qi Li, Cong Wang, Chao Shen, Lingchen Zhao, Peipei Jiang, Zheng Fang, Shenyi Zhang


## 2024-01-17
+ [MITS-GAN: Safeguarding Medical Imaging from Tampering with Generative Adversarial Networks](https://arxiv.org/abs/2401.09624)

	Giovanni Pasqualino, Luca Guarnera, Alessandro Ortis, Sebastiano Battiato


+ [An Optimal Transport Approach for Computing Adversarial Training Lower Bounds in Multiclass Classification](https://arxiv.org/abs/2401.09191)

	Nicolas Garcia Trillos, Matt Jacobs, Jakwang Kim, Matthew Werenski


+ [Artwork Protection Against Neural Style Transfer Using Locally Adaptive Adversarial Color Attack](https://arxiv.org/abs/2401.09673)

	Zhongliang Guo, Kaixuan Wang, Weiye Li, Yifei Qian, Ognjen Arandjelović, Lei Fang


## 2024-01-16
+ [ Bag of Tricks to Boost Adversarial Transferability](https://arxiv.org/abs/2401.08734)

	Zeliang Zhang, Rongyi Zhu, Wei Yao, Xiaosen Wang, Chenliang Xu


+ [ Towards Efficient and Certified Recovery from Poisoning Attacks in Federated Learning](https://arxiv.org/abs/2401.08216)

	Yu Jiang, Jiyuan Shen, Ziyao Liu, Chee Wei Tan, Kwok-Yan Lam


+ [ PPR: Enhancing Dodging Attacks while Maintaining Impersonation Attacks on Face Recognition Systems](https://arxiv.org/abs/2401.08903)

	Fengfan Zhou, Heifei Ling


+ [ A Generative Adversarial Attack for Multilingual Text Classifiers](https://arxiv.org/abs/2401.08255)

	Tom Roth, Inigo Jauregi Unanue, Alsharif Abuadbba, Massimo Piccardi



## 2024-01-15
+ [ Left-right Discrepancy for Adversarial Attack on Stereo Networks](https://arxiv.org/abs/2401.07188)

	Pengfei Wang, Xiaofei Hui, Beijia Lu, Nimrod Lilith, Jun Liu, Sameer Alam


+ [ Exploring Adversarial Attacks against Latent Diffusion Model from the Perspective of Adversarial Transferability](https://arxiv.org/abs/2401.07087)

	Junxi Chen, Junhao Dong, Xiaohua Xie


+ [ Crafter: Facial Feature Crafting against Inversion-based Identity Theft on Deep Models](https://arxiv.org/abs/2401.07205)

	Shiming Wang, Zhe Ji, Liyao Xiang, Hao Zhang, Xinbing Wang, Chenghu Zhou, Bo Li


+ [ Robustness Against Adversarial Attacks via Learning Confined Adversarial Polytopes](https://arxiv.org/abs/2401.06373)

	Shayan Mohajer Hamidi; Linfeng Ye


## 2024-01-12
+ [ An Analytical Framework for Modeling and Synthesizing Malicious Attacks on ACC Vehicles](https://arxiv.org/abs/2401.06916)

	Shian Wang


+ [ Adversarial Examples are Misaligned in Diffusion Model Manifolds](https://arxiv.org/abs/2401.06637)

	Peter Lorenz, Ricard Durall, Janis Keuper


+ [ How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs](https://arxiv.org/abs/2401.06373)

	Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, Weiyan Shi


## 2024-01-11
+ [ Combating Adversarial Attacks with Multi-Agent Debate](https://arxiv.org/abs/2401.05998)

	Steffi Chern, Zhen Fan, Andy Liu


+ [ GE-AdvGAN: Improving the transferability of adversarial samples by gradient editing-based adversarial generative model](https://arxiv.org/abs/2401.06031)

	Zhiyu Zhu, Huaming Chen, Xinyi Wang, Jiayu Zhang, Zhibo Jin, Kim-Kwang Raymond Choo


+ [ Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks](https://arxiv.org/abs/2401.05949)

	Shuai Zhao, Meihuizi Jia, Luu Anh Tuan, Jinming Wen


+ [ Can We Trust the Unlabeled Target Data? Towards Backdoor Attack and Defense on Model Adaptation](https://arxiv.org/abs/2401.06030)

	Lijun Sheng, Jian Liang, Ran He, Zilei Wang, Tieniu Tan


## 2024-01-10
+ [ Revisiting Adversarial Training at Scale](https://arxiv.org/abs/2401.04727)

	Zeyu Wang, Xianhang Li, Hongru Zhu, Cihang Xie


+ [ SoK: Facial Deepfake Detectors](https://arxiv.org/abs/2401.04364)

	Binh M. Le, Jiwon Kim, Shahroz Tariq, Kristen Moore, Alsharif Abuadbba, Simon S. Woo


+ [ Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training](https://arxiv.org/abs/2401.05566)

	Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M. Ziegler, Tim Maxwell, Newton Cheng, Adam Jermyn, Amanda Askell, Ansh Radhakrishnan, Cem Anil, David Duvenaud, Deep Ganguli, Fazl Barez, Jack Clark, Kamal Ndousse, Kshitij Sachan, Michael Sellitto, Mrinank Sharma, Nova DasSarma, Roger Grosse, Shauna Kravec, Yuntao Bai, Zachary Witten, Marina Favaro, Jan Brauner, Holden Karnofsky, Paul Christiano, Samuel R. Bowman, Logan Graham, Jared Kaplan, Sören Mindermann, Ryan Greenblatt, Buck Shlegeris, Nicholas Schiefer, Ethan Perez


+ [ TrustLLM: Trustworthiness in Large Language Models](https://arxiv.org/abs/2401.05561)

	Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zhengliang Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bhavya Kailkhura, Caiming Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes, Neil Zhenqiang Gong, Philip S. Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong Chen, Tianming Liu, Tianyi Zhou, Willian Wang, Xiang Li, Xiangliang Zhang, Xiao Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen, Yue Zhao


+ [ Exploring Vulnerabilities of No-Reference Image Quality Assessment Models: A Query-Based Black-Box Method](https://arxiv.org/abs/2401.05217)

	Chenxi Yang, Yujia Liu, Dingquan Li, Tingting Jiang


## 2024-01-09
+ [ Deep Anomaly Detection in Text](https://arxiv.org/abs/2401.02971)

	Andrei Manolache


+ [ Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning  for Safe and Efficient Autonomous Driving](https://arxiv.org/abs/2401.03160)

	Zilin Huang, Zihao Sheng, Chengyuan Ma, Sikai Chen


+ [ An Investigation of Large Language Models for Real-World Hate Speech  Detection](https://arxiv.org/abs/2401.03346)

	Keyan Guo, Alexander Hu, Jaden Mu, Ziheng Shi, Ziming Zhao, Nishant Vishwamitra, Hongxin Hu


+ [ LLM-Powered Code Vulnerability Repair with Reinforcement Learning and  Semantic Reward](https://arxiv.org/abs/2401.03374)

	Nafis Tanveer Islam, Joseph Khoury, Andrew Seong, Gonzalo De La Torre Parra, Elias Bou-Harb, Peyman Najafirad


+ [ GLOCALFAIR: Jointly Improving Global and Local Group Fairness in  Federated Learning](https://arxiv.org/abs/2401.03562)

	Syed Irfan Ali Meerza, Luyang Liu, Jiaxin Zhang, Jian Liu


+ [ Few-Shot Causal Representation Learning for Out-of-Distribution  Generalization on Heterogeneous Graphs](https://arxiv.org/abs/2401.03597)

	Pengfei Ding, Yan Wang, Guanfeng Liu, Nan Wang


+ [ A Large-scale Empirical Study on Improving the Fairness of Deep Learning  Models](https://arxiv.org/abs/2401.03695)

	Junjie Yang, Jiajun Jiang, Zeyu Sun, Junjie Chen


+ [ The Butterfly Effect of Altering Prompts: How Small Changes and  Jailbreaks Affect Large Language Model Performance](https://arxiv.org/abs/2401.03729)

	Abel Salinas, Fred Morstatter


+ [ Transferable Learned Image Compression-Resistant Adversarial  Perturbations](https://arxiv.org/abs/2401.03115)

	Yang Sui, Zhuohang Li, Ding Ding, Xiang Pan, Xiaozhong Xu, Shan Liu, Zhenzhong Chen


+ [ Adaptive Boosting with Fairness-aware Reweighting Technique for Fair  Classification](https://arxiv.org/abs/2401.03097)

	Xiaobin Song, Zeyuan Liu, Benben Jiang


+ [ Accurate and Scalable Estimation of Epistemic Uncertainty for Graph  Neural Networks](https://arxiv.org/abs/2401.03350)

	Puja Trivedi, Mark Heimann, Rushil Anirudh, Danai Koutra, Jayaraman J. Thiagarajan


## 2024-01-08
+ [ Logits Poisoning Attack in Federated Distillation](http://arxiv.org/abs/2401.03685)

    Yuhan Tang, Zhiyuan Wu, Bo Gao, Tian Wen, Yuwei Wang, Sheng Sun


## 2024-01-07
+ [ Invisible Reflections: Leveraging Infrared Laser Reflections to Target Traffic Sign Perception](http://arxiv.org/abs/2401.03582)

    Takami Sato, Sri Hrushikesh Varma Bhupathiraju, Michael Clifford, Takeshi Sugawara, Qi Alfred Chen, Sara Rampazzi


+ [ Data-Driven Subsampling in the Presence of an Adversarial Actor](http://arxiv.org/abs/2401.03488)

    Abu Shafin Mohammad Mahdee Jameel, Ahmed P. Mohamed, Jinho Yi, Aly El Gamal, Akshay Malhotra


+ [ ROIC-DM: Robust Text Inference and Classification via Diffusion Model](http://arxiv.org/abs/2401.03514)

    Shilong Yuan, Wei Yuan, Tieke HE


## 2024-01-06
+ [ Data-Dependent Stability Analysis of Adversarial Training](http://arxiv.org/abs/2401.03156)

    Yihan Wang, Shuang Liu, Xiao-Shan Gao


+ [ End-to-End Anti-Backdoor Learning on Images and Time Series](http://arxiv.org/abs/2401.03215)

    Yujing Jiang, Xingjun Ma, Sarah Monazam Erfani, Yige Li, James Bailey


## 2024-01-05
+ [ Transferable Learned Image Compression-Resistant Adversarial Perturbations](http://arxiv.org/abs/2401.03115)

    Yang Sui, Zhuohang Li, Ding Ding, Xiang Pan, Xiaozhong Xu, Shan Liu, Zhenzhong Chen


+ [ Enhancing targeted transferability via feature space fine-tuning](http://arxiv.org/abs/2401.02727)

    Hui Zeng, Biwei Chen, Anjie Peng


+ [ Calibration Attack: A Framework For Adversarial Attacks Targeting Calibration](http://arxiv.org/abs/2401.02718)

    Stephen Obadinma, Xiaodan Zhu, Hongyu Guo


+ [ A backdoor attack against link prediction tasks with graph neural networks](http://arxiv.org/abs/2401.02663)

    Jiazhu Dai, Haoyu Sun


+ [ MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance](http://arxiv.org/abs/2401.02906)

    Renjie Pi, Tianyang Han, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong, Jipeng Zhang, Tong Zhang


## 2024-01-04
+ [ Vulnerabilities Unveiled: Adversarially Attacking a Multimodal Vision Langauge Model for Pathology Imaging](http://arxiv.org/abs/2401.02565)   

    Jai Prakash Veerla, Poojitha Thota, Partha Sai Guttikonda, Shirin Nilizadeh, Jacob M. Luber


+ [ A Random Ensemble of Encrypted models for Enhancing Robustness against Adversarial Examples](http://arxiv.org/abs/2401.02633)

    Ryota Iijima, Sayaka Shiota, Hitoshi Kiya


+ [ AdvSQLi: Generating Adversarial SQL Injections against Real-world WAF-as-a-service](http://arxiv.org/abs/2401.02615)

    Zhenqing Qu, Xiang Ling, Ting Wang, Xiang Chen, Shouling Ji, Chunming Wu


+ [ Evasive Hardware Trojan through Adversarial Power Trace](http://arxiv.org/abs/2401.02342)

    Behnam Omidi, Khaled N. Khasawneh, Ihsen Alouani


+ [ Object-oriented backdoor attack against image captioning](http://arxiv.org/abs/2401.02600)

    Meiling Li, Nan Zhong, Xinpeng Zhang, Zhenxing Qian, Sheng Li


+ [ DEM: A Method for Certifying Deep Neural Network Classifier Outputs in Aerospace](http://arxiv.org/abs/2401.02283)

    Guy Katz, Natan Levy, Idan Refaeli, Raz Yerushalmi


+ [ Secure Control of Connected and Automated Vehicles Using Trust-Aware Robust Event-Triggered Control Barrier Functions](http://arxiv.org/abs/2401.02306)

    H M Sabbir Ahmad, Ehsan Sabouni, Akua Dickson, Wei Xiao, Christos G. Cassandras, Wenchao Li


## 2024-01-03
+ [ Towards Robust Semantic Segmentation against Patch-based Attack via Attention Refinement](http://arxiv.org/abs/2401.01750)

    Zheng Yuan, Jie Zhang, Yude Wang, Shiguang Shan, Xilin Chen


+ [ Spy-Watermark: Robust Invisible Watermarking for Backdoor Attack](http://arxiv.org/abs/2401.02031)

    Ruofei Wang, Renjie Wan, Zongyu Guo, Qing Guo, Rui Huang


+ [ FullLoRA-AT: Efficiently Boosting the Robustness of Pretrained Vision Transformers](http://arxiv.org/abs/2401.01752)

    Zheng Yuan, Jie Zhang, Shiguang Shan


+ [ Integrated Cyber-Physical Resiliency for Power Grids under IoT-Enabled Dynamic Botnet Attacks](http://arxiv.org/abs/2401.01963)

    Yuhan Zhao, Juntao Chen, Quanyan Zhu


+ [ Enhancing Generalization of Invisible Facial Privacy Cloak via Gradient Accumulation](http://arxiv.org/abs/2401.01575)

    Xuannan Liu, Yaoyao Zhong, Weihong Deng, Hongzhi Shi, Xingchen Cui, Yunfeng Yin, Dongchao Wen


## 2024-01-02
+ [ Safety and Performance, Why Not Both? Bi-Objective Optimized Model  Compression against Heterogeneous Attacks Toward AI Software Deployment](https://arxiv.org/abs/2401.00996)

	Jie Zhu, Leye Wang, Xiao Han, Anmin Liu, Tao Xie


+ [ Teach Large Language Models to Forget Privacy](https://arxiv.org/abs/2401.00870)

	Ran Yan, Yujun Li, Wenqian Li, Peihua Mai, Yan Pang, Yinchuan Li


+ [ PPBFL: A Privacy Protected Blockchain-based Federated Learning Model](https://arxiv.org/abs/2401.01204)

	Yang Li, Chunhe Xia, Wanshuang Lin, Tianbo Wang


+ [ LLbezpeky: Leveraging Large Language Models for Vulnerability Detection](https://arxiv.org/abs/2401.01269)

	Noble Saji Mathews, Yelizaveta Brus, Yousra Aafer, Mei Nagappan, Shane McIntosh


+ [ Noise-NeRF: Hide Information in Neural Radiance Fields using Trainable  Noise](https://arxiv.org/abs/2401.01216)

	Qinglong Huang, Yong Liao, Yanbin Hao, Pengyuan Zhou


+ [ Imperio: Language-Guided Backdoor Attacks for Arbitrary Model Control](https://arxiv.org/abs/2401.01085)

	Ka-Ho Chow, Wenqi Wei, Lei Yu


+ [ Efficient Sparse Least Absolute Deviation Regression with Differential  Privacy](https://arxiv.org/abs/2401.01294)

	Weidong Liu, Xiaojun Mao, Xiaofei Zhang, Xin Zhang


+ [ Opening A Pandora's Box: Things You Should Know in the Era of Custom  GPTs](https://arxiv.org/abs/2401.00905)

	Guanhong Tao, Siyuan Cheng, Zhuo Zhang, Junmin Zhu, Guangyu Shen, Xiangyu Zhang


+ [ A Novel Evaluation Framework for Assessing Resilience Against Prompt  Injection Attacks in Large Language Models](https://arxiv.org/abs/2401.00991)

	Daniel Wankit Yip, Aysan Esmradi, Chun Fai Chan


+ [ Detection and Defense Against Prominent Attacks on Preconditioned  LLM-Integrated Virtual Assistants](https://arxiv.org/abs/2401.00994)

	Chun Fai Chan, Daniel Wankit Yip, Aysan Esmradi


## 2024-01-01
+ [ Red Teaming for Large Language Models At Scale: Tackling Hallucinations  on Mathematics Tasks](https://arxiv.org/abs/2401.00290)

	Aleksander Buszydlik, Karol Dobiczek, Michał Teodor Okoń, Konrad Skublicki, Philip Lippmann, Jie Yang


+ [ A & B == B & A: Triggering Logical Reasoning Failures in Large Language  Models](https://arxiv.org/abs/2401.00757)

	Yuxuan Wan, Wenxuan Wang, Yiliu Yang, Youliang Yuan, Jen-tse Huang, Pinjia He, Wenxiang Jiao, Michael R. Lyu


+ [ SHARE: Single-view Human Adversarial REconstruction](https://arxiv.org/abs/2401.00343)

	Shreelekha Revankar, Shijia Liao, Yu Shen, Junbang Liang, Huaishu Peng, Ming Lin


+ [ SSL-OTA: Unveiling Backdoor Threats in Self-Supervised Learning for  Object Detection](https://arxiv.org/abs/2401.00137)

	Qiannan Wang, Changchun Yin, Liming Fang, Lu Zhou, Zhe Liu, Run Wang, Chenhao Lin


+ [ Adversarially Trained Actor Critic for offline CMDPs](https://arxiv.org/abs/2401.00629)

	Honghao Wei, Xiyue Peng, Xin Liu, Arnob Ghosh


## 2023-12-31
+ [ Is It Possible to Backdoor Face Forgery Detection with Natural Triggers](http://arxiv.org/abs/2401.00414)

    Xiaoxuan Han, Songlin Yang, Wei Wang, Ziwen He, Jing Dong


## 2023-12-30
+ [ Explainability-Driven Leaf Disease Classification using Adversarial Training and Knowledge Distillation](http://arxiv.org/abs/2401.00334)

    Sebastian-Vasile Echim, Iulian-Marius Tăiatu, Dumitru-Clementin Cercel, Florin Pop       


+ [ CamPro: Camera-based Anti-Facial Recognition](http://arxiv.org/abs/2401.00151)  

    Wenjun Zhu, Yuan Sun, Jiani Liu, Yushi Cheng, Xiaoyu Ji, Wenyuan Xu


+ [ TPatch: A Triggered Physical Adversarial Patch](http://arxiv.org/abs/2401.00148)

    Wenjun Zhu, Xiaoyu Ji, Yushi Cheng, Shibo Zhang, Wenyuan Xu


+ [ A clean-label graph backdoor attack method in node classification task](http://arxiv.org/abs/2401.00163)

    Xiaogang Xing, Ming Xu, Yujing Bai, Dongdong Yang


## 2023-12-29
+ [ Jatmo: Prompt Injection Defense by Task-Specific Finetuning](http://arxiv.org/abs/2312.17673)

    Julien Piet, Maha Alrashed, Chawin Sitawarin, Sizhe Chen, Zeming Wei, Elizabeth Sun, Basel Alomair, David Wagner


+ [ Towards Faithful Explanations for Text Classification with Robustness Improvement and Explanation Guided Training](http://arxiv.org/abs/2312.17591)

    Dongfang Li, Baotian Hu, Qingcai Chen, Shan He


## 2023-12-28
+ [ Layer Attack Unlearning: Fast and Accurate Machine Unlearning via Layer  Level Attack and Knowledge Distillation](https://arxiv.org/abs/2312.16823)

	Hyunjune Kim, Sangyong Lee, Simon S. Woo


+ [ Adversarial Representation with Intra-Modal and Inter-Modal Graph  Contrastive Learning for Multimodal Emotion Recognition](https://arxiv.org/abs/2312.16778)

	Yuntao Shou, Tao Meng, Wei Ai, Keqin Li


+ [ SoK: Taming the Triangle -- On the Interplays between Fairness,  Interpretability and Privacy in Machine Learning](https://arxiv.org/abs/2312.16191)

	Julien Ferry, Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Mohamed Siala


+ [ Adversarial Attacks on Image Classification Models: Analysis and Defense](http://arxiv.org/abs/2312.16880)

    Jaydip Sen, Abhiraj Sen, Ananda Chatterjee


+ [ BlackboxBench: A Comprehensive Benchmark of Black-box Adversarial Attacks](http://arxiv.org/abs/2312.16979)

    Meixi Zheng, Xuanchen Yan, Zihao Zhu, Hongrui Chen, Baoyuan Wu


+ [ Attack Tree Analysis for Adversarial Evasion Attacks](http://arxiv.org/abs/2312.16957)

    Yuki Yamaguchi, Toshiaki Aoki


+ [ DOEPatch: Dynamically Optimized Ensemble Model for Adversarial Patches Generation](http://arxiv.org/abs/2312.16907)

    Wenyi Tan, Yang Li, Chenxing Zhao, Zhunga Liu, Quan Pan


+ [ Securing NextG Systems against Poisoning Attacks on Federated Learning: A Game-Theoretic Solution](http://arxiv.org/abs/2312.17164)

    Yalin E. Sagduyu, Tugba Erpek, Yi Shi


+ [ Timeliness: A New Design Metric and a New Attack Surface](http://arxiv.org/abs/2312.17220)

    Priyanka Kaswan, Sennur Ulukus


## 2023-12-27
+ [ Adversarial Data Poisoning for Fake News Detection: How to Make a Model  Misclassify a Target News without Modifying It](https://arxiv.org/abs/2312.15228)

	Federico Siciliano, Luca Maiano, Lorenzo Papa, Federica Baccin, Irene Amerini, Fabrizio Silvestri


+ [ Adversarial Attacks on LoRa Device Identification and Rogue Signal Detection with Deep Learning](http://arxiv.org/abs/2312.16715)

    Yalin E. Sagduyu, Tugba Erpek


+ [ Domain Generalization with Vital Phase Augmentation](http://arxiv.org/abs/2312.16451)

    Ingyun Lee, Wooju Lee, Hyun Myung


## 2023-12-26
+ [ From Text to Multimodal: A Comprehensive Survey of Adversarial Example Generation in Question Answering Systems](http://arxiv.org/abs/2312.16156) 

    Gulsum Yigit, Mehmet Fatih Amasyali


+ [ Natural Adversarial Patch Generation Method Based on Latent Diffusion Model](http://arxiv.org/abs/2312.16401)

    Xianyi Chen, Fazhan Liu, Dong Jiang, Kai Yan


+ [ Universal Pyramid Adversarial Training for Improved ViT Performance](http://arxiv.org/abs/2312.16339)

    Ping-yeh Chiang, Yipin Zhou, Omid Poursaeed, Satya Narayan Shukla, Ashish Shah, Tom Goldstein, Ser-Nam Lim


+ [ Robust Survival Analysis with Adversarial Regularization](http://arxiv.org/abs/2312.16019)

    Michael Potter, Stefano Maxenti, Michael Everett


## 2023-12-25
+ [ GanFinger: GAN-Based Fingerprint Generation for Deep Neural Network Ownership Verification](http://arxiv.org/abs/2312.15617)

    Huali Ren, Anli Yan, Xiaojun Ren, Pei-Gen Ye, Chong-zhi Gao, Zhili Zhou, Jin Li


+ [ Adversarial Item Promotion on Visually-Aware Recommender Systems by Guided Diffusion](http://arxiv.org/abs/2312.15826)

    Lijian Chen, Wei Yuan, Tong Chen, Quoc Viet Hung Nguyen, Lizhen Cui, Hongzhi Yin


+ [ Punctuation Matters! Stealthy Backdoor Attack for Language Models](http://arxiv.org/abs/2312.15867)

    Xuan Sheng, Zhicheng Li, Zhaoyang Han, Xiangmao Chang, Piji Li


## 2023-12-24
+ [ Benchmarking and Defending Against Indirect Prompt Injection Attacks on  Large Language Models](https://arxiv.org/abs/2312.14197)

	Jingwei Yi, Yueqi Xie, Bin Zhu, Keegan Hines, Emre Kiciman, Guangzhong Sun, Xing Xie, Fangzhao Wu


+ [ Hierarchical Multi-Agent Reinforcement Learning for Assessing False-Data  Injection Attacks on Transportation Networks](https://arxiv.org/abs/2312.14625)

	Taha Eghtesad, Sirui Li, Yevgeniy Vorobeychik, Aron Laszka


+ [ AutoAugment Input Transformation for Highly Transferable Targeted  Attacks](https://arxiv.org/abs/2312.14218)

	Haobo Lu, Xin Liu, Kun He


+ [ Can Machines Learn Robustly, Privately, and Efficiently?](https://arxiv.org/abs/2312.14712)

	Youssef Allouah, Rachid Guerraoui, John Stephan


## 2023-12-23
+ [ Adversarial Data Poisoning for Fake News Detection: How to Make a Model Misclassify a Target News without Modifying It](http://arxiv.org/abs/2312.15228)

    Federico Siciliano, Luca Maiano, Lorenzo Papa, Federica Baccin, Irene Amerini, Fabrizio Silvestri


+ [ Pre-trained Trojan Attacks for Visual Recognition](http://arxiv.org/abs/2312.15172)

    Aishan Liu, Xinwei Zhang, Yisong Xiao, Yuguang Zhou, Siyuan Liang, Jiakai Wang, Xianglong Liu, Xiaochun Cao, Dacheng Tao


## 2023-12-22
+ [ MEAOD: Model Extraction Attack against Object Detectors](http://arxiv.org/abs/2312.14677)

    Zeyu Li, Chenghui Shi, Yuwen Pu, Xuhong Zhang, Yu Li, Jinbao Li, Shouling Ji


+ [ Asymmetric Bias in Text-to-Image Generation with Adversarial Attacks](http://arxiv.org/abs/2312.14440)

    Haz Sameen Shahgir, Xianghao Kong, Greg Ver Steeg, Yue Dong


+ [ Understanding the Regularity of Self-Attention with Optimal Transport](http://arxiv.org/abs/2312.14820)

    Valérie Castin, Pierre Ablin, Gabriel Peyré


+ [ Attacking Byzantine Robust Aggregation in High Dimensions](http://arxiv.org/abs/2312.14461)

    Sarthak Choudhary, Aashish Kolluri, Prateek Saxena


+ [ SODA: Protecting Proprietary Information in On-Device Machine Learning Models](http://arxiv.org/abs/2312.15036)

    Akanksha Atrey, Ritwik Sinha, Saayan Mitra, Prashant Shenoy


+ [ Energy-based learning algorithms for analog computing: a comparative study](http://arxiv.org/abs/2312.15103)

    Benjamin Scellier, Maxence Ernoult, Jack Kendall, Suhas Kumar


+ [ Adaptive Domain Inference Attack](http://arxiv.org/abs/2312.15088)

    Yuechun Gu, Keke Chen


## 2023-12-21
+ [ Adversarial Markov Games: On Adaptive Decision-Based Attacks and  Defenses](https://arxiv.org/abs/2312.13435)

	Ilias Tsingenopoulos, Vera Rimmer, Davy Preuveneers, Fabio Pierazzi, Lorenzo Cavallaro, Wouter Joosen


+ [ Using GPT-4 Prompts to Determine Whether Articles Contain Functional  Evidence Supporting or Refuting Variant Pathogenicity](https://arxiv.org/abs/2312.13521)

	Samuel J. Aronson, Kalotina Machini, Pranav Sriraman, Jiyeon Shin, Emma R. Henricks, Charlotte Mailly, Angie J. Nottage, Michael Oates, Matthew S. Lebo


+ [ SADA: Semantic adversarial unsupervised domain adaptation for Temporal  Action Localization](https://arxiv.org/abs/2312.13377)

	David Pujol-Perich, Albert Clapés, Sergio Escalera


+ [ ARBiBench: Benchmarking Adversarial Robustness of Binarized Neural  Networks](https://arxiv.org/abs/2312.13575)

	Peng Zhao, Jiehua Zhang, Bowen Peng, Longguang Wang, YingMei Wei, Yu Liu, Li Liu


+ [ Where and How to Attack? A Causality-Inspired Recipe for Generating  Counterfactual Adversarial Examples](https://arxiv.org/abs/2312.13628)

	Ruichu Cai, Yuxuan Zhu, Jie Qiao, Zefeng Liang, Furui Liu, Zhifeng Hao


+ [ Manipulating Trajectory Prediction with Backdoors](https://arxiv.org/abs/2312.13863)

	Kaouther Massoud, Kathrin Grosse, Mickael Chen, Matthieu Cord, Patrick Pérez, Alexandre Alahi


## 2023-12-20
+ [ Progressive Poisoned Data Isolation for Training-time Backdoor Defense](https://arxiv.org/abs/2312.12724)

	Yiming Chen, Haiwei Wu, Jiantao Zhou


+ [ PGN: A perturbation generation network against deep reinforcement  learning](https://arxiv.org/abs/2312.12904)

	Xiangjuan Li, Feifan Li, Yang Li, Quan Pan


+ [ Learning and Forgetting Unsafe Examples in Large Language Models](https://arxiv.org/abs/2312.12736)

	Jiachen Zhao, Zhun Deng, David Madras, James Zou, Mengye Ren


+ [ Adaptive Distribution Masked Autoencoders for Continual Test-Time  Adaptation](https://arxiv.org/abs/2312.12480)

	Jiaming Liu, Ran Xu, Senqiao Yang, Renrui Zhang, Qizhe Zhang, Zehui Chen, Yandong Guo, Shanghang Zhang


+ [ Misalign, Contrast then Distill: Rethinking Misalignments in  Language-Image Pretraining](https://arxiv.org/abs/2312.12661)

	Bumsoo Kim, Yeonsik Jo, Jinhyung Kim, Seung Hwan Kim


+ [ Mutual-modality Adversarial Attack with Semantic Perturbation](https://arxiv.org/abs/2312.12768)

	Jingwen Ye, Ruonan Yu, Songhua Liu, Xinchao Wang


+ [ Trust, But Verify: A Survey of Randomized Smoothing Techniques](https://arxiv.org/abs/2312.12608)

	Anupriya Kumari, Devansh Bhardwaj, Sukrit Jindal, Sarthak Gupta


+ [ Stability of Graph Convolutional Neural Networks through the lens of  small perturbation analysis](https://arxiv.org/abs/2312.12934)

	Lucia Testa, Claudio Battiloro, Stefania Sardellitti, Sergio Barbarossa


+ [ Neural Stochastic Differential Equations with Change Points: A  Generative Adversarial Approach](https://arxiv.org/abs/2312.13152)

	Zhongchang Sun, Yousef El-Laham, Svitlana Vyetrenko


+ [ SkyMask: Attack-agnostic Robust Federated Learning with Fine-grained  Learnable Masks](https://arxiv.org/abs/2312.12484)

	Peishen Yan, Hao Wang, Tao Song, Yang Hua, Ruhui Ma, Ningxin Hu, Mohammad R. Haghighat, Haibing Guan


+ [ Can Large Language Models Identify And Reason About Security  Vulnerabilities? Not Yet](https://arxiv.org/abs/2312.12575)

	Saad Ullah, Mingji Han, Saurabh Pujar, Hammond Pearce, Ayse Coskun, Gianluca Stringhini


## 2023-12-19
+ [ A Red Teaming Framework for Securing AI in Maritime Autonomous Systems](https://arxiv.org/abs/2312.11500)

	Mathew J. Walter, Aaron Barrett, Kimberly Tam


+ [ Maatphor: Automated Variant Analysis for Prompt Injection Attacks](https://arxiv.org/abs/2312.11513)

	Ahmed Salem, Andrew Paverd, Boris Köpf


+ [ Robust Communicative Multi-Agent Reinforcement Learning with Active  Defense](https://arxiv.org/abs/2312.11545)

	Lebin Yu, Yunbo Qiu, Quanming Yao, Yuan Shen, Xudong Zhang, Jian Wang


+ [ Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation  in ultra low-data regimes](https://arxiv.org/abs/2312.12112)

	Nabeel Seedat, Nicolas Huynh, Boris van Breugel, Mihaela van der Schaar


+ [ Bypassing the Safety Training of Open-Source LLMs with Priming Attacks](https://arxiv.org/abs/2312.12321)

	Jason Vega, Isha Chaudhary, Changming Xu, Gagandeep Singh


+ [ Chasing Fairness in Graphs: A GNN Architecture Perspective](https://arxiv.org/abs/2312.12369)

	Zhimeng Jiang, Xiaotian Han, Chao Fan, Zirui Liu, Na Zou, Ali Mostafavi, Xia Hu


+ [ Adversarial AutoMixup](https://arxiv.org/abs/2312.11954)

	Huafeng Qin, Xin Jin, Yun Jiang, Mounim A. El-Yacoubi, Xinbo Gao


+ [ Prompting Hard or Hardly Prompting: Prompt Inversion for Text-to-Image  Diffusion Models](https://arxiv.org/abs/2312.12416)

	Shweta Mahajan, Tanzila Rahman, Kwang Moo Yi, Leonid Sigal


+ [ A Study on Transferability of Deep Learning Models for Network Intrusion  Detection](https://arxiv.org/abs/2312.11550)

	Shreya Ghosh, Abu Shafin Mohammad Mahdee Jameel, Aly El Gamal


+ [ Android Malware Detection with Unbiased Confidence Guarantees](https://arxiv.org/abs/2312.11559)

	Harris Papadopoulos, Nestoras Georgiou, Charalambos Eliades, Andreas Konstantinidis


+ [ Terrapin Attack: Breaking SSH Channel Integrity By Sequence Number  Manipulation](https://arxiv.org/abs/2312.12422)

	Fabian Bäumer, Marcus Brinkmann, Jörg Schwenk


## 2023-12-18
+ [ Annotation-Free Automatic Music Transcription with Scalable Synthetic  Data and Adversarial Domain Confusion](https://arxiv.org/abs/2312.10402)

	Gakusei Sato, Taketo Akama


+ [ SAME: Sample Reconstruction Against Model Extraction Attacks](https://arxiv.org/abs/2312.10578)

	Yi Xie, Jie Zhang, Shiqian Zhao, Tianwei Zhang, Xiaofeng Chen


+ [ Synthesizing Black-box Anti-forensics DeepFakes with High Visual Quality](https://arxiv.org/abs/2312.10713)

	Bing Fan, Shu Hu, Feng Ding


+ [ Unmasking Deepfake Faces from Videos Using An Explainable Cost-Sensitive  Deep Learning Approach](https://arxiv.org/abs/2312.10740)

	Faysal Mahmud, Yusha Abdullah, Minhajul Islam, Tahsin Aziz


+ [ DataElixir: Purifying Poisoned Dataset to Mitigate Backdoor Attacks via  Diffusion Models](https://arxiv.org/abs/2312.11057)

	Jiachen Zhou, Peizhuo Lv, Yibing Lan, Guozhu Meng, Kai Chen, Hualong Ma


+ [ Adv-Diffusion: Imperceptible Adversarial Face Identity Attack via Latent  Diffusion Model](https://arxiv.org/abs/2312.11285)

	Decheng Liu, Xijun Wang, Chunlei Peng, Nannan Wang, Ruiming Hu, Xinbo Gao


+ [ Bengali Intent Classification with Generative Adversarial BERT](https://arxiv.org/abs/2312.10679)

	Mehedi Hasan, Mohammad Jahid Ibna Basher, Md. Tanvir Rouf Shawon


+ [ Perturbation-Invariant Adversarial Training for Neural Ranking Models:  Improving the Effectiveness-Robustness Trade-Off](https://arxiv.org/abs/2312.10329)

	Yu-An Liu, Ruqing Zhang, Mingkun Zhang, Wei Chen, Maarten de Rijke, Jiafeng Guo, Xueqi Cheng


+ [ Closing the Gap: Achieving Better Accuracy-Robustness Tradeoffs Against  Query-Based Attacks](https://arxiv.org/abs/2312.10132)

	Pascal Zimmer, Sébastien Andreina, Giorgia Azzurra Marson, Ghassan Karame


+ [ Anomaly Score: Evaluating Generative Models and Individual Generated  Images based on Complexity and Vulnerability](https://arxiv.org/abs/2312.10634)

	Jaehui Hwang, Junghyuk Lee, Jong-Seok Lee


+ [ The Ultimate Combo: Boosting Adversarial Example Transferability by  Composing Data Augmentations](https://arxiv.org/abs/2312.11309)

	Zebin Yun, Achi-Or Weingarten, Eyal Ronen, Mahmood Sharif


+ [ On Robustness to Missing Video for Audiovisual Speech Recognition](https://arxiv.org/abs/2312.10088)

	Oscar Chang, Otavio Braga, Hank Liao, Dmitriy Serdyuk, Olivier Siohan


+ [ Rethinking Robustness of Model Attributions](https://arxiv.org/abs/2312.10534)

	Sandesh Kamath, Sankalp Mittal, Amit Deshpande, Vineeth N Balasubramanian


+ [ The Pros and Cons of Adversarial Robustness](https://arxiv.org/abs/2312.10911)

	Yacine Izza, Joao Marques-Silva


+ [ PPIDSG: A Privacy-Preserving Image Distribution Sharing Scheme with GAN  in Federated Learning](https://arxiv.org/abs/2312.10380)

	Yuting Ma, Yuanzhi Yao, Xiaohua Xu


+ [ TrojFSP: Trojan Insertion in Few-shot Prompt Tuning](https://arxiv.org/abs/2312.10467)

	Mengxin Zheng, Jiaqi Xue, Xun Chen, YanShan Wang, Qian Lou, Lei Jiang


+ [ TrojFair: Trojan Fairness Attacks](https://arxiv.org/abs/2312.10508)

	Mengxin Zheng, Jiaqi Xue, Yi Sheng, Lei Yang, Qian Lou, Lei Jiang


+ [ Adversarially Balanced Representation for Continuous Treatment Effect  Estimation](https://arxiv.org/abs/2312.10570)

	Amirreza Kazemi, Martin Ester


+ [ Model Stealing Attack against Graph Classification with Authenticity,  Uncertainty and Diversity](https://arxiv.org/abs/2312.10943)

	Zhihao Zhu, Chenwang Wu, Rui Fan, Yi Yang, Defu Lian, Enhong Chen


+ [ MISA: Unveiling the Vulnerabilities in Split Federated Learning](https://arxiv.org/abs/2312.11026)

	Wei Wan, Yuxuan Ning, Shengshan Hu1, Lulu Xue, Minghui Li, Leo Yu Zhang, Hai Jin


+ [ Uncertainty-based Fairness Measures](https://arxiv.org/abs/2312.11299)

	Selim Kuzucu, Jiaee Cheong, Hatice Gunes, Sinan Kalkan


+ [ Improving Environment Robustness of Deep Reinforcement Learning  Approaches for Autonomous Racing Using Bayesian Optimization-based Curriculum  Learning](https://arxiv.org/abs/2312.10557)

	Rohan Banerjee, Prishita Ray, Mark Campbell


+ [ Harnessing Inherent Noises for Privacy Preservation in Quantum Machine  Learning](https://arxiv.org/abs/2312.11126)

	Keyi Ju, Xiaoqi Qin, Hui Zhong, Xinyue Zhang, Miao Pan, Baoling Liu


+ [ UltraClean: A Simple Framework to Train Robust Neural Networks against  Backdoor Attacks](https://arxiv.org/abs/2312.10657)

	Bingyin Zhao, Yingjie Lao


+ [ A Mutation-Based Method for Multi-Modal Jailbreaking Attack Detection](https://arxiv.org/abs/2312.10766)

	Xiaoyu Zhang, Cen Zhang, Tianlin Li, Yihao Huang, Xiaojun Jia, Xiaofei Xie, Yang Liu, Chao Shen


+ [ Federated learning with differential privacy and an untrusted aggregator](https://arxiv.org/abs/2312.10789)

	Kunlong Liu, Trinabh Gupta


+ [ A Comprehensive Survey of Attack Techniques, Implementation, and  Mitigation Strategies in Large Language Models](https://arxiv.org/abs/2312.10982)

	Aysan Esmradi, Daniel Wankit Yip, Chun Fai Chan


## 2023-12-17
+ [ Investigating Responsible AI for Scientific Research: An Empirical Study](https://arxiv.org/abs/2312.09561)

	Muneera Bano, Didar Zowghi, Pip Shea, Georgina Ibarra


+ [ Continual Adversarial Defense](https://arxiv.org/abs/2312.09481)

	Qian Wang, Yaoyao Liu, Hefei Ling, Yingwei Li, Qihao Liu, Ping Li, Jiazhong Chen, Alan Yuille, Ning Yu


+ [ SlowTrack: Increasing the Latency of Camera-based Perception in  Autonomous Driving Using Adversarial Examples](https://arxiv.org/abs/2312.09520)

	Chen Ma, Ningfei Wang, Qi Alfred Chen, Chao Shen


+ [ Embodied Adversarial Attack: A Dynamic Robust Physical Attack in  Autonomous Driving](https://arxiv.org/abs/2312.09554)

	Yitong Sun, Yao Huang, Xingxing Wei


+ [ Adversarial Robustness on Image Classification with $k$-means](https://arxiv.org/abs/2312.09533)

	Rollin Omari, Junae Kim, Paul Montague


+ [ Fragility, Robustness and Antifragility in Deep Learning](https://arxiv.org/abs/2312.09821)

	Chandresh Pravin, Ivan Martino, Giuseppe Nicosia, Varun Ojha


+ [ Reliable Probabilistic Classification with Neural Networks](https://arxiv.org/abs/2312.09912)

	Harris Papadopoulos


+ [ A Malware Classification Survey on Adversarial Attacks and Defences](https://arxiv.org/abs/2312.09636)

	Mahesh Datta Sai Ponnuru, Likhitha Amasala, Tanu Sree Bhimavarapu, Guna Chaitanya Garikipati


+ [ Silent Guardian: Protecting Text from Malicious Exploitation by Large  Language Models](https://arxiv.org/abs/2312.09669)

	Jiawei Zhao, Kejiang Chen, Xiaojian Yuan, Yuang Qi, Weiming Zhang, Nenghai Yu


## 2023-12-16
+ [ Towards Inductive Robustness: Distilling and Fostering Wave-induced  Resonance in Transductive GCNs Against Graph Adversarial Attacks](https://arxiv.org/abs/2312.08651)

	Ao Liu, Wenshan Li, Tao Li, Beibei Li, Hanyuan Huang, Pan Zhou


## 2023-12-15
+ [ PhasePerturbation: Speech Data Augmentation via Phase Perturbation for  Automatic Speech Recognition](https://arxiv.org/abs/2312.08571)

	Chengxi Lei, Satwinder Singh, Feng Hou, Xiaoyun Jia, Ruili Wang


+ [ Data and Model Poisoning Backdoor Attacks on Wireless Federated  Learning, and the Defense Mechanisms: A Comprehensive Survey](https://arxiv.org/abs/2312.08667)

	Yichen Wan, Youyang Qu, Wei Ni, Yong Xiang, Longxiang Gao, Ekram Hossain


+ [ The Earth is Flat because...: Investigating LLMs' Belief towards  Misinformation via Persuasive Conversation](https://arxiv.org/abs/2312.09085)

	Rongwu Xu, Brian S. Lin, Shujian Yang, Tianqi Zhang, Weiyan Shi, Tianwei Zhang, Zhixuan Fang, Wei Xu, Han Qiu


+ [ Privacy Constrained Fairness Estimation for Decision Trees](https://arxiv.org/abs/2312.08413)

	Florian van der Steen, Fré Vink, Heysem Kaya


+ [ Scalable Ensemble-based Detection Method against Adversarial Attacks for  speaker verification](https://arxiv.org/abs/2312.08622)

	Haibin Wu, Heng-Cheng Kuo, Yu Tsao, Hung-yi Lee

	
## 2023-12-14
+ [ AVA: Inconspicuous Attribute Variation-based Adversarial Attack bypassing DeepFake Detection](http://arxiv.org/abs/2312.08675)

    Xiangtao Meng, Li Wang, Shanqing Guo, Lei Ju, Qingchuan Zhao


+ [ On the Difficulty of Defending Contrastive Learning against Backdoor Attacks](http://arxiv.org/abs/2312.09057)

    Changjiang Li, Ren Pang, Bochuan Cao, Zhaohan Xi, Jinghui Chen, Shouling Ji, Ting Wang


+ [ Detection and Defense of Unlearnable Examples](http://arxiv.org/abs/2312.08898)

    Yifan Zhu, Lijia Yu, Xiao-Shan Gao


+ [ Improve Robustness of Reinforcement Learning against Observation Perturbations via $l_\infty$ Lipschitz Policy Networks](http://arxiv.org/abs/2312.08751)

    Buqing Nie, Jingtian Ji, Yangqing Fu, Yue Gao


+ [ Data and Model Poisoning Backdoor Attacks on Wireless Federated Learning, and the Defense Mechanisms: A Comprehensive Survey](http://arxiv.org/abs/2312.08667)     

    Yichen Wan, Youyang Qu, Wei Ni, Yong Xiang, Longxiang Gao, Ekram Hossain


+ [ DRAM-Locker: A General-Purpose DRAM Protection Mechanism against Adversarial DNN Weight Attacks](http://arxiv.org/abs/2312.09027)

    Ranyang Zhou, Sabbir Ahmed, Arman Roohi, Adnan Siraj Rakin, Shaahin Angizi


+ [ Forbidden Facts: An Investigation of Competing Objectives in Llama-2](http://arxiv.org/abs/2312.08793)

    Tony T. Wang, Miles Wang, Kaivu Hariharan, Nir Shavit


+ [ Coevolutionary Algorithm for Building Robust Decision Trees under Minimax Regret](http://arxiv.org/abs/2312.09078)

    Adam Żychowski, Andrew Perrault, Jacek Mańdziuk


+ [ Exploring Transferability for Randomized Smoothing](http://arxiv.org/abs/2312.09020)

    Kai Qiu, Huishuai Zhang, Zhirong Wu, Stephen Lin


+ [ Split-Ensemble: Efficient OOD-aware Ensemble via Task and Model Splitting](http://arxiv.org/abs/2312.09148)

    Anthony Chen, Huanrui Yang, Yulu Gan, Denis A Gudovskiy, Zhen Dong, Haofan Wang, Tomoyuki Okuno, Yohei Nakata, Shanghang Zhang, Kurt Keutzer


## 2023-12-13
+ [ Defenses in Adversarial Machine Learning: A Survey](http://arxiv.org/abs/2312.08890)

    Baoyuan Wu, Shaokui Wei, Mingli Zhu, Meixi Zheng, Zihao Zhu, Mingda Zhang, Hongrui Chen, Danni Yuan, Li Liu, Qingshan Liu


+ [ Robust Few-Shot Named Entity Recognition with Boundary Discrimination and Correlation Purification](http://arxiv.org/abs/2312.07961)

    Xiaojun Xue, Chunxia Zhang, Tianxiang Xu, Zhendong Niu


+ [ Universal Adversarial Framework to Improve Adversarial Robustness for Diabetic Retinopathy Detection](http://arxiv.org/abs/2312.08193)

    Samrat Mukherjee, Dibyanayan Bandyopadhyay, Baban Gain, Asif Ekbal


+ [ Towards Inductive Robustness: Distilling and Fostering Wave-induced Resonance in Transductive GCNs Against Graph Adversarial Attacks](http://arxiv.org/abs/2312.08651)

    Ao Liu, Wenshan Li, Tao Li, Beibei Li, Hanyuan Huang, Pan Zhou


+ [ Scalable Ensemble-based Detection Method against Adversarial Attacks for speaker verification](http://arxiv.org/abs/2312.08622)

    Haibin Wu, Heng-Cheng Kuo, Yu Tsao, Hung-yi Lee


+ [ Accelerating the Global Aggregation of Local Explanations](http://arxiv.org/abs/2312.07991)

    Alon Mor, Yonatan Belinkov, Benny Kimelfeld


+ [ Erasing Self-Supervised Learning Backdoor by Cluster Activation Masking](http://arxiv.org/abs/2312.07955)

    Shengsheng Qian, Yifei Wang, Dizhan Xue, Shengjie Zhang, Huaiwen Zhang, Changsheng Xu


+ [ Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models](http://arxiv.org/abs/2312.08303)

    Jiang Zhang, Qiong Wu, Yiming Xu, Cheng Cao, Zheng Du, Konstantinos Psounis


## 2023-12-12
+ [ Patch-MI: Enhancing Model Inversion Attacks via Patch-Based  Reconstruction](https://arxiv.org/abs/2312.07040)

	Jonggyu Jang, Hyeonsu Lyu, Hyun Jong Yang


+ [ Radio Signal Classification by Adversarially Robust Quantum Machine Learning](http://arxiv.org/abs/2312.07821)

    Yanqiu Wu, Eromanga Adermann, Chandra Thapa, Seyit Camtepe, Hajime Suzuki, Muhammad 
Usman


+ [ SSTA: Salient Spatially Transformed Attack](http://arxiv.org/abs/2312.07258)

    Renyang Liu, Wei Zhou, Sixin Wu, Jun Zhao, Kwok-Yan Lam


+ [ DTA: Distribution Transform-based Attack for Query-Limited Scenario](http://arxiv.org/abs/2312.07245)

    Renyang Liu, Wei Zhou, Xin Jin, Song Gao, Yuanyu Wang, Ruxin Wang


+ [ May the Noise be with you: Adversarial Training without Adversarial Examples](http://arxiv.org/abs/2312.08877)

    Ayoub Arous, Andres F Lopez-Lopera, Nael Abu-Ghazaleh, Ihsen Alouani


+ [ Focus on Hiders: Exploring Hidden Threats for Enhancing Adversarial Training](http://arxiv.org/abs/2312.07067)

    Qian Li, Yuxiao Hu, Yinpeng Dong, Dongxiao Zhang, Yuntian Chen


+ [ Attacking the Loop: Adversarial Attacks on Graph-based Loop Closure Detection](http://arxiv.org/abs/2312.06991)

    Jonathan J. Y. Kim, Martin Urschler, Patricia J. Riddle, Jorg S. Wicker


+ [ Collapse-Oriented Adversarial Training with Triplet Decoupling for Robust Image Retrieval](http://arxiv.org/abs/2312.07364)

    Qiwei Tian, Chenhao Lin, Qian Li, Zhengyu Zhao, Chao Shen


+ [ ReRoGCRL: Representation-based Robustness in Goal-Conditioned Reinforcement Learning](http://arxiv.org/abs/2312.07392)

    Xiangyu Yin, Sihao Wu, Jiaxu Liu, Meng Fang, Xingyu Zhao, Xiaowei Huang, Wenjie Ruan

+ [ Robust MRI Reconstruction by Smoothed Unrollin](http://arxiv.org/abs/2312.07784)

    Shijun Liang, Van Hoang Minh Nguyen, Jinghan Jia, Ismail Alkhouri, Sijia Liu, Saiprasad Ravishankar


+ [ Cost Aware Untargeted Poisoning Attack against Graph Neural Networks,](http://arxiv.org/abs/2312.07158)

    Yuwei Han, Yuni Lai, Yulin Zhu, Kai Zhou


+ [ EdgePruner: Poisoned Edge Pruning in Graph Contrastive Learning](http://arxiv.org/abs/2312.07022)

    Hiroya Kato, Kento Hasegawa, Seira Hidano, Kazuhide Fukushima


+ [ Causality Analysis for Evaluating the Security of Large Language Models](http://arxiv.org/abs/2312.07876)

    Wei Zhao, Zhe Li, Jun Sun


+ [ SimAC: A Simple Anti-Customization Method against Text-to-Image Synthesis of Diffusion Models](http://arxiv.org/abs/2312.07865)

    Feifei Wang, Zhentao Tan, Tianyi Wei, Yue Wu, Qidong Huang


+ [ Eroding Trust In Aerial Imagery: Comprehensive Analysis and Evaluation Of Adversarial Attacks In Geospatial Systems](http://arxiv.org/abs/2312.07389)

    Michael Lanier, Aayush Dhakal, Zhexiao Xiong, Arthur Li, Nathan Jacobs, Yevgeniy Vorobeychik


+ [ Securing Graph Neural Networks in MLaaS: A Comprehensive Realization of Query-based 
Integrity Verification](http://arxiv.org/abs/2312.07870)

    Bang Wu, Xingliang Yuan, Shuo Wang, Qi Li, Minhui Xue, Shirui Pan


+ [ Majority is Not Required: A Rational Analysis of the Private Double-Spend Attack from a Sub-Majority Adversary](http://arxiv.org/abs/2312.07709)

    Yanni Georghiades, Rajesh Mishra, Karl Kreder, Sriram Vishwanath


+ [ Divide-and-Conquer Attack: Harnessing the Power of LLM to Bypass the  Censorship of Text-to-Image Generation Model](https://arxiv.org/abs/2312.07130)

	Yimo Deng, Huangxun Chen


+ [ Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an  In-Context Attack](https://arxiv.org/abs/2312.06924)

	Yu Fu, Yufei Li, Wen Xiao, Cong Liu, Yue Dong


+ [ Adversarial Semi-Supervised Domain Adaptation for Semantic Segmentation:  A New Role for Labeled Target Samples](https://arxiv.org/abs/2312.07370)

	Marwa Kechaou, Mokhtar Z. Alaya, Romain Hérault, Gilles Gasso


+ [ Dynamic Adversarial Attacks on Autonomous Driving Systems](https://arxiv.org/abs/2312.06701)

	Amirhosein Chahe, Chenan Wang, Abhishek Jeyapratap, Kaidi Xu, Lifeng Zhou


+ [ AI Control: Improving Safety Despite Intentional Subversion](https://arxiv.org/abs/2312.06942)

	Ryan Greenblatt, Buck Shlegeris, Kshitij Sachan, Fabien Roger


+ [ Task-Agnostic Privacy-Preserving Representation Learning for Federated  Learning Against Attribute Inference Attacks](https://arxiv.org/abs/2312.06989)

	Caridad Arroyo Arevalo, Sayedeh Leila Noorbakhsh, Yun Dong, Yuan Hong, Binghui Wang


## 2023-12-11
+ [ Towards Transferable Adversarial Attacks with Centralized Perturbation](http://arxiv.org/abs/2312.06199)

    Shangbo Wu, Yu-an Tan, Yajie Wang, Ruinan Ma, Wencong Ma, Yuanzhang Li


+ [ MalPurifier: Enhancing Android Malware Detection with Adversarial Purification against Evasion Attacks](http://arxiv.org/abs/2312.06423)

    Yuyang Zhou, Guang Cheng, Zongyao Chen, Shui Yu


+ [ Sparse but Strong: Crafting Adversarially Robust Graph Lottery Tickets](http://arxiv.org/abs/2312.06568)

    Subhajit Dutta Chowdhury, Zhiyu Ni, Qingyuan Peng, Souvik Kundu, Pierluigi Nuzzo    


+ [ Reward Certification for Policy Smoothed Reinforcement Learning](http://arxiv.org/abs/2312.06436)

    Ronghui Mu, Leandro Soriano Marcolino, Tianle Zhang, Yanghao Zhang, Xiaowei Huang, Wenjie Ruan


+ [ Activation Gradient based Poisoned Sample Detection Against Backdoor Attacks](http://arxiv.org/abs/2312.06230)

    Danni Yuan, Shaokui Wei, Mingda Zhang, Li Liu, Baoyuan Wu


+ [ Poisoned ChatGPT Finds Work for Idle Hands: Exploring Developers' Coding Practices with Insecure Suggestions from Poisoned AI Models](http://arxiv.org/abs/2312.06227)

    Sanghak Oh, Kiho Lee, Seonhye Park, Doowon Kim, Hyoungshick Kim


+ [ Promoting Counterfactual Robustness through Diversity](http://arxiv.org/abs/2312.06564)

    Francesco Leofante, Nico Potyka


+ [ Robust Graph Neural Network based on Graph Denoising](http://arxiv.org/abs/2312.06557)

	Victor M. Tenorio, Samuel Rey, Antonio G. Marques


+ [ Privacy Preserving Multi-Agent Reinforcement Learning in Supply Chains](https://arxiv.org/abs/2312.05686)

	Ananta Mukherjee, Peeyush Kumar, Boling Yang, Nishanth Chandran, Divya Gupta


+ [ Exploring the Limits of ChatGPT in Software Security Applications](https://arxiv.org/abs/2312.05275)

	Fangzhou Wu, Qingzhao Zhang, Ati Priya Bajaj, Tiffany Bao, Ning Zhang, Ruoyu "Fish" Wang, Chaowei Xiao


+ [ Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer  Inputs of Language Models in Federated Learning](https://arxiv.org/abs/2312.05720)

	Jianwei Li, Sheng Liu, Qi Lei


+ [ MalPurifier: Enhancing Android Malware Detection with Adversarial  Purification against Evasion Attacks](https://arxiv.org/abs/2312.06423)

	Yuyang Zhou, Guang Cheng, Zongyao Chen, Shui Yu


+ [ GTA: Gated Toxicity Avoidance for LM Performance Preservation](https://arxiv.org/abs/2312.06122)

	Heegyu Kim, Hyunsouk Cho


+ [ Initialization Matters for Adversarial Transfer Learning](https://arxiv.org/abs/2312.05716)

	Andong Hua, Jindong Gu, Zhiyu Xue, Nicholas Carlini, Eric Wong, Yao Qin


+ [ Adversarial Camera Patch: An Effective and Robust Physical-World Attack  on Object Detectors](https://arxiv.org/abs/2312.06163)

	Kalibinuer Tiliwalidi


+ [ CAD: Photorealistic 3D Generation via Adversarial Distillation](https://arxiv.org/abs/2312.06663)

	Ziyu Wan, Despoina Paschalidou, Ian Huang, Hongyu Liu, Bokui Shen, Xiaoyu Xiang, Jing Liao, Leonidas Guibas


+ [ Improving Adversarial Robust Fairness via Anti-Bias Soft Label  Distillation](https://arxiv.org/abs/2312.05508)

	Shiji Zhao, Xizhe Wang, Xingxing Wei


+ [ Model Extraction Attacks Revisited](https://arxiv.org/abs/2312.05386)

	Jiacheng Liang, Ren Pang, Changjiang Li, Ting Wang


+ [ Poisoning $\times$ Evasion: Symbiotic Adversarial Robustness for Graph  Neural Networks](https://arxiv.org/abs/2312.05502)

	Ege Erdogan, Simon Geisler, Stephan Günnemann


+ [ A Practical Survey on Emerging Threats from AI-driven Voice Attacks: How  Vulnerable are Commercial Voice Control Systems?](https://arxiv.org/abs/2312.06010)

	Yuanda Wang, Qiben Yan, Nikolay Ivanov, Xun Chen


+ [ DiffAIL: Diffusion Adversarial Imitation Learning](https://arxiv.org/abs/2312.06348)

	Bingzheng Wang, Yan Zhang, Teng Pang, Guoqiang Wu, Yilong Yin


+ [ Security and Reliability Evaluation of Countermeasures implemented using  High-Level Synthesis](https://arxiv.org/abs/2312.06268)

	Amalia Artemis Koufopoulou, Kalliopi Xevgeni, Athanasios Papadimitriou, Mihalis Psarakis, David Hely

## 2023-12-10
+ [ zkFDL: An efficient and privacy-preserving decentralized federated  learning with zero knowledge proof](https://arxiv.org/abs/2312.04579)

	Mojtaba Ahmadi, Reza Nourmohammadi


+ [ Towards Sample-specific Backdoor Attack with Clean Labels via Attribute  Trigger](https://arxiv.org/abs/2312.04584)

	Yiming Li, Mingyan Zhu, Junfeng Guo, Tao Wei, Shu-Tao Xia, Zhan Qin



+ [ DeceptPrompt: Exploiting LLM-driven Code Generation via Adversarial  Natural Language Instructions](https://arxiv.org/abs/2312.04730)

	Fangzhou Wu, Xiaogeng Liu, Chaowei Xiao


+ [ Forcing Generative Models to Degenerate Ones: The Power of Data  Poisoning Attacks](https://arxiv.org/abs/2312.04748)

	Shuli Jiang, Swanand Ravindra Kadhe, Yi Zhou, Ling Cai, Nathalie Baracaldo


+ [ BELT: Old-School Backdoor Attacks can Evade the State-of-the-Art Defense  with Backdoor Exclusivity Lifting](https://arxiv.org/abs/2312.04902)

	Huming Qiu, Junjie Sun, Mi Zhang, Xudong Pan, Min Yang


+ [ SA-Attack: Improving Adversarial Transferability of Vision-Language  Pre-training Models via Self-Augmentation](https://arxiv.org/abs/2312.04913)

	Bangyan He, Xiaojun Jia, Siyuan Liang, Tianrui Lou, Yang Liu, Xiaochun Cao


+ [ MIMIR: Masked Image Modeling for Mutual Information-based Adversarial  Robustness](https://arxiv.org/abs/2312.04960)

	Xiaoyun Xu, Shujian Yu, Jingzheng Wu, Stjepan Picek


+ [ On the Inadequacy of Similarity-based Privacy Metrics: Reconstruction  Attacks against "Truly Anonymous Synthetic Data''](https://arxiv.org/abs/2312.05114)

	Georgi Ganev, Emiliano De Cristofaro


+ [ MimicDiffusion: Purifying Adversarial Perturbation via Mimicking Clean  Diffusion Model](https://arxiv.org/abs/2312.04802)

	Kaiyu Song, Hanjiang Lai


+ [ Annotation-Free Group Robustness via Loss-Based Resampling](https://arxiv.org/abs/2312.04893)

	Mahdi Ghaznavi, Hesam Asadollahzadeh, HamidReza Yaghoubi Araghi, Fahimeh Hosseini Noohdani, Mohammad Hossein Rohban, Mahdieh Soleymani Baghshah


+ [ Diffence: Fencing Membership Privacy With Diffusion Models](https://arxiv.org/abs/2312.04692)

	Yuefeng Peng, Ali Naseh, Amir Houmansadr


+ [ HC-Ref: Hierarchical Constrained Refinement for Robust Adversarial  Training of GNNs](https://arxiv.org/abs/2312.04879)

	Xiaobing Pei, Haoran Yang, Gang Shen


+ [ FedBayes: A Zero-Trust Federated Learning Aggregation to Defend Against  Adversarial Attacks](https://arxiv.org/abs/2312.04587)

	Marc Vucovich, Devin Quinn, Kevin Choi, Christopher Redino, Abdul Rahman, Edward Bowen


+ [ TrustFed: A Reliable Federated Learning Framework with Malicious-Attack  Resistance](https://arxiv.org/abs/2312.04597)

	Hangn Su, Jianhong Zhou, Xianhua Niu, Gang Feng


+ [ Topology-Based Reconstruction Prevention for Decentralised Learning](https://arxiv.org/abs/2312.05248)

	Florine W. Dekker, Zekeriya Erkin, Mauro Conti


## 2023-12-09
+ [ Cognitive Dissonance: Why Do Language Model Outputs Disagree with  Internal Representations of Truthfulness?](https://arxiv.org/abs/2312.03729)

	Kevin Liu, Stephen Casper, Dylan Hadfield-Menell, Jacob Andreas


+ [ On the Learnability of Watermarks for Language Models](https://arxiv.org/abs/2312.04469)

	Chenchen Gu, Xiang Lisa Li, Percy Liang, Tatsunori Hashimoto


+ [ Defense against ML-based Power Side-channel Attacks on DNN Accelerators  with Adversarial Attacks](https://arxiv.org/abs/2312.04035)

	Xiaobei Yan, Chip Hong Chang, Tianwei Zhang


+ [ GaitGuard: Towards Private Gait in Mixed Reality](https://arxiv.org/abs/2312.04470)

	Diana Romero, Ruchi Jagdish Patel, Athina Markopolou, Salma Elmalaki


## 2023-12-08
+ [ An Evaluation of State-of-the-Art Large Language Models for Sarcasm  Detection](https://arxiv.org/abs/2312.03706)

	Juliann Zhou


+ [ Negotiating with LLMS: Prompt Hacks, Skill Gaps, and Reasoning Deficits](https://arxiv.org/abs/2312.03720)

	Johannes Schneider, Steffi Haag, Leona Chandra Kruse


+ [ DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt  Engineer](https://arxiv.org/abs/2312.03724)

	Junyuan Hong, Jiachen T. Wang, Chenhui Zhang, Zhangheng Li, Bo Li, Zhangyang Wang


+ [ Making Translators Privacy-aware on the User's Side](https://arxiv.org/abs/2312.04068)

	Ryoma Sato


+ [ Adversarial Denoising Diffusion Model for Unsupervised Anomaly Detection](https://arxiv.org/abs/2312.04382)

	Jongmin Yu, Hyeontaek Oh, Jinhong Yang


+ [ RoAST: Robustifying Language Models via Adversarial Perturbation with  Selective Training](https://arxiv.org/abs/2312.04032)

	Jaehyung Kim, Yuning Mao, Rui Hou, Hanchao Yu, Davis Liang, Pascale Fung, Qifan Wang, Fuli Feng, Lifu Huang, Madian Khabsa


+ [ The Potential of Vision-Language Models for Content Moderation of  Children's Videos](https://arxiv.org/abs/2312.03936)

	Syed Hammad Ahmed, Shengnan Hu, Gita Sukthankar


+ [ On the Impact of Multi-dimensional Local Differential Privacy on  Fairness](https://arxiv.org/abs/2312.04404)

	karima Makhlouf, Heber H. Arcolezi, Sami Zhioua, Ghassen Ben Brahim, Catuscia Palamidessi


+ [ FreqFed: A Frequency Analysis-Based Approach for Mitigating Poisoning  Attacks in Federated Learning](https://arxiv.org/abs/2312.04432)

	Hossein Fereidooni, Alessandro Pegoraro, Phillip Rieger, Alexandra Dmitrienko, Ahmad-Reza Sadeghi


+ [ SoK: Unintended Interactions among Machine Learning Defenses and Risks](https://arxiv.org/abs/2312.04542)

	Vasisht Duddu, Sebastian Szyller, N. Asokan


## 2023-12-07
+ [ GaitGuard: Towards Private Gait in Mixed Reality](https://arxiv.org/abs/2312.04470)

	Diana Romero, Ruchi Jagdish Patel, Athina Markopolou, Salma Elmalaki


+ [ Exploring the Robustness of Model-Graded Evaluations and Automated  Interpretability](https://arxiv.org/abs/2312.03721)

	Simon Lermen, Ondřej Kvapil


+ [ On The Fairness Impacts of Hardware Selection in Machine Learning](https://arxiv.org/abs/2312.03886)

	Sree Harsha Nelaturu, Nishaanth Kanna Ravichandran, Cuong Tran, Sara Hooker, Ferdinando Fioretto


+ [ Detecting and Restoring Non-Standard Hands in Stable Diffusion Generated  Images](https://arxiv.org/abs/2312.04236)

	Yiqun Zhang, Zhenyue Qin, Yang Liu, Dylan Campbell


+ [ Adversarial Learning for Feature Shift Detection and Correction](https://arxiv.org/abs/2312.04546)

	Miriam Barrabes, Daniel Mas Montserrat, Margarita Geleta, Xavier Giro-i-Nieto, Alexander G. Ioannidis


+ [ OT-Attack: Enhancing Adversarial Transferability of Vision-Language Models via Optimal Transport Optimization](http://arxiv.org/abs/2312.04403)

    Dongchen Han, Xiaojun Jia, Yang Bai, Jindong Gu, Yang Liu, Xiaochun Cao


+ [ FreqFed: A Frequency Analysis-Based Approach for Mitigating Poisoning Attacks in Federated Learning](http://arxiv.org/abs/2312.04432)

    Hossein Fereidooni, Alessandro Pegoraro, Phillip Rieger, Alexandra Dmitrienko, Ahmad-Reza Sadeghi


## 2023-12-06
+ [ Defense against ML-based Power Side-channel Attacks on DNN Accelerators with Adversarial Attacks](http://arxiv.org/abs/2312.04035)

    Xiaobei Yan, Chip Hong Chang, Tianwei Zhang


+ [ Defense Against Adversarial Attacks using Convolutional Auto-Encoders](http://arxiv.org/abs/2312.03520)

    Shreyasi Mandal


+ [ Node-aware Bi-smoothing: Certified Robustness against Graph Injection Attacks](http://arxiv.org/abs/2312.03979)

    Yuni Lai, Yulin Zhu, Bailin Pan, Kai Zhou


+ [ Privacy-preserving quantum federated learning via gradient hiding](https://arxiv.org/abs/2312.04447)

	Changhao Li, Niraj Kumar, Zhixin Song, Shouvanik Chakrabarti, Marco Pistoia


+ [ RoAST: Robustifying Language Models via Adversarial Perturbation with Selective Training](http://arxiv.org/abs/2312.04032)

    Jaehyung Kim, Yuning Mao, Rui Hou, Hanchao Yu, Davis Liang, Pascale Fung, Qifan Wang, 
Fuli Feng, Lifu Huang, Madian Khabsa


+ [ Analyzing the Inherent Response Tendency of LLMs: Real-World  Instructions-Driven Jailbreak](https://arxiv.org/abs/2312.04127)

	Yanrui Du, Sendong Zhao, Ming Ma, Yuhan Chen, Bing Qin


+ [ Detecting Voice Cloning Attacks via Timbre Watermarking](http://arxiv.org/abs/2312.03410)

    Chang Liu, Jie Zhang, Tianwei Zhang, Xi Yang, Weiming Zhang, Nenghai Yu


+ [ Identity-Obscured Neural Radiance Fields: Privacy-Preserving 3D Facial  Reconstruction](https://arxiv.org/abs/2312.04106)

	Jiayi Kong, Baixin Xu, Xurui Song, Chen Qian, Jun Luo, Ying He


+ [ Learn to Unlearn for Deep Neural Networks: Minimizing Unlearning  Interference with Gradient Projection](https://arxiv.org/abs/2312.04095)

	Tuan Hoang, Santu Rana, Sunil Gupta, Svetha Venkatesh


+ [ Synthesizing Physical Backdoor Datasets: An Automated Framework Leveraging Deep Generative Models](http://arxiv.org/abs/2312.03419)

    Sze Jue Yang, Chinh D. La, Quang H. Nguyen, Eugene Bagdasaryan, Kok-Seng Wong, Anh Tuan Tran, Chee Seng Chan, Khoa D. Doan


+ [ MICRO: Model-Based Offline Reinforcement Learning with a Conservative Bellman Operator](http://arxiv.org/abs/2312.03991)

    Xiao-Yin Liu, Xiao-Hu Zhou, Guo-Tao Li, Hao Li, Mei-Jiang Gui, Tian-Yu Xiang, De-Xing 
Huang, Zeng-Guang Hou


## 2023-12-05
+ [ Generating Visually Realistic Adversarial Patch](http://arxiv.org/abs/2312.03030)

    Xiaosen Wang, Kunyu Wang


+ [ ScAR: Scaling Adversarial Robustness for LiDAR Object Detection](http://arxiv.org/abs/2312.03085)

    Xiaohu Lu, Hayder Radha


+ [ A Simple Framework to Enhance the Adversarial Robustness of Deep Learning-based Intrusion Detection System](http://arxiv.org/abs/2312.03245)

    Xinwei Yuan, Shu Han, Wei Huang, Hongliang Ye, Xianglong Kong, Fan Zhang


+ [ Realistic Scatterer Based Adversarial Attacks on SAR Image Classifiers](http://arxiv.org/abs/2312.02912)

    Tian Ye, Rajgopal Kannan, Viktor Prasanna, Carl Busart, Lance Kaplan


+ [ Class Incremental Learning for Adversarial Robustness](http://arxiv.org/abs/2312.03289)

    Seungju Cho, Hongshin Lee, Changick Kim


+ [ (Provable) Adversarial Robustness for Group Equivariant Tasks: Graphs, Point Clouds, Molecules, and More](http://arxiv.org/abs/2312.02708)

    Jan Schuchardt, Yan Scholten, Stephan Günnemann


+ [ On the Robustness of Large Multimodal Models Against Image Adversarial Attacks](http://arxiv.org/abs/2312.03777)

    Xuanimng Cui, Alejandro Aparcedo, Young Kyun Jang, Ser-Nam Lim


+ [ Scaling Laws for Adversarial Attacks on Language Model Activations](http://arxiv.org/abs/2312.02780)

    Stanislav Fort


+ [ Indirect Gradient Matching for Adversarial Robust Distillation](http://arxiv.org/abs/2312.03286)

    Hongsin Lee, Seungju Cho, Changick Kim


+ [ Robust Backdoor Detection for Deep Learning via Topological Evolution Dynamics](http://arxiv.org/abs/2312.02673)

    Xiaoxing Mo, Yechao Zhang, Leo Yu Zhang, Wei Luo, Nan Sun, Shengshan Hu, Shang Gao, Yang Xiang


+ [ Prompt Optimization via Adversarial In-Context Learning](http://arxiv.org/abs/2312.02614)

    Xuan Long Do, Yiran Zhao, Hannah Brown, Yuxi Xie, James Xu Zhao, Nancy F. Chen, Kenji 
Kawaguchi, Michael Qizhe Xie, Junxian He


+ [ Privacy-Preserving Task-Oriented Semantic Communications Against Model Inversion Attacks](http://arxiv.org/abs/2312.03252)

    Yanhu Wang, Shuaishuai Guo, Yiqin Deng, Haixia Zhang, Yuguang Fang


+ [ Machine Vision Therapy: Multimodal Large Language Models Can Enhance Visual Robustness via Denoising In-Context Learning](http://arxiv.org/abs/2312.02546)

    Zhuo Huang, Chang Liu, Yinpeng Dong, Hang Su, Shibao Zheng, Tongliang Liu


## 2023-12-04
+ [ Adversarial Medical Image with Hierarchical Feature Hiding](http://arxiv.org/abs/2312.01679)

    Qingsong Yao, Zecheng He, Yuexiang Li, Yi Lin, Kai Ma, Yefeng Zheng, S. Kevin Zhou    


+ [ InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models](http://arxiv.org/abs/2312.01886)

    Xunguang Wang, Zhenlan Ji, Pingchuan Ma, Zongjie Li, Shuai Wang


+ [ Singular Regularization with Information Bottleneck Improves Model's Adversarial Robustness](http://arxiv.org/abs/2312.02237)

    Guanlin Li, Naishan Zheng, Man Zhou, Jie Zhang, Tianwei Zhang


+ [ Two-stage optimized unified adversarial patch for attacking visible-infrared cross-modal detectors in the physical world](http://arxiv.org/abs/2312.01789)

    Chengyin Hu, Weiwen Shi


+ [ Auto DP-SGD: Dual Improvements of Privacy and Accuracy via Automatic Clipping Threshold and Noise Multiplier Estimation](http://arxiv.org/abs/2312.02400)

    Sai Venkatesh Chilukoti, Md Imran Hossen, Liqun Shan, Vijay Srinivas Tida, Xiai Hei   


+ [ Rejuvenating image-GPT as Strong Visual Representation Learners](http://arxiv.org/abs/2312.02147)

    Sucheng Ren, Zeyu Wang, Hongru Zhu, Junfei Xiao, Alan Yuille, Cihang Xie


## 2023-12-03
+ [ QuantAttack: Exploiting Dynamic Quantization to Attack Vision Transformers](http://arxiv.org/abs/2312.02220)

    Amit Baras, Alon Zolfi, Yuval Elovici, Asaf Shabtai


+ [ OCGEC: One-class Graph Embedding Classification for DNN Backdoor Detection](http://arxiv.org/abs/2312.01585)

    Haoyu Jiang, Haiyang Yu, Nan Li, Ping Yi


+ [ Evaluating the Security of Satellite Systems](http://arxiv.org/abs/2312.01330)

    Roy Peled, Eran Aizikovich, Edan Habler, Yuval Elovici, Asaf Shabtai


+ [ Exploring Adversarial Robustness of LiDAR-Camera Fusion Model in Autonomous Driving](http://arxiv.org/abs/2312.01468)

    Bo Yang, Xiaoyu Ji, Xiaoyu Ji, Xiaoyu Ji, Xiaoyu Ji


## 2023-12-02
+ [ TranSegPGD: Improving Transferability of Adversarial Examples on Semantic Segmentation](http://arxiv.org/abs/2312.02207)

    Xiaojun Jia, Jindong Gu, Yihao Huang, Simeng Qin, Qing Guo, Yang Liu, Xiaochun Cao    


+ [ Rethinking PGD Attack: Is Sign Function Necessary? (98%](http://arxiv.org/abs/2312.01260)

    Junjie Yang, Tianlong Chen, Xuxi Chen, Zhangyang Wang, Yingbin Liang


+ [ PROFL: A Privacy-Preserving Federated Learning Method with Stringent Defense Against Poisoning Attacks](http://arxiv.org/abs/2312.01045)

    Yisheng Zhong, Li-Ping Wang


+ [ Mendata: A Framework to Purify Manipulated Training Data](http://arxiv.org/abs/2312.01281)

    Zonghao Huang, Neil Gong, Michael K. Reiter


## 2023-12-01
+ [ PyraTrans: Learning Attention-Enriched Multi-Scale Pyramid Network from Pre-Trained Transformers for Effective Malicious URL Detection](http://arxiv.org/abs/2312.00508)

    Ruitong Liu, Yanbin Wang, Zhenhao Guo, Haitao Xu, Zhan Qin, Wenrui Ma, Fan Zhang      


+ [ Survey of Security Issues in Memristor-based Machine Learning Accelerators for RF Analysis](http://arxiv.org/abs/2312.00942)

    William Lillis, Max Cohen Hoffing, Wayne Burleson


+ [ Deep Generative Attacks and Countermeasures for Data-Driven Offline Signature Verification](http://arxiv.org/abs/2312.00987)

    An Ngo, MinhPhuong Cao, Rajesh Kumar


+ [ Temperature Balancing, Layer-wise Weight Analysis, and Neural Network Training](http://arxiv.org/abs/2312.00359)

    Yefan Zhou, Tianyu Pang, Keqin Liu, Charles H. Martin, Michael W. Mahoney, Yaoqing Yang


## 2023-11-30
+ [ Improving Faithfulness for Vision Transformers](https://arxiv.org/abs/2311.17983)

	Lijie Hu, Yixin Liu, Ninghao Liu, Mengdi Huai, Lichao Sun, Di Wang


+ [ TrustMark: Universal Watermarking for Arbitrary Resolution Images](https://arxiv.org/abs/2311.18297)

	Tu Bui, Shruti Agarwal, John Collomosse


+ [ Unnatural Error Correction: GPT-4 Can Almost Perfectly Handle Unnatural  Scrambled Text](https://arxiv.org/abs/2311.18805)

	Qi Cao, Takeshi Kojima, Yutaka Matsuo, Yusuke Iwasawa


+ [ ROBBIE: Robust Bias Evaluation of Large Generative Language Models](https://arxiv.org/abs/2311.18140)

	David Esiobu, Xiaoqing Tan, Saghar Hosseini, Megan Ung, Yuchen Zhang, Jude Fernandes, Jane Dwivedi-Yu, Eleonora Presani, Adina Williams, Eric Michael Smith


+ [ What Do Llamas Really Think? Revealing Preference Biases in Language  Model Representations](https://arxiv.org/abs/2311.18812)

	Raphael Tang, Xinyu Zhang, Jimmy Lin, Ferhan Ture


+ [ Improving Adversarial Transferability via Model Alignment](https://arxiv.org/abs/2311.18495)

	Avery Ma, Amir-massoud Farahmand, Yangchen Pan, Philip Torr, Jindong Gu


+ [ Poisoning Attacks Against Contrastive Recommender Systems](https://arxiv.org/abs/2311.18244)

	Zongwei Wang, Junliang Yu, Min Gao, Hongzhi Yin, Bin Cui, Shazia Sadiq


## 2023-11-29
+ [ Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention](http://arxiv.org/abs/2311.17400)

 Lujia Shen, Yuwen Pu, Shouling Ji, Changjiang Li, Xuhong Zhang, Chunpeng Ge, Ting Wang  


+ [ Group-wise Sparse and Explainable Adversarial Attacks](http://arxiv.org/abs/2311.17434)

 Shpresim Sadiku, Moritz Wagner, Sebastian Pokutta


+ [ Quantum Neural Networks under Depolarization Noise: Exploring White-Box Attacks and Defenses](http://arxiv.org/abs/2311.17458)

 David Winderl, Nicola Franco, Jeanette Miriam Lorenz


+ [ On the Adversarial Robustness of Graph Contrastive Learning Methods](http://arxiv.org/abs/2311.17853)

 Filippo Guerranti, Zinuo Yi, Anna Starovoit, Rafiq Kamel, Simon Geisler, Stephan Günnemann


+ [ Adversarial Robust Memory-Based Continual Learner](http://arxiv.org/abs/2311.17608)

 Xiaoyue Mi, Fan Tang, Zonghan Yang, Danding Wang, Juan Cao, Peng Li, Yang Liu


+ [ TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4](http://arxiv.org/abs/2311.17429)

 Zihao Tan, Qingliang Chen, Yongjian Huang, Chen Liang


+ [ Topology-Preserving Adversarial Training](http://arxiv.org/abs/2311.17607)  

 Xiaoyue Mi, Fan Tang, Yepeng Weng, Danding Wang, Juan Cao, Sheng Tang, Peng Li, Yang Liu

+ [ Query-Relevant Images Jailbreak Large Multi-Modal Models](http://arxiv.org/abs/2311.17600)

 Xin Liu, Yichen Zhu, Yunshi Lan, Chao Yang, Yu Qiao


+ [ Analyzing and Explaining Image Classifiers via Diffusion Guidance](http://arxiv.org/abs/2311.17833)

 Maximilian Augustin, Yannic Neuhaus, Matthias Hein


+ [ SenTest: Evaluating Robustness of Sentence Encoders](http://arxiv.org/abs/2311.17722)

 Tanmay Chavan, Shantanu Patankar, Aditya Kane, Omkar Gokhale, Geetanjali Kale, Raviraj Joshi


+ [ CLIPC8: Face liveness detection algorithm based on image-text pairs and contrastive learning](http://arxiv.org/abs/2311.17583)

 Xu Liu, Shu Zhou, Yurong Song, Wenzhe Luo, Xin Zhang


## 2023-11-28
+ [ Unveiling the Implicit Toxicity in Large Language Models](http://arxiv.org/abs/2311.17391)

 Jiaxin Wen, Pei Ke, Hao Sun, Zhexin Zhang, Chengfei Li, Jinfeng Bai, Minlie Huang       


+ [ Vulnerability Analysis of Transformer-based Optical Character Recognition to Adversarial Attacks](http://arxiv.org/abs/2311.17128)

 Lucas Beerens, Desmond J. Higham


+ [ NeRFTAP: Enhancing Transferability of Adversarial Patches on Face Recognition using Neural Radiance Fields](http://arxiv.org/abs/2311.17332)

 Xiaoliang Liu, Furao Shen, Feng Han, Jian Zhao, Changhai Nie


+ [ Efficient Key-Based Adversarial Defense for ImageNet by Using Pre-trained Model](http://arxiv.org/abs/2311.16577)

 AprilPyone MaungMaung, Isao Echizen, Hitoshi Kiya


+ [ RADAP: A Robust and Adaptive Defense Against Diverse Adversarial Patches on Face Recognition](http://arxiv.org/abs/2311.17339)

 Xiaoliang Liu, Furao Shen, Jian Zhao, Changhai Nie


+ [ 1-Lipschitz Layers Compared: Memory, Speed, and Certifiable Robustness](http://arxiv.org/abs/2311.16833)

 Bernd Prach, Fabio Brau, Giorgio Buttazzo, Christoph H. Lampert


+ [ Scalable Extraction of Training Data from (Production) Language Models](http://arxiv.org/abs/2311.17035)

 Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace, Florian Tramèr, Katherine Lee   


+ [ Cooperative Abnormal Node Detection with Adversary Resistance: A Probabilistic Approach](http://arxiv.org/abs/2311.16661)

 Yingying Huangfu, Tian Bai


+ [ Shadows Don't Lie and Lines Can't Bend! Generative Models don't know Projective Geometry](http://arxiv.org/abs/2311.17138)

 Ayush Sarkar, Hanlin Mai, Amitabh Mahapatra, Svetlana Lazebnik, D. A. Forsyth, Anand Bhattad


+ [ On robust overfitting: adversarial training induced distribution matters](http://arxiv.org/abs/2311.16526)

 Runzhi Tian, Yongyi Mao


+ [ Understanding the (Extra-)Ordinary: Validating Deep Model Decisions with Prototypical Concept-based Explanations](http://arxiv.org/abs/2311.16681)

 Maximilian Dreyer, Reduan Achtibat, Wojciech Samek, Sebastian Lapuschkin


## 2023-11-27
+ [Rethinking Mixup for Improving the Adversarial Transferability](https://arxiv.org/abs/2311.17087)

Xiaosen Wang, Zeyuan Yin

  
+ [ Microarchitectural Security of AWS Firecracker VMM for Serverless Cloud Platforms](http://arxiv.org/abs/2311.15999)

	Zane Worcester Polytechnic Institute Weissman, Thore University of Lübeck Tiemann, Thomas University of Lübeck Eisenbarth, Berk Worcester Polytechnic Institute Sunar


+ [ Scale-Dropout: Estimating Uncertainty in Deep Neural Networks Using  Stochastic Scale](https://arxiv.org/abs/2311.15816)

	Soyed Tuhin Ahmed, Kamal Danouchi, Michael Hefenbrock, Guillaume Prenat, Lorena Anghel, Mehdi B. Tahoori


+ [ Instruct2Attack: Language-Guided Semantic Adversarial Attacks](http://arxiv.org/abs/2311.15551)

	Jiang Liu, Chen Wei, Yuxiang Guo, Heng Yu, Alan Yuille, Soheil Feizi, Chun Pong Lau, Rama Chellappa


+ [ A Survey on Vulnerability of Federated Learning: A Learning Algorithm  Perspective](https://arxiv.org/abs/2311.16065)

	Xianghua Xie, Chen Hu, Hanchi Ren, Jingjing Deng


+ [ Distributed Attacks over Federated Reinforcement Learning-enabled Cell Sleep Control](http://arxiv.org/abs/2311.15894)

	Han Zhang, Hao Zhou, Medhat Elsayed, Majid Bavand, Raimundas Gaigalas, Yigit Ozcan, Melike Erol-Kantarci


+ [ How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs](http://arxiv.org/abs/2311.16101)

	Haoqin Tu, Chenhang Cui, Zijun Wang, Yiyang Zhou, Bingchen Zhao, Junlin Han, Wangchunshu Zhou, Huaxiu Yao, Cihang Xie


+ [ Trainwreck: A damaging adversarial attack on image classifiers](https://arxiv.org/abs/2311.14772)

	Jan Zahálka


+ [ Adversaral Doodles: Interpretable and Human-drawable Attacks Provide  Describable Insights](https://arxiv.org/abs/2311.15994)

	Ryoya Nara, Yusuke Matsui


+ [ Automated discovery of trade-off between utility, privacy and fairness  in machine learning models](https://arxiv.org/abs/2311.15691)

	Bogdan Ficiu, Neil D. Lawrence, Andrei Paleyes


+ [ Attend Who is Weak: Enhancing Graph Condensation via Cross-Free  Adversarial Training](https://arxiv.org/abs/2311.15772)

	Xinglin Li, Kun Wang, Hanhui Deng, Yuxuan Liang, Di Wu


+ [ Rethinking Privacy in Machine Learning Pipelines from an Information  Flow Control Perspective](https://arxiv.org/abs/2311.15792)

	Lukas Wutschitz, Boris Köpf, Andrew Paverd, Saravan Rajmohan, Ahmed Salem, Shruti Tople, Santiago Zanella-Béguelin, Menglin Xia, Victor Rühle


## 2023-11-26
+ [ Confidence Is All You Need for MI Attacks](https://arxiv.org/abs/2311.15373)

	Abhishek Sinha, Himanshi Tibrewal, Mansi Gupta, Nikhar Waghela, Shivank Garg


+ [ Mixing Classifiers to Alleviate the Accuracy-Robustness Trade-Off](https://arxiv.org/abs/2311.15165)

	Yatong Bai, Brendon G. Anderson, Somayeh Sojoudi


+ [ Adversarial Purification of Information Masking](http://arxiv.org/abs/2311.15339)

	Sitong Liu, Zhichao Lian, Shuangquan Zhang, Liang Xiao


+ [ Having Second Thoughts? Let's hear it](http://arxiv.org/abs/2311.15356)

	Jung H. Lee, Sujith Vijayan


## 2023-11-25
+ [ Effective Backdoor Mitigation Depends on the Pre-training Objective](https://arxiv.org/abs/2311.14948)

	Sahil Verma, Gantavya Bhatt, Avi Schwarzschild, Soumye Singhal, Arnav Mohanty Das, Chirag Shah, John P Dickerson, Jeff Bilmes


+ [ Robust Graph Neural Networks via Unbiased Aggregation](https://arxiv.org/abs/2311.14934)

	Ruiqi Feng, Zhichao Hou, Tyler Derr, Xiaorui Liu


+ [ Mixing Classifiers to Alleviate the Accuracy-Robustness Trade-Off](http://arxiv.org/abs/2311.15165)

	Yatong Bai, Brendon G. Anderson, Somayeh Sojoudi


+ [ Robust Graph Neural Networks via Unbiased Aggregation](http://arxiv.org/abs/2311.14934)

	Ruiqi Feng, Zhichao Hou, Tyler Derr, Xiaorui Liu


+ [ Effective Backdoor Mitigation Depends on the Pre-training Objective](http://arxiv.org/abs/2311.14948)

	Sahil Verma, Gantavya Bhatt, Avi Schwarzschild, Soumye Singhal, Arnav Mohanty Das, Chirag Shah, John P Dickerson, Jeff Bilmes


## 2023-11-24
+ [ Universal Jailbreak Backdoors from Poisoned Human Feedback](https://arxiv.org/abs/2311.14455)

	Javier Rando, Florian Tramèr


+ [ How to ensure a safe control strategy? Towards a SRL for urban transit  autonomous operation](https://arxiv.org/abs/2311.14457)

	Zicong Zhao


+ [ Potential Societal Biases of ChatGPT in Higher Education: A Scoping  Review](https://arxiv.org/abs/2311.14381)

	Ming Li, Ariunaa Enkhtur, Beverley Anne Yamamoto, Fei Cheng


+ [ AI-based Attack Graph Generation](https://arxiv.org/abs/2311.14342)

	Sangbeom Park, Jaesung Lee, Jeongdo Yoo, Min Geun Song, Hyosun Lee, Jaewoong Choi, Chaeyeon Sagong, Huy Kang Kim


+ [ Segment (Almost) Nothing: Prompt-Agnostic Adversarial Attacks on Segmentation Models](http://arxiv.org/abs/2311.14450)

	Francesco Croce, Matthias Hein


+ [ Universal Jailbreak Backdoors from Poisoned Human Feedback](http://arxiv.org/abs/2311.14455)

	Javier Rando, Florian Tramèr


## 2023-11-23
+ [ Exploring Methods for Cross-lingual Text Style Transfer: The Case of  Text Detoxification](https://arxiv.org/abs/2311.13937)

	Daryna Dementieva, Daniil Moskovskiy, David Dale, Alexander Panchenko


+ [ Efficient Trigger Word Insertion](https://arxiv.org/abs/2311.13957)

	Yueqi Zeng, Ziqiang Li, Pengfei Xia, Lei Liu, Bin Li


+ [ ACT: Adversarial Consistency Models](https://arxiv.org/abs/2311.14097)

	Fei Kong, Jinhao Duan, Lichao Sun, Hao Cheng, Renjing Xu, Hengtao Shen, Xiaofeng Zhu, Xiaoshuang Shi, Kaidi Xu


+ [ Robust and Interpretable COVID-19 Diagnosis on Chest X-ray Images using  Adversarial Training](https://arxiv.org/abs/2311.14227)

	Karina Yang, Alexis Bennett, Dominique Duncan


+ [ When Side-Channel Attacks Break the Black-Box Property of Embedded Artificial Intelligence](http://arxiv.org/abs/2311.14005)

	Benoit Coqueret, Mathieu Carbone, Olivier Sentieys, Gabriel Zaid


+ [ Adversarial defense based on distribution transfer](http://arxiv.org/abs/2311.13841)

	Jiahao Chen, Diqun Yan, Li Dong


+ [ Robust and Interpretable COVID-19 Diagnosis on Chest X-ray Images using Adversarial Training](http://arxiv.org/abs/2311.14227)

	Karina Yang, Alexis Bennett, Dominique Duncan


## 2023-11-22
+ [ Prompt Risk Control: A Rigorous Framework for Responsible Deployment of  Large Language Models](https://arxiv.org/abs/2311.13628)

	Thomas P. Zollo, Todd Morrill, Zhun Deng, Jake C. Snell, Toniann Pitassi, Richard Zemel


+ [ A Theoretical Insight into Attack and Defense of Gradient Leakage in  Transformer](https://arxiv.org/abs/2311.13624)

	Chenyang Li, Zhao Song, Weixin Wang, Chiwun Yang


+ [ OASIS: Offsetting Active Reconstruction Attacks in Federated Learning](https://arxiv.org/abs/2311.13739)

	Tre' R. Jeter, Truc Nguyen, Raed Alharbi, My T. Thai


+ [ RAEDiff: Denoising Diffusion Probabilistic Models Based Reversible  Adversarial Examples Self-Generation and Self-Recovery](https://arxiv.org/abs/2311.12858)

	Fan Xing, Xiaoyi Zhou, Xuefeng Fan, Zhuo Tian, Yan Zhao


+ [ A Survey of Adversarial CAPTCHAs on its History, Classification and  Generation](https://arxiv.org/abs/2311.13233)

	Zisheng Xu, Qiao Yan, F. Richard Yu, Victor C. M. Leung


+ [ Panda or not Panda? Understanding Adversarial Attacks with Interactive Visualization](http://arxiv.org/abs/2311.13656)

	Yuzhe You, Jarvis Tse, Jian Zhao


+ [ Security and Privacy Challenges in Deep Learning Models](http://arxiv.org/abs/2311.13744)

	Gopichandh Golla


+ [ A Somewhat Robust Image Watermark against Diffusion-based Editing Models](http://arxiv.org/abs/2311.13713)

	Mingtian Tan, Tianhao Wang, Somesh Jha


+ [ Adversarial sample generation and training using geometric masks for  accurate and resilient license plate character recognition](https://arxiv.org/abs/2311.12857)

	Bishal Shrestha, Griwan Khakurel, Kritika Simkhada, Badri Adhikari


+ [ Attention Deficit is Ordered! Fooling Deformable Vision Transformers  with Collaborative Adversarial Patches](https://arxiv.org/abs/2311.12914)

	Quazi Mishkatul Alam, Bilel Tarchoun, Ihsen Alouani, Nael Abu-Ghazaleh


+ [ SD-NAE: Generating Natural Adversarial Examples with Stable Diffusion](https://arxiv.org/abs/2311.12981)

	Yueqian Lin, Jingyang Zhang, Yiran Chen, Hai Li


+ [ Hard Label Black Box Node Injection Attack on Graph Neural Networks](https://arxiv.org/abs/2311.13244)

	Yu Zhou, Zihao Dong, Guofeng Zhang, Jingchen Tang


+ [ Transfer Attacks and Defenses for Large Language Models on Coding Tasks](https://arxiv.org/abs/2311.13445)

	Chi Zhang, Zifan Wang, Ravi Mangal, Matt Fredrikson, Limin Jia, Corina Pasareanu


## 2023-11-21
+ [ Boost Adversarial Transferability by Uniform Scale and Mix Mask Method](https://arxiv.org/abs/2311.12051)

	Tao Wang, Zijian Ying, Qianmu Li, zhichao Lian


+ [ BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive  Learning](https://arxiv.org/abs/2311.12075)

	Siyuan Liang, Mingli Zhu, Aishan Liu, Baoyuan Wu, Xiaochun Cao, Ee-Chien Chang


+ [ Investigating Weight-Perturbed Deep Neural Networks With Application in  Iris Presentation Attack Detection](https://arxiv.org/abs/2311.12764)

	Renu Sharma, Redwan Sony, Arun Ross


+ [ Iris Presentation Attack: Assessing the Impact of Combining Vanadium  Dioxide Films with Artificial Eyes](https://arxiv.org/abs/2311.12773)

	Darshika Jauhari, Renu Sharma, Cunjian Chen, Nelson Sepulveda, Arun Ross


+ [ ODDR: Outlier Detection & Dimension Reduction Based Defense Against  Adversarial Patches](https://arxiv.org/abs/2311.12084)

	Nandish Chattopadhyay, Amira Guesmi, Muhammad Abdullah Hanif, Bassem Ouni, Muhammad Shafique


+ [ Attacking Motion Planners Using Adversarial Perception Errors](https://arxiv.org/abs/2311.12722)

	Jonathan Sadeghi, Nicholas A. Lord, John Redford, Romain Mueller


+ [ Masked Autoencoders Are Robust Neural Architecture Search Learners](https://arxiv.org/abs/2311.12086)

	Yiming Hu, Xiangxiang Chu, Bo Zhang


+ [ Adversarial Reweighting Guided by Wasserstein Distance for Bias  Mitigation](https://arxiv.org/abs/2311.12684)

	Xuan Zhao, Simone Fabbrizzi, Paula Reyero Lobo, Siamak Ghodsi, Klaus Broelemann, Steffen Staab, Gjergji Kasneci


+ [ Attacks of fairness in Federated Learning](https://arxiv.org/abs/2311.12715)

	Joseph Rance, Filip Svoboda


+ [ DefensiveDR: Defending against Adversarial Patches using Dimensionality  Reduction](https://arxiv.org/abs/2311.12211)

	Nandish Chattopadhyay, Amira Guesmi, Muhammad Abdullah Hanif, Bassem Ouni, Muhammad Shafique


## 2023-11-20
+ [ Safety-aware Causal Representation for Trustworthy Reinforcement  Learning in Autonomous Driving ](https://arxiv.org/abs/2311.10747)

	Haohong Lin, Wenhao Ding, Zuxin Liu, Yaru Niu, Jiacheng Zhu, Yuming Niu, Ding Zhao


+ [ Assessing Prompt Injection Risks in 200+ Custom GPTs ](https://arxiv.org/abs/2311.11538)

	Jiahao Yu, Yuhang Wu, Dong Shu, Mingyu Jin, Xinyu Xin


+ [ Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI  Systems ](https://arxiv.org/abs/2311.11796)

	Guangjing Wang, Ce Zhou, Yuanda Wang, Bocheng Chen, Hanqing Guo, Qiben Yan


+ [ Generating Valid and Natural Adversarial Examples with Large Language Models ](http://arxiv.org/abs/2311.11861)

	Zimu Wang, Wei Wang, Qi Chen, Qiufeng Wang, Anh Nguyen


+ [ AdvGen: Physical Adversarial Attack on Face Presentation Attack Detection Systems ](http://arxiv.org/abs/2311.11753)

	Sai Amrit Patnaik, Shivali Chansoriya, Anil K. Jain, Anoop M. Namboodiri


+ [ Understanding Variation in Subpopulation Susceptibility to Poisoning Attacks ](http://arxiv.org/abs/2311.11544)

	Evan Rose, Fnu Suya, David Evans


+ [ Training robust and generalizable quantum models ](http://arxiv.org/abs/2311.11871)

	Julian Berberich, Daniel Fink, Daniel Pranjić, Christian Tutschku, Christian Holm       


+ [ BrainWash: A Poisoning Attack to Forget in Continual Learning ](http://arxiv.org/abs/2311.11995)

	Ali Abbasi, Parsa Nooralinejad, Hamed Pirsiavash, Soheil Kolouri


## 2023-11-19
+ [ Robust Network Slicing: Multi-Agent Policies, Adversarial Attacks, and  Defensive Strategies ](https://arxiv.org/abs/2311.11206)

	Feng Wang, M. Cenk Gursoy, Senem Velipasalar


+ [ Adversarial Prompt Tuning for Vision-Language Models ](http://arxiv.org/abs/2311.11261)

	Jiaming Zhang, Xingjun Ma, Xin Wang, Lingyu Qiu, Jiaqi Wang, Yu-Gang Jiang, Jitao Sang  


+ [ TextGuard: Provable Defense against Backdoor Attacks on Text  Classification ](https://arxiv.org/abs/2311.11225)

	Hengzhi Pei, Jinyuan Jia, Wenbo Guo, Bo Li, Dawn Song


## 2023-11-18
+ [ Improving Adversarial Transferability by Stable Diffusion ](http://arxiv.org/abs/2311.11017)

 	Jiayang Liu, Siyu Zhu, Siyuan Liang, Jie Zhang, Han Fang, Weiming Zhang, Ee-Chien Chang 


+ [ Attention-Based Real-Time Defenses for Physical Adversarial Attacks in Vision Applications ](http://arxiv.org/abs/2311.11191)

	Giulio Rossolini, Alessandro Biondi, Giorgio Buttazzo


+ [ PACOL: Poisoning Attacks Against Continual Learners ](https://arxiv.org/abs/2311.10919)

	Huayu Li, Gregory Ditzler


+ [ Robust Network Slicing: Multi-Agent Policies, Adversarial Attacks, and Defensive Strategies ](http://arxiv.org/abs/2311.11206)

 	Feng Wang, M. Cenk Gursoy, Senem Velipasalar


## 2023-11-17
+ [ Robustness Enhancement in Neural Networks with Alpha-Stable Training  Noise ](https://arxiv.org/abs/2311.10803)

	Xueqiong Yuan, Jipeng Li, Ercan Engin Kuruoğlu

	
+ [ Breaking Temporal Consistency: Generating Video Universal Adversarial Perturbations Using Image Models ](http://arxiv.org/abs/2311.10366)

 	Hee-Seon Kim, Minji Son, Minbeom Kim, Myung-Joon Kwon, Changick Kim


+ [ PACOL: Poisoning Attacks Against Continual Learners ](http://arxiv.org/abs/2311.10919)

	Huayu Li, Gregory Ditzler


+ [ Two-Factor Authentication Approach Based on Behavior Patterns for Defeating Puppet Attacks ](http://arxiv.org/abs/2311.10389)

	Wenhao Wang, Guyue Li, Zhiming Chu, Haobo Li, Daniele Faccio


## 2023-11-16
+ [Breaking Boundaries: Balancing Performance and Robustness in Deep Wireless Traffic Forecasting](http://arxiv.org/abs/2311.09790)

    Ilbert Romain, V. Hoang Thai, Zhang Zonghua, Palpanas Themis


+ [ Hijacking Large Language Models via Adversarial In-Context Learning](http://arxiv.org/abs/2311.09948)

    Yao Qiang, Xiangyu Zhou, Dongxiao Zhu


+ [ Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking](http://arxiv.org/abs/2311.09827)

    Nan Xu, Fei Wang, Ben Zhou, Bang Zheng Li, Chaowei Xiao, Muhao Chen


+ [ Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations](http://arxiv.org/abs/2311.09763)

    Wenjie Mo, Jiashu Xu, Qin Liu, Jiongxiao Wang, Jun Yan, Chaowei Xiao, Muhao Chen


+ [ On the Exploitability of Reinforcement Learning with Human Feedback for Large Language Models](http://arxiv.org/abs/2311.09641)

    Jiongxiao Wang, Junlin Wu, Muhao Chen, Yevgeniy Vorobeychik, Chaowei Xiao


+ [ Towards more Practical Threat Models in Artificial Intelligence Security](http://arxiv.org/abs/2311.09994)

    Kathrin Grosse, Lukas Bieringer, Tarek Richard Besold, Alexandre Alahi


## 2023-11-15
+ [ Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts](http://arxiv.org/abs/2311.09127)

    Yuanwei Wu, Xiang Li, Yixin Liu, Pan Zhou, Lichao Sun


+ [ Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment](http://arxiv.org/abs/2311.09433)  

    Haoran Wang, Kai Shu


+ [ Fast Certification of Vision-Language Models Using Incremental Randomized Smoothing](http://arxiv.org/abs/2311.09024)

    A K Iowa State University Nirala, A New York University Joshi, C New York University Hegde, S Iowa State University Sarkar


+ [ Adversarially Robust Spiking Neural Networks Through Conversion](http://arxiv.org/abs/2311.09266)

    Ozan Özdenizci, Robert Legenstein


+ [ Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization](http://arxiv.org/abs/2311.09096)

    Zhexin Zhang, Junxiao Yang, Pei Ke, Minlie Huang


+ [ Privacy Threats in Stable Diffusion Models](http://arxiv.org/abs/2311.09355)

    Thomas Cilloni, Charles Fleming, Charles Walter


+ [ How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities](http://arxiv.org/abs/2311.09447)

    Lingbo Mo, Boshi Wang, Muhao Chen, Huan Sun


+ [ MirrorNet: A TEE-Friendly Framework for Secure On-device DNN Inference](http://arxiv.org/abs/2311.09489)

    Ziyu Liu, Yukui Luo, Shijin Duan, Tong Zhou, Xiaolin Xu


+ [ Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models](http://arxiv.org/abs/2311.09428)

    Yueqing Liang, Lu Cheng, Ali Payani, Kai Shu


+ [ JAB: Joint Adversarial Prompting and Belief Augmentation](http://arxiv.org/abs/2311.09473)

    Ninareh Mehrabi, Palash Goyal, Anil Ramakrishna, Jwala Dhamala, Shalini Ghosh, Richard Zemel, Kai-Wei Chang, Aram Galstyan, Rahul Gupta


## 2023-11-14
+ [ Towards Improving Robustness Against Common Corruptions in Object Detectors Using Adversarial Contrastive Learning](http://arxiv.org/abs/2311.07928)

    Shashank Kotyan, Danilo Vasconcellos Vargas


+ [ Physical Adversarial Examples for Multi-Camera Systems](http://arxiv.org/abs/2311.08539)

    Ana Răduţoiu, Jan-Philipp Schulze, Philip Sperl, Konstantin Böttinger


+ [ DALA: A Distribution-Aware LoRA-Based Adversarial Attack against Pre-trained Language Models](http://arxiv.org/abs/2311.08598)

    Yibo Wang, Xiangjue Dong, James Caverlee, Philip S. Yu


+ [ On The Relationship Between Universal Adversarial Attacks And Sparse Representations](http://arxiv.org/abs/2311.08265)

    Dana Weitzner, Raja Giryes


+ [ A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily](http://arxiv.org/abs/2311.08268)   

    Peng Ding, Jun Kuang, Dan Ma, Xuezhi Cao, Yunsen Xian, Jiajun Chen, Shujian Huang


+ [ Multi-Set Inoculation: Assessing Model Robustness Across Multiple Challenge Sets](http://arxiv.org/abs/2311.08662)

    Vatsal Gupta, Pranshu Pandya, Tushar Kataria, Vivek Gupta, Dan Roth


+ [ The Perception-Robustness Tradeoff in Deterministic Image Restoration](http://arxiv.org/abs/2311.09253)

    Guy Ohayon, Tomer Michaeli, Michael Elad


## 2023-11-13
+ [ Adversarial Purification for Data-Driven Power System Event Classifiers with Diffusion Models](http://arxiv.org/abs/2311.07110)

    Yuanbin Cheng, Koji Yamashita, Jim Follum, Nanpeng Yu


+ [ Parrot-Trained Adversarial Examples: Pushing the Practicality of Black-Box Audio Attacks against Speaker Recognition Models](http://arxiv.org/abs/2311.07780)

    Rui Duan, Zhe Qu, Leah Ding, Yao Liu, Zhuo Lu


+ [ An Extensive Study on Adversarial Attack against Pre-trained Models of Code](http://arxiv.org/abs/2311.07553)

    Xiaohu Du, Ming Wen, Zichao Wei, Shangwen Wang, Hai Jin


+ [ Untargeted Black-box Attacks for Social Recommendations](http://arxiv.org/abs/2311.07127)

    Wenqi Fan, Shijie Wang, Xiao-yong Wei, Xiaowei Mei, Qing Li


+ [ On the Robustness of Neural Collapse and the Neural Collapse of Robustness](http://arxiv.org/abs/2311.07444)

    Jingtong Su, Ya Shi Zhang, Nikolaos Tsilivis, Julia Kempe


+ [ Tabdoor: Backdoor Vulnerabilities in Transformer-based Neural Networks for Tabular Data ](http://arxiv.org/abs/2311.07550)

    Bart Pleiter, Behrad Tajalli, Stefanos Koffas, Gorka Abad, Jing Xu, Martha Larson, Stjepan Picek


## 2023-11-12
+ [ Learning Globally Optimized Language Structure via Adversarial Training](http://arxiv.org/abs/2311.06771)

	Xuwang Yin


+ [ Contractive Systems Improve Graph Neural Networks Against Adversarial Attacks](http://arxiv.org/abs/2311.06942)

    Moshe Eliasof, Davide Murari, Ferdia Sherry, Carola-Bibiane Schönlieb


+ [ Analytical Verification of Deep Neural Network Performance for Time-Synchronized Distribution System State Estimation](http://arxiv.org/abs/2311.06973)

        Behrouz Azimian, Shiva Moshtagh, Anamitra Pal, Shanshan Ma


+ [ DialMAT: Dialogue-Enabled Transformer with Moment-Based Adversarial Training](http://arxiv.org/abs/2311.06855)

    Kanta Kaneda, Ryosuke Korekata, Yuiga Wada, Shunya Nagashima, Motonari Kambara, Yui Iioka, Haruka Matsuo, Yuto Imai, Takayuki Nishimura, Komei Sugiura


## 2023-11-10
+ [ Robust Adversarial Attacks Detection for Deep Learning based Relative  Pose Estimation for Space Rendezvous](https://arxiv.org/abs/2311.05992)

	Ziwei Wang, Nabil Aouf, Jose Pizarro, Christophe Honvault


+ [ Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the  Wild](https://arxiv.org/abs/2311.06237)

	Nanna Inie, Jonathan Stray, Leon Derczynski


+ [ Scale-MIA: A Scalable Model Inversion Attack against Secure Federated  Learning via Latent Space Reconstruction](https://arxiv.org/abs/2311.05808)

	Shanghao Shi, Ning Wang, Yang Xiao, Chaoyu Zhang, Yi Shi, Y.Thomas Hou, Wenjing Lou


+ [ Does Differential Privacy Prevent Backdoor Attacks in Practice?](https://arxiv.org/abs/2311.06227)

	Fereshteh Razmi, Jian Lou, Li Xiong


+ [Flatness-aware Adversarial Attack](https://arxiv.org/abs/2311.06423)

	Mingyuan Fan, Xiaodan Li, Cen Chen, Yinggui Wang


+ [Robust Adversarial Attacks Detection for Deep Learning based Relative Pose Estimation for Space Rendezvous.](https://arxiv.org/abs/2311.05992)

	Ziwei Wang, Nabil Aouf, Jose Pizarro, Christophe Honvault


+ [Fight Fire with Fire: Combating Adversarial Patch Attacks using Pattern-randomized Defensive Patches](https://arxiv.org/abs/2311.06122)
	
	Jianan Feng, Jiachun Li, Changqing Miao, Jianjun Huang, Wei You, Wenchang Shi, Bin Liang


+ [Resilient and constrained consensus against adversarial attacks: A distributed MPC framework](https://arxiv.org/abs/2311.05935)

	Henglai Wei, Kunwu Zhang, Hui Zhang, Yang Shi


+ [CALLOC: Curriculum Adversarial Learning for Secure and Robust Indoor Localization](https://arxiv.org/abs/2311.06361)

	Danish Gufran, Sudeep Pasricha

+ [Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration](https://arxiv.org/abs/2311.06062)

	Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, Tao Jiang


## 2023-11-09
+ [ Counter-Empirical Attacking based on Adversarial Reinforcement Learning  for Time-Relevant Scoring System](https://arxiv.org/abs/2311.05144)

	Xiangguo Sun, Hong Cheng, Hang Dong, Bo Qiao, Si Qin, Qingwei Lin


+ [ SynFacePAD 2023: Competition on Face Presentation Attack Detection Based  on Privacy-aware Synthetic Training Data](https://arxiv.org/abs/2311.05336)

	Meiling Fang, Marco Huber, Julian Fierrez, Raghavendra Ramachandra, Naser Damer, Alhasan Alkhaddour, Maksim Kasantcev, Vasiliy Pryadchenko, Ziyuan Yang, Huijie Huangfu, Yingyu Chen, Yi Zhang, Yuchen Pan, Junjun Jiang, Xianming Liu, Xianyun Sun, Caiyong Wang, Xingyu Liu, Zhaohua Chang, Guangzhe Zhao, Juan Tapia, Lazaro Gonzalez-Soler, Carlos Aravena, Daniel Schulz


+ [ Counter-Empirical Attacking based on Adversarial Reinforcement Learning  for Time-Relevant Scoring System](https://arxiv.org/abs/2311.05144)

	Xiangguo Sun, Hong Cheng, Hang Dong, Bo Qiao, Si Qin, Qingwei Lin


+ [ SCAAT: Improving Neural Network Interpretability via Saliency  Constrained Adaptive Adversarial Training](https://arxiv.org/abs/2311.05143)

	Rui Xu, Wenkang Qin, Peixiang Huang, Haowang, Lin Luo


+ [ Training Robust Deep Physiological Measurement Models with Synthetic  Video-based Data](https://arxiv.org/abs/2311.05371)

	Yuxuan Ou, Yuzhe Zhang, Yuntang Wang, Shwetak Patel, Daniel McDuf, Xin Liu

+ [ FigStep: Jailbreaking Large Vision-language Models via Typographic  Visual Prompts](https://arxiv.org/abs/2311.05608)

	Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, Xiaoyun Wang


## 2023-11-8
+ [ Familiarity-Based Open-Set Recognition Under Adversarial Attacks](https://arxiv.org/abs/2311.05006)

	Philip Enevoldsen, Christian Gundersen, Nico Lang, Serge Belongie, Christian Igel


+ [ Edge-assisted U-Shaped Split Federated Learning with Privacy-preserving  for Internet of Things](https://arxiv.org/abs/2311.04944)

	Hengliang Tang, Zihang Zhao, Detian Liu, Yang Cao, Shiqiang Zhang, Siqing You


+ [ DEMASQ: Unmasking the ChatGPT Wordsmith](https://arxiv.org/abs/2311.05019)

	Kavita Kumari, Alessandro Pegoraro, Hossein Fereidooni, Ahmad-Reza Sadeghi


+ [ On the steerability of large language models toward data-driven personas](https://arxiv.org/abs/2311.04978)

	Junyi Li, Ninareh Mehrabi, Charith Peris, Palash Goyal, Kai-Wei Chang, Aram Galstyan, Richard Zemel, Rahul Gupta


## 2023-11-7
+ [ FD-MIA: Efficient Attacks on Fairness-enhanced Models](https://arxiv.org/abs/2311.03865)

	Huan Tian, Guangsheng Zhang, Bo Liu, Tianqing Zhu, Ming Ding, Wanlei Zhou


+ [ Unveiling Safety Vulnerabilities of Large Language Models](https://arxiv.org/abs/2311.04124)
	
	George Kour, Marcel Zalmanovici, Naama Zwerdling, Esther Goldbraich, Ora Nova Fandina, Ateret Anaby-Tavor, Orna Raz, Eitan Farchi


## 2023-11-6
+ [ A Preference Learning Approach to Develop Safe and Personalizable  Autonomous Vehicles](https://arxiv.org/abs/2311.02099)

	Ruya Karagulle, Nikos Arechiga, Andrew Best, Jonathan DeCastro, Necmiye Ozay


+ [ Making Harmful Behaviors Unlearnable for Large Language Models](https://arxiv.org/abs/2311.02105)

	Xin Zhou, Yi Lu, Ruotian Ma, Tao Gui, Qi Zhang, Xuanjing Huang


+ [ Uncertainty Quantification of Deep Learning for Spatiotemporal Data:  Challenges and Opportunities](https://arxiv.org/abs/2311.02485)

	Wenchong He, Zhe Jiang


+ [ On the Intersection of Self-Correction and Trust in Language Models](https://arxiv.org/abs/2311.02801)

	Satyapriya Krishna


+ [ Preserving Privacy in GANs Against Membership Inference Attack](https://arxiv.org/abs/2311.03172)

	Mohammadhadi Shateri, Francisco Messina, Fabrice Labeau, Pablo Piantanida


+ [ DeepInception: Hypnotize Large Language Model to Be Jailbreaker](https://arxiv.org/abs/2311.03191)

	Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, Bo Han


## 2023-11-5
+ [ Pilot-Based Key Distribution and Encryption for Secure Coherent Passive  Optical Networks](https://arxiv.org/abs/2311.02554)

	Haide Wang, Ji Zhou, Qingxin Lu, Jianrui Zeng, Yongqing Liao, Weiping Liu, Changyuan Yu, Zhaohui Li


+ [ ELEGANT: Certified Defense on the Fairness of Graph Neural Networks](https://arxiv.org/abs/2311.02757)

	Yushun Dong; Binchi Zhang; Hanghang Tong; Jundong Li


## 2023-11-4
+ [ From Trojan Horses to Castle Walls: Unveiling Bilateral Backdoor Effects  in Diffusion Models](https://arxiv.org/abs/2311.02373)

	Zhuoshi Pan, Yuguang Yao, Gaowen Liu, Bingquan Shen, H. Vicky Zhao, Ramana Rao Kompella, Sijia Liu


## 2023-11-3
+ [ Can AI Mitigate Human Perceptual Biases? A Pilot Study](https://arxiv.org/abs/2311.00706)

	Ross Geuy, Nate Rising, Tiancheng Shi, Meng Ling, Jian Chen


+ [ SCPO: Safe Reinforcement Learning with Safety Critic Policy Optimization](https://arxiv.org/abs/2311.00880)

	Jaafar Mhamed, Shangding Gu


+ [ Robust Identity Perceptual Watermark Against Deepfake Face Swapping](https://arxiv.org/abs/2311.01357)

	Tianyi Wang, Mengxiao Huang, Harry Cheng, Bin Ma, Yinglong Wang


+ [ A Call to Arms: AI Should be Critical for Social Media Analysis of  Conflict Zones](https://arxiv.org/abs/2311.00810)

	Afia Abedin, Abdul Bais, Cody Buntain, Laura Courchesne, Brian McQuinn, Matthew E. Taylor, Muhib Ullah


+ [ Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game](https://arxiv.org/abs/2311.01011)

	Sam Toyer, Olivia Watkins, Ethan Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor Darrell, Alan Ritter, Stuart Russell


+ [ MIST: Defending Against Membership Inference Attacks Through  Membership-Invariant Subspace Training](https://arxiv.org/abs/2311.00919)

	Jiacheng Li, Ninghui Li, Bruno Ribeiro


+ [ Reputation Systems for Supply Chains: The Challenge of Achieving Privacy  Preservation](https://arxiv.org/abs/2311.01060)

	Lennart Bader, Jan Pennekamp, Emildeon Thevaraj, Maria Spiß, Salil S. Kanhere, Klaus Wehrle


## 2023-11-2
+ [ Optimal Cost Constrained Adversarial Attacks For Multiple Agent Systems](https://arxiv.org/abs/2311.00859)

	Ziqing Lu, Guanlin Liu, Lifeng Cai, Weiyu Xu


+ [ SCPO: Safe Reinforcement Learning with Safety Critic Policy Optimization](https://arxiv.org/abs/2311.00880)

	Jaafar Mhamed, Shangding Gu


+ [ Generate and Pray: Using SALLMS to Evaluate the Security of LLM  Generated Code](https://arxiv.org/abs/2311.00889)

	Mohammed Latif Siddiq, Joanna C. S. Santos


+ [ Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Lehman Go  Indifferent](https://arxiv.org/abs/2311.01205)

	Lorenz Kummer, Samir Moustafa, Nils N. Kriege, Wilfried N. Gansterer


+ [ Towards Evaluating Transfer-based Attacks Systematically, Practically, and Fairly](https://arxiv.org/abs/2311.01323)

	Qizhang Li, Yiwen Guo, Wangmeng Zuo, Hao Chen


+ [ Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game](https://arxiv.org/abs/2311.01011)

	Sam Toyer, Olivia Watkins, Ethan Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor Darrell, Alan Ritter, Stuart Russell


+ [ In Defense of Softmax Parametrization for Calibrated and Consistent  Learning to Defer](https://arxiv.org/abs/2311.01106)

	Yuzhou Cao, Hussein Mozannar, Lei Feng, Hongxin Wei, Bo An


+ [ MIST: Defending Against Membership Inference Attacks Through  Membership-Invariant Subspace Training](https://arxiv.org/abs/2311.00919)

	Jiacheng Li, Ninghui Li, Bruno Ribeiro


## 2023-11-1
+ [ FAIRLABEL: Correcting Bias in Labels](https://arxiv.org/abs/2311.00638)

	Srinivasan H Sengamedu, Hien Pham


+ [ Probing Explicit and Implicit Gender Bias through LLM Conditional Text  Generation](https://arxiv.org/abs/2311.00306)

	Xiangjue Dong, Yibo Wang, Philip S. Yu, James Caverlee


+ [ Robustness Tests for Automatic Machine Translation Metrics with  Adversarial Attacks](https://arxiv.org/abs/2311.00508)

	Yichen Huang, Timothy Baldwin


+ [ Medi-CAT: Contrastive Adversarial Training for Medical Image  Classification](https://arxiv.org/abs/2311.00154)

	Pervaiz Iqbal Khan, Andreas Dengel, Sheraz Ahmed


+ [ Uncertainty quantification and out-of-distribution detection using  surjective normalizing flows](https://arxiv.org/abs/2311.00377)

	Simon Dirmeier, Ye Hong, Yanan Xin, Fernando Perez-Cruz


+ [ NEO-KD: Knowledge-Distillation-Based Adversarial Training for Robust  Multi-Exit Neural Networks](https://arxiv.org/abs/2311.00428)

	Seokil Ham, Jungwuk Park, Dong-Jun Han, Jaekyun Moon


## 2023-10-31
+ [ Unmasking Bias and Inequities: A Systematic Review of Bias Detection and  Mitigation in Healthcare Artificial Intelligence Using Electronic Health  Records](https://arxiv.org/abs/2310.19917)

	Feng Chen, Liqin Wang, Julie Hong, Jiaqi Jiang, Li Zhou


+ [ Is Robustness Transferable across Languages in Multilingual Neural  Machine Translation?](https://arxiv.org/abs/2310.20162)

	Leiyu Pan, Supryadi, Deyi Xiong


+ [ LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B](https://arxiv.org/abs/2310.20624)

	Simon Lermen, Charlie Rogers-Smith, Jeffrey Ladish


+ [ DEPN: Detecting and Editing Privacy Neurons in Pretrained Language  Models](https://arxiv.org/abs/2310.20138)

	Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, Deyi Xiong


+ [ Verification of Neural Networks Local Differential Classification  Privacy](https://arxiv.org/abs/2310.20299)

	Roie Reshef, Anan Kabaha, Olga Seleznova, Dana Drachsler-Cohen


+ [ Initialization Matters: Privacy-Utility Analysis of Overparameterized  Neural Networks](https://arxiv.org/abs/2310.20579)

	Jiayuan Ye, Zhenyu Zhu, Fanghui Liu, Reza Shokri, Volkan Cevher


## 2023-10-30
+ [ PriPrune: Quantifying and Preserving Privacy in Pruned Federated  Learning](https://arxiv.org/abs/2310.19958)

	Tianyue Chu, Mengwei Yang, Nikolaos Laoutaris, Athina Markopoulou


+ [ LipSim: A Provably Robust Perceptual Similarity Metric](https://arxiv.org/abs/2310.18274)

	Sara Ghazanfari, Alexandre Araujo, Prashanth Krishnamurthy, Farshad Khorrami, Siddharth Garg


+ [ Counterfactual Fairness for Predictions using Generative Adversarial  Networks](https://arxiv.org/abs/2310.17687)

	Yuchen Ma, Dennis Frauen, Valentyn Melnychuk, Stefan Feuerriegel


## 2023-10-29
+ [ Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection  Method](https://arxiv.org/abs/2310.17918)

	Yukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang Xing, Chong Meng, Shuaiqiang Wang, Zhicong Cheng, Zhaochun Ren, Dawei Yin


+ [ Fine tuning Pre trained Models for Robustness Under Noisy Labels](https://arxiv.org/abs/2310.17668)

	Sumyeong Ahn, Sihyeon Kim, Jongwoo Ko, Se-Young Yun


+ [ Adversarial Anomaly Detection using Gaussian Priors and Nonlinear  Anomaly Scores](https://arxiv.org/abs/2310.18091)

	Fiete Lüer, Tobias Weber, Maxim Dolgich, Christian Böhm


+ [ $α$-Mutual Information: A Tunable Privacy Measure for Privacy  Protection in Data Sharing](https://arxiv.org/abs/2310.18241)

	MirHamed Jafarzadeh Asl, Mohammadhadi Shateri, Fabrice Labeau


+ [ BlackJack: Secure machine learning on IoT devices through hardware-based  shuffling](https://arxiv.org/abs/2310.17804)

	Karthik Ganesan, Michal Fishkin, Ourong Lin, Natalie Enright Jerger



## 2023-10-27
+ [ ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in  Real-World User-AI Conversation](https://arxiv.org/abs/2310.17389)

	Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, Jingbo Shang


+ [ Where you go is who you are -- A study on machine learning based  semantic privacy attacks](https://arxiv.org/abs/2310.17643)

	Nina Wiedemann, Ourania Kounadi, Martin Raubal, Krzysztof Janowicz


+ [ A near-autonomous and incremental intrusion detection system through  active learning of known and unknown attacks](https://arxiv.org/abs/2310.17430)

	Lynda Boukela, Gongxuan Zhang, Meziane Yacoub, Samia Bouzefrane


## 2023-10-26
+ [ CBD: A Certified Backdoor Detector Based on Local Dominant Probability](https://arxiv.org/abs/2310.17498)

	Zhen Xiang, Zidi Xiong, Bo Li


+ [ A Survey on Transferability of Adversarial Examples across Deep Neural  Networks](https://arxiv.org/abs/2310.17626)

	Jindong Gu, Xiaojun Jia, Pau de Jorge, Wenqain Yu, Xinwei Liu, Avery Ma, Yuan Xun, Anjun Hu, Ashkan Khakzar, Zhijiang Li, Xiaochun Cao, Philip Torr


+ [ Uncertainty-weighted Loss Functions for Improved Adversarial Attacks on  Semantic Segmentation](https://arxiv.org/abs/2310.17436)

	Kira Maag, Asja Fischer


+ [ Detecting stealthy cyberattacks on adaptive cruise control vehicles: A  machine learning approach](https://arxiv.org/abs/2310.17091)

	Tianyi Li, Mingfeng Shang, Shian Wang, Raphael Stern


+ [ SoK: Pitfalls in Evaluating Black-Box Attacks](https://arxiv.org/abs/2310.17534)

	Fnu Suya, Anshuman Suri, Tingwei Zhang, Jingtao Hong, Yuan Tian, David Evans


+ [ Defending Against Transfer Attacks From Public Models](https://arxiv.org/abs/2310.17645)

	Chawin Sitawarin, Jaewon Chang, David Huang, Wesson Altoyan, David Wagner


+ [ Proving Test Set Contamination in Black Box Language Models](https://arxiv.org/abs/2310.17623)

	Yonatan Oren, Nicole Meister, Niladri Chatterji, Faisal Ladhak, Tatsunori B. Hashimoto


+ [ Detection Defenses: An Empty Promise against Adversarial Patch Attacks  on Optical Flow](https://arxiv.org/abs/2310.17403)

	Erik Scheurer, Jenny Schmalfuss, Alexander Lis, Andrés Bruhn


## 2023-10-25
+ [ Trust, but Verify: Robust Image Segmentation using Deep Learning](https://arxiv.org/abs/2310.16999)

	Fahim Ahmed Zaman, Xiaodong Wu, Weiyu Xu, Milan Sonka, Raghuraman Mudumbai


+ [ Break it, Imitate it, Fix it: Robustness by Generating Human-Like  Attacks](https://arxiv.org/abs/2310.16955)

	Aradhana Sinha, Ananth Balashankar, Ahmad Beirami, Thi Avrahami, Jilin Chen, Alex Beutel


+ [ Improving Few-shot Generalization of Safety Classifiers via Data  Augmented Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2310.16959)

	Ananth Balashankar, Xiao Ma, Aradhana Sinha, Ahmad Beirami, Yao Qin, Jilin Chen, Alex Beutel


+ [ Confounder Balancing in Adversarial Domain Adaptation for Pre-Trained  Large Models Fine-Tuning](https://arxiv.org/abs/2310.16062)

	Shuoran Jiang, Qingcai Chen, Yang Xiang, Youcheng Pan, Xiangping Wu


## 2023-10-24
+ [ Enhancing Large Language Models for Secure Code Generation: A  Dataset-driven Study on Vulnerability Mitigation](https://arxiv.org/abs/2310.16263)

	Jiexin Wang, Liuwen Cao, Xitong Luo, Zhiping Zhou, Jiayuan Xie, Adam Jatowt, Yi Cai


+ [ Segue: Side-information Guided Generative Unlearnable Examples for  Facial Privacy Protection in Real World](https://arxiv.org/abs/2310.16061)

	Zhiling Zhang, Jie Zhang, Kui Zhang, Wenbo Zhou, Weiming Zhang, Nenghai Yu


+ [ Defense Against Model Extraction Attacks on Recommender Systems](https://arxiv.org/abs/2310.16335)

	Sixiao Zhang, Hongzhi Yin, Hongxu Chen, Cheng Long


+ [ Robust and Actively Secure Serverless Collaborative Learning](https://arxiv.org/abs/2310.16678)

	Olive Franzese, Adam Dziedzic, Christopher A. Choquette-Choo, Mark R. Thomas, Muhammad Ahmad Kaleem, Stephan Rabanser, Congyu Fang, Somesh Jha, Nicolas Papernot, Xiao Wang


+ [ AI Hazard Management: A framework for the systematic management of root  causes for AI risks](https://arxiv.org/abs/2310.16727)

	Ronald Schnitzer, Andreas Hapfelmeier, Sven Gaube, Sonja Zillner


+ [ FLTrojan: Privacy Leakage Attacks against Federated Language Models  Through Selective Weight Tampering](https://arxiv.org/abs/2310.16152)

	Md Rafi Ur Rashid, Vishnu Asutosh Dasu, Kang Gu, Najrin Sultana, Shagufta Mehnaz


+ [ Poison is Not Traceless: Fully-Agnostic Detection of Poisoning Attacks](https://arxiv.org/abs/2310.16224)

	Xinglong Chang, Katharina Dost, Gillian Dobbie, Jörg Wicker


+ [ A model for multi-attack classification to improve intrusion detection  performance using deep learning approaches](https://arxiv.org/abs/2310.16380)

	Arun Kumar Silivery, Ram Mohan Rao Kovvur


## 2023-10-23
+ [ 3D Masked Autoencoders for Enhanced Privacy in MRI Scans](https://arxiv.org/abs/2310.15778)

	Lennart Alexander Van der Goten, Kevin Smith


+ [ Self-Guard: Empower the LLM to Safeguard Itself](https://arxiv.org/abs/2310.15851)

	Zezhong Wang, Fangkai Yang, Lu Wang, Pu Zhao, Hongru Wang, Liang Chen, Qingwei Lin, Kam-Fai Wong


+ [ The Janus Interface: How Fine-Tuning in Large Language Models Amplifies  the Privacy Risks](https://arxiv.org/abs/2310.15469)

	Xiaoyi Chen, Siyuan Tang, Rui Zhu, Shijun Yan, Lei Jin, Zihao Wang, Liya Su, XiaoFeng Wang, Haixu Tang


+ [ Deceptive Fairness Attacks on Graphs via Meta Learning](https://arxiv.org/abs/2310.15653)

	Jian Kang, Yinglong Xia, Ross Maciejewski, Jiebo Luo, Hanghang Tong


+ [ Momentum Gradient-based Untargeted Attack on Hypergraph Neural Networks](https://arxiv.org/abs/2310.15656)

	Yang Chen, Stjepan Picek, Zhonglin Ye, Zhaoyang Wang, Haixing Zhao


## 2023-10-22
+ [ Fundamental Limits of Membership Inference Attacks on Machine Learning  Models](https://arxiv.org/abs/2310.13786)

	Eric Aubinais, Elisabeth Gassiat, Pablo Piantanida


+ [ Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models](https://arxiv.org/abs/2310.13828)

	Shawn Shan, Wenxin Ding, Josephine Passananti, Haitao Zheng, Ben Y. Zhao


+ [ MoPe: Model Perturbation-based Privacy Attacks on Language Models](https://arxiv.org/abs/2310.14369)

	Marvin Li, Jason Wang, Jeffrey Wang, Seth Neel


+ [ On existence, uniqueness and scalability of adversarial robustness  measures for AI classifiers](https://arxiv.org/abs/2310.14421)

	Illia Horenko


+ [ AutoDAN: Automatic and Interpretable Adversarial Attacks on Large  Language Models](https://arxiv.org/abs/2310.15140)

	Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, Tong Sun


+ [ Toward Stronger Textual Attack Detectors](https://arxiv.org/abs/2310.14001)

	Pierre Colombo, Marine Picot, Nathan Noiry, Guillaume Staerman, Pablo Piantanida


+ [ CT-GAT: Cross-Task Generative Adversarial Attack based on  Transferability](https://arxiv.org/abs/2310.14265)

	Minxuan Lv, Chengwei Dai, Kun Li, Wei Zhou, Songlin Hu


+ [ Data-Free Knowledge Distillation Using Adversarially Perturbed OpenGL  Shader Images](https://arxiv.org/abs/2310.13782)

	Logan Frank, Jim Davis


+ [ Bi-discriminator Domain Adversarial Neural Networks with Class-Level  Gradient Alignment](https://arxiv.org/abs/2310.13959)

	Chuang Zhao, Hongke Zhao, Hengshu Zhu, Zhenya Huang, Nan Feng, Enhong Chen, Hui Xiong


+ [ ADoPT: LiDAR Spoofing Attack Detection Based on Point-Level Temporal  Consistency](https://arxiv.org/abs/2310.14504)

	Minkyoung Cho, Yulong Cao, Zixiang Zhou, Z. Morley Mao


+ [ F$^2$AT: Feature-Focusing Adversarial Training via Disentanglement of  Natural and Perturbed Patterns](https://arxiv.org/abs/2310.14561)

	Yaguan Qian, Chenyu Zhao, Zhaoquan Gu, Bin Wang, Shouling Ji, Wei Wang, Boyang Zhou, Pan Zhou


+ [ Semantic-Aware Adversarial Training for Reliable Deep Hashing Retrieval](https://arxiv.org/abs/2310.14637)

	Xu Yuan, Zheng Zhang, Xunguang Wang, Lin Wu


+ [ On the Detection of Image-Scaling Attacks in Machine Learning](https://arxiv.org/abs/2310.15085)

	Erwin Quiring, Andreas Müller, Konrad Rieck


+ [ Adversarial Attacks on Fairness of Graph Neural Networks](https://arxiv.org/abs/2310.13822)

	Binchi Zhang, Yushun Dong, Chen Chen, Yada Zhu, Minnan Luo, Jundong Li


+ [ Competitive Advantage Attacks to Decentralized Federated Learning](https://arxiv.org/abs/2310.13862)

	Yuqi Jia, Minghong Fang, Neil Zhenqiang Gong


+ [ Enhancing Accuracy-Privacy Trade-off in Differentially Private Split  Learning](https://arxiv.org/abs/2310.14434)

	Ngoc Duy Pham, Khoa Tran Phan, Naveen Chilamkurti


## 2023-10-21
+ [ GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for  Reasoning Problems](https://arxiv.org/abs/2310.12397)

	Kaya Stechly, Matthew Marquez, Subbarao Kambhampati


+ [ Prompt Injection Attacks and Defenses in LLM-Integrated Applications](https://arxiv.org/abs/2310.12815)

	Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, Neil Zhenqiang Gong


+ [ Probing LLMs for hate speech detection: strengths and vulnerabilities](https://arxiv.org/abs/2310.12860)

	Sarthak Roy, Ashish Harshavardhan, Animesh Mukherjee, Punyajoy Saha


## 2023-10-20
+ [ PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models](https://arxiv.org/abs/2310.12439)

	Hongwei Yao, Jian Lou, Zhan Qin


+ [ Segment Anything Meets Universal Adversarial Perturbation](https://arxiv.org/abs/2310.12431)

	Dongshen Han, Sheng Zheng, Chaoning Zhang


+ [ Fast Model Debias with Machine Unlearning](https://arxiv.org/abs/2310.12560)

	Ruizhe Chen, Jianfei Yang, Huimin Xiong, Jianhong Bai, Tianxiang Hu, Jin Hao, Yang Feng, Joey Tianyi Zhou, Jian Wu, Zuozhu Liu


## 2023-10-19
+ [ Automatic Hallucination Assessment for Aligned Large Language Models via  Transferable Adversarial Attacks](https://arxiv.org/abs/2310.12516)

	Xiaodong Yu, Hao Cheng, Xiaodong Liu, Dan Roth, Jianfeng Gao


+ [ Attack Prompt Generation for Red Teaming and Defending Large Language  Models](https://arxiv.org/abs/2310.12505)

	Boyi Deng, Wenjie Wang, Fuli Feng, Yang Deng, Qifan Wang, Xiangnan He


+ [ Recoverable Privacy-Preserving Image Classification through Noise-like  Adversarial Examples](https://arxiv.org/abs/2310.12707)

	Jun Liu, Jiantao Zhou, Jinyu Tian, Weiwei Sun


+ [ REVAMP: Automated Simulations of Adversarial Attacks on Arbitrary  Objects in Realistic Scenes](https://arxiv.org/abs/2310.12243)

	Matthew Hull, Zijie J. Wang, Duen Horng Chau


+ [ Generating Robust Adversarial Examples against Online Social Networks  (OSNs)](https://arxiv.org/abs/2310.12708)

	Jun Liu, Jiantao Zhou, Haiwei Wu, Weiwei Sun, Jinyu Tian


+ [ OODRobustBench: benchmarking and analyzing adversarial robustness under  distribution shift](https://arxiv.org/abs/2310.12793)

	Lin Li, Yifei Wang, Chawin Sitawarin, Michael Spratling


+ [ CAT: Closed-loop Adversarial Training for Safe End-to-End Driving](https://arxiv.org/abs/2310.12432)

	Linrui Zhang, Zhenghao Peng, Quanyi Li, Bolei Zhou


+ [ Knowledge from Uncertainty in Evidential Deep Learning](https://arxiv.org/abs/2310.12663)

	Cai Davies, Marc Roig Vilamala, Alun D. Preece, Federico Cerutti, Lance M. Kaplan, Supriyo Chakraborty


+ [ Learn from the Past: A Proxy based Adversarial Defense Framework to  Boost Robustness](https://arxiv.org/abs/2310.12713)

	Yaohua Liu, Jiaxin Gao, Zhu Liu, Xianghao Jiao, Xin Fan, Risheng Liu


+ [ PrivInfer: Privacy-Preserving Inference for Black-box Large Language  Model](https://arxiv.org/abs/2310.12214)

	Meng Tong, Kejiang Chen, Yuang Qi, Jie Zhang, Weiming Zhang, Nenghai Yu


+ [ Privacy Preserving Large Language Models: ChatGPT Case Study Based  Vision and Framework](https://arxiv.org/abs/2310.12523)

	Imdad Ullah, Najm Hassan, Sukhpal Singh Gill, Basem Suleiman, Tariq Ahamed Ahanger, Zawar Shah, Junaid Qadir, Salil S. Kanhere


+ [ SecurityNet: Assessing Machine Learning Vulnerabilities on Public Models](https://arxiv.org/abs/2310.12665)

	Boyang Zhang, Zheng Li, Ziqing Yang, Xinlei He, Michael Backes, Mario Fritz, Yang Zhang


## 2023-10-18
+ [ Adversarial Robustness Unhardening via Backdoor Attacks in Federated  Learning](https://arxiv.org/abs/2310.11594)

	Taejin Kim, Jiarui Li, Shubhranshu Singh, Nikhil Madaan, Carlee Joe-Wong


+ [ WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks  Against Deep Neural Networks](https://arxiv.org/abs/2310.11595)

	Jun Xia, Zhihao Yue, Yingbo Zhou, Zhiwei Ling, Xian Wei, Mingsong Chen


+ [ The Efficacy of Transformer-based Adversarial Attacks in Security  Domains](https://arxiv.org/abs/2310.11597)

	Kunyang Li, Kyle Domico, Jean-Charles Noirot Ferrand, Patrick McDaniel


+ [ Black-Box Training Data Identification in GANs via Detector Networks](https://arxiv.org/abs/2310.12063)

	Lukman Olagoke, Salil Vadhan, Seth Neel


+ [ A Cautionary Tale: On the Role of Reference Data in Empirical Privacy  Defenses](https://arxiv.org/abs/2310.12112)

	Caelin G. Kaplan, Chuan Xu, Othmane Marfoq, Giovanni Neglia, Anderson Santana de Oliveira


+ [ Domain-Generalized Face Anti-Spoofing with Unknown Attacks](https://arxiv.org/abs/2310.11758)

	Zong-Wei Hong, Yu-Chen Lin, Hsuan-Tung Liu, Yi-Ren Yeh, Chu-Song Chen


+ [ To Generate or Not? Safety-Driven Unlearned Diffusion Models Are Still  Easy To Generate Unsafe Images ... For Now](https://arxiv.org/abs/2310.11868)

	Yimeng Zhang, Jinghan Jia, Xin Chen, Aochuan Chen, Yihua Zhang, Jiancheng Liu, Ke Ding, Sijia Liu


+ [ Exploring Decision-based Black-box Attacks on Face Forgery Detection](https://arxiv.org/abs/2310.12017)

	Zhaoyu Chen, Bo Li, Kaixun Jiang, Shuang Wu, Shouhong Ding, Wenqiang Zhang


+ [ Revisiting Transferable Adversarial Image Examples: Attack  Categorization, Evaluation Guidelines, and New Insights](https://arxiv.org/abs/2310.11850)

	Zhengyu Zhao, Hanwei Zhang, Renjue Li, Ronan Sicre, Laurent Amsaleg, Michael Backes, Qi Li, Chao Shen


+ [ In defense of parameter sharing for model-compression](https://arxiv.org/abs/2310.11611)

	Aditya Desai, Anshumali Shrivastava


+ [ Adversarial Training for Physics-Informed Neural Networks](https://arxiv.org/abs/2310.11789)

	Yao Li, Shengzhu Shi, Zhichang Guo, Boying Wu


+ [ Quantifying Privacy Risks of Prompts in Visual Prompt Learning](https://arxiv.org/abs/2310.11970)

	Yixin Wu, Rui Wen, Michael Backes, Pascal Berrang, Mathias Humbert, Yun Shen, Yang Zhang



## 2023-10-17
+ [ Demystifying Poisoning Backdoor Attacks from a Statistical Perspective](https://arxiv.org/abs/2310.10780)

	Xun Xian, Ganghua Wang, Jayanth Srinivasa, Ashish Kundu, Xuan Bi, Mingyi Hong, Jie Ding


+ [ Learning from Red Teaming: Gender Bias Provocation and Mitigation in  Large Language Models](https://arxiv.org/abs/2310.11079)

	Hsuan Su, Cheng-Chu Cheng, Hua Farn, Shachi H Kumar, Saurav Sahay, Shang-Tse Chen, Hung-yi Lee


+ [ Quantifying Language Models' Sensitivity to Spurious Features in Prompt  Design or: How I learned to start worrying about prompt formatting](https://arxiv.org/abs/2310.11324)

	Melanie Sclar, Yejin Choi, Yulia Tsvetkov, Alane Suhr


+ [ Functional Invariants to Watermark Large Transformers](https://arxiv.org/abs/2310.11446)

	Fernandez Pierre, Couairon Guillaume, Furon Teddy, Douze Matthijs


+ [ Fake News in Sheep's Clothing: Robust Fake News Detection Against  LLM-Empowered Style Attacks](https://arxiv.org/abs/2310.10830)

	Jiaying Wu, Bryan Hooi


+ [ Survey of Vulnerabilities in Large Language Models Revealed by  Adversarial Attacks](https://arxiv.org/abs/2310.10844)

	Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, Nael Abu-Ghazaleh


+ [ Will the Prince Get True Love's Kiss? On the Model Sensitivity to Gender  Perturbation over Fairytale Texts](https://arxiv.org/abs/2310.10865)

	Christina Chance, Da Yin, Dakuo Wang, Kai-Wei Chang


+ [ Backdoor Attack through Machine Unlearning](https://arxiv.org/abs/2310.10659)

	Peixin Zhang, Jun Sun, Mingtian Tan, Xinyu Wang


+ [ Regularization properties of adversarially-trained linear regression](https://arxiv.org/abs/2310.10807)

	Antônio H. Ribeiro, Dave Zachariah, Francis Bach, Thomas B. Schön


+ [ Locally Differentially Private Graph Embedding](https://arxiv.org/abs/2310.11060)

	Zening Li, Rong-Hua Li, Meihao Liao, Fusheng Jin, Guoren Wang


+ [ Last One Standing: A Comparative Analysis of Security and Privacy of  Soft Prompt Tuning, LoRA, and In-Context Learning](https://arxiv.org/abs/2310.11397)

	Rui Wen, Tianhao Wang, Michael Backes, Yang Zhang, Ahmed Salem


+ [ Unbiased Watermark for Large Language Models](https://arxiv.org/abs/2310.10669)

	Zhengmian Hu, Lichang Chen, Xidong Wu, Yihan Wu, Hongyang Zhang, Heng Huang


## 2023-10-16
+ [ DANAA: Towards transferable attacks with double adversarial neuron  attribution](https://arxiv.org/abs/2310.10427)

	Zhibo Jin, Zhiyu Zhu, Xinyi Wang, Jiayu Zhang, Jun Shen, Huaming Chen


+ [ Privacy in Large Language Models: Attacks, Defenses and Future  Directions](https://arxiv.org/abs/2310.10383)

	Haoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi Hu, Chunkit Chan, Yangqiu Song


+ [ Prompt Packer: Deceiving LLMs through Compositional Instruction with  Hidden Attacks](https://arxiv.org/abs/2310.10077)

	Shuyu Jiang, Xingshu Chen, Rui Tang


+ [ ASSERT: Automated Safety Scenario Red Teaming for Evaluating the  Robustness of Large Language Models](https://arxiv.org/abs/2310.09624)

	Alex Mei, Sharon Levy, William Yang Wang


+ [ Orthogonal Uncertainty Representation of Data Manifold for Robust  Long-Tailed Learning](https://arxiv.org/abs/2310.10090)

	Yanbiao Ma, Licheng Jiao, Fang Liu, Shuyuan Yang, Xu Liu, Lingling Li


+ [ Quantifying Assistive Robustness Via the Natural-Adversarial Frontier](https://arxiv.org/abs/2310.10610)

	Jerry Zhi-Yang He, Zackory Erickson, Daniel S. Brown, Anca D. Dragan


## 2023-10-15
+ [ SCME: A Self-Contrastive Method for Data-free and Query-Limited Model  Extraction Attack](https://arxiv.org/abs/2310.09792)

	Renyang Liu, Jinhong Zhang, Kwok-Yan Lam, Jun Zhao, Wei Zhou


+ [ AFLOW: Developing Adversarial Examples under Extremely Noise-limited  Settings](https://arxiv.org/abs/2310.09795)

	Renyang Liu, Jinhong Zhang, Haoran Li, Jin Zhang, Yuanyu Wang, Wei Zhou


+ [ Black-box Targeted Adversarial Attack on Segment Anything (SAM)](https://arxiv.org/abs/2310.10010)

	Sheng Zheng, Chaoning Zhang


+ [ Explore the Effect of Data Selection on Poison Efficiency in Backdoor  Attacks](https://arxiv.org/abs/2310.09744)

	Ziqiang Li, Pengfei Xia, Hong Sun, Yueqi Zeng, Wei Zhang, Bin Li


+ [ Model Inversion Attacks on Homogeneous and Heterogeneous Graph Neural  Networks](https://arxiv.org/abs/2310.09800)

	Renyang Liu, Wei Zhou, Jinhong Zhang, Xiaoyuan Liu, Peiyuan Si, Haoran Li


+ [ Evaluating Robustness of Visual Representations for Object Assembly Task  Requiring Spatio-Geometrical Reasoning](https://arxiv.org/abs/2310.09943)

	Chahyon Ku, Carl Winge, Ryan Diaz, Wentao Yuan, Karthik Desingh


+ [ Is Certifying $\ell_p$ Robustness Still Worthwhile?](https://arxiv.org/abs/2310.09361)

	Ravi Mangal, Klas Leino, Zifan Wang, Kai Hu, Weicheng Yu, Corina Pasareanu, Anupam Datta, Matt Fredrikson


+ [ MAGIC: Detecting Advanced Persistent Threats via Masked Graph  Representation Learning](https://arxiv.org/abs/2310.09831)

	Zian Jia, Yun Xiong, Yuhong Nan, Yao Zhang, Jinjing Zhao, Mi Wen


+ [ A Comprehensive Study of Privacy Risks in Curriculum Learning](https://arxiv.org/abs/2310.10124)

	Joann Qiongna Chen, Xinlei He, Zheng Li, Yang Zhang, Zhou Li


+ [ Prime Match: A Privacy-Preserving Inventory Matching System](https://arxiv.org/abs/2310.09621)

	Antigoni Polychroniadou, Gilad Asharov, Benjamin Diamond, Tucker Balch, Hans Buehler, Richard Hua, Suwen Gu, Greg Gimler, Manuela Veloso


+ [ BufferSearch: Generating Black-Box Adversarial Texts With Lower Queries](https://arxiv.org/abs/2310.09652)

	Wenjie Lv, Zhen Wang, Yitao Zheng, Zhehua Zhong, Qi Xuan, Tianyi Chen


## 2023-10-14
+ [ Defending Our Privacy With Backdoors](https://arxiv.org/abs/2310.08320)

	Dominik Hintersdorf, Lukas Struppek, Daniel Neider, Kristian Kersting


+ [ Samples on Thin Ice: Re-Evaluating Adversarial Pruning of Neural  Networks](https://arxiv.org/abs/2310.08073)

	Giorgio Piras, Maura Pintor, Ambra Demontis, Battista Biggio


## 2023-10-13
+ [ Towards the Vulnerability of Watermarking Artificial Intelligence  Generated Content](https://arxiv.org/abs/2310.07726)

	Guanlin Li, Yifei Chen, Jie Zhang, Jiwei Li, Shangwei Guo, Tianwei Zhang


+ [ Sentinel: An Aggregation Function to Secure Decentralized Federated  Learning](https://arxiv.org/abs/2310.08097)

	Chao Feng, Alberto Huertas Celdran, Janosch Baltensperger, Enrique Tomas Matınez Bertran, Gerome Bovet, Burkhard Stiller


+ [ Trustworthy Machine Learning](https://arxiv.org/abs/2310.08215)

	Bálint Mucsányi, Michael Kirchhof, Elisa Nguyen, Alexander Rubinstein, Seong Joon Oh


+ [ Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders](https://arxiv.org/abs/2310.08571)

	Jan Dubiński, Stanisław Pawlak, Franziska Boenisch, Tomasz Trzciński, Adam Dziedzic

## 2023-10-12
+ [ Effects of Human Adversarial and Affable Samples on BERT  Generalizability](https://arxiv.org/abs/2310.08008)

	Aparna Elangovan, Jiayuan He, Yuan Li, Karin Verspoor


+ [ Concealed Electronic Countermeasures of Radar Signal with Adversarial  Examples](https://arxiv.org/abs/2310.08292)

	Ruinan Ma, Canjie Zhu, Mingfeng Lu, Yunjie Li, Yu-an Tan, Ruibin Zhang, Ran Tao


+ [ Jailbreaking Black Box Large Language Models in Twenty Queries](https://arxiv.org/abs/2310.08419)

	Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J. Pappas, Eric Wong


+ [ Towards Robust Multi-Modal Reasoning via Model Selection](https://arxiv.org/abs/2310.08446)

	Xiangyan Liu, Rongxue Li, Wei Ji, Tao Lin


+ [ Improving Fast Minimum-Norm Attacks with Hyperparameter Optimization](https://arxiv.org/abs/2310.08177)

	Giuseppe Floris, Raffaele Mura, Luca Scionis, Giorgio Piras, Maura Pintor, Ambra Demontis, Battista Biggio


+ [ Invisible Threats: Backdoor Attack in OCR Systems](https://arxiv.org/abs/2310.08259)

	Mauro Conti, Nicola Farronato, Stefanos Koffas, Luca Pajola, Stjepan Picek


+ [ Promoting Robustness of Randomized Smoothing: Two Cost-Effective  Approaches](https://arxiv.org/abs/2310.07780)

	Linbo Liu, Trong Nghia Hoang, Lam M. Nguyen, Tsui-Wei Weng


+ [ Towards Causal Deep Learning for Vulnerability Detection](https://arxiv.org/abs/2310.07958)

	Md Mahbubur Rahman, Ira Ceka, Chengzhi Mao, Saikat Chakraborty, Baishakhi Ray, Wei Le


## 2023-10-11
+ [ RobustEdge: Low Power Adversarial Detection for Cloud-Edge Systems](https://arxiv.org/abs/2310.06845)

	Abhishek Moitra, Abhiroop Bhattacharjee, Youngeun Kim, Priyadarshini Panda


+ [ Genetic Algorithm-Based Dynamic Backdoor Attack on Federated  Learning-Based Network Traffic Classification](https://arxiv.org/abs/2310.06855)

	Mahmoud Nazzal, Nura Aljaafari, Ahmed Sawalmeh, Abdallah Khreishah, Muhammad Anan, Abdulelah Algosaibi, Mohammed Alnaeem, Adel Aldalbahi, Abdulaziz Alhumam, Conrado P. Vizcarra, Shadan Alhamed


+ [ Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation](https://arxiv.org/abs/2310.06987)

	Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, Danqi Chen


+ [ No Privacy Left Outside: On the (In-)Security of TEE-Shielded DNN  Partition for On-Device ML](https://arxiv.org/abs/2310.07152)

	Ziqi Zhang, Chen Gong, Yifeng Cai, Yuanyuan Yuan, Bingyan Liu, Ding Li, Yao Guo, Xiangqun Chen


+ [ Boosting Black-box Attack to Deep Neural Networks with Conditional  Diffusion Models](https://arxiv.org/abs/2310.07492)

	Renyang Liu, Wei Zhou, Tianwei Zhang, Kangjie Chen, Jun Zhao, Kwok-Yan Lam


+ [ Composite Backdoor Attacks Against Large Language Models](https://arxiv.org/abs/2310.07676)

	Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang


+ [ Comparing the robustness of modern no-reference image- and video-quality  metrics to adversarial attacks](https://arxiv.org/abs/2310.06958)

	Anastasia Antsiferova, Khaled Abud, Aleksandr Gushchin, Sergey Lavrushkin, Ekaterina Shumitskaya, Maksim Velikanov, Dmitriy Vatolin


+ [ Robust Safe Reinforcement Learning under Adversarial Disturbances](https://arxiv.org/abs/2310.07207)

	Zeyang Li, Chuxiong Hu, Shengbo Eben Li, Jia Cheng, Yunan Wang


## 2023-10-10
+ [ Fingerprint Attack: Client De-Anonymization in Federated Learning](https://arxiv.org/abs/2310.05960)

	Qiongkai Xu, Trevor Cohn, Olga Ohrimenko


+ [ Suppressing Overestimation in Q-Learning through Adversarial Behaviors](https://arxiv.org/abs/2310.06286)

	HyeAnn Lee, Donghwan Lee


+ [ Jailbreak and Guard Aligned Language Models with Only Few In-Context  Demonstrations](https://arxiv.org/abs/2310.06387)

	Zeming Wei, Yifei Wang, Yisen Wang


+ [ Multilingual Jailbreak Challenges in Large Language Models](https://arxiv.org/abs/2310.06474)

	Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing


+ [ A Semantic Invariant Robust Watermark for Large Language Models](https://arxiv.org/abs/2310.06356)

	Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng, Lijie Wen


+ [ Adversarial Masked Image Inpainting for Robust Detection of Mpox and  Non-Mpox](https://arxiv.org/abs/2310.06318)

	Yubiao Yue, Zhenzhang Li


+ [ Leveraging Diffusion-Based Image Variations for Robust Training on  Poisoned Data](https://arxiv.org/abs/2310.06372)

	Lukas Struppek, Martin B. Hentschel, Clifton Poth, Dominik Hintersdorf, Kristian Kersting


+ [ Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield  but Also a Catalyst for Model Inversion Attacks](https://arxiv.org/abs/2310.06549)

	Lukas Struppek, Dominik Hintersdorf, Kristian Kersting


+ [ Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK  Approach](https://arxiv.org/abs/2310.06112)

	Shaopeng Fu, Di Wang


+ [ PAC-Bayesian Spectrally-Normalized Bounds for Adversarially Robust  Generalization](https://arxiv.org/abs/2310.06182)

	Jiancong Xiao, Ruoyu Sun, Zhi-quan Luo


+ [ Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach](https://arxiv.org/abs/2310.06396)

	Kai Zhao, Qiyu Kang, Yang Song, Rui She, Sijie Wang, Wee Peng Tay


+ [ Exploring adversarial attacks in federated learning for medical imaging](https://arxiv.org/abs/2310.06227)

	Erfan Darzi, Florian Dubost, N.M. Sijtsema, P.M.A van Ooijen


## 2023-10-09

+ [ The Troubling Emergence of Hallucination in Large Language Models -- An  Extensive Definition, Quantification, and Prescriptive Remediations](https://arxiv.org/abs/2310.04988)

	Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S.M Towhidul Islam Tonmoy, Aman Chadha, Amit P. Sheth, Amitava Das


+ [ Large Language Models Can Be Good Privacy Protection Learners](https://arxiv.org/abs/2310.02469)

	Yijia Xiao, Yiqiao Jin, Yushi Bai, Yue Wu, Xianjun Yang, Xiao Luo, Wenchao Yu, Xujiang Zhao, Yanchi Liu, Haifeng Chen, Wei Wang, Wei Cheng


+ [ LoFT: Local Proxy Fine-tuning For Improving Transferability Of  Adversarial Attacks Against Large Language Model](https://arxiv.org/abs/2310.04445)

	Muhammad Ahmed Shah, Roshan Sharma, Hira Dhamyal, Raphael Olivier, Ankit Shah, Dareen Alharthi, Hazim T Bukhari, Massa Baali, Soham Deshmukh, Michael Kuhlmann, Bhiksha Raj, Rita Singh


+ [ AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language  Models](https://arxiv.org/abs/2310.04451)

	Xiaogeng Liu, Nan Xu, Muhao Chen, Chaowei Xiao



+ [ Understanding and Improving Adversarial Attacks on Latent Diffusion  Model](https://arxiv.org/abs/2310.04687)

	Boyang Zheng, Chumeng Liang, Xiaoyu Wu, Yan Liu


+ [ Robustness-enhanced Uplift Modeling with Adversarial Feature  Desensitization](https://arxiv.org/abs/2310.04693)

	Zexu Sun, Bowei He, Ming Ma, Jiakai Tang, Yuchen Wang, Chen Ma, Dugang Liu



+ [ Better Safe than Sorry: Pre-training CLIP against Targeted Data  Poisoning and Backdoor Attacks](https://arxiv.org/abs/2310.05862)

	Wenhan Yang, Jingdong Gao, Baharan Mirzasoleiman


+ [ BRAINTEASER: Lateral Thinking Puzzles for Large Language Model](https://arxiv.org/abs/2310.05057)

	Yifan Jiang, Filip Ilievski, Kaixin Ma


+ [ Do Large Language Models Know about Facts?](https://arxiv.org/abs/2310.05177)

	Xuming Hu, Junzhe Chen, Xiaochuan Li, Yufei Guo, Lijie Wen, Philip S. Yu, Zhijiang Guo


+ [ SC-Safety: A Multi-round Open-ended Question Adversarial Safety  Benchmark for Large Language Models in Chinese](https://arxiv.org/abs/2310.05818)

	Liang Xu, Kangkang Zhao, Lei Zhu, Hang Xue


+ [ IPMix: Label-Preserving Data Augmentation Method for Training Robust  Classifiers](https://arxiv.org/abs/2310.04780)

	Zhenglin Huang, Xianan Bao, Na Zhang, Qingqi Zhang, Xiaomei Tu, Biao Wu, Xi Yang


+ [ VLAttack: Multimodal Adversarial Attacks on Vision-Language Tasks via  Pre-trained Models](https://arxiv.org/abs/2310.04655)

	Ziyi Yin, Muchao Ye, Tianrong Zhang, Tianyu Du, Jinguo Zhu, Han Liu, Jinghui Chen, Ting Wang, Fenglong Ma


+ [ GReAT: A Graph Regularized Adversarial Training Method](https://arxiv.org/abs/2310.05336)

	Samet Bayram, Kenneth Barner


+ [ Generating Less Certain Adversarial Examples Improves Robust  Generalization](https://arxiv.org/abs/2310.04539)

	Minxing Zhang, Michael Backes, Xiao Zhang


+ [ Protecting Sensitive Data through Federated Co-Training](https://arxiv.org/abs/2310.05696)

	Amr Abourayya, Jens Kleesiek, Kanishka Rao, Erman Ayday, Bharat Rao, Geoff Webb, Michael Kamp


+ [ Tight Certified Robustness via Min-Max Representations of ReLU Neural  Networks](https://arxiv.org/abs/2310.04916)

	Brendon G. Anderson, Samuel Pfrommer, Somayeh Sojoudi


## 2023-10-08
+ [ Lightweight Boosting Models for User Response Prediction Using  Adversarial Validation](https://arxiv.org/abs/2310.03778)

	Hyeonwoo Kim, Wonsung Lee


+ [ Assessing Robustness via Score-Based Adversarial Image Generation](https://arxiv.org/abs/2310.04285)

	Marcel Kollovieh, Lukas Gosch, Yan Scholten, Marten Lienen, Stephan Günnemann


+ [ Indirect Meltdown: Building Novel Side-Channel Attacks from  Transient-Execution Attacks](https://arxiv.org/abs/2310.04183)

	Daniel Weber, Fabian Thomas, Lukas Gerlach, Ruiyi Zhang, Michael Schwarz


## 2023-10-07
+ [ Benchmarking Local Robustness of High-Accuracy Binary Neural Networks  for Enhanced Traffic Sign Recognition](https://arxiv.org/abs/2310.03033)

	Andreea Postovan, Mădălina Eraşcu
	

+ [ Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models](https://arxiv.org/abs/2310.03123)

	Zihao Lin, Yan Sun, Yifan Shi, Xueqian Wang, Lifu Huang, Li Shen, Dacheng Tao


+ [ Fine-tuning Aligned Language Models Compromises Safety, Even When Users  Do Not Intend To!](https://arxiv.org/abs/2310.03693)

	Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, Peter Henderson


+ [ Ask for Alice: Online Covert Distress Signal in the Presence of a Strong  Adversary](https://arxiv.org/abs/2310.03237)

	Hayyu Imanda, Kasper Rasmussen


## 2023-10-06
+ [ Misusing Tools in Large Language Models With Visual Adversarial Examples](https://arxiv.org/abs/2310.03185)

	Xiaohan Fu, Zihan Wang, Shuheng Li, Rajesh K. Gupta, Niloofar Mireshghallah, Taylor Berg-Kirkpatrick, Earlence Fernandes


+ [ Robust Representation Learning via Asymmetric Negative Contrast and  Reverse Attention](https://arxiv.org/abs/2310.03358)

	Nuoyan Zhou, Decheng Liu, Dawei Zhou, Xinbo Gao, Nannan Wang


+ [ SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks](https://arxiv.org/abs/2310.03684)

	Alexander Robey, Eric Wong, Hamed Hassani, George J. Pappas


+ [ A Formalism and Approach for Improving Robustness of Large Language  Models Using Risk-Adjusted Confidence Scores](https://arxiv.org/abs/2310.03283)

	Ke Shen, Mayank Kejriwal


+ [ Shielding the Unseen: Privacy Protection through Poisoning NeRF with  Spatial Deformation](https://arxiv.org/abs/2310.03125)

	Yihan Wu, Brandon Y. Feng, Heng Huang


+ [ CSI: Enhancing the Robustness of 3D Point Cloud Recognition against  Corruption](https://arxiv.org/abs/2310.03360)

	Zhuoyuan Wu, Jiachen Sun, Chaowei Xiao


+ [ OMG-ATTACK: Self-Supervised On-Manifold Generation of Transferable  Evasion Attacks](https://arxiv.org/abs/2310.03707)

	Ofir Bar Tal, Adi Haviv, Amit H. Bermano


+ [ Untargeted White-box Adversarial Attack with Heuristic Defence Methods  in Real-time Deep Learning based Network Intrusion Detection System](https://arxiv.org/abs/2310.03334)

	Khushnaseeb Roshan, Aasim Zafar, Sheikh Burhan Ul Haque


+ [ Targeted Adversarial Attacks on Generalizable Neural Radiance Fields](https://arxiv.org/abs/2310.03578)

	Andras Horvath, Csaba M. Jozsa


+ [ Adversarial Machine Learning for Social Good: Reframing the Adversary as  an Ally](https://arxiv.org/abs/2310.03614)

	Shawqi Al-Maliki, Adnan Qayyum, Hassan Ali, Mohamed Abdallah, Junaid Qadir, Dinh Thai Hoang, Dusit Niyato, Ala Al-Fuqaha


+ [ Raze to the Ground: Query-Efficient Adversarial HTML Attacks on  Machine-Learning Phishing Webpage Detectors](https://arxiv.org/abs/2310.03166)

	Biagio Montaruli, Luca Demetrio, Maura Pintor, Luca Compagna, Davide Balzarotti, Battista Biggio


+ [ Regret Analysis of Distributed Online Control for LTI Systems with  Adversarial Disturbances](https://arxiv.org/abs/2310.03206)

	Ting-Jui Chang, Shahin Shahrampour


+ [ Certifiably Robust Graph Contrastive Learning](https://arxiv.org/abs/2310.03312)

	Minhua Lin, Teng Xiao, Enyan Dai, Xiang Zhang, Suhang Wang


+ [ An Integrated Algorithm for Robust and Imperceptible Audio Adversarial  Examples](https://arxiv.org/abs/2310.03349)

	Armin Ettenhofer, Jan-Philipp Schulze, Karla Pizzi


## 2023-10-05
+ [ Low-Resource Languages Jailbreak GPT-4](https://arxiv.org/abs/2310.02446)

	Zheng-Xin Yong, Cristina Menghini, Stephen H. Bach


+ [ Discovering General Reinforcement Learning Algorithms with Adversarial  Environment Design](https://arxiv.org/abs/2310.02782)

	Matthew Thomas Jackson, Minqi Jiang, Jack Parker-Holder, Risto Vuorio, Chris Lu, Gregory Farquhar, Shimon Whiteson, Jakob Nicolaus Foerster


+ [ Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models](https://arxiv.org/abs/2310.02949)

	Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, Dahua Lin


+ [ Can Large Language Models Provide Security & Privacy Advice? Measuring  the Ability of LLMs to Refute Misconceptions](https://arxiv.org/abs/2310.02431)

	Yufan Chen, Arjun Arunasalam, Z. Berkay Celik


+ [ SlowFormer: Universal Adversarial Patch for Attack on Compute and Energy  Efficiency of Inference Efficient Vision Transformers](https://arxiv.org/abs/2310.02544)

	KL Navaneet, Soroush Abbasi Koohpayegani, Essam Sleiman, Hamed Pirsiavash


+ [ Splitting the Difference on Adversarial Training](https://arxiv.org/abs/2310.02480)

	Matan Levi, Aryeh Kontorovich


+ [ A Recipe for Improved Certifiable Robustness: Capacity and Data](https://arxiv.org/abs/2310.02513)

	Kai Hu, Klas Leino, Zifan Wang, Matt Fredrikson


+ [ Jailbreaker in Jail: Moving Target Defense for Large Language Models](https://arxiv.org/abs/2310.02417)

	Bocheng Chen, Advait Paliwal, Qiben Yan


## 2023-10-04
+ [ Identifying and Mitigating Privacy Risks Stemming from Language Models:  A Survey](https://arxiv.org/abs/2310.01424)

	Victoria Smith, Ali Shahin Shamsabadi, Carolyn Ashurst, Adrian Weller


+ [ Fooling the Textual Fooler via Randomizing Latent Representations](https://arxiv.org/abs/2310.01452)

	Duy C. Hoang, Quang H. Nguyen, Saurav Manchanda, MinLong Peng, Kok-Seng Wong, Khoa D. Doan


+ [ On the Safety of Open-Sourced Large Language Models: Does Alignment  Really Prevent Them From Being Misused?](https://arxiv.org/abs/2310.01581)

	Hangfan Zhang, Zhimeng Guo, Huaisheng Zhu, Bochuan Cao, Lu Lin, Jinyuan Jia, Jinghui Chen, Dinghao Wu


+ [ Towards Stable Backdoor Purification through Feature Shift Tuning](https://arxiv.org/abs/2310.01875)

	Rui Min, Zeyu Qin, Li Shen, Minhao Cheng


+ [ Defending Against Authorship Identification Attacks](https://arxiv.org/abs/2310.01568)

	Haining Wang


+ [ Can Language Models be Instructed to Protect Personal Information?](https://arxiv.org/abs/2310.02224)

	Yang Chen, Ethan Mendes, Sauvik Das, Wei Xu, Alan Ritter


+ [ Adversarial Client Detection via Non-parametric Subspace Monitoring in  the Internet of Federated Things](https://arxiv.org/abs/2310.01537)

	Xianjian Xie, Xiaochen Xian, Dan Li, Andi Wang


+ [ Fool Your (Vision and) Language Model With Embarrassingly Simple  Permutations](https://arxiv.org/abs/2310.01651)

	Yongshuo Zong, Tingyang Yu, Bingchen Zhao, Ruchika Chavhan, Timothy Hospedales


+ [ Beyond Labeling Oracles: What does it mean to steal ML models?](https://arxiv.org/abs/2310.01959)

	Avital Shafran, Ilia Shumailov, Murat A. Erdogdu, Nicolas Papernot


+ [ FLEDGE: Ledger-based Federated Learning Resilient to Inference and  Backdoor Attacks](https://arxiv.org/abs/2310.02113)

	Jorge Castillo, Phillip Rieger, Hossein Fereidooni, Qian Chen, Ahmad Sadeghi


+ [ Waveform Manipulation Against DNN-based Modulation Classification  Attacks](https://arxiv.org/abs/2310.01894)

	Dimitrios Varkatzas, Antonios Argyriou


## 2023-10-03
+ [ Adversarial Driving Behavior Generation Incorporating Human Risk  Cognition for Autonomous Vehicle Evaluation](https://arxiv.org/abs/2310.00029)

	Zhen Liu, Hang Gao, Hao Ma, Shuo Cai, Yunfeng Hu, Ting Qu, Hong Chen, Xun Gong


+ [ Certified Robustness via Dynamic Margin Maximization and Improved  Lipschitz Regularization](https://arxiv.org/abs/2310.00116)

	Mahyar Fazlyab, Taha Entesari, Aniket Roy, Rama Chellappa


+ [ Beyond Random Noise: Insights on Anonymization Strategies from a Latent  Bandit Study](https://arxiv.org/abs/2310.00221)

	Alexander Galozy, Sadi Alawadi, Victor Kebande, Sławomir Nowaczyk


+ [ Understanding the Robustness of Randomized Feature Defense Against  Query-Based Adversarial Attacks](https://arxiv.org/abs/2310.00567)

	Quang H. Nguyen, Yingjie Lao, Tung Pham, Kok-Seng Wong, Khoa D. Doan


+ [ Faithful Explanations of Black-box NLP Models Using LLM-generated  Counterfactuals](https://arxiv.org/abs/2310.00603)

	Yair Gat, Nitay Calderon, Amir Feder, Alexander Chapanin, Amit Sharma, Roi Reichart


+ [ A Survey of Robustness and Safety of 2D and 3D Deep Learning Models  Against Adversarial Attacks](https://arxiv.org/abs/2310.00633)

	Yanjie Li, Bin Xie, Songtao Guo, Yuanyuan Yang, Bin Xiao



+ [ All Languages Matter: On the Multilingual Safety of Large Language  Models](https://arxiv.org/abs/2310.00905)

	Wenxuan Wang, Zhaopeng Tu, Chang Chen, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, Michael R. Lyu


+ [ Large Language Model-Powered Smart Contract Vulnerability Detection: New  Perspectives](https://arxiv.org/abs/2310.01152)

	Sihao Hu, Tiansheng Huang, Fatih İlhan, Selim Fukan Tekin, Ling Liu



+ [ Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language  Models](https://arxiv.org/abs/2310.00322)

	Chengdong Ma, Ziran Yang, Minquan Gao, Hai Ci, Jun Gao, Xuehai Pan, Yaodong Yang


+ [ Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2310.00648)

	Lauren Hong, Ting Wang


+ [ Robustness of AI-Image Detectors: Fundamental Limits and Practical  Attacks](https://arxiv.org/abs/2310.00076)

	Mehrdad Saberi, Vinu Sankar Sadasivan, Keivan Rezaei, Aounon Kumar, Atoosa Chegini, Wenxiao Wang, Soheil Feizi


+ [ Human-Producible Adversarial Examples](https://arxiv.org/abs/2310.00438)

	David Khachaturov, Yue Gao, Ilia Shumailov, Robert Mullins, Ross Anderson, Kassem Fawaz


+ [ Black-box Attacks on Image Activity Prediction and its Natural Language  Explanations](https://arxiv.org/abs/2310.00503)

	Alina Elena Baia, Valentina Poggioni, Andrea Cavallaro


+ [ GhostEncoder: Stealthy Backdoor Attacks with Dynamic Triggers to  Pre-trained Encoders in Self-supervised Learning](https://arxiv.org/abs/2310.00626)

	Qiannan Wang, Changchun Yin, Zhe Liu, Liming Fang, Run Wang, Chenhao Lin


+ [ Counterfactual Image Generation for adversarially robust and  interpretable Classifiers](https://arxiv.org/abs/2310.00761)

	Rafael Bischof, Florian Scheidegger, Michael A. Kraus, A. Cristiano I. Malossi


+ [ Practical Membership Inference Attacks Against Large-Scale Multi-Modal  Models: A Pilot Study](https://arxiv.org/abs/2310.00108)

	Myeongseob Ko, Ming Jin, Chenguang Wang, Ruoxi Jia


+ [ Understanding Adversarial Transferability in Federated Learning](https://arxiv.org/abs/2310.00616)

	Yijiang Li, Ying Gao, Haohan Wang


+ [ On the Onset of Robust Overfitting in Adversarial Training](https://arxiv.org/abs/2310.00607)

	Chaojian Yu, Xiaolong Shi, Jun Yu, Bo Han, Tongliang Liu


+ [ Balancing Efficiency vs. Effectiveness and Providing Missing Label  Robustness in Multi-Label Stream Classification](https://arxiv.org/abs/2310.00665)

	Sepehr Bakhshi, Fazli Can


+ [ Adversarial Explainability: Utilizing Explainable Machine Learning in  Bypassing IoT Botnet Detection Systems](https://arxiv.org/abs/2310.00070)

	Mohammed M. Alani, Atefeh Mashatan, Ali Miri


+ [ Source Inference Attacks: Beyond Membership Inference Attacks in  Federated Learning](https://arxiv.org/abs/2310.00222)

	Hongsheng Hu, Xuyun Zhang, Zoran Salcic, Lichao Sun, Kim-Kwang Raymond Choo, Gillian Dobbie


## 2023-10-02
+ [ AIR: Threats of Adversarial Attacks on Deep Learning-Based Information  Recovery](https://arxiv.org/abs/2309.16706)

	Jinyin Chen, Jie Ge, Shilian Zheng, Linhui Ye, Haibin Zheng, Weiguo Shen, Keqiang Yue, Xiaoniu Yang


+ [ General Lipschitz: Certified Robustness Against Resolvable Semantic  Transformations via Transformation-Dependent Randomized Smoothing](https://arxiv.org/abs/2309.16710)

	Dmitrii Korzh, Mikhail Pautov, Olga Tsymboi, Ivan Oseledets


+ [ Investigating Human-Identifiable Features Hidden in Adversarial  Perturbations](https://arxiv.org/abs/2309.16878)

	Dennis Y. Menn, Tzu-hsun Feng, Sriram Vishwanath, Hung-yi Lee


+ [ Medical Foundation Models are Susceptible to Targeted Misinformation  Attacks](https://arxiv.org/abs/2309.17007)

	Tianyu Han, Sven Nebelung, Firas Khader, Tianci Wang, Gustav Mueller-Franzes, Christiane Kuhl, Sebastian Försch, Jens Kleesiek, Christoph Haarburger, Keno K. Bressem, Jakob Nikolas Kather, Daniel Truhn



+ [ Adversarial Machine Learning in Latent Representations of Neural  Networks](https://arxiv.org/abs/2309.17401)

	Milin Zhang, Mohammad Abdi, Francesco Restuccia


+ [ Can Sensitive Information Be Deleted From LLMs? Objectives for Defending  Against Extraction Attacks](https://arxiv.org/abs/2309.17410)

	Vaidehi Patil, Peter Hase, Mohit Bansal


+ [ LatticeGen: A Cooperative Framework which Hides Generated Text in a  Lattice for Privacy-Aware Generation on Cloud](https://arxiv.org/abs/2309.17157)

	Mengke Zhang, Tianxing He, Tianle Wang, Fatemehsadat Mireshghallah, Binyi Chen, Hao Wang, Yulia Tsvetkov


+ [ Towards Robust Offline-to-Online Reinforcement Learning via Uncertainty  and Smoothness](https://arxiv.org/abs/2309.16973)

	Xiaoyu Wen, Xudong Yu, Rui Yang, Chenjia Bai, Zhen Wang


+ [ Efficient Biologically Plausible Adversarial Training](https://arxiv.org/abs/2309.17348)

	Matilde Tristany Farinha, Thomas Ortner, Giorgia Dellaferrera, Benjamin Grewe, Angeliki Pantazi


+ [ Adversarial Imitation Learning from Visual Observations using Latent  Information](https://arxiv.org/abs/2309.17371)

	Vittorio Giammarino, James Queeney, Ioannis Ch. Paschalidis


## 2023-10-01
+ [ Towards Efficient and Trustworthy AI Through  Hardware-Algorithm-Communication Co-Design](https://arxiv.org/abs/2309.15942)

	Bipin Rajendran, Osvaldo Simeone, Bashir M. Al-Hashimi


+ [ VDC: Versatile Data Cleanser for Detecting Dirty Samples via  Visual-Linguistic Inconsistency](https://arxiv.org/abs/2309.16211)

	Zihao Zhu, Mingda Zhang, Shaokui Wei, Bingzhe Wu, Baoyuan Wu


## 2023-09-30


+ [ Recent Advances of Differential Privacy in Centralized Deep Learning: A  Systematic Survey](https://arxiv.org/abs/2309.16398)

	Lea Demelius, Roman Kern, Andreas Trügler


+ [ Robust Offline Reinforcement Learning -- Certify the Confidence Interval](https://arxiv.org/abs/2309.16631)

	Jiarui Yao, Simon Shaolei Du


## 2023-09-29
+ [ Adversarial Examples Might be Avoidable: The Role of Data Concentration  in Adversarial Robustness](https://arxiv.org/abs/2309.16096)

	Ambar Pal, Jeremias Sulam, René Vidal


+ [ Parameter-Saving Adversarial Training: Reinforcing Multi-Perturbation  Robustness via Hypernetworks](https://arxiv.org/abs/2309.16207)

	Huihui Gong, Minjing Dong, Siqi Ma, Seyit Camtepe, Surya Nepal, Chang Xu


+ [ On the Trade-offs between Adversarial Robustness and Actionable  Explanations](https://arxiv.org/abs/2309.16452)

	Satyapriya Krishna, Chirag Agarwal, Himabindu Lakkaraju


+ [ Resisting Backdoor Attacks in Federated Learning via Bidirectional  Elections and Individual Perspective](https://arxiv.org/abs/2309.16456)

	Zhen Qin, Feiyi Chen, Chen Zhi, Xueqiang Yan, Shuiguang Deng


+ [ Towards Poisoning Fair Representations](https://arxiv.org/abs/2309.16487)

	Tianci Liu, Haoyu Wang, Feijie Wu, Hengtong Zhang, Pan Li, Lu Su, Jing Gao


+ [ Compilation as a Defense: Enhancing DL Model Attack Robustness via  Tensor Optimization](https://arxiv.org/abs/2309.16577)

	Stefan Trawicki, William Hackett, Lewis Birch, Neeraj Suri, Peter Garraghan


+ [ Cyber Sentinel: Exploring Conversational Agents in Streamlining Security  Tasks with GPT-4](https://arxiv.org/abs/2309.16422)

	Mehrdad Kaheh, Danial Khosh Kholgh, Panos Kostakos

## 2023-09-28

+ [ How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking  Unrelated Questions](https://arxiv.org/abs/2309.15840)

	Lorenzo Pacchiardi, Alex J. Chan, Sören Mindermann, Ilan Moscovitz, Alexa Y. Pan, Yarin Gal, Owain Evans, Jan Brauner


+ [ The Robust Semantic Segmentation UNCV2023 Challenge Results](https://arxiv.org/abs/2309.15478)

	Xuanlong Yu, Yi Zuo, Zitao Wang, Xiaowen Zhang, Jiaxuan Zhao, Yuting Yang, Licheng Jiao, Rui Peng, Xinyi Wang, Junpei Zhang, Kexin Zhang, Fang Liu, Roberto Alcover-Couso, Juan C. SanMiguel, Marcos Escudero-Viñolo, Hanlin Tian, Kenta Matsui, Tianhao Wang, Fahmy Adan, Zhitong Gao, Xuming He, Quentin Bouniot, Hossein Moghaddam, Shyam Nandan Rai, Fabio Cermelli, Carlo Masone, Andrea Pilzer, Elisa Ricci, Andrei Bursuc, Arno Solin, Martin Trapp, Rui Li, Angela Yao, Wenlong Chen, Ivor Simpson, Neill D. F. Campbell, Gianni Franchi


+ [ A Unified View of Differentially Private Deep Generative Modeling](https://arxiv.org/abs/2309.15696)

	Dingfan Chen, Raouf Kerkouche, Mario Fritz


+ [ On Computational Entanglement and Its Interpretation in Adversarial  Machine Learning](https://arxiv.org/abs/2309.15669)

	YenLung Lai, Xingbo Dong, Zhe Jin


+ [ Automatic Feature Fairness in Recommendation via Adversaries](https://arxiv.org/abs/2309.15418)

	Hengchang Hu, Yiming Cao, Zhankui He, Samson Tan, Min-Yen Kan


## 2023-09-27
+ [ Bias Assessment and Mitigation in LLM-based Code Generation](https://arxiv.org/abs/2309.14345)

	Dong Huang, Qingwen Bu, Jie Zhang, Xiaofei Xie, Junjie Chen, Heming Cui


+ [ Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM](https://arxiv.org/abs/2309.14348)

	Bochuan Cao, Yuanpu Cao, Lu Lin, Jinghui Chen


+ [ Survey of Social Bias in Vision-Language Models](https://arxiv.org/abs/2309.14381)

	Nayeon Lee, Yejin Bang, Holy Lovenia, Samuel Cahyawijaya, Wenliang Dai, Pascale Fung


+ [ XGV-BERT: Leveraging Contextualized Language Model and Graph Neural  Network for Efficient Software Vulnerability Detection](https://arxiv.org/abs/2309.14677)

	Vu Le Anh Quan, Chau Thuan Phat, Kiet Van Nguyen, Phan The Duy, Van-Hau Pham


+ [ DifAttack: Query-Efficient Black-Box Attack via Disentangled Feature  Space](https://arxiv.org/abs/2309.14585)

	Liu Jun, Zhou Jiantao, Zeng Jiandian, Jinyu Tian


+ [ Structure Invariant Transformation for better Adversarial  Transferability](https://arxiv.org/abs/2309.14700)

	Xiaosen Wang, Zeliang Zhang, Jianping Zhang


+ [ Frugal Satellite Image Change Detection with Deep-Net Inversion](https://arxiv.org/abs/2309.14781)

	Hichem Sahbi, Sebastien Deschamps


+ [ The Surveillance AI Pipeline](https://arxiv.org/abs/2309.15084)

	Pratyusha Ria Kalluri, William Agnew, Myra Cheng, Kentrell Owens, Luca Soldaini, Abeba Birhane


+ [ Unveiling Fairness Biases in Deep Learning-Based Brain MRI  Reconstruction](https://arxiv.org/abs/2309.14392)

	Yuning Du, Yuyang Xue, Rohan Dharmakumar, Sotirios A. Tsaftaris


+ [ LogGPT: Log Anomaly Detection via GPT](https://arxiv.org/abs/2309.14482)

	Xiao Han, Shuhan Yuan, Mohamed Trabelsi


+ [ Privacy-preserving and Privacy-attacking Approaches for Speech and Audio  -- A Survey](https://arxiv.org/abs/2309.15087)

	Yuchen Liu, Apu Kapadia, Donald Williamson

## 2023-09-26
+ [ Investigating Efficient Deep Learning Architectures For Side-Channel  Attacks on AES](https://arxiv.org/abs/2309.13170)

	Yohaï-Eliel Berreby, Laurent Sauvage


+ [ Towards Green AI in Fine-tuning Large Language Models via Adaptive  Backpropagation](https://arxiv.org/abs/2309.13192)

	Kai Huang, Hanyun Yin, Heng Huang, Wei Gao


+ [ Defending Pre-trained Language Models as Few-shot Learners against  Backdoor Attacks](https://arxiv.org/abs/2309.13256)

	Zhaohan Xi, Tianyu Du, Changjiang Li, Ren Pang, Shouling Ji, Jinghui Chen, Fenglong Ma, Ting Wang


+ [ LLMs as Counterfactual Explanation Modules: Can ChatGPT Explain  Black-box Text Classifiers?](https://arxiv.org/abs/2309.13340)

	Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, Huan Liu


+ [ Seeing Is Not Always Believing: Invisible Collision Attack and Defence  on Pre-Trained Models](https://arxiv.org/abs/2309.13579)

	Minghang Deng, Zhong Zhang, Junming Shao


+ [ PRIS: Practical robust invertible network for image steganography](https://arxiv.org/abs/2309.13620)

	Hang Yang, Yitian Xu, Xuhua Liu, Xiaodong Ma


+ [ GHN-QAT: Training Graph Hypernetworks to Predict Quantization-Robust  Parameters of Unseen Limited Precision Neural Networks](https://arxiv.org/abs/2309.13773)

	Stone Yun, Alexander Wong


+ [ Can LLM-Generated Misinformation Be Detected?](https://arxiv.org/abs/2309.13788)

	Canyu Chen, Kai Shu


+ [ RBFormer: Improve Adversarial Robustness of Transformer by Robust Bias](https://arxiv.org/abs/2309.13245)

	Hao Cheng, Jinhao Duan, Hui Li, Lyutianyang Zhang, Jiahang Cao, Ping Wang, Jize Zhang, Kaidi Xu, Renjing Xu


+ [ DFRD: Data-Free Robustness Distillation for Heterogeneous Federated  Learning](https://arxiv.org/abs/2309.13546)

	Kangyang Luo, Shuai Wang, Yexuan Fu, Xiang Li, Yunshi Lan, Ming Gao


+ [ Vulnerabilities in Video Quality Assessment Models: The Challenge of  Adversarial Attacks](https://arxiv.org/abs/2309.13609)

	Ao-Xiang Zhang, Yu Ran, Weixuan Tang, Yuan-Gen Wang


+ [ Video Adverse-Weather-Component Suppression Network via Weather  Messenger and Adversarial Backpropagation](https://arxiv.org/abs/2309.13700)

	Yijun Yang, Angelica I. Aviles-Rivero, Huazhu Fu, Ye Liu, Weiming Wang, Lei Zhu


+ [ Combining Two Adversarial Attacks Against Person Re-Identification  Systems](https://arxiv.org/abs/2309.13763)

	Eduardo de O. Andrade, Igor Garcia Ballhausen Sampaio, Joris Guérin, José Viterbo


+ [ Adversarial Attacks on Video Object Segmentation with Hard Region  Discovery](https://arxiv.org/abs/2309.13857)

	Ping Li, Yu Zhang, Li Yuan, Jian Zhao, Xianghua Xu, Xiaoqin Zhang


+ [ SurrogatePrompt: Bypassing the Safety Filter of Text-To-Image Models via  Substitution](https://arxiv.org/abs/2309.14122)

	Zhongjie Ba, Jieming Zhong, Jiachen Lei, Peng Cheng, Qinglong Wang, Zhan Qin, Zhibo Wang, Kui Ren



+ [ Spatial-frequency channels, shape bias, and adversarial robustness](https://arxiv.org/abs/2309.13190)

	Ajay Subramanian, Elena Sizikova, Najib J. Majaj, Denis G. Pelli


+ [ Beyond Fairness: Age-Harmless Parkinson's Detection via Voice](https://arxiv.org/abs/2309.13292)

	Yicheng Wang, Xiaotian Han, Leisheng Yu, Na Zou


+ [ Improving Robustness of Deep Convolutional Neural Networks via  Multiresolution Learning](https://arxiv.org/abs/2309.13752)

	Hongyan Zhou, Yao Liang


+ [ Invisible Watermarking for Audio Generation Diffusion Models](https://arxiv.org/abs/2309.13166)

	Xirong Cao, Xiang Li, Divyesh Jadav, Yanzhao Wu, Zhehui Chen, Chen Zeng, Wenqi Wei


+ [ On the Effectiveness of Adversarial Samples against Ensemble  Learning-based Windows PE Malware Detectors](https://arxiv.org/abs/2309.13841)

	Trong-Nghia To, Danh Le Kim, Do Thi Thu Hien, Nghi Hoang Khoa, Hien Do Hoang, Phan The Duy, Van-Hau Pham


## 2023-09-25
+ [ Provably Robust and Plausible Counterfactual Explanations for Neural  Networks via Robust Optimisation](https://arxiv.org/abs/2309.12545)

	Junqi Jiang, Jianglin Lan, Francesco Leofante, Antonio Rago, Francesca Toni


+ [ HANS, are you clever? Clever Hans Effect Analysis of Neural Systems](https://arxiv.org/abs/2309.12481)

	Leonardo Ranaldi, Fabio Massimo Zanzotto


+ [ Privacy Assessment on Reconstructed Images: Are Existing Evaluation  Metrics Faithful to Human Perception?](https://arxiv.org/abs/2309.13038)

	Xiaoxiao Sun, Nidham Gazagnadou, Vivek Sharma, Lingjuan Lyu, Hongdong Li, Liang Zheng


+ [ Improving Machine Learning Robustness via Adversarial Training](https://arxiv.org/abs/2309.12593)

	Long Dang, Thushari Hapuarachchi, Kaiqi Xiong, Jing Lin


+ [ On Data Fabrication in Collaborative Vehicular Perception: Attacks and  Countermeasures](https://arxiv.org/abs/2309.12955)

	Qingzhao Zhang, Shuowei Jin, Jiachen Sun, Xumiao Zhang, Ruiyang Zhu, Qi Alfred Chen, Z. Morley Mao


+ [ Robotic Handling of Compliant Food Objects by Robust Learning from  Demonstration](https://arxiv.org/abs/2309.12856)

	Ekrem Misimi, Alexander Olofsson, Aleksander Eilertsen, Elling Ruud Øye, John Reidar Mathiassen


## 2023-09-24
+ [ Distilling Adversarial Prompts from Safety Benchmarks: Report for the  Adversarial Nibbler Challenge](https://arxiv.org/abs/2309.11575)

	Manuel Brack, Patrick Schramowski, Kristian Kersting


+ [ The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A"](https://arxiv.org/abs/2309.12288)

	Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, Owain Evans


## 2023-09-23
+ [ Bad Actor, Good Advisor: Exploring the Role of Large Language Models in  Fake News Detection](https://arxiv.org/abs/2309.12247)

	Beizhe Hu, Qiang Sheng, Juan Cao, Yuhui Shi, Yang Li, Danding Wang, Peng Qi


+ [ A Chinese Prompt Attack Dataset for LLMs with Evil Content](https://arxiv.org/abs/2309.11830)

	Chengyuan Liu, Fubang Zhao, Lizhi Qing, Yangyang Kang, Changlong Sun, Kun Kuang, Fei Wu


+ [ Vulnerability of 3D Face Recognition Systems to Morphing Attacks](https://arxiv.org/abs/2309.12118)

	Sanjeet Vardam, Luuk Spreeuwers


+ [ Towards Differential Privacy in Sequential Recommendation: A Noisy Graph  Neural Network Approach](https://arxiv.org/abs/2309.11515)

	Wentao Hu, Hui Fang


## 2023-09-22
+ [ CATS: Conditional Adversarial Trajectory Synthesis for  Privacy-Preserving Trajectory Data Publication Using Deep Learning Approaches](https://arxiv.org/abs/2309.11587)

	Jinmeng Rao, Song Gao, Sijia Zhu


+ [ How Robust is Google's Bard to Adversarial Image Attacks?](https://arxiv.org/abs/2309.11751)

	Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang, Yichi Zhang, Yu Tian, Hang Su, Jun Zhu


+ [ Knowledge Sanitization of Large Language Models](https://arxiv.org/abs/2309.11852)

	Yoichi Ishibashi, Hidetoshi Shimodaira


+ [ On the Relationship between Skill Neurons and Robustness in Prompt  Tuning](https://arxiv.org/abs/2309.12263)

	Leon Ackermann, Xenia Ohmer


+ [ TextCLIP: Text-Guided Face Image Generation And Manipulation Without  Adversarial Training](https://arxiv.org/abs/2309.11923)

	Xiaozhou You, Jian Zhang


+ [ Dictionary Attack on IMU-based Gait Authentication](https://arxiv.org/abs/2309.11766)

	Rajesh Kumar, Can Isik, Chilukuri K. Mohan
  

+ [ Privacy-Preserving In-Context Learning with Differentially Private  Few-Shot Generation](https://arxiv.org/abs/2309.11765)

	Xinyu Tang, Richard Shin, Huseyin A. Inan, Andre Manoel, Fatemehsadat Mireshghallah, Zinan Lin, Sivakanth Gopi, Janardhan Kulkarni, Robert Sim


+ [ MarkNerf:Watermarking for Neural Radiance Field](https://arxiv.org/abs/2309.11747)

	Lifeng Chen, Jia Liu, Yan Ke, Wenquan Sun, Weina Dong, Xiaozhong Pan


+ [ DeepTheft: Stealing DNN Model Architectures through Power Side Channel](https://arxiv.org/abs/2309.11894)

	Yansong Gao, Huming Qiu, Zhi Zhang, Binghui Wang, Hua Ma, Alsharif Abuadbba, Minhui Xue, Anmin Fu, Surya Nepal


## 2023-09-21
+ [ When to Trust AI: Advances and Challenges for Certification of Neural  Networks](https://arxiv.org/abs/2309.11196)

	Marta Kwiatkowska, Xiyue Zhang


+ [ C$\cdot$ASE: Learning Conditional Adversarial Skill Embeddings for  Physics-based Characters](https://arxiv.org/abs/2309.11351)

	Zhiyang Dou, Xuelin Chen, Qingnan Fan, Taku Komura, Wenping Wang


+ [ What Learned Representations and Influence Functions Can Tell Us About  Adversarial Examples](https://arxiv.org/abs/2309.10916)

	Shakila Mahjabin Tonni, Mark Dras


+ [ PRAT: PRofiling Adversarial aTtacks](https://arxiv.org/abs/2309.11111)

	Rahul Ambati, Naveed Akhtar, Ajmal Mian, Yogesh Singh Rawat


+ [ It's Simplex! Disaggregating Measures to Improve Certified Robustness](https://arxiv.org/abs/2309.11005)

	Andrew C. Cullen, Paul Montague, Shijie Liu, Sarah M. Erfani, Benjamin I.P. Rubinstein


+ [ Learning Patient Static Information from Time-series EHR and an Approach  for Safeguarding Privacy and Fairness](https://arxiv.org/abs/2309.11373)

	Wei Liao, Joel Voldman


+ [ Fed-LSAE: Thwarting Poisoning Attacks against Federated Cyber Threat  Detection System via Autoencoder-based Latent Space Inspection](https://arxiv.org/abs/2309.11053)

	Tran Duc Luong, Vuong Minh Tien, Nguyen Huu Quyen, Do Thi Thu Hien, Phan The Duy, Van-Hau Pham


## 2023-09-20
+ [ GPTFUZZER : Red Teaming Large Language Models with Auto-Generated  Jailbreak Prompts](https://arxiv.org/abs/2309.10253) 

  Jiahao Yu, Xingwei Lin, Xinyu Xing


+ [ Exploring the Dark Side of AI: Advanced Phishing Attack Design and  Deployment Using ChatGPT](https://arxiv.org/abs/2309.10463) 

  Nils Begou, Jeremy Vinoy, Andrzej Duda, Maciej Korczynski


+ [ Transferable Adversarial Attack on Image Tampering Localization](https://arxiv.org/abs/2309.10243)

  Yuqi Wang, Gang Cao, Zijie Lou, Haochen Zhu


+ [ RECALL+: Adversarial Web-based Replay for Continual Learning in Semantic  Segmentation](https://arxiv.org/abs/2309.10479)

  Chang Liu, Giulia Rizzoli, Francesco Barbato, Umberto Michieli, Yi Niu, Pietro Zanuttigh


+ [ Realistic Website Fingerprinting By Augmenting Network Trace](https://arxiv.org/abs/2309.10147)

  Alireza Bahramali, Ardavan Bozorgi, Amir Houmansadr


+ [ Love or Hate? Share or Split? Privacy-Preserving Training Using Split  Learning and Homomorphic Encryption](https://arxiv.org/abs/2309.10517) 

  Tanveer Khan, Khoa Nguyen, Antonis Michalas, Alexandros Bakas


+ [ Disentangled Information Bottleneck guided Privacy-Protective JSCC for  Image Transmission](https://arxiv.org/abs/2309.10263)

  Lunan Sun, Yang Yang, Mingzhe Chen, Caili Guo


+ [ SPFL: A Self-purified Federated Learning Method Against Poisoning  Attacks](https://arxiv.org/abs/2309.10607) 

  Zizhen Liu, Weiyang He, Chip-Hong Chang, Jing Ye, Huawei Li, Xiaowei Li


## 2023-09-19
+ [ Plug in the Safety Chip: Enforcing Constraints for LLM-driven Robot Agents](https://arxiv.org/abs/2309.09919)

  Ziyi Yang, Shreyas S. Raman, Ankit Shah, Stefanie Tellex


+ [ Efficient Avoidance of Vulnerabilities in Auto-completed Smart Contract  Code Using Vulnerability-constrained Decoding](https://arxiv.org/abs/2309.09826)

  André Storhaug, Jingyue Li, Tianyuan Hu


+ [ Bias of AI-Generated Content: An Examination of News Produced by Large  Language Models](https://arxiv.org/abs/2309.09825)

  Xiao Fang, Shangkun Che, Minjia Mao, Hongzhe Zhang, Ming Zhao, Xiaohang Zhao


+ [ Reducing Adversarial Training Cost with Gradient Approximation](https://arxiv.org/abs/2309.09464)

  Huihui Gong, Shuo Yang, Siqi Ma, Seyit Camtepe, Surya Nepal, Chang Xu


+ [ Stealthy Physical Masked Face Recognition Attack via Adversarial Style  Optimization](https://arxiv.org/abs/2309.09480)

  Huihui Gong, Minjing Dong, Siqi Ma, Seyit Camtepe, Surya Nepal, Chang Xu


+ [ Evaluating Adversarial Robustness with Expected Viable Performance](https://arxiv.org/abs/2309.09928)

  Ryan McCoppin, Colin Dawson, Sean M. Kennedy, Leslie M. Blaha

## 2023-09-17

+ [ Convex Latent-Optimized Adversarial Regularizers for Imaging Inverse  Problems](https://arxiv.org/abs/2309.09250)

  Huayu Wang, Chen Luo, Taofeng Xie, Qiyu Jin, Guoqing Chen, Zhuo-Xu Cui, Dong Liang

## 2023-09-16


+ [ Robust Backdoor Attacks on Object Detection in Real World](https://arxiv.org/abs/2309.08953)

  Yaguan Qian, Boyuan Ji, Shuke He, Shenhui Huang, Xiang Ling, Bin Wang, Wei Wang


+ [ Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and  Nationality Bias in Generative Models](https://arxiv.org/abs/2309.08902)

  Mahammed Kamruzzaman, Md. Minul Islam Shovon, Gene Louis Kim
 

+ [ Context-aware Adversarial Attack on Named Entity Recognition](https://arxiv.org/abs/2309.08999)

  Shuguang Chen, Leonardo Neves, Thamar Solorio
 
## 2023-09-15

+ [ Adversarial Attacks on Tables with Entity Swap](https://arxiv.org/abs/2309.08650)

  Aneta Koleva, Martin Ringsquandl, Volker Tresp


+ [ A More Secure Split: Enhancing the Security of Privacy-Preserving Split  Learning](https://arxiv.org/abs/2309.08697)

  Tanveer Khan, Khoa Nguyen, Antonis Michalas

  
+ [ Detecting Unknown Attacks in IoT Environments: An Open Set Classifier  for Enhanced Network Intrusion Detection](https://arxiv.org/abs/2309.07461)

  Yasir Ali Farrukh, Syed Wali, Irfan Khan, Nathaniel D. Bastian


+ [ Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated  Text](https://arxiv.org/abs/2309.07689)

  Mahdi Dhaini, Wessel Poelman, Ege Erdogan


+ [ Keep your Identity Small: Privacy-preserving Client-side Fingerprinting](https://arxiv.org/abs/2309.07563)

  Alberto Fernandez-de-Retana, Igor Santos-Grueiro


+ [ Fake News Detectors are Biased against Texts Generated by Large Language  Models](https://arxiv.org/abs/2309.08674)

  Jinyan Su, Terry Yue Zhuo, Jonibek Mansurov, Di Wang, Preslav Nakov
